
        
   SPYD  
        



Validation:
  - used by:
     - main config
     - plugin config (CONF.runnerConfig|reporterConfig)
        - required to use any configuration property
        - handle errors to add plugin name
        - including common properties, e.g. CONF.reporterConfig.{id}.output (which must do path resolution)
     - not used to validate task files (runner-specific)
        - reason: it runs in runner process, which might be different language
        - runner should instead do the following on their own: default values, normalization, throwing errors
        - try to use shared library between runners of same language
           - including using the same validation code as main|plugin config for Node.js runners
  - validate(object, config, opts)
  - config.VARR.schema OBJ
     - JSON schema
  - config.VARR.validate(VAL, "VARR", OPTS)[->"ERROR"]
     - OPTS:
        - parent OBJ
        - root OBJ
  - validate against unknown properties
     - unless "*" in VARR
     - include automatic suggestion
  - config.VARR.example VAL
     - shown on validation error
  - config.VARR.normalize(VAL, "VARR", OPTS)->VAL
     - same OPTS as validate()
  - config.VARR.default VAL
     - applied if undefined
  - opts.cwd STR[(VAL, "VARR")]
     - used when resolving file paths
        - done when config.VARR.schema.pattern "path"
     - def: process.cwd()
     - when loading config:
        - must keep "CWD" of each VARR as an OBJ since this depends on how config was loaded, and this is known only before validation
        - replace current `PATH_CONFIG_PROPS` logic
        - including for CONF.tasks
     - FUNC can return empty STR to not do path resolution
        - used by CONF.config when prefixed with "npm:" for example
  - opts.validPropName REGEXP
     - use /^[a-z][a-zA-Z\d]*$/
        - i.e. use case (not _ - .) as delimiter
     - reason: when using "*" in VARR, property names are user-defined
  - VARR:
     - dot-delimited
     - can include "*" for either:
        - array indices
        - dynamic properties
  - add comments:
     - no warnings, only errors because:
        - simpler to specify
        - prefer failing hard
        - prefer semver major with breaking changes over deprecation period
        - does not work well with previews since that clears screen
        - simpler to implement
           - warnings would need to be returned from start() and passed to parent

--------------------

Report flow:
  - reportStart():
     - applySince()
     - normalizeHistory()
        - add report-related properties, only history
        - add historyResult empty ARR
        - normalizeNonStatsAll()
           - not changed during run, not reporter-specific
           - result.screenWidth|Height
        - normalizeStatsAll()
           - changed during run, not reporter-specific
           - stats.diff[Precise]
           - result.dimensions + result.combinations sorting
        - normalizeNonStatsEach()
           - not changed during run, reporter-specific
           - add titles
        - normalizeStatsEach()
           - changed during run, reporter-specific
           - assigned to reporter.history
           - apply CONF.show* to stats
              - fix time series reporter: it can now use history results to know whether median or medianMin|Max should be used
     - normalizeTargetResult()
        - non-stats-related, reporter-specific, for target result only
        - add each reporter.footerParam|String
        - omit result.id|timestamp
        - normalizeNonStatsAll()
        - normalizeNonStatsEach()
           - assigned to reporter.resultProps
  - reportCompute():
     - normalizeComputedReport()
        - merge historyResult
        - normalizeStatsAll()
        - normalizeStatsEach()
        - apply each reporter.footerParam
        - merge each reporter.resultProps
        - merge each reporter.history
           - append target result to it

id|timestamp:
  - timezone:
     - local if `reporter.tty: true` (output stdout + interactive), UTC otherwise
        - reason: interactive terminal expects single viewer, but other outputs might be shared and viewed by several people
     - for both result.* and footer
  - result.id|timestamp passed to reporter
     - to result.history[*] (including last)
        - reason: time series reporters
     - but not to top-level result.*
        - reason: should rely on footer instead
  - result.timestamp "YYYY-MM-DD HH:MM:SS"
     - for both reporters and footer
     - footer appends " UTC" if UTC (nothing appended if local)

Figure out exactly which stat can be undefined, and when
  - document it in comments
  - handle every case where it might be
  - check codebase (not only reporters) for logic using stats, to ensure it checks for undefined

Fix linting

prettifyStats()
  - extract it outside of reporters
  - make it format specific, e.g. markdown does not need padding|colors

--reporter="" CLI flag works but not -r=""

Box plot reporter:
  - name "box"
  - show: min, p25, median (or medianMin|medianMax), p75, max
  - abscissa min|max is same for all
  - each vertical bar should have a different color, shown in abscissa numbers too, to help distinguish
     - probably: median[Min|Max] cyan, p25|p75 gray, min|max white
  - medianMin|Max should be separated by dash, not space
  - also shown in `dev` reporter

--------------------

CONF.select|limit:
  - use "and" keyword instead of guessing dimensions
     - reasons:
        - knowing dimensions requires loading all results
        - making union|intersection implicit is confusing for users
     - "not" still applies to whole STR (i.e. current behavior)
     - do not allow "and": at beginning|end, after "not" or after another "and"
  - use partial substring match (STR.includes())
     - add comments:
        - use cases:
           - allow selecting a group of related tasks
           - faster to type on the CLI
        - simpler and as useful as globbing or regexp
        - guessing if a selector is meant to target a default id might fail when using a substring, but this is not a problem because:
           - only impacts variations and systems (only if multiple system dimensions) since the other default ids are persisted
           - most substrings matching "primary|main_*" would match all of the other ids of the dimension, making the selectors not useful for users, i.e. unlikely
  - case insensitive
     - including when applying default ids
     - including "not|and"

Dimensions:
  - remove result.dimensions OBJ_ARR
     - including adding titles to result.dimensions
  - combination.dimensions.DIMENSION "ID"
     - DIMENSION: "task", "step", "runner", "system.{systemDimension}", "variation.{variationDimension}"
  - on report:
     - filter out unnecessary dimensions from combination.dimensions.DIMENSION
        - i.e. any dimension with only 1 id
        - exception: if all dimensions are unnecessary, keep task dimension
     - when adding titles, convert combination.dimensions.DIMENSION "ID" to OBJ: id STR, title STR, titlePadded STR
        - re-use then remove the current addTitles() logic
     - sort result.combinations by mean stats.median
        - do not add combination.*Rank nor dimensions.*.mean|rank
        - sorted dimension by dimension
           - sorting order: step, task, runner, system, variation
        - reporters rows|columns computation use result.combinations ARR order
           - when grouping dimensions (e.g. tables), filter out dimensions with no combinations, which can happen due to:
              - CONF.select
              - runnerConfig variations being runner-specific
     - not persisted on stores:
        - reasons:
           - CONF.select might remove some ids
           - CONF.since might add some ids
  - getCombinationName(combination)->STR:
     - 'DIMENSION "ID",...'
     - ids, not titles
     - for: `dev`, limit errors, measure errors
        - not reporters
     - should re-use some of the normalization happening for reporters:
        - result.combinations dimension ARR order
        - filter out unnecessary dimensions
  - getCombinationTitle[Padded](combination)->STR:
     - titles, not ids
        - using result.dimensions.*.title[Padded]
     - for reporters

Cross-dimensions duplicate id validation:
  - for newly measured result with `run`:
     - throw error
     - only for the given result's ids, not including the previous results' ids
        - reasons:
           - would depend on CONF.since, making it fail or not depending on it
           - since CONF.since defaults to 0, it would not be useful most of the times
           - we would still need a separate way to handle cross-dimensions duplicate ids between results due to:
              - multiple branches results (since we do not load all results from all branches)
              - manual edits of result files
  - between different results:
     - prepend `DIMENSION_` to id
        - use _ instead of .
     - not done on the dimension with the most recent result

CONF.system "ID" -> CONF.system[.{systemDimension}] "ID"
  - DIMENSION is "system[.{systemDimension}]"
     - i.e. can use several dimensions
     - getCombinationName() should use only systemDimension
  - system id is "ID", without systemDimension
  - default systemDimension: "machine"
  - default "ID": "primary_{systemDimension}"
  - when sorting result.combinations per dimension, system sorted:
     - after runner, before variation
     - by CONF.system OBJ order
  - result.systems[*].dimensions:
     - before merging: OBJ where key is dimension, value is id
     - after merging:
        - OBJ_ARR_ARR: id STR, title STR, dimension STR
        - second ARR is when merging combinations with same shared props
     - reported as comma-separated list of space-separated lists
     - remove result.systems[*].id|title
  - merging:
     - create shared systems in decreasing number of shared dimensions:
        - first look for properties shared on all system dimensions. On any match:
           - create a shared system OBJ for it, unless either:
              - no shared properties
                 - except top-level one, at systems[0], always defined
              - another shared system has exact same properties:
                 - just append dimensions ids to it instead
           - remove props from the matching combinations, so they are not in two different system OBJs
        - then same for all dimensions minus 1|2|...
           - using all possible sets of "dimensions minus 1|2|..."
     - undefined values:
        - like other values when grouping
        - but cleaned after grouping
           - i.e. not printed by reporters
  - add comments:
     - reasons for allowing multiple system dimensions (e.g. CONF.system.os "linux") instead of a single one (e.g. CONF.system "os_linux")
        - separating those in reporting looks nicer
        - easier to configure titles
        - better dimension-wise sorting in reporting
        - easier to select (CONF.select|limit)

Variations:
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation dimension"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - any CONF.runnerConfig.{runnerId}.PROP
        - cartesian product only to combinations with that runner
  - DIMENSION is "variation.{variationDimension}"
     - i.e. each variable property is a dimension
     - variationDimension is property VARR (dot-delimited)
     - getCombinationName() should remove "variation." prefix from DIMENSION
  - variation id is 'ID', not 'PROP.ID'
  - configuration normalization|validation should apply to each VAL
     - including yargs flags parsing
  - when sorting result.combinations per dimension, variations sorted:
     - at end
     - input, then concurrency, then runnerConfig
     - input: sorted by CONF.inputs OBJ order
     - runnerConfig sorted by:
        - CONF.runner ARR order
        - then config prop name, alphabetically
  - add combination.config CONF_OBJ
     - not persisted in history nor used in report
     - contains config, with combination-specific variations
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR STR (commaSpace-separated list)
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - should still set combination.dimensions.{variationDimension} on those combinations, but with id "", title "", titlePadded " ... "
     - when filtering unnecessary dimensions, should exclude ones with id ""
  - result.variations.{variationDimension} VAL
     - persisted in history
        - using the most recent value
     - on report:
        - set combination.dimensions.{variationDimension}.value VAL
           - should only be reported by reporters able to show details
        - remove result.variations

Default ids:
  - add default ids:
     - done:
        - right after results load
        - before CONF.since merge
     - for all possible dimensions with default ids
        - figure out all possible dimensions by using dimensions of all loaded results
           - add comment: do not also use currently measured runners' possible configuration properties
              - because this is not available in `show|remove`, which would make CONF.select|limit behave differently between commands
     - i.e. missing dimension should behave like the dimension with its default id, including in:
        - CONF.select|limit
        - diff
        - reporting (e.g. when mixed with other results not missing the dimension)
     - per dimension:
        - task|runner: never missing
        - step: "main"
           - assigned on result creation, i.e. saved
        - system: "primary_DIMENSION"
           - assigned on result creation, i.e. saved, except if multiple system dimensions
        - variation: "main_PROP"
           - not assigned on result creation, i.e. not saved
           - PROP is only last property
           - exception: PROP is last two properties (_-separated) when two variable properties have same last property
  - CONF.select:
     - when retrieving combination ids:
        - if CONF.select contains default ids
           - except for persisted default ids: "main" and "primary_machine"
        - then add those default to the combination ids, if the combination is missing the relevant dimensions
     - reason: selection is applied before results are loaded, i.e. before all dimensions are known and default ids are applied
  - validate that new result ids do not match possible default ids (unless same dimension):
     - "main": step
     - "main_*": variations
     - "primary_*": system
  - add comments:
     - purpose is when a result is missing a dimension that other results have, that result should still:
        - be comparable in the history
           - e.g. only one step was used (undefined) but another step was added, forcing to rename the first step but it should still be comparable
        - be reported nicely
        - be selectable with CONF.select
     - history matching problem:
        - when:
           - a result misses a dimension
           - it must be compared with results with that dimension
           - those other results have multiple ids of that dimension
        - then it becomes difficult to know which id should be used to compare
        - possible solutions (others than default ids):
           - missing dimension never matches an id from a non-missing dimension
              - problem: creates discontinuity in history
           - missing dimension matches any ids from a non-missing dimension
              - problem: non-limear history and most likely wrong results

--------------------

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each iteration, shallow copy that OBJ
     - i.e. each iteration has fresh copy
     - i.e. cannot communicate with next iteration
        - reason: might accidentally get previous iteration state, especially if property is set considitionally
        - other reason: ensure proper garbage collection
        - exception: top-level state, or in properties created during before()
  - passed as `this`
     - check whether FUNC.call(context) or FUNC.bind(context) is faster
        - probably bind() since it can be done once during init
     - should document that does not work with arrow function nor bound function, i.e. must be avoided
     - try to validate against binding step functions
        - only if multiple steps
  - should be passed to step function even when single step
     - reasons:
        - stats do not vary depending on whether there is a single step or not
        - more monomorphic
  - forbid inputId named `context`, to allow other runners to pass `context` alongside named arguments
  - add comments about:
     - advantages over using a named argument:
        - cannot re-assign `this`, i.e. no need to validate it
        - simpler syntax (no need to retrieve from arguments)
        - intuitive|common pattern in JavaScript
        - clearer separation between inputs and context
     - advantages over top-level scope (which can still be used)
        - not shared between tasks
        - not shared between iterations
        - does not require declaring a variable
     - problems with alternatives to single `context` OBJ:
        - separate `context` arguments for input and output (to next step)
           - information meant for later steps must be passed between several steps
        - `context` argument for input, return for output (to next step)
           - information meant for later steps must be passed between several steps
        - `{stepId}` argument for input, return for output
           - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
           - name conflict with any core argument
              - except inputs, which are validated against duplicates with steps
           - more complex to explain:
              - return vs context
              - `stepId` argument name
              - when repeating a step, only last iteration's return value is used

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - in documentation:
     - encourage `export const TASK = { STEP() {...}, ... };`
     - as opposed to `const TASK_STEP = function() {...}; export const TASK = { STEP: TASK_STEP, ... };`
  - validate against tasks with no steps (besides before|after)
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
        - i.e. must return `tasks` `{ taskId: ['main'] }` to parent
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - processes:
     - at benchmark start, when runner communicates available tasks to parent, it should also return available steps
        - returned as `tasks: { taskId: 'stepId'_ARR, ... }`
     - all steps of a given combination use same process
  - remove beforeEach|afterEach
     - rename beforeAll|afterAll to before|after
     - add comment that runners should avoid specific case for reserved exported names, since users might use different case convention for stepIds
  - each step is a combination dimension
  - implementation:
     - runner:
        - on start, returns steps to parent
           - ARR in execution order
        - on measure, gets OBJ:
           - maxLoops NUM
           - steps OBJ_ARR:
              - id "stepId", repeat NUM
              - ARR in execution order
        - do {
            const context = { ...beforeContext }
            for (const { id, repeat } of steps) {
              const step = steps[id]
              startTime()
              while (repeat--) {
                step.apply(context, args)
              }
              endTime()
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in steps execution order
     - parent:
        - `measureDuration` is for whole sample (it is already the case)
        - `maxLoops` = 100ms / sum(steps.map((step) => step.median * step.repeat))
        - `combination.steps` OBJ_ARR: id "stepId", ...
           - for all step-wise state: measures, bufferedMeasures, stats, loops, times, repeat, calibrated
           - not for: everything related to minLoopDuration, samples
        - total `benchmarkDuration` does not vary with number of steps
           - fix preview logic (at the moment, it uses combinations.length * CONF.duration)
           - also `measureDuration` measures sum of all steps
           - add comments why:
              - new steps are more likely to be due to splitting existing steps than adding new ones
              - adding steps does not increase `measureDuration`, i.e. decrease preview responsiveness
              - with CONF.select, all steps are still run.
                It is simpler to explain this by documenting that steps never influence total benchmark duration
  - error handling:
     - keep current logic i.e. exceptions are propagated and in:
        - `before` -> do not call later steps nor `after`
        - any other steps -> do not call later steps, but call `after`
           - exceptions in `after` itself are ignored
        - `after` -> nothing else to call
     - reasons:
        - ensures `after` does cleanup, but only if `before` completed
        - but assumes that exception leaves bad state, i.e. should not run additional steps, and `after` might fail
  - result.steps OBJ_ARR: id 'STR'
     - for all steps, even if not measured
     - ARR is sorted by step execution order
        - two tasks might run steps in different order
           - including between different results
        - i.e. use the step mean order:
           - percentage of step index within its task's steps ARR
              - 0 for first step, 1 for last step
           - take the arithmetic mean between all tasks defining that step
           - if two steps equal, use stepId alphabetical order
        - step groups:
           - right before their earliest child
           - if two step groups have same earliest child, decide using:
              - if latest child is earlier, comes first
              - otherwise, use stepId alphabetical order
  - reporting:
     - report one separate table per step
        - step title should be in top-left corner
     - sorting between tables: by result.steps ARR order
  - excluding steps with CONF.select:
     - like any other combination dimensions:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
        - not taken into account to decide whether to stop combinations (based on rmoe)
           - including in the preview duration estimation
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one combination for that task exist
        - i.e. parent process measuring logic ignores steps:
           - at the beginning of measuring logic, combinations with same task but different steps are grouped
           - parent process does not pass any information to runner process about steps, and runner runs them all
           - at the end of measuring logic, combinations are ungrouped to different steps
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.select
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.select already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - `maxLoops` should be divided by number of steps, for the current combination, since all steps measures are in-memory at once
  - add comments about:
     - complex step order:
        - problems:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allow reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as special kinds of steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.repeat error handling, etc.

Step groups:
  - behave like steps except:
     - specified with CONF.stepsConfig.{stepId}.group 'stepId'_ARR
        - ignored if empty ARR
        - reasons for the syntax:
           - allow non-consecutive steps
           - not verbose (unlike using stepId, e.g. using stepId common prefixes)
     - stats are based on aggregation of other steps stats
        - use other steps stats, not `measures` because the number of `measures` might differ between steps when CONF.repeat true
           - add comment that could in principle use `measures` when CONF.repeat false, but does not because:
              - it would make stats differ between CONF.repeat true|false
                 - this is confusing and might lead some users to use CONF.repeat false
                 - using CONF.repeat true|false should only change precision, not accuracy
              - it would give better stats for step groups, discouraging CONF.repeat true
              - it is slower
        - how:
           - samples|minLoopDuration: any
           - mean|quantiles|median|min|max|stdev|loops|times: add
           - repeat: Math.round(loops / times)
           - histogram:
              - among all histograms first buckets, find one with smallest frequency:
                 - create a bucket with:
                    - frequency: smallestFrequency
                    - low|high: sum of all histograms first bucket's low|high
                 - for each first bucket:
                    - if smallestFrequency === firstBucketFrequency:
                       - discard bucket, i.e. next one becomes first bucket for that histogram
                       - also if >= (due to possible rounding error)
                    - otherwise, update:
                       - frequency: subtract smallestFrequency to it
                       - low|high: kept as is
                 - repeat
              - re-distribute buckets:
                 - so buckets width is uniform, and so there are 1000 buckets
                 - do it by summing and interpolating
        - add comment that this assumes steps measures are positively corrolated
           - for: quantiles|median|min|max, stdev, histogram
           - if not, result is a bit inaccurate, but remains precise
  - persisted in history
     - as opposed to being dynamically computed during reporting
     - reason: allows not losing history when:
        - step group change which steps it includes
        - or their names
        - or whether it is a step group or a normal step
  - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
     - reason: users might want to see children only when need details
        - and vice-versa
  - can target another step group
     - error if cycle
  - special stepId "all":
     - only allowed in stepsConfig.{stepId}.group ARR
        - when present, do not allow other values in ARR
     - select all available steps
        - including ones not measured|included
     - forbid "all" as a stepId name, either normal step or step group

Automatic repeat:
  - `repeat` vs `scale`:
     - both passed to runner.measure()
     - both are per step (not per task)
     - repeat is inside timestamp, scale outside
     - passed to runner:
        - as OBJ: maxLoops NUM, steps OBJ_ARR: id 'stepId', scale NUM, repeat NUM
        - do {
            const context = { ...beforeContext }
            for (const { id, scale, repeat } of steps) {
              const step = steps[id]
              while (scale--) {
                startTime()
                while (repeat--) {
                  step.apply(context, args)
                }
                endTime()
              }
            }
          } while (maxLoops--)
     - goal:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: fast steps should be run more often than slow steps because:
           - they are less precise, i.e. each iteration brings more value
           - they take a smaller percentage of the overall CONF.duration
  - `repeat` NUM: keep current logic as is
  - `scale` NUM
     - always passed to runner.measure()
     - value:
        - Math.round(maxStepDuration / currentStepDuration)
           - maxStepDuration = for current task, median duration of slowest step
           - currentStepDuration = median duration of current step
        - i.e. always 1 if single step
  - CONF[.tasksConfig.{taskId}].repeat BOOL
     - def: false
     - if false and task has multiple steps, then:
        - for all steps of that task
        - `repeat` and `scale` are always 1
     - if true and task has multiple steps, then:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - including: cannot both read+write same property in neither arguments nor top-level scope
           - including:
              - stateful class instances like event emitters and streams
              - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.repeat true, increase step function complexity (including increasing input size)
           - split step into its own task
  - report imprecise steps
     - only if CONF.repeat false and multiple steps
        - reason: result might be slightly imprecise due to approximation of the repeat algorithm
     - when, if repeat had been used, it would have been >1
     - set combination.imprecise BOOL
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - reasons why CONF.repeat:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.repeat and `scale`:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over CONF.duration
           - problem: does not work with CONF.duration 0|1
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

Manual mode:
  - opt-in
     - ignore all of this unless CONF.[stepsConfig.{stepId}.]manual defined for that step
     - reasons:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
  - CONF.[stepsConfig.{stepId}.]manual "UNIT"
     - if no stepId: all steps
     - i.e. same step from different tasks have same unit
        - including if single step for all tasks
  - use hardcoded list of units:
     - list:
        - duration: fs ps ns us|μs ms s m h d
           - i.e. allow custom duration
              - could be useful when task file is measuring another process, e.g. time spent on a server
        - %
        - bytes: B KB|KiB MB|MiB GB|GiB TB|TiB PB|PiB
           - also ...b (bits not bytes)
        - counts: ops
     - enum validation:
        - reasons (as opposed to allow custom counts units):
           - simpler to explain
           - no need for case insensitivity
           - no need to validate max length
     - reasons why no empty string units:
        - ambiguous as user might either intend to use it to specify CONF.unit should not be used, or should be displayed with no units
        - forces distinguishing between different units
  - repeat loop still used
     - because automatic duration still measured, for CONF.rate
     - but do not set combination.imprecise
  - pass `steps[*].manual` true to runner:
     - each measure should then be an ARR of two values:
        - automatic duration NUM
        - step return VAL
  - must return NUM from step function
     - reasons, as opposed to set `measure` argument:
        - argument could be destructured, leading to assignment not working
        - argument would be used for too many things: inputs, message passing, manual measures
        - clear that return value has this type of semantics
     - reason why NUM instead of OBJ: works for every language, including cli runner
  - parent validates NUM:
     - for:
        - all tasks of a given step
        - all measures of a given combination
     - allow:
        - 0
        - floats
     - do not allow:
        - negative floats
        - not numbers
        - NaN and Infinity
        - undefined
  - combination.stats:
     - used for manual measures NUM
     - automatic durations are still:
        - measured (for CONF.rate) in combination.durationStats
        - used for calibration: maxLoops, scale
  - persisted at result.steps[*].manual "UNIT"
  - re-use existing unit-specific logic for:
     - automatic scaling
        - e.g. 'ns' -> 's' or 'B' -> 'MB'
     - significant digits|decimals
  - reporting sorting:
     - duration, %: asc
     - bytes, count: desc
     - do not allow configuring|overridding for the moment, to keep things simple, because most users won't need it
  - when merging combinations from different results with same stepId but different unit:
     - if same unit "kind" (duration, %, bytes|bits, count): allow comparing by normalizing stats during mergeResults():
        - find the lowest scale among all units, then multiply to it
        - if mixed manual + auto durations, turn all to manual durations
           - i.e. copy combination.stats to combination.durationStats
           - only if manual unit is duration
        - reason: not losing history when:
           - changing unit scale
           - switching from auto to manual duration
     - if different unit "kind":
        - only keep most recent unit, filtering out previous combinations with different unit kind
        - i.e. units are not a combination dimension

Rate:
  - CONF.[stepsConfig.{stepId}.]rate BOOL
     - def: false
  - reporting-only
     - not persisted in history
     - reporting flag
  - changes the reported value:
     - duration: 1/medianDuration, i.e. times per duration
     - %, bytes, count: value/medianDuration, i.e. scales the left side
  - sorting order:
     - duration: inverted
     - %, bytes, count: kept
  - reported unit:
     - duration, count: "ops/TIME_UNIT"
     - %, bytes: "UNIT/TIME_UNIT"
  - automatic scaling
     - duration, %, count: focused on TIME_UNIT
     - bytes: focused on UNIT, leaving TIME_UNIT as "s"

--------------------

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `dev` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - median measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for beforeEach, main and afterEach
  - always use await on beforeAll|afterAll, i.e. allow both sync and async
  - remove task.async BOOL

--------------------

`spyd` history branch:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - directory is "{gitRoot}/history/results/"
        - git root lookup is using same logic as other places which look for it, i.e. CONF.cwd
     - automatic commits should be prefixed with `[skip ci]`
     - individual results:
        - at /history/results/YYYY-MM-DD--HH-MM-SS--BRANCH--{result.id}.json
           - encode|decode characters in BRANCH:
              - [[:alnum:]-_] left as is
              - any others like percent encoding but using x instead of %
                 - including . and /
                 - including x if followed by two [0-9a-f]
                 - a-f lowercase only
              - extract to own library
        - one immutable FILE.json per result
           - i.e. single OBJ
        - format is JSON
           - reason: fast
  - add comments:
     - pros of using a separate git branch:
        - instead of:
           - using regular files in codebase
           - git hash-object + git cat-file
        - does not pollute git log
           - especially in CI where a single commit might have many CI jobs and results
        - does not require git push --force on the codebase
        - automatically handle committing
        - semi-automatically handle git push|pull
           - can update in CI without requiring developers to pull all the time
        - allows multiple files
        - easier to make it skip CI
        - does not create many tags
        - ensures all branches can always be used for branches comparison with CONF.since
        - easy to understand
        - no risk of pruning
        - versioned
     - only store history with git at the moment
        - however possible approach in the future, if there is a need for it: CONF.history STR:
           - "git"
           - "PATH"
           - CRUD stores plugins
     - why concatenated results are not cached:
        - simpler, i.e. no need to check the file, nor update it after each `spyd bench --save`, `spyd sync`, etc.
        - performance benefits not big enough since results are partially loaded in batches
        - manual edits of files would require some way to invalidate cache, e.g. a `spyd prune` command
        - takes space on the cache directory, i.e. might require automatic|manual pruning
     - why using individual results instead of single file for all results:
        - fast to create new results
        - does not create git conflicts
        - concurrent safe
        - small file size impact in git history
        - easier to edit
        - allow loading only few results instead of all

Require git:
  - only for:
     - CONF.save
     - `sync` command
     - CONF.since branch|commit|tag|"parent"
        - i.e. optional for CONF.since resultId
  - require:
     - `git` binary is executable (i.e. `git --version` has exit code 0)
     - minimum version of `git`
     - there is a `.git` in `[.../]{CONF.cwd}`
  - add comments:
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository

Loading results in batches:
  - done batch by batch, with size incrementing exponentially (2**n)
     - instead of all at once
     - in order, using timestamp in result's filename
  - performed iteratively until can determine no more results needed, in three stages:
     - target delta can be resolved
     - since delta can be resolved
     - beforeSinceResults can be results
        - sinceResult + beforeSinceResults and sinceResult + afterSinceResults contain same combinations (after `select` filtering)
        - i.e. sinceResult merging would be same even if more previous results were loaded
  - ensure that delta resolution:
     - apply `select` filtering first
        - i.e. for each batch, files must be JSON loaded
        - including delta formats: NUM, git branch
     - only use results from correct branch:
        - target delta: current branch
        - sinceResult's branch (if git reference of result id) or current branch (otherwise)
     - for since delta: only use results earlier than target result
  - figuring out if delta is resolved
     - by format:
        - NUM -> >= NUM results
        - "first" -> right away with all results
        - id|timestamp|duration -> right away with result index
        - git branch -> latest result in that branch
           - i.e. use BRANCH in filename but not in `systems[0].git.branch`
        - git commit|tag:
           - find `git` commit author date then use it like in timestamp delta format
           - i.e. does not use `systems[0].git.tag|commit`

`spyd sync` command:
  - on `spyd` git branch:
     - git pull --rebase
        - if merge conflict:
           - try to automatically solve by including additions from both branches
              - add comment that this might happen with history/renamed_branches.yml
              - then retry `git pull --rebase`
           - otherwise fail
     - then git push
        - not run if we know locally there is nothing to push
  - stdin|stdout|stderr "inherit" on git pull|push
     - reasons:
        - allow entering passwords
        - show any error message such as: authentication, git hooks, network, etc.
        - provide with progress
     - not on other git commands
     - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
     - only if CONF.quiet false
  - add comments:
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull

Multiple branches reporting:
  - only report one branch + targetResult
     - branch is:
        - if CONF.since is a git reference or a result id, use its branch
           - branch is resolved:
              - git branch: use it
              - git tag|commit: use the same git logic used to get the current branch, but with a git tag|commit
              - result id: get branch from the result's filename
        - otherwise, current branch
           - instead of using env-ci, should use the best library to guess the branch from a specific commit reference
              - ensure cwd can be specified
              - could also just inline the git calls made by the library
           - if no branch found, use the `undefined` branch
     - filter out any results from the branch later than targetResult, before applying sinceDelta
        - using timestamp + branch in result's filename
        - reasons:
           - ensure benchmark's history remains same as new results are being added
           - ensure history is sorted by timestamp, including the target result
           - parent branch's newer results are likely to contain performance improvements that were not merged in the current branch yet when the result was taken
     - all results without a git branch are treated as if part of a single `undefined` branch
     - result's branch is persisted when the benchmark is saved
  - CONF.since "parent":
     - same as CONF.since "branch" with parent branch
        - i.e. last commit of parent branch, since this is the one the child branch will merge to
     - if no parent branch, like "first" in current branch
  - add comments:
     - problems:
        - hard to know which is parent's branch parent commit using only the benchmarks data:
           - need to use commit ids, which might have changed since benchmark was run due to rebasing
           - user intent is ambiguous: even if branch has not been rebased to parent yet, might intend to do so
        - parent branch's benchmarks' timestamps might interleave with current branch's
        - current branch might have been rebased 0|1|n times onto parent branch
           - parent and current branch's performance improvements are mixed and hard to distinguish
        - grouping results per branch and showing several branches leads to:
           - more visually complex time series
           - harder to understand history since each branch has its own progression
        - should be easy to understand|explain
        - should preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - should not be impacted by rebasing
        - allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
     - discarded solutions:
        - report all results without taking branches into account
        - report all results, grouped by branches
        - report all results of parent branches, grouped by branches
           - variant: only show earlier results
        - let user decide with a CONF.branch configuration property

Renaming branches:
  - on results load, should also get a list of branch renames:
     - do it using the current git reflog
  - use the list of branch renames:
     - to normalize all results branches to their latest names, including when:
        - resolving CONF.since's branch
        - reporting
     - the result's filename and contents also contains the branch name but this name might be old, which is ok since it is fixed by branch renames
  - persist branch renames:
     - reason: git reflog lasts only for 90 days by default
     - done during CONF.save
        - reason: otherwise, user might not expect having to run `spyd sync` again
     - when saving, should automatically create its own git commit
        - using same logic as git commits for results saving (including `[skip ci]`), but as a separate commit
           - reason: cleaner, and limit impact of merge conflicts
     - only append new branch renames, not ones already persisted
        - reason for append-only: limit potential for merge conflicts
     - only add renames for branches used in at least one result
     - persisted to history/renamed_branched.yml
        - OBJ_ARR: from 'BRANCH', to 'BRANCH2'
        - reason for YAML array: limit potential for merge conflicts
     - always loaded and merged with current git reflog, with lower priority

--------------------

Find ways to improve precision even more???

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information

CONF.debug BOOL
  - meant as bug report attachment, not meant for users to debug themselves
  - saved debug information to "{process.cwd()}/spyd_debug_logs.yml"
     - not printed to stdout
     - no way to configure location
     - saved at end once, using try/finally wrapping the programmatic entry points
        - not streamed, so it does not impact benchmark
  - does not change the logic otherwise
     - including reporting and previews
  - file is YAML:
     - using document stream
     - of OBJ:
        - event "EVENT"
        - event-specific properties
  - include:
     - envinfo
     - resolved config
     - task files
     - runner.versions
     - combinations
     - samples
        - including: duration spent, estimated time left, progress bar percentage
  - for all commands
  - interface is debugLog(debug, "EVENT", EVENT_OBJ)
     - debug is undefined if CONF.debug false, mutable ARR otherwise
  - add to GitHub issue templates

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

Learn package 'simple-statistics' and|or 'jstat' and use it in spyd, where needed

--------------------

Add TypeScript support:
  - to:
     - spyd.ts
     - tasks.ts
  - export types of those too

Add output formats:
  - report.reportMarkdown()->PROMISE_STR
     - for CONF.output "*.md|markdown|mdown|mkd|mkdn" or "README|readme"
     - def: reportTerminal() return value in ``` block
     - no padding
     - allows two reporters with same output
        - concatenated like terminal formats, i.e. with newlines
     - footer: appended as Markdown
  - report.reportHtml()->PROMISE_STR
     - for CONF.output "*.html|htm"
     - file should be whole (i.e. have <html> tag)
     - reporter can save other files (imported by the main file) in the same directory or subdirectory
     - def: reportMarkdown() return value rendered to HTML
     - no padding
     - does not allow two reporters with same output
        - reasons:
           - hard to concatenate
           - can emulate concatenation using a parent HTML file
     - footer: inject to any element with id "spyd-footer"
  - report.reportSvg()->PROMISE_STR
     - for CONF.output "*.svg|png|jpg|jpeg|webp"
     - def: reportHtml() return value rendered to SVG
     - converts SVG to PNG|JPEG|WebP
     - no padding
     - does not allow two reporters with same output
     - footer: added with svg manipulation
  - add default value for reportTerminal(): using markdown|html|svg return as is

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
        - abscissa:
           - only show start|end
           - format should be date-only if different days, time-only otherwise
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - stacked bar graph for multiple stages benchmarks
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

GitHub action

GitHub PRs integration

Add other runners:
  - HTTP
  - JSON-RPC
  - graphQL
  - gRPC
  - WebSocket
  - TCP
  - chrome (maybe using puppetter)
  - firefox (maybe using puppetter-firefox)
  - selenium
  - bash
  - go

SaaS:
  - reporting dashboard
  - files:
     - either connect to GitHub repository
     - or edit file with online IDE, saved in the server
  - perform benchmark in-browser
  - public URLs, for sharing

--------------------

Learn the whole Terminal section in edl
  - including finishing ANSI sequences and terminal emulators
  - use cli table library
     - ensure this is still responsive (by creating multiple table if too wide)
        - if no library does this, create own library on top of another

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Terminal-histogram:
  - separate to own repository
  - add features:
     - color themable (using terminal-theme)
     - left bar with percentages
     - can specify number of abscissa ticks
        - or number of columns per tick
        - stack labels, i.e. might need to stack deeper than one level
    - allow minimum ordinate to be either 0 or minimum value
    - labelling columns
    - custom unit for ordinate
    - when too many columns:
       - if labeled: break into several histograms
       - otherwise: extrapolate

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

--------------------

Manually try all features with each Node.js version

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Positioning (in documentation), in priority order:
  - whole benchmarking flow, not just measuring:
     - reporting (pretty, configurable, live preview, multiple formats, insertion, hardware/software info)
     - combinations (variations, inputs, systems, steps, selection)
     - comparing
     - history (including time series)
     - debugging (spyd dev, nice error messages)
     - sharing for others to run
     - testing (limits)
     - concurrency benchmarking
     - CI
     - GitHub PRs
  - simplicity
     - no library|APIs, only export functions
     - no need to specify duration nor number of loops
     - can work with no|minimal configuration, including on-the-fly
  - precision
     - high precision
     - configurable precision
     - report statistical significance (including for diffs)
     - not only average, also: min|max, stdev, histogram, quantiles
  - platform|language-agnostic
     - including CLI, Node.js, TypeScript
     - including comparing Node.js versions

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both medians (for accuracy) and stdev (for precision)

--------------------

Add repo of spyd benchmarks:
  - contributors can add any
  - only for:
     - JavaScript
     - core Node.js or JavaScript, no modules
  - each benchmark is a directory with a single benchmark
     - optional spys.yml
  - README shows all results
     - as run in CI
     - each shows: title, tasks file content, reported result
     - a hardcoded list is maintained for sorting
     - created by a build task
  - binary for users to run any of the benchmarks on their machine
     - including with npx

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2020.stateofjs.com/en-US/resources/
  - https://javascriptkicks.com/submit

Ideas for articles about benchmarking:
  - choice between precision and accuracy
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes

------------------------

Steps:
  - stats/samples:
     - measureCost estimation: step-wise or not???
     - getMaxLoops()???
        - probably:
           - compute for each step, using targetSampleDuration divided by steps.length
           - then find the slowest amount of maxLoops
              - use it for all steps
              - use scale 1
           - use `scale` = Math.round(stepMaxLoops / topMaxLoops), for each step
           - what if no `scale` (CONF.repeat false)???
     - sampleMeasures|sampleMedian|sampleLoops: step-wise
