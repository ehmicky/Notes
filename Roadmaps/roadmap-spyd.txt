
        
   SPYD  
        



Validation:
  - used by:
     - main config
     - plugin config (CONF.runnerConfig|reporterConfig)
        - required to use any configuration property
        - handle errors to add plugin name
        - including common properties, e.g. CONF.reporterConfig.{id}.output (which must do path resolution)
     - not used to validate task files (runner-specific)
        - reason: it runs in runner process, which might be different language
        - runner should instead do the following on their own: default values, normalization, throwing errors
        - try to use shared library between runners of same language
           - including using the same validation code as main|plugin config for Node.js runners
  - validate(object, config, opts)
  - config.VARR.schema OBJ
     - JSON schema
  - config.VARR.validate(VAL, "VARR", OPTS)[->"ERROR"]
     - OPTS:
        - parent OBJ
        - root OBJ
  - validate against unknown properties
     - unless "*" in VARR
     - include automatic suggestion
  - config.VARR.example VAL
     - shown on validation error
  - config.VARR.normalize(VAL, "VARR", OPTS)->VAL
     - same OPTS as validate()
  - config.VARR.default VAL
     - applied if undefined
  - opts.cwd STR[(VAL, "VARR")]
     - used when resolving file paths
        - done when config.VARR.schema.pattern "path"
     - def: process.cwd()
     - when loading config:
        - must keep "CWD" of each VARR as an OBJ since this depends on how config was loaded, and this is known only before validation
        - replace current `PATH_CONFIG_PROPS` logic
        - including for CONF.tasks
     - FUNC can return empty STR to not do path resolution
        - used by CONF.config when prefixed with "npm:" for example
  - opts.validPropName REGEXP
     - use /^[a-z][a-zA-Z\d]*$/
        - i.e. use case (not _ - .) as delimiter
     - reason: when using "*" in VARR, property names are user-defined
  - VARR:
     - dot-delimited
     - can include "*" for either:
        - array indices
        - dynamic properties
  - add comments:
     - no warnings, only errors because:
        - simpler to specify
        - prefer failing hard
        - prefer semver major with breaking changes over deprecation period
        - does not work well with previews since that clears screen
        - simpler to implement
           - warnings would need to be returned from start() and passed to parent

--------------------

Cross-dimensions duplicate id validation:
  - for newly measured result with `run`:
     - throw error
     - only for the given result's ids, not including the previous results' ids
        - reasons:
           - would depend on CONF.since, making it fail or not depending on it
           - since CONF.since defaults to 0, it would not be useful most of the times
           - we would still need a separate way to handle cross-dimensions duplicate ids between results due to:
              - multiple branches results (since we do not load all results from all branches)
              - manual edits of result files
  - between different results:
     - prepend `DIMENSION_` to id
        - use _ instead of .
     - not done on the dimension with the most recent result

CONF.system "ID" -> CONF.system[.{systemDimension}] "ID"
  - DIMENSION is "system[.{systemDimension}]"
     - i.e. can use several dimensions
     - combination.dimensions.system -> combination.dimensions['system.{systemDimension}']
     - getCombinationName() should use only systemDimension
  - system id is "ID", without systemDimension
  - default systemDimension: "machine"
  - default "ID": "primary_{systemDimension}"
  - when sorting result.combinations per dimension, system sorted:
     - after runner, before variation
     - by CONF.system OBJ order
  - result.systems[*].dimensions:
     - before merging: OBJ where key is dimension, value is id
     - after merging:
        - OBJ_ARR_ARR: id STR, title STR, dimension STR
        - second ARR is when merging combinations with same shared props
     - reported as comma-separated list of space-separated lists
     - remove result.systems[*].id|title
  - merging:
     - create shared systems in decreasing number of shared dimensions:
        - first look for properties shared on all system dimensions. On any match:
           - create a shared system OBJ for it, unless either:
              - no shared properties
                 - except top-level one, at systems[0], always defined
              - another shared system has exact same properties:
                 - just append dimensions ids to it instead
           - remove props from the matching combinations, so they are not in two different system OBJs
        - then same for all dimensions minus 1|2|...
           - using all possible sets of "dimensions minus 1|2|..."
     - undefined values:
        - like other values when grouping
        - but cleaned after grouping
           - i.e. not printed by reporters
  - add comments:
     - reasons for allowing multiple system dimensions (e.g. CONF.system.os "linux") instead of a single one (e.g. CONF.system "os_linux")
        - separating those in reporting looks nicer
        - easier to configure titles
        - better dimension-wise sorting in reporting
        - easier to select (CONF.select|limit)

Variations:
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation dimension"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - any CONF.runnerConfig.{runnerId}.PROP
        - cartesian product only to combinations with that runner
  - DIMENSION is "variation.{variationDimension}"
     - i.e. each variable property is a dimension
     - add combination.dimensions['variation.{variationDimension}']
     - variationDimension is property VARR (dot-delimited)
     - getCombinationName() should remove "variation." prefix from DIMENSION
  - variation id is 'ID', not 'PROP.ID'
  - configuration normalization|validation should apply to each VAL
     - including yargs flags parsing
  - when sorting result.combinations per dimension, variations sorted:
     - at end
     - input, then concurrency, then runnerConfig
     - input: sorted by CONF.inputs OBJ order
     - runnerConfig sorted by:
        - CONF.runner ARR order
        - then config prop name, alphabetically
  - add combination.config CONF_OBJ
     - not persisted in history nor used in report
     - contains config, with combination-specific variations
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR STR (commaSpace-separated list)
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - should still set combination.dimensions.{variationDimension} on those combinations, but with id "", title "", titlePadded " ... "
     - when filtering unnecessary dimensions, should exclude ones with id ""
  - each runnerConfig variation should run its own `init` stage
     - `noDimensions` of `init` stage should take it into account
  - add comments:
     - why we only persist variation ids, not values
        - value might change but user intend the id to be constant, which is more relevant
        - value might be big to store, or have secrets
        - value might be not nice to report (e.g. big objects)

Default ids:
  - add default ids:
     - done:
        - right after results load
        - before CONF.since merge
     - for all possible dimensions with default ids
        - figure out all possible dimensions by using dimensions of all loaded results
           - add comment: do not also use currently measured runners' possible configuration properties
              - because this is not available in `show|remove`, which would make CONF.select|limit behave differently between commands
     - i.e. missing dimension should behave like the dimension with its default id, including in:
        - CONF.select|limit
        - diff
        - reporting (e.g. when mixed with other results not missing the dimension)
     - per dimension:
        - task|runner: never missing
        - step: "main"
           - assigned on result creation, i.e. saved
        - system: "primary_DIMENSION"
           - assigned on result creation, i.e. saved, except if multiple system dimensions
        - variation: "main_PROP"
           - not assigned on result creation, i.e. not saved
           - PROP is only last property
           - exception: PROP is last two properties (_-separated) when two variable properties have same last property
  - CONF.select:
     - when retrieving combination ids:
        - if CONF.select contains default ids
           - except for persisted default ids: "main" and "primary_machine"
           - should be checked case-insensitively
           - add comment: this guess might fail due to using partial substring matching, but this is not a problem because:
              - only impacts variations and systems (only if multiple system dimensions) since the other default ids are persisted
              - most substrings matching "primary|main_*" would match all of the other ids of the dimension, making the selectors not useful for users, i.e. unlikely
        - then add those default to the combination ids, if the combination is missing the relevant dimensions
     - reason: selection is applied before results are loaded, i.e. before all dimensions are known and default ids are applied
  - validate that new result ids do not match possible default ids (unless same dimension):
     - "main": step
     - "main_*": variations
     - "primary_*": system
  - add comments:
     - purpose is when a result is missing a dimension that other results have, that result should still:
        - be comparable in the history
           - e.g. only one step was used (undefined) but another step was added, forcing to rename the first step but it should still be comparable
        - be reported nicely
        - be selectable with CONF.select
     - history matching problem:
        - when:
           - a result misses a dimension
           - it must be compared with results with that dimension
           - those other results have multiple ids of that dimension
        - then it becomes difficult to know which id should be used to compare
        - possible solutions (others than default ids):
           - missing dimension never matches an id from a non-missing dimension
              - problem: creates discontinuity in history
           - missing dimension matches any ids from a non-missing dimension
              - problem: non-limear history and most likely wrong results

--------------------

Task argument:
  - should be single argument OBJ: context OBJ2, inputs OBJ3
  - add comment about reasons:
     - clear distinction between context and inputs:
        - readonly vs read-write
        - set in config file vs in tasks file
        - combination-scoped vs iteration-scoped
     - avoids `this` since it:
        - does not work on tasks using arrow or bound functions
        - requires nested functions to use arrow or bound functions
     - extensible to more properties for future features
     - allows any type (unlike environment variables)
     - test-friendly and linting-friendly (unlike global variables)

Inputs immutability:
  - prevent mutations:
     - for:
        - arg.inputs = value
        - arg.inputs.inputId = value
     - reason:
        - could lead to iterations to influence each other by using shared state
           - encourage users to use context instead
           - although users can still bypass it by using top-level state or by setting context in before()
        - simpler for users to reason about inputs as constants
  - use a Proxy instead of Object.freeze()
     - show a nice error message
     - check performance impact
  - allow mutating arg.context[.VARR] = value
     - next iteration should see the mutation, i.e. whole `arg` object should be passed

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each main loop iteration, shallow clone context from before()
     - potential performance optimization: when this is an empty OBJ, create a new inline empty OBJ instead of shallow copying an existing one
        - empty OBJ condition should only happen once
        - see if worth it performance-wise
     - add comment: context properties set in before() are shared by all iterations
        - i.e. any mutation is kept in next iteration
        - this can be used as global changing state, e.g. incrementing id
           - however users should avoid when possible to keep iterations independent from each other
  - do not shallow nor deep clone context before each scale|repeat iteration
     - reasons:
        - deep cloning is not possible on some JavaScript types, e.g. classes
        - shallow cloning would mean top properties would be new on each iteration, but not deep ones
           - this might confuse users
           - for example, renaming context.var++ to context.deep.var++ would change semantics
        - cloning has a performance impact
     - add comment with CONF.repeat true that users should consider that context might have been set by previous iteration of current step
  - passed to step function even when single step
     - reasons:
        - stats do not vary depending on whether there is a single step or not
        - more monomorphic
  - forbid inputId named `context`, to allow other runners to pass `context` alongside named arguments
  - add comments about:
     - advantages over top-level scope (which can still be used)
        - not shared between tasks
        - not shared between iterations
        - does not require declaring a variable
     - problems with alternatives to single `context` OBJ:
        - separate `context` arguments for input and output (to next step)
           - information meant for later steps must be passed between several steps
        - `context` argument for input, return for output (to next step)
           - information meant for later steps must be passed between several steps
        - `{stepId}` argument for input, return for output
           - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
           - name conflict with any core argument
              - except inputs, which are validated against duplicates with steps
           - more complex to explain:
              - return vs context
              - `stepId` argument name
              - when repeating a step, only last iteration's return value is used

--------------------

before|after special steps:
  - on runner start:
     - runner does not know that some steps are special, i.e. return all to parent as tasks.taskId "stepId"_ARR
     - parent handles "before|after" as special steps:
        - separated from steps, i.e. tasks OBJ: id "taskId", steps "stepId"_ARR, hooks "hookId"_ARR
  - runner.hook()
     - runner.before|after() -> runner.hook()
     - parent only sends "hook" event if child defined the hook
     - event payload: step "hookId"
        - using hookId retrieved during start
     - unlike runner.measure():
        - run step function only once
        - any context change is persisted for future runner.hook|measure()

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - in documentation:
     - encourage `export const TASK = { STEP() {...}, ... };`
     - as opposed to `const TASK_STEP = function() {...}; export const TASK = { STEP: TASK_STEP, ... };`
  - validate against tasks with no steps (besides before|after)
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
        - i.e. must return `tasks` `{ taskId: ['main'] }` to parent
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - before|after special steps:
     - remove beforeEach|afterEach
        - rename beforeAll|afterAll to before|after
     - alongside other steps but not considered a step:
        - run only once
        - not reported
        - not a dimension
        - cannot be excluded by CONF.select
        - no impact on CONF.repeat
        - end() is run even on error or cancel
     - can use `context`:
        - in before:
           - context is empty
           - context mutations are passed to any other steps
        - in after:
           - receives only the context mutations from `before`, not individual steps
     - validate that before|after is first|last in steps OBJ order
  - each non-special step is a combination dimension
     - add combination.dimensions.step
     - when sorting combinations: sorted first, before tasks
  - implementation:
     - runner:
        - on start:
           - return all `tasks: { taskId: 'stepId'_ARR, ... }`
              - ARR in execution order
        - on measure, gets OBJ:
           - maxLoops NUM
           - series OBJ_ARR:
              - id "stepId", repeat NUM
              - ARR in execution order
        - do {
            const arg = { context: { ...beforeContext }, inputs }
            for (const { id, repeat } of series) {
              const step= steps[id]
              startTime()
              while (repeat--) {
                step(arg)
              }
              endTime()
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in `series` execution order
     - all steps of a given combination use same process
     - measureEachCombination() [un]groups combinations with same dimensions except stepId into series, and iterates over those instead
        - rename "combination" to "series" for any group of steps with same other dimensions
     - getSampleState() and getStats() are iterated over sampleState.series[*]
     - sampleState:
        - measureDuration|totalDuration|sampleDurationMean: not-step-wise
        - allSamples: not-step-wise
        - all other properties: step-wise, at sampleState.series OBJ_ARR including: step 'stepId', repeat, calibration, stats, sampleMeasures|sampleMedian|sampleLoops
     - sample payload:
        - payload.series[*].repeat NUM: sampleState.repeat
        - payload.maxLoops NUM:
           - not step-wise
           - if first sample (measureDuration === 0): 1
           - otherwise: keep current logic except include steps in `repeatGrowth` computation
              - renamed to `growth`
              - sumEachStep(repeatGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - sampleMedianWeight = sampleMedian / sampleMediansSum
                    - sampleMediansSum = sum of each step sampleMedian
                    - if sampleMedianSum 0: sampleMedianWeight is 1 for each step
                    - if sampleMedianSum not 0 but sampleMedian 0: sampleMedianWeight is 0 for that step
              - maxMeasures threshold: divide (MAX_MEASURES - measures.length) by series.length
     - previewStats:
        - combinationEnd: keep most logic except:
           - compute samplesLeft for each step, and only use the Math.max()
        - skipping logic (uncalibrated or mean === 0) should be step-wise
           - preview should be skipped only if all steps are skipped
     - tweak isRemainingCombination logic for multiple steps:
        - stopped, `dev` stage: keep as is (not-step-wise)
        - mean === 0: series.some()
        - maxMeasures: use sum of all samplesState.step[*].measures.length
        - loops === 0 (CONF.precision 0): series.some()
        - rmoe: series.some()
     - minLoopDuration:
        - each step has its own minLoopDuration
        - multiply TARGET_DURATION by series.length
        - maxMeasures should use sum of series[*].measures.length
        - hasEnoughMeasures(): use series.every()
        - run steps normally otherwise, i.e. the runner executes them serially but with `repeat: 0`
     - truncateLogs: not-step-wise
     - dev command: run all steps at once, without headers in-between
  - separate steps in reporters:
     - reason: they might have different units or CONF.rate (i.e. min|max)
     - for:
        - tables (debug|history): each step should have its own table
        - boxplot: each step should have its own min|max
  - steps excluded by CONF.select:
     - like any other combination dimensions:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
        - not taken into account in:
           - isRemainingCombinationLogic logic
           - preview logic, including duration estimation
           - minLoopDuration
           - payload computation
              - except repeat always 1
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one step for that task is selected
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.select
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.select already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - error handling:
     - keep current logic i.e. exceptions are propagated
        - in:
           - `before` -> do not call later steps nor `after`
           - any other steps -> do not call later steps, but call `after`
              - exceptions in `after` itself are ignored
           - `after` -> nothing else to call
        - reasons:
           - ensures `after` does cleanup, but only if `before` completed
           - but assumes that exception leaves bad state, i.e. should not run additional steps, and `after` might fail
     - child should pass an optional stepId alongside error string, so that parent can print it
  - add comments about:
     - complex step order:
        - limitations of current solution:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allows reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as regular steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.repeat error handling, etc.
     - syntax alternatives for before|after not as good:
        - top-level before|after{taskId} function:
           - requires either picking a specific case or allowing multiple
           - top-level like task functions, although different, creating confusion
           - requires validating that it is not defined twice in multiple task files
           - less clear that `this` is shared with steps
        - before|after[.{taskId}], i.e. before|after is either a function or an object:
           - use same OBJ syntax as steps but with tasks instead, making before|after > task > step hierarchy unclear
     - why sampleTargetDuration does not increase with number of steps:
        - new steps are more likely to be due to splitting existing steps than adding new ones
        - keep preview responsive
     - we do not store steps execution order because not used for the moment

"each" special task
  - runner event payloads:
     - "start" event payload: never pass taskId
     - "hook" event payload: add task "taskId"
     - "measure" event payload: series[*].id "stepId" -> series[*].task "taskId", series[*].step "stepId"
  - "each" special task:
     - handled only by parent process
        - runner process does not know about special tasks
     - after runner start, parent adds each stepId from "each" task to other tasks' stepIds|hookIds
        - then remove "each" taskId
        - including during init
     - the taskId "each" is kept even when merged to another task, so it can be passed to "hook|measure" event payload
  - merging:
     - "each" task's steps can be both "before|after" and normal steps
     - lower priority, i.e. can be overridden by individual tasks
        - for both "before|after" and normal steps
     - prepended, i.e. first in steps order of each task, unless either:
        - overridden by task
        - stepId starts with "after", case-insensitively
     - only applies to tasks inside same file
        - reasons:
           - easier to reason about
           - allows scoping to a group of tasks
           - no possibility of redefining it in multiple files
           - required since different task files have different processes
  - add comments:
     - example use cases for regular steps in "all":
        - create temporary directory when all tasks write to filesystem
        - create temporary REST entity when all tasks modify REST entities
        - create temporary data when all tasks modify arguments
           - e.g. sorting arrays
     - reasons why shared steps are nested under "each" instead of being top-level:
        - keep top-level for tasks and nested level for steps
        - extensible if more properties were to be added to tasks

Step groups:
  - behave like steps except:
     - specified with CONF.stepsConfig.{stepId}.group 'stepId'_ARR
        - ignored if empty ARR
        - reasons for the syntax:
           - allow non-consecutive steps
           - not verbose (unlike using stepId, e.g. using stepId common prefixes)
     - stats are based on aggregation of other steps stats
        - use other steps stats, not `measures` because the number of `measures` might differ between steps when CONF.repeat true
           - add comment that could in principle use `measures` when CONF.repeat false, but does not because:
              - it would make stats differ between CONF.repeat true|false
                 - this is confusing and might lead some users to use CONF.repeat false
                 - using CONF.repeat true|false should only change precision, not accuracy
              - it would give better stats for step groups, discouraging CONF.repeat true
              - it is slower
        - how:
           - samples|minLoopDuration: any
           - mean|quantiles|median|min|max|stdev|loops|times: add
           - repeat: Math.round(loops / times)
           - histogram:
              - among all histograms first buckets, find one with smallest frequency:
                 - create a bucket with:
                    - frequency: smallestFrequency
                    - low|high: sum of all histograms first bucket's low|high
                 - for each first bucket:
                    - if smallestFrequency === firstBucketFrequency:
                       - discard bucket, i.e. next one becomes first bucket for that histogram
                       - also if >= (due to possible rounding error)
                    - otherwise, update:
                       - frequency: subtract smallestFrequency to it
                       - low|high: kept as is
                 - repeat
              - re-distribute buckets:
                 - so buckets width is uniform, and so there are 1000 buckets
                 - do it by summing and interpolating
        - add comment that this assumes steps measures are positively corrolated
           - for: quantiles|median|min|max, stdev, histogram
           - if not, result is a bit inaccurate, but remains precise
  - persisted in history
     - as opposed to being dynamically computed during reporting
     - reason: allows not losing history when:
        - step group change which steps it includes
        - or their names
        - or whether it is a step group or a normal step
  - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
     - reason: users might want to see children only when need details
        - and vice-versa
  - can target another step group
     - error if cycle
  - special stepId "all":
     - only allowed in stepsConfig.{stepId}.group ARR
        - when present, do not allow other values in ARR
     - select all available steps
        - including ones not measured|included
     - forbid "all" as a stepId name, either normal step or step group

Automatic repeat:
  - rename `scale` in prettifyStats() to `factor`
  - CONF[.tasksConfig.{taskId}].repeat BOOL
     - def: false
  - runner:
     - params OBJ: maxLoops NUM, series OBJ_ARR: task 'taskId', step 'stepId', scale NUM, repeat NUM
     - do {
         const arg = { context: { ...beforeContext }, inputs }
         for (const { task, step, scale, repeat } of series) {
           const stepFunc = tasks[task][step]
           while (scale--) {
             startTime()
             while (repeat--) {
               stepFunc(arg)
             }
             endTime()
           }
         }
       } while (maxLoops--)
  - payload:
     - payload.series[*].repeat NUM:
        - if CONF.repeat false and task has multiple steps: 1
     - payload.series[*].scale NUM:
        - step-wise
        - also set at sampleState.series[*].scale[Last]
        - if:
           - CONF.repeat false: 1
           - first sample (sampleMedian === undefined): 1
           - at least one step has no rstdev: 1
              - happens if either <5 measures or mean 0
                 - i.e. always the case when uncalibrated
              - reason: ensure all steps have a rstdev as soon as possible
           - otherwise:
              - steps with rstdev 0:
                 - scale = 1
                 - excluded from "lowest(rstdev**2)" below
              - step with lowest(rstdev**2): scale = 1
              - other steps:
                 - scale = Math.round(current(rstdev**2) / lowest(rstdev**2))
                 - scale is Math.min()'d with both:
                    - Math.round(lowestRstdevStep.sampleMedian / currentStep.sampleMedian)
                       - reason:
                          - prevent `scale` from increasing sample duration too much
                          - based on the fact that rstdev tends to be correlated with complexity, which is itself correlated with median duration
                       - if currentStep.sampleMedian 0: 1
                    - Math.floor((MAX_MEASURES - measures.length) / (series.length - 1))
                 - then scale is Math.max()'d with 1
              - reasons:
                 - repeat imprecise steps more so that all steps reach CONF.precision roughly at the same time
                 - keep scale stable across the run, since difference scales can lead to different engine optimization
                    - which we use we don't use rmoe**2 instead of rstdev**2: although it would be more accurate, it would be less stable
                 - produce same results regardless of CONF.precision
     - payload.maxLoops NUM:
        - if:
           - first sample (measureDuration === 0): 1
           - CONF.repeat false: same as current behavior (i.e. use `growth`)
           - CONF.repeat true:
              - use growth = sumEachStep(repeatGrowth * scaleGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - scaleGrowth = scale / scaleLast
              - maxMeasures: divide (MAX_MEASURES - measures.length) by sum of all series[*].scale
                 - i.e. if all series[*].scale 1, divide by series.length
              - multiply each payload.series[*].scale by maxLoops
                 - including if some steps have no rstdev
                 - but excluding if first sample
              - set payload.maxLoops 1
                 - reasons:
                    - each step has bigger loops, i.e. less cross-step optimization influence
                    - gives more flexibility to slow down some steps over others
     - steps excluded by CONF.select:
        - payload.series[*].scale: always 1
  - combination.imprecise BOOL
     - true when all of:
        - CONF.repeat false
        - multiple steps
        - if repeat had been used, it would have been >1
     - reporting:
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - repeat vs scale:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: make benchmark faster by ensuring each step reaches its rmoe target at same time
     - requirements for tasks with CONF.repeat true:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - i.e. cannot both read+write same property in neither `context` nor top-level scope, including:
           - stateful class instances like event emitters and streams
           - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.repeat true, increase step function complexity (including increasing input size)
           - split step into its own task
     - reasons why CONF.repeat:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.repeat and `scale`:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

--------------------

Add comment about preferred ways to loop, by preference:
  - remove manual loop and let core automatically loop
  - if has init|end (to measure separately): use steps
  - if init|end step slow: use CONF.repeat true
  - if init|end step also stateful: use CONF.repeat false, non-"auto" unit and return chunk ARR in main step
     - also preferred if manual looping happens inside library code
  - if main step does not have access to measures to return, and unit is duration: use CONF.repeat false, unit "count" and CONF.rate true in main step

combination.unit STR:
  - persisted in results
  - used and removed by prettifyStats() normalization
  - at the moment, hardcoded to "auto"

Split measures vs duration ARRs
  - runner: rename `measures` to `durations`
  - sample stage
     - assume that runner might return a `measures` ARR alongside the `durations` ARR
        - it is just forwarded as is to the transformation stage
        - renamed to `sampleMeasures`
     - uses `durations` ARR, not `measures` ARR nor stats
        - use durations, not measures for: repeat, sampleMedian
        - use sampleLoops, not stats.loops
        - except stats.rmoe (for `scale`)
     - `durations` ARR is only for current sample
        - no accumulation between samples
           - including no mergeSort()
     - validation of the response general shape:
        - check:
           - `durations`:
              - not undefined
              - is array
              - each element:
                 - forbid: undefined, not float, NaN, Infinity, negative
                 - allow: 0, float
           - `measures` (unless undefined):
              - is array
              - has same length as `durations`
        - those are plugin errors
        - does not inspect `measures` ARR individual elements
           - this is done by transformation stage instead
  - transformation stage:
     - transform sampleState to a sampleMeasures ARR
     - if not calibrated, skip stage and return empty sampleMeasures ARR
     - in order:
        - validation of each `sampleMeasures` ARR element
           - unit-specific
           - user errors (not plugin errors)
        - unit-specific normalization of each `sampleMeasures` ARR element
           - if unit "auto": none, but set from `durations` ARR
        - sort `sampleMeasures`
           - unless unit "auto": because already sorted
  - stats computation stage
     - does not use any sampleState
        - use sampleMeasures and measures, not `durations` ARR
           - merge sampleMeasures to measures with mergeSort()
        - use sampleMeasures.length, not sampleLoops
           - i.e. after chunk ARRs flattening
           - including for `times` calculation
        - except: calibrated, allSamples, durationState
  - stats|preview computation stage:
     - skipped when samplesMeasures is empty, which happens when either:
        - not calibrated
        - task has returned empty chunk ARRs as custom measures
  - maxMeasures check in isRemainingCombination() uses measures ARR, not durations ARR

Allow custom measures:
  - pass payload.sendMeasures BOOL to runner
     - if true, runner must return a `measures` VAL_ARR in response, using tasks return values
     - runner does not inspect nor validate measures, it just serializes and sends them to parent
  - step functions must then return NUM:
     - reasons, as opposed to mutating a `measure` argument:
        - argument could be destructured, leading to assignment not working
        - confusion with inputs also used as arguments
        - clear that return value has this semantics
     - reason why NUM instead of OBJ:
        - simple
        - works for every language, including cli runner

Allow tasks to return chunks, i.e. ARR of custom measures:
  - allow task to:
     - mix returning ARR and not ARR
        - returning VAL and [VAL] should behave the same
     - return empty chunk ARR
        - should behave similarly to not having measured
  - flatten chunk ARR of custom measures into a `sampleMeasures` single ARR:
     - done during transformation stage, after unit-specific normalization and before sorting
  - sample stage ignores chunk ARRs, but not later stages:
     - sampleLoops ignores chunk ARRS
        - but not sampleMeasures.length nor stats.loops, since those are done after chunk ARR flattening
     - sample response general shape validation ignores chunk ARRs (since it does not look into ARR items)
        - but ARR items validation recurse over chunk ARRs
  - sampleLoopSize:
     - mean length of step's returned chunk ARRs for the last sample
        - computed when flattening chunk ARRs
        - i.e. 1 when no chunk ARRs
        - Math.min() with 1 (in case the task only returns empty chunk ARRs)
     - used as part of maxMeasures in:
        - `scale` as part of the Math.min():
            - Math.floor((MAX_MEASURES - measures.length) / (series.length - 1))
           -> Math.floor((MAX_MEASURES - measures.length) / (series.length - 1) / currentStep.sampleLoopSize)
        - `maxLoop`:
            - (MAX_MEASURES - measures.length) / sum(series[*].scale)
           -> (MAX_MEASURES - measures.length) / sum(series[*].scale * series[*].sampleLoopSize)
  - add comments:
     - meant to be equivalent to having run the task several times returning each chunk ARR element

CONF.[stepsConfig.{stepId}.]rate BOOL
  - def: false
  - applied on report time
     - not persisted in history
     - reporting flag
  - applyRate():
     - done:
        - after combination sorting
           - i.e. kept as is
           - add comment: do not allow configuring|overridding sorting for the moment, to keep things simple, because most users won't need it
        - before prettifyStats()
     - invert:
        - invert stats values (1/NUM)
           - of all stats of mainKind
           - for quantiles: each ARR element
           - for histogram: each start|end
        - swap min|max and meanMin|Max
        - reverse ARR order of quantiles|histogram
     - stats 0:
        - if a stat is 0, make it undefined
        - histogram:
           - if max 0: whole histogram undefined
           - if min 0: first bucket's min = bucketMax / 2
        - quantiles:
           - if max 0: whole quantiles undefined
           - if min 0:
              - find first non-0 quantile
              - replace all quantiles before it by its value divided by 2
        - fix reporters so they take into account those new possibilities of stats being undefined
  - with prettifyStats(): change factor|unitSuffix|unitPrefix

CONF.[stepsConfig.{stepId}.]unit "UNIT"
  - if no stepId: all steps
  - i.e. same step from different tasks have same unit
     - including if single step for all tasks
  - persisted during `run` at combination.unit
     - cannot be changed during reporting

prettifyStats() factor|unit:
  - rename `unit` suffix to `unitSuffix`
  - add `unitPrefix`
     - empty string for most units
  - rename "duration|percentage|count" stat kinds to "duration|percentage|countKind"
  - split "durationKind" stat kind into:
     - "durationKind":
        - for minLoopDuration only
        - `factor` is based on itself (like other stat kinds except mainKind)
        - factor|unitPrefix|unitSuffix always use "durationUnit"
     - "mainKind":
        - for all others: mean[Min|Max]|median|min|max|quantiles|stdev|moe
        - `factor` is based on a single stat (mean[Min]|median)
        - factor|unitPrefix|unitSuffix depend on both combination.unit and CONF.rate
  - rename existing types for factor|unitSuffix|unitPrefix to: "durationUnit", "countUnit", "percentageUnit"
  - add new types for factor|unitSuffix|unitPrefix:
     - inverseCountUnit
        - factor: like countUnit
        - unitSuffix: like countUnit but inverse sign
        - unitPrefix: "1/"
     - durationRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: ops/s|ms|us|ns|ps|fs
     - bytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: B|KB|MB|GB|TB|PB/s
     - inverseBytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: s|ms|us|ns|ps|fs/B

Units:
  - Unit groups:
     - "autoOnly":
        - only use auto duration
        - set `sampleMeasures` using `durations` ARR
        - persisted unit: ns for 1 operation
        - payload.sendMeasures false
        - parent validates that `response.measures` is undefined
     - "mixed":
        - use both auto duration and custom measure
        - set measures to custom NUM / auto duration (in ns)
           - during measure normalization stage
           - chunk ARR elements use the same duration but divided by chunk ARR.length
           - if custom NUM is 0: result is 0 (i.e. same handling as any other NUM)
           - if auto duration 0: filter out the measure
              - even if custom NUM also 0
        - persisted unit: ops/ns
        - reason why we do not persist both the auto duration and measure:
           - would require whole logic to be duplicated for both: stats, preview duration estimation, isRemainingCombination(), saving, etc.
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - use durations.slice().sort() instead of durations.sort() when retrieving sampleMedian
           - reason: must keep its order since it correlates to `measures` order
     - "customOnly":
        - only use custom measure
        - persist task return value as a NUM
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - persisted unit: depends on unit
        - still compute durations
           - including repeat loop
           - reason: needs it for medianWeight (maxLoops)
           - but:
              - `calibrated` always true
                 - by setting it initially to true
                 - reason: uncalibrated durations impact sample durations but not custom measures precision nor accuracy
              - combination.imprecise always false
  - Available units:
     - "auto"
        - default value
        - group: "autoOnly"
        - no return
        - normalization: none, except one from group "autoOnly"
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "duration"
        - group: "customOnly"
        - return float (ns)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "count"
        - group: "mixed"
        - return float (ops)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: durationRate
           - CONF.rate true: durationUnit
        - add comment: possible intents depends on whether NUM is specified by:
           - code used by task: task does not know NUM and forwards it, i.e. check how many times something occured (e.g. number of writes)
           - task code: i.e. task performs a loop and returns loop size
              - in this case, returning chunk ARR of durations is better instead, if possible
     - "bytes"
        - group: "mixed"
        - return integer (bytes)
        - validation:
           - forbid: undefined, not integer, NaN, Infinity, negative
           - allow: 0, integer
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: bytesRate
           - CONF.rate true: inverseBytesRate
     - "boolean"
        - group: "customOnly"
        - return BOOL
        - validation:
           - forbid: undefined, not BOOL
        - normalization:
           - set to ARR with two numbers: `true` count, total count
           - use BIGNUMs
        - stat kind:
           - CONF.rate false: percentageUnit
           - CONF.rate true: inverseCountUnit
     - "number"
        - group: "customOnly"
        - return float
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: countUnit
           - CONF.rate true: inverseCountUnit
  - when merging combinations from different results with same stepId but different unit:
     - filter out older combinations with incompatible units
     - compatible units:
        - any unit with group "durationOnly" or "mixed", even if different unit
        - units with group "customOnly", only if same unit
  - add comments:
     - why CONF.unit enum values are generic words, not specific units, is to make it clear that:
        - this does not set the specific unit being reported
        - unit `factor` happens at reporting time, not measuring time
     - why default is "auto": makes returning values from task functions opt-in:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
     - we allow measures being 0
        - including for any stats, including mean
        - this is useful for most units:
           - a task might randomly not perform an action
              - measures being 0 remove the need for manually looping until the action is performed
           - a task might have a real mean of 0
              - e.g. task which never writes (0B/s), task which always return false (0%), etc.

CONF.unit "boolean":
  - no sorting
  - no outliers
  - mean: `true` count / total count
     - mean 1 is handled like mean 0
        - including when checking whether to skip stdev-related stats
  - meanMin|Max: same as other units
  - quantiles|min|max|median|histogram: undefined
     - i.e. reporters show it the same way as not measured yet combinations
  - stdev|moe
     - estimated with a normal distribution approximation:
        - variance is mean * (1 - mean) instead
        - rest is same, including:
           - stdev = sqrt(variance)
           - standardError, tvalue, moe
     - add comments about statistical significance:
        - BOOLs require binomial confidence interval
        - normal distribution approximation is not as accurate as Wilson score interval
        - however:
           - most of the inaccuracy is when both percentage is very low|high and sample size is low
              - however CONF.precision threshold makes sample size always high enough that the difference is minimal
           - both `scale` computation and preview duration estimation rely on the assumption that rmoe is proportional to sqrt(loops)
  - rstdev|rmoe: divide by (1-mean) instead of mean if mean > 0.5
     - reason: inverting BOOL in task should not change them
  - CONF.precision thresholds are: 100-10-1-.1% instead
     - i.e. 10 times higher
     - reason:
        - binomial distribution has much higher rstdev: minimum 100% (for mean 0.5), and higher as mean gets lower|higher
        - i.e. using same CONF.precision thresholds would be too slow
  - diff: same as other units
     - add comments:
        - why we do not make diff:
           - behave the same for mean and 1 - mean
              - e.g. diff(.2, .1) === diff(.8, .9)
              - i.e. inverting BOOL does not change it
              - reasons:
                 - not intuitive, e.g. diff(.8, .6) is -100%
                 - signedness would be ambiguous, e.g. diff(.8, .9) could be seen either as:
                    - positive: new value is higher
                    - negative: to make it behave like 1 - mean
           - absolute instead of relative to mean:
              - inconsistent with other units
              - inconsistent with rmoe and CONF.precision
              - harder for users when percentage is very low|high
           - show mean|diff as 0-50% true|false:
              - confusing when a results includes percentages both above and below 50%
              - generally confusing
        - users should invert BOOL or not so the mean is <.5, making the diff|limit more intuitive
           - e.g. .2 -> .1 is twice lower, i.e. -50%, but when inverted, .8 -> .9, i.e. +12.5%
  - steps with unit "boolean" are ignored by maxMeasures, both in isRemainingCombination() and in maxLoop computation

--------------------

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for each step
  - always use await on before|after, i.e. allow both sync and async
  - remove task.async BOOL

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `dev` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - mean measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

--------------------

`spyd` history branch:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - directory is "{gitRoot}/history/results/"
        - git root lookup is using same logic as other places which look for it, i.e. CONF.cwd
     - automatic commits should be prefixed with `[skip ci]`
     - individual results:
        - at /history/results/YYYY-MM-DD--HH-MM-SS--BRANCH--{result.id}.json
           - encode|decode characters in BRANCH:
              - [[:alnum:]-_] left as is
              - any others like percent encoding but using x instead of %
                 - including . and /
                 - including x if followed by two [0-9a-f]
                 - a-f lowercase only
              - extract to own library
        - one immutable FILE.json per result
           - i.e. single OBJ
        - format is JSON
           - reason: fast
  - add comments:
     - pros of using a separate git branch:
        - instead of:
           - using regular files in codebase
           - git hash-object + git cat-file
        - does not pollute git log
           - especially in CI where a single commit might have many CI jobs and results
        - does not require git push --force on the codebase
        - automatically handle committing
        - semi-automatically handle git push|pull
           - can update in CI without requiring developers to pull all the time
        - allows multiple files
        - easier to make it skip CI
        - does not create many tags
        - ensures all branches can always be used for branches comparison with CONF.since
        - easy to understand
        - no risk of pruning
        - versioned
     - only store history with git at the moment
        - however possible approach in the future, if there is a need for it: CONF.history STR:
           - "git"
           - "PATH"
           - CRUD stores plugins
     - why concatenated results are not cached:
        - simpler, i.e. no need to check the file, nor update it after each `spyd bench --save`, `spyd sync`, etc.
        - performance benefits not big enough since results are partially loaded in batches
        - manual edits of files would require some way to invalidate cache, e.g. a `spyd prune` command
        - takes space on the cache directory, i.e. might require automatic|manual pruning
     - why using individual results instead of single file for all results:
        - fast to create new results
        - does not create git conflicts
        - concurrent safe
        - small file size impact in git history
        - easier to edit
        - allow loading only few results instead of all

Require git:
  - only for:
     - CONF.save
     - `sync` command
     - CONF.since branch|commit|tag|"parent"
        - i.e. optional for CONF.since resultId
  - require:
     - `git` binary is executable (i.e. `git --version` has exit code 0)
     - minimum version of `git`
     - there is a `.git` in `[.../]{CONF.cwd}`
  - add comments:
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository

Loading results in batches:
  - done batch by batch, with size incrementing exponentially (2**n)
     - instead of all at once
     - in order, using timestamp in result's filename
  - performed iteratively until can determine no more results needed, in three stages:
     - target delta can be resolved
     - since delta can be resolved
     - beforeSinceResults can be results
        - sinceResult + beforeSinceResults and sinceResult + afterSinceResults contain same combinations (after `select` filtering)
        - i.e. sinceResult merging would be same even if more previous results were loaded
  - ensure that delta resolution:
     - apply `select` filtering first
        - i.e. for each batch, files must be JSON loaded
        - including delta formats: NUM, git branch
     - only use results from correct branch:
        - target delta: current branch
        - sinceResult's branch (if git reference of result id) or current branch (otherwise)
     - for since delta: only use results earlier than target result
  - figuring out if delta is resolved
     - by format:
        - NUM -> >= NUM results
        - "first" -> right away with all results
        - id|timestamp|duration -> right away with result index
        - git branch -> latest result in that branch
           - i.e. use BRANCH in filename but not in `systems[0].git.branch`
        - git commit|tag:
           - find `git` commit author date then use it like in timestamp delta format
           - i.e. does not use `systems[0].git.tag|commit`

`spyd sync` command:
  - on `spyd` git branch:
     - git pull --rebase
        - if merge conflict:
           - try to automatically solve by including additions from both branches
              - add comment that this might happen with history/renamed_branches.yml
              - then retry `git pull --rebase`
           - otherwise fail
     - then git push
        - not run if we know locally there is nothing to push
  - stdin|stdout|stderr "inherit" on git pull|push
     - reasons:
        - allow entering passwords
        - show any error message such as: authentication, git hooks, network, etc.
        - provide with progress
     - not on other git commands
     - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
     - only if CONF.quiet false
  - add comments:
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull

Multiple branches reporting:
  - only report one branch + targetResult
     - branch is:
        - if CONF.since is a git reference or a result id, use its branch
           - branch is resolved:
              - git branch: use it
              - git tag|commit: use the same git logic used to get the current branch, but with a git tag|commit
              - result id: get branch from the result's filename
        - otherwise, current branch
           - instead of using env-ci, should use the best library to guess the branch from a specific commit reference
              - ensure cwd can be specified
              - could also just inline the git calls made by the library
           - if no branch found, use the `undefined` branch
     - filter out any results from the branch later than targetResult, before applying sinceDelta
        - using timestamp + branch in result's filename
        - reasons:
           - ensure benchmark's history remains same as new results are being added
           - ensure history is sorted by timestamp, including the target result
           - parent branch's newer results are likely to contain performance improvements that were not merged in the current branch yet when the result was taken
     - all results without a git branch are treated as if part of a single `undefined` branch
     - result's branch is persisted when the benchmark is saved
  - CONF.since "parent":
     - same as CONF.since "branch" with parent branch
        - i.e. last commit of parent branch, since this is the one the child branch will merge to
     - if no parent branch, like "first" in current branch
  - add comments:
     - problems:
        - hard to know which is parent's branch parent commit using only the benchmarks data:
           - need to use commit ids, which might have changed since benchmark was run due to rebasing
           - user intent is ambiguous: even if branch has not been rebased to parent yet, might intend to do so
        - parent branch's benchmarks' timestamps might interleave with current branch's
        - current branch might have been rebased 0|1|n times onto parent branch
           - parent and current branch's performance improvements are mixed and hard to distinguish
        - grouping results per branch and showing several branches leads to:
           - more visually complex time series
           - harder to understand history since each branch has its own progression
        - should be easy to understand|explain
        - should preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - should not be impacted by rebasing
        - allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
     - discarded solutions:
        - report all results without taking branches into account
        - report all results, grouped by branches
        - report all results of parent branches, grouped by branches
           - variant: only show earlier results
        - let user decide with a CONF.branch configuration property

Renaming branches:
  - on results load, should also get a list of branch renames:
     - do it using the current git reflog
  - use the list of branch renames:
     - to normalize all results branches to their latest names, including when:
        - resolving CONF.since's branch
        - reporting
     - the result's filename and contents also contains the branch name but this name might be old, which is ok since it is fixed by branch renames
  - persist branch renames:
     - reason: git reflog lasts only for 90 days by default
     - done during CONF.save
        - reason: otherwise, user might not expect having to run `spyd sync` again
     - when saving, should automatically create its own git commit
        - using same logic as git commits for results saving (including `[skip ci]`), but as a separate commit
           - reason: cleaner, and limit impact of merge conflicts
     - only append new branch renames, not ones already persisted
        - reason for append-only: limit potential for merge conflicts
     - only add renames for branches used in at least one result
     - persisted to history/renamed_branched.yml
        - OBJ_ARR: from 'BRANCH', to 'BRANCH2'
        - reason for YAML array: limit potential for merge conflicts
     - always loaded and merged with current git reflog, with lower priority

--------------------

Find ways to improve precision even more???

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information
  - in runners:
     - send: errorMessage STR, errorType STR
     - errorType:
        - default to "plugin"
        - unless "plugin": user error
        - parent process prepends a specific string based on the error type
           - i.e. runner does not need to
     - specific error types:
        - "ipcSerialization"
           - when serializing payload
           - if CONF.manual, parent handles it as a user error, i.e. a task returned a non-JSON-serializable value
           - otherwise, plugin error
        - "tasksLoad": tasks file load
        - "tasksSyntax": tasks file syntax error
        - "stepRun": when running a step function
        - "config": runnerConfig validation

CONF.debug BOOL
  - meant as bug report attachment, not meant for users to debug themselves
  - saved debug information to "{process.cwd()}/spyd_debug_logs.yml"
     - not printed to stdout
     - no way to configure location
     - saved at end once, using try/finally wrapping the programmatic entry points
        - not streamed, so it does not impact benchmark
  - does not change the logic otherwise
     - including reporting and previews
  - file is YAML:
     - using document stream
     - of OBJ:
        - event "EVENT"
        - event-specific properties
  - include:
     - envinfo
     - resolved config
     - task files
     - runner.versions
     - combinations
     - samples
        - including: duration spent, estimated time left, progress bar percentage
  - for all commands
  - interface is debugLog(debug, "EVENT", EVENT_OBJ)
     - debug is undefined if CONF.debug false, mutable ARR otherwise
  - add to GitHub issue templates

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

Fix --help CLI flag output: it has odd newlines breaks

Learn package 'simple-statistics' and|or 'jstat' and use it in spyd, where needed

Find performance bottlenecks and optimize them

--------------------

Add TypeScript support:
  - to:
     - spyd.ts
     - tasks.ts
  - export types of those too

Add output formats:
  - report.reportMarkdown()->PROMISE_STR
     - for CONF.output "*.md|markdown|mdown|mkd|mkdn" or "README|readme"
     - def: reportTerminal() return value in ``` block
     - no padding
     - allows two reporters with same output
        - concatenated like terminal formats, i.e. with newlines
     - footer: appended as Markdown
  - report.reportHtml()->PROMISE_STR
     - for CONF.output "*.html|htm"
     - file should be whole (i.e. have <html> tag)
     - reporter can save other files (imported by the main file) in the same directory or subdirectory
     - def: reportMarkdown() return value rendered to HTML
     - no padding
     - does not allow two reporters with same output
        - reasons:
           - hard to concatenate
           - can emulate concatenation using a parent HTML file
     - footer: inject to any element with id "spyd-footer"
  - report.reportSvg()->PROMISE_STR
     - for CONF.output "*.svg|png|jpg|jpeg|webp"
     - def: reportHtml() return value rendered to SVG
     - converts SVG to PNG|JPEG|WebP
     - no padding
     - does not allow two reporters with same output
     - footer: added with svg manipulation
  - add default value for reportTerminal(): using markdown|html|svg return as is

Reporters:
  - types:
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for means
        - with full CLI width for slowest mean
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
        - abscissa:
           - only show start|end
           - format should be date-only if different days, time-only otherwise
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - stacked bar graph for multiple stages benchmarks
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - no JSON???
     - reasons:
        - might be misused in place of CONF.save (when intent is persisting)
        - might be misused in place of external reporters (when intent is external reporting)
        - increase importance of forward compatibility, because JSON consumers might not be versioned unlike reporters

GitHub action

GitHub PRs integration

Add other runners:
  - HTTP
  - JSON-RPC
  - graphQL
  - gRPC
  - WebSocket
  - TCP
  - chrome (maybe using puppetter)
  - firefox (maybe using puppetter-firefox)
  - selenium
  - bash
  - go

SaaS:
  - reporting dashboard
  - files:
     - either connect to GitHub repository
     - or edit file with online IDE, saved in the server
  - perform benchmark in-browser
  - public URLs, for sharing

--------------------

Learn the whole Terminal section in edl
  - including finishing ANSI sequences and terminal emulators
  - use cli table library
     - ensure this is still responsive (by creating multiple table if too wide)
        - if no library does this, create own library on top of another

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Terminal-histogram:
  - separate to own repository
  - add features:
     - color themable (using terminal-theme)
     - left bar with percentages
     - can specify number of abscissa ticks
        - or number of columns per tick
        - stack labels, i.e. might need to stack deeper than one level
    - allow minimum ordinate to be either 0 or minimum value
    - labelling columns
    - custom unit for ordinate
    - when too many columns:
       - if labeled: break into several histograms
       - otherwise: extrapolate

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

--------------------

Manually try all features with each Node.js version

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Positioning (in documentation), in priority order:
  - whole benchmarking flow, not just measuring:
     - reporting (pretty, configurable, live preview, multiple formats, insertion, hardware/software info)
     - combinations (variations, inputs, systems, steps, selection)
     - comparing
     - history (including time series)
     - debugging (spyd dev, nice error messages)
     - sharing for others to run
     - testing (limits)
     - concurrency benchmarking
     - CI
     - GitHub PRs
  - simplicity
     - no library|APIs, only export functions
     - no need to specify duration nor number of loops
     - can work with no|minimal configuration, including on-the-fly
  - precision
     - high precision
     - configurable precision
     - report statistical significance (including for diffs)
     - not only average, also: min|max, stdev, histogram, quantiles
  - platform|language-agnostic
     - including CLI, Node.js, TypeScript
     - including comparing Node.js versions

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both means (for accuracy) and stdev (for precision)

--------------------

Add repo of spyd benchmarks:
  - contributors can add any
  - only for:
     - JavaScript
     - core Node.js or JavaScript, no modules
  - each benchmark is a directory with a single benchmark
     - optional spys.yml
  - README shows all results
     - as run in CI
     - each shows: title, tasks file content, reported result
     - a hardcoded list is maintained for sorting
     - created by a build task
  - binary for users to run any of the benchmarks on their machine
     - including with npx

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2020.stateofjs.com/en-US/resources/
  - https://javascriptkicks.com/submit

Ideas for articles about benchmarking:
  - choice between accuracy vs precision
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes

------------------------

Possible solution for environment variation:
  - applies:
     - to any distribution where nearby measures are more likely to have value close to each other
        - as opposed to being fully independent from each other
     - i.e. more generic than just product of two distributions as in our case
  - assumption: there is a maximim neighborhood size where measures beyond this are independent from each other
     - i.e. grouping with that size makes the resulting distribution have fully independent measures
        - i.e. can use central limit theorem for moe
  - i.e. only need to find the maximal ratio actualStdev/expectedStdev and return it, so that, when computing moe, consumer either:
     - divides sampleSize by square of it
     - multiplies stdev by it
        - maybe result in stat.adjustedStdev, i.e. can be used by diffPrecise
           - would be different for different sample sizes when below groupSize though, since statefulness decreases logarithmatically with higher sample size
           - i.e. this is the stdev after sample size is high enough
  - things to check:
     - check the group index being picked
        - it seems that it is usually too high???
           - if so, why???
           - this leads to big meanRatio stdev when period is low
           - use low period like 2**2 to debug this
        - maybe, when searching for startIndex, allow decrementing startIndex if it fits when also decrementing endIndex???
           - i.e shift both ends
     - debugging using the meanRatio percentiles, mean and stdev, tweak:
        - significance rate
        - whether to use Math.floor() or ceil() with invalidMax
        - clusterFactor
           - findings about smaller clusterFactor???
              - makes accuracy+precision fluctuate less when sample size changes
              - does not impact precision much, in average
                 - except when groupCount is close to maximum: then precision decreased a little
              - slightly decreases accuracy
     - MIN_GROUP_SIZE:
        - does not currently work
           - make sure it applies to groupSize, not clusterSize
        - increase it?
           - or have some way to take groups with wide confidence interval less into account???
        - use only a single group, with groupSize === clusterSize???
     - CLUSTER_FACTOR:
        - ensure it works
        - figure out how much time complexity it adds
           - it seems groupCount n -> duration log(n)
           - try to improve it?
        - find best value
     - check if resulting ratio can be used to find the perfect moe of the distribution
  - does not work when sample size is small???
     - use minimum duration???
        - make it higher when CONF.precision higher
        - verify assumption that longer duration reduces ratio between measured environment variation and real variation
           - even when number of samples is same, i.e. try it using timeout pauses
  - use result in stat.rmoe too, i.e. to end benchmark???
     - could lead to ending too soon for tasks with very low rstdev since sample size might be too low to detect environment variation???
  - performance issues???
     - tweak performance
     - cannot sort, i.e. outliers require ARR.filter() on all measures
     - due to outliers, cannot compute sums incrementally
     - done on each sample
        - skip doing it sometimes, reusing previous value, with exponential gaps???

Possible logic to find environment variation:
  - main situation:
     - product of two uncorrelated distributions:
        - task: independent variables
        - environment: variables depend on:
           - previous one, i.e. stateful on time
              - solutions:
                 - moe takes environment changes into account
                 - increase CONF.precision to make benchmark run longer, which softens the statefulness on time by making that state fluctuate more
           - global state that can change between runs
              - e.g. OS upgrade, different machine, etc.
              - solutions:
                 - use CONF.system to signify environment differences
                 - keep machine load constant as much as possible
                    - during run
                       - e.g. not using other applications while waiting for benchmark to end
                    - between runs
                       - e.g. using CI
     - i.e. result is lognormal
  - logic:
     - goal is to guess both distributions from their product
     - this relies on the fact that one distribution's measures are stateful on time
        - i.e. sample size reduces first distribution's moe by sqrt(n), but other distribution much less
        - i.e. when grouping elements and taking mean
           - first distribution's stdev as divided by sqrt(moe) and closer in shape to normal distribution
           - second distribution does not change much
           - i.e. the product is closer to the second distribution than to the first
        - the second distribution statefulness is finite:
           - when grouped at a specific group size (the "period"), the statefulness is all within the group's timeline, but not in-between grous
           - the period is the group size when the product of both distributions is closest to the second distribution
  - random generation:
     - multiply by another distribution instead of a periodic rate
     - second distribution should use
        - index % period instead of Math.random()
        - realMeanRate as mean
        - realStdevRate as stdev
     - ensure existing logic is still accurate
  - possible simplifications???
     - only look for combined moe, not necessarily mean + stdev of both distributions
     - assume that distributions are added, not multiplied
        - not an issue regardless of means, but can be an issue if first distribution has high stdev???
  - when multiplying normal distribution, the result mean and variance is:
     - mean: meanA * meanB
     - variance: (varianceA + meanSquaredA) * (varianceB + meanSquaredB) - meanSquaredA * meanSquaredB
  - guessing period:
     - ratio is not minimal at period. Instead, the ratio stops decreasing???
  - guessing first distribution stdev:
     - using difference of stdev between after grouping and in total, since all of that difference comes from first distribution stdev / sqrt(n)
  - guessing realMeanRate:
     - using group with highest ratio
     - is ratio the same as realMeanRate???
        - confirm this works using generated random values
  - guessing realMeanStdev
     - use mean of the stdevs of each cluster, and compare difference with what is expected???
  - then implement in spyd codebase:
     - confirm the guessed environment variation roughly matches what it seems to be
        - i.e. when multiplied to moe, the resulting meanMin|Max encompasses the real mean ~95% of the times, providing machine load is constant
     - stages:
        - initially, for simplicity while confirming it works, keep unsorted measures ARR and compute everything once at end
        - then, keep track of each group of measures as sums, computed sample by sample

Difference between reported and real moe???
  - environment variation:
     - different types of environment variation:
        - small changes due to OS optimization and caching
        - bigger temporary slowdowns due to recurring OS background processes
        - temporary slowdowns due to other programs on the machine either:
           - left in background
           - user interacting with them while benchmark is running
     - environment variations are hard to measure:
        - their frequency can be hard to predict: does not follow clear patterns, user-driven
        - their amplitude is variable and not necessarily bounded
        - therefore, tasks' distribution is ever-changing
           - especially its mean and stdev, not so much its general shape
           - how much it changes between each sample is itself very variable in time
              - i.e. it randomly alternates between period of high and low precision
        - this creates imprecision
        - this also makes it hard to compute moe because it relies on the central limit theorem which assumes the underlying distribution to be constant
        - the variation shifts continuously in time, i.e.:
           - the result looks like a single distribution
           - but each quantile's frequency varies a lot between runs, which makes the overall precision much lower than statistical moe
           - this also makes it impossible to distinguish individual distributions|modes in the final distribution
     - distinct from statistical variation
        - low statistical moe does not reduce it
        - i.e. high environment variation does not change when to stop benchmark run
           - since longer benchmarks are mostly meant to reduce statistical variation
        - however, higher sample size:
           - blends more temporary and recurring environment changes into a single distribution, making the reported stdev|moe include them, i.e. less of a problem
           - makes engine optimization last for a smaller percentage of the whole run, i.e. easier to distinguish from environment variation detection
     - cannot reduce the amount of environment variation???
        - except for not doing anything while benchmark is running
        - although can maybe measure it and add it to diffPrecise and reported moe???
     - this creates the following problems:
        - makes reported moe not match user's experience of re-running the benchmark???
        - make runs results differ from each other???
        - makes diffPrecise less useful???
           - if can measure environment variation, multiply stdev by it???
           - or use quantiles instead of welch t-test??? or use both???
     - possible solutions???
        - measure environment variation and take it into account:
           - use time dimension
              - reason: the only thing which distinguishes environment variation from task regular variation is that its mean|stdev changes over time
              - i.e. cannot use sorted measures since those are timeless
              - split measures into NUM by chronological order then check mean|stdev???
                 - subtract expected variation, i.e. moe (using sample size of each chunk)???
           - use individual measures, not samples???
              - since environment variation is independent from sampling
              - for performance reason, use a binary search (using factor 2 or maybe lower)???
           - use welch t-test to see if a group of measures is different distribution???
           - what if slow task with very few measures???
           - store both:
              - non-adjusted moe: used to stop running benchmark, since cannot improve environment variation, can just measure it better
              - adjusted moe: reported
        - do not report moe
           - remove CONF.showPrecision
           - remove meanMin|Max
        - report quantile range instead of moe
           - also useful for users to get an idea of the distribution real range
              - although some of it is due to environment variation
           - p25-p75??? p5-p95??? p2.5-p97.5???
              - it looks like p5-95 or p2.5-p97.5 are better to capture "mean is usually in this range"
              - but p25-p75 has a smaller range, making it more visually interesting
           - rename CONF.showPrecision to CONF.showRange???
           - is boxplot reporter enough instead???
           - works better with central stat being median instead of mean???
              - pro: removes influence of outliers removal logic on the central stat
                 - still needed for min|max and histogram???
              - con: less useful to users who want summed duration when task is repeated in hot path
              - con: does not work well with small integer measures
              - con: might encourage some users to manually loop to overcome some of the cons
              - con: stdev|moe are meant to work on mean, not median
              - con: more different from other benchmark tools, i.e. might result in surprising differences
              - con: less precise with very skewed distributions
  - confirm theory: the fact that a task is lognormal does not change its moe accuracy too much
     - try to see at which sample size this changes
  - findings:
     - currently:
        - real moe is 2-20 times higher than reported moe
        - real moe decreases with higher CONF.precision, but slower than reported moe
     - automatic outliers logic is useful:
        - removing outliers:
           - improves stdev and min|max a lot
           - does not change mean too much, when sample size is large enough
        - using fixed outliers percentages:
           - requires knowing that the best value for a given distribution before-hand
           - even when known, leads to similar stats as when automatically computing those percentages instead
        - outliers are spread through the run, not only at beginning
     - using repeat loop does not impact cross-runs precision
     - when the measures follow a perfect normal distribution, not influenced by machine, cross-runs precision is perfect
     - repeating a task changes the following, until eventually stabilizing: mean|median, stdev, max, outliersMin|Max
        - usually improves, but sometimes opposite
        - stripping earlier measures barely changes stats, providing sample size is high enough
           - instead, it is more important to ensure sample size is high enough so that most of the measures were optimized
     - environment variation:
        - even when just repeating same task
        - even when task is optimized
        - sometimes, very temporary, for just one sample
           - other times, persist for several minutes
     - each process comes with stats variation
        - i.e. running several processes instead of 1 for the same total amount of measures improves precision
        - however, not by much and require too many processes to have an effect
        - i.e. not worth it, especially considering:
           - implementation complexity
           - less precise preview duration
           - confusing previews due to multiple slow starts
           - having to load top-level scope of tasks file and before() multiple times

Non-normal distribution:
  - distribution not always normal:
     - unit "boolean": binomial
     - other units:
        - often lognormal
        - but can be other distributions in some occasions
     - unit "count|bytes": mixed with discrete normal or lognormal
  - only a problem for [r]moe, not other statistics
  - less of a problem with higher sample size due to central limit theorem
     - from my tests, seems not to be a problem at all when sample size high???
  - cannot use traditional methods to compute moe for lognormal distributions because:
     - too slow to compute
     - the distribution might in some cases not be normal nor lognormal
     - too complex, making it harder to compute preview duration
  - multiply any moe by a constant number???
     - or make it depend on how much data fits a normal distribution???
  - make moe as skewed as distribution???
     - make meanMin|Max = mean +|- moe * factorMin|Max
     - either:
        - make factorMin + factorMax = 1, i.e. same total interval???
        - or one factor is 1 and the other is >= 1???
     - what to use as skew factor???
        - not good because does not take into account distribution:
           - amount of measures of each side of mean|median
           - min|max difference with mean|median
        - ratio of stdev on each side of mean|median??? i.e. after removing the other half but keeping mean|median as is

Engine optimization:
  - how to fix the fact that some tasks get faster as they repeat???
  - maybe only stop running when NUM% earlier sample have same mean as current one???
     - NUM% is based on number of measures, not samples
     - use a welch t-test
     - can use tStat / tValue ratio as part of preview durations computation???
  - also check for stdev difference???
  - is checking for mean difference useless since if there is one, stdev should be too high for rmoe target to be reached???
     - and if optimization is too slow, cannot detect it with mean difference, not know for sure unless running longer
     - i.e. only good solution is increasing CONF.precision???
  - what if it makes low CONF.precision last for too long???
     - different significance rate per CONF.precision???
     - not applied to CONF.precision 0 and|or 1???

Significance rate:
  - increase significance rate from 95%???
     - values:
        - 95%: 0% longer runs in average, and rstdev is multiplied by (for 2,3,4,5 samples): 42, 4.8, 2.6, 2
        - 99%: 40% longer and: 260, 13, 5.4, 3.7
        - 99.5%: 75% longer and: 10000, 26, 8.9, 5.5
        - 99.9%: 150% longer and: 26000, 130, 27, 13
  - higher significance rate than 95% when sample size is low, since magnitude of moe error is then bigger???

MIN_STDEV_LOOPS:
  - compute correct value for unit "boolean"???
     - i.e. check quantiles of moe difference for the 5% times it is wrong, based on sampleSize, using similar logic as Desktop/gaussian.js
  - different value too for:
     - unit group "mixed"???
        - with small integers???
     - unit "integer"???

Unit group mixed using both integers and durations:
  - not a problem since: ???
     - if integer range small, rstdev big (since integers are multiplying factors), i.e. rmoe big
     - if integer range wide, closer to a continous distribution, i.e. no stdev problem

stdev|moe with always same measure:
  - use n+1???
     - i.e. compute stdev but as if one measure was 1 off
        - note: stdev computed differently for unit "boolean" and other units
     - reason: if n+1 still below rmoe threshold, no need to continue
     - other reason: give a sense of precision range with some statistical meaning
     - does not work for unit boolean since very low proportions have very high rmoe???
  - or use Wilson score interval???
     - use for other units than "boolean" too???
        - reason: since repeated measures, assume:
           - discrete distribution
           - any divergent measure would be only off by 1
           - therefore same as a probability|binomial situation
  - impact on diffPrecise???

stdev when rounded:
  - when measures are rounded and sample size low, sample stdev might be too low, leading to rmoe too low???
     - rounding due to:
        - float epsilon (2e-16)
        - integers (1)
     - including stdev 0 (always same measures)
        - might be real one
     - including with mean 0
        - might be real one
        - or 0|1 with unit "boolean"
  - main problem is preventing too low threshold (making run stop too early) not too high (making run slower)
  - moe imprecision is same as stdev imprecision
     - e.g. if stdev twice too high, so is moe
  - use variance confidence interval, i.e. chi-squared test???
     - variance * (n-1) / chi-value
     - i.e. stdev * Math.sqrt(n - 1) / Math.sqrt(chi-value)
     - chi-value:
        - degrees of freedom: n - 1
        - asymetric (underestimations are bigger than overestimations)
        - must use half on each side, e.g. 95% -> 0.025|0.975
        - Math.sqrt(n - 1) / Math.sqrt(chi-value) for n 2-... and 50-51:
           - 0.025 (inverted): 2.24, 1.92, 1.76, 1.67, 1.60, 1.55, ..., 1.1971, 1.1951
           - 0.975: 31.9, 6.26, 3.73, 2.87, 2.45, 2.20, 2.04, 1.91, 1.83, 1.75, 1.69, ..., 1.2461, 1.2430

Add comments:
  - user should set CONF.precision up to the highest duration they are willing to wait
  - some tasks get optimized as they repeat
     - by either runtime or OS
     - often done at specific thresholds
        - i.e. cannot know in advance whether there is a future optimization threshold until it has been hit
     - this can lead to benchmark to end with unoptimized stdev and mean
     - solution: user should increase CONF.precision
  - mean is more robust than the median for strongly multimodal distributions
  - rmoeMin|Max:
     - unlike moe, rmoe can be impacted by mean's inaccuracy (due to being estimated)
        - this can make run end earlier than expected
     - it would be more accurate to use rmoeMin|Max instead of rmoe
        - using as moe / meanMin|Max instead of moe / mean
        - rmoeMax would be used like rmoe, i.e. mostly to check to see if combination should end
     - for a given rmoe, rmoeMin|Max is [rmoe/(1+rmoe), rmoe/(1-rmoe)]
        - the higher rmoe threshold, the more likely to reach too early
        - with our CONF.precision thresolds, rmoeMax is rmoe multiplied by: 2 (50%), 1.11 (10%), 1.05 (5%), 1.01 (1%), 1.005 (.5%), 1.001 (.1%)
     - while it would be more accurate, we do not use rmoeMin|Max because:
        - the difference with rmoe is:
           - too low to be worth the added complexity
           - changing when run ends, but not the stats themselves (including moe)
           - constant for a given CONF.precision, i.e. could be considered part of the rmoe threshold itself
              - e.g. 10% rmoe threshold is actually always [9.1%, 11.1%]
        - it is not a problem even when mean inaccuracy is high
           - because that is only likely when rstdev is high and sample size low
           - which is proportional to a higher [r]moe
  - distribution of most duration-based measures:
     - are normal-to-lognormal
        - most of the times (but not always) longer tail are on the slow side
     - have multiple modes
        - multiple modes sizes can go from barely noticeable to big, separate modes of same size
        - the central one is often higher than the others
        - some modes are only present when sample size is small
        - this is less common for logic with higher complexity
     - if they use randomness as input:
        - might have multiple very separate modes
        - might have a more uniform distribution
     - outliers:
        - most have 0-5% slow outliers which are due to both:
           - lognormal long tail
           - engine optimization, OS background processes, etc.
        - many (but not all) have 0-0.5% fast outliers
        - a few have a large amount 5-30% of slow outliers which are due to either:
           - distribution having multiple modes
           - engine optimization, i.e. slow outliers are less frequent as sample size increases
        - rarely, there is a large amount 1-10% of fast outliers
  - impact of changing MIN_STDEV_LOOPS:
     - lower value:
        - faster first preview
        - runs with slow mean and low rstdev finish faster
     - higher value: more likely to:
        - end run too early
        - report inaccurate first previews
