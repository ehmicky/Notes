
        
   SPYD  
        



Default CONF.output:
  - can be specified per reporter at reporter.defaultOutput
  - if unspecified, use "stdout"
     - i.e. "terminal" format
  - reason: indicates most likely format and filename

Default CONF.quiet|colors|showDiff:
  - false|true|true if CONF.output is "stdout" and stdout is interactive
  - applied per-reporter
  - remove reporter.quiet

report.report{Format}():
  - reasons:
     - differentiate between reporter intent (dev, histogram, etc.) and output format
     - smaller list of reporters
     - allows knowing in advance which format is supported
  - alternatives not as good:
     - report() returning all formats at once
        - reason: requires computing unused formats
     - format passed as argument to report()
     - different modules, named spyd-reporter-{reporter}-{format}
        - reasons:
           - requires uninstalling/installing to change format
           - harder for publisher
  - format is guessed based on CONF.output
     - "terminal" is the fallback otherwise
     - automatically create intermediary directories (except when "terminal|external")
  - when report.report{Format}() does not exist, each format has its own default to re-use another format's return value
  - formats:
     - report.reportTerminal()->PROMISE_STR
        - no default value
     - report.reportExternal()->PROMISE
        - meant for: separate program, network requests, desktop notifications
        - no default value
        - for CONF.output "external"

Concatenating result of reporters with same report.output is format-specific:
  - terminal: newline joined
  - external: errors

Footer:
  - added by core, not reporter
  - logic is format-specific:
     - terminal: appended as text
     - external: nothing
  - if some reporters have same report.output, done on their concatenated return value
  - id|timestamp|systems still passed but:
     - only to reporters with format "external"
     - only if report.showMetadata|showSystems true (using current logic)

--------------------

Programmatic usage:
  - do not return reporter outputs:
     - can report to a file and load that file instead if needed
  - let exceptions propagate
  - `spyd run|show|remove` return result
     - same result as reporters, i.e. post normalization
        - reasons:
           - simpler to document
           - every returned properties might be useful
     - use CONF.show* true for all of them
        - reasons:
           - do not tie programmatic return value with CONF.reporter
           - every returned properties might be useful
  - `spyd dev`: return nothing
  - add comment that default CONF same in CLI and programmatic:
     - i.e. reports to stdout by default and use default config lookup
     - reasons:
        - easier to switch between CLI and programmatic
        - consistent

Validation:
  - used by:
     - main config
     - plugin config (CONF.runner|reporter)
        - required to use any configuration property
        - handle errors to add plugin name
     - not used to validate task files (runner-specific)
        - reason: it runs in runner process, which might be different language
        - runner should instead do the following on their own: default values, normalization, throwing errors
        - try to use shared library between runners of same language
           - including using the same validation code as main|plugin config for Node.js runners
  - validate(object, config, opts)
  - config.VARR.schema OBJ
     - JSON schema
  - config.VARR.validate(VAL, "VARR", OPTS)[->"ERROR"]
     - OPTS:
        - parent OBJ
        - root OBJ
  - validate against unknown properties
     - unless "*" in VARR
     - include automatic suggestion
  - config.VARR.example VAL
     - shown on validation error
  - config.VARR.normalize(VAL, "VARR", OPTS)->VAL
     - same OPTS as validate()
  - config.VARR.default VAL
     - applied if undefined
  - opts.cwd STR[(VAL, "VARR")]
     - used when resolving file paths
        - done when config.VARR.schema.pattern "path"
     - def: process.cwd()
     - when loading config:
        - must keep "CWD" of each VARR as an OBJ since this depends on how config was loaded, and this is known only before validation
        - replace current `PATH_CONFIG_PROPS` logic
        - including for CONF.tasks
     - FUNC can return empty STR to not do path resolution
        - used by CONF.config when prefixed with "npm:" for example
  - opts.validPropName REGEXP
     - use /^[a-z][a-zA-Z\d]*$/
        - i.e. use case (not _ - .) as delimiter
     - reason: when using "*" in VARR, property names are user-defined
  - VARR:
     - dot-delimited
     - can include "*" for either:
        - array indices
        - dynamic properties
  - add comments:
     - no warnings, only errors because:
        - simpler to specify
        - prefer failing hard
        - prefer semver major with breaking changes over deprecation period
        - does not work well with previews since that clears screen
        - simpler to implement
           - warnings would need to be returned from start() and passed to parent

--------------------

Remove outliers for all stats:
  - including:
     - mean, quantiles
     - min|max (remove them)
  - reason:
     - make all stats consistent with each other
     - avoid two different sets of min|max (with|without outliers)
  - rename
     - low|high to min|max
     - medianLow|High to medianMin|Max

reporter.debugStats BOOL
  - def: false
  - true for `debug` reporter
  - undocumented
  - if false, do not pass:
     - mean
        - add comment that we must ensure median is the main one used, so different reporters are consistent, and also because it is used in sorting combinations, and also it is less precise
     - times
        - add comment that it is a bad indicator of precision, and also might be confused as an indicator of speed due to other benchmark libraries showing it like that
     - rstdev|rmoe|moe
        - add comment that medianMin|Max is more useful for end users, including to allow seeing if two combinations overlap
     - minLoopDuration, samples, repeat, loops
        - add comment that those are internal

Box plot reporter:
  - name "box"
  - show: min, p25, median (or medianMin|medianMax), p75, max
  - abscissa min|max is same for all
  - also shown in `dev` reporter

--------------------

Combinations filtering:
  - list all unique ids from all results
     - including currently measured result
     - then find their categories
  - order:
     - getCombinations()
     - fetchResults()
     - listIdsCategories()
     - selectCombinations()
  - use category when filtering
     - reason we need to know category: non-matching ids resolve differently if in different categories or not
     - unknown category (i.e. unknown id) should error
        - use levenstein distance to suggest typo (if close enough)
  - duplicate id validation should use all results ids
     - i.e. no new result id has a duplicate in a previous result with a different category

Categories:
  - combination.categories.CATEGORY "ID"
  - remove current result.categories OBJ_ARR
  - on report (not persisted in stores):
     - filter out unnecessary categories from combination.categories.CATEGORY
        - i.e. any category with 0|1 id
        - exception: if all categories are unnecessary, keep task
        - reason this is not persisted: when merging results, those categories might get additional ids
     - convert combination.categories.CATEGORY "ID" to OBJ:
        - id STR
        - title STR, titlePadded STR: using CONF.titles
           - done on load. CONF.showTitles just omit those properties later
     - sort result.combinations by mean stats.median
        - do not add combination.*Rank nor categories.*.mean|rank
        - same priority order as result.categories ARR order
        - reason this is not persisted: order might change depending on CONF.select
     - set result.categories "CATEGORY"_ARR:
        - with all categories
        - sorted by: step, task, runner, system, variation
  - reporters rows|columns computation:
     - use result.categories, including ARR order
     - preserve result.combinations ARR order
     - when grouping categories (e.g. tables), filter out dimensions with no combinations, which can happen due to:
        - CONF.select
        - runnerConfig variations being runner-specific
  - getCombinationName(combination)->STR:
     - 'CATEGORY "ID",...'
     - ids, not titles
     - for: `dev`, limit errors, measure errors
     - should re-use some of the normalization happening for reporters:
        - result.categories ARR order
        - filter out unnecessary categories
     - not meant for reporters
  - CATEGORY: "task", "step", "runner", "system.{systemCategory}", "variation.{variationCategory}"
  - remove current row|column|addTitles() logic

Default ids:
  - add default ids on results load
     - for all possible categories with default ids
        - figure out possible variation categories by using all results categories
           - do not also use currently measured runners' possible configuration properties
              - because this is not available in `show|remove`, which would make CONF.select|limit behave differently between commands
     - i.e. missing category should behave like the category with its default id, including in:
        - CONF.select|limit
        - diff
        - reporting (e.g. when mixed with other results not missing the category)
     - per category:
        - task|runner: never missing
        - step: "main"
        - variation: "*_main" (see below)
        - system: "system_*" (see below)
  - when saving, remove default ids, to save space
  - validate that new result ids do not match possible default ids (unless same category):
     - "main": step
     - "*_main": variations
     - "system_*": system
  - add comments about:
     - problems with alternatives:
        - alternatives:
           - different categories -> different combinations
           - missing category match any id of that category
           - new category matches missing category, but not vice-versa (i.e. can add but not remove)
        - problems:
           - new category have multiple ids, and it is hard to know which ones the user expects to be compared to the previous missing category
           - same problem when removing a category, but in the other direction
           - when making a new result with a missing category, but previous results have that category, they should be kept in history
              - this happens for example when running locally but still want to see latest CI results

CONF.system "ID" -> CONF.system[.{systemCategory}] "ID"
  - CATEGORY is "system[.{systemCategory}]"
     - i.e. can use several categories
     - getCombinationName() should use only systemCategory
        - if systemCategory is default "main", use "system" instead
  - system id is "ID", without systemCategory
  - default systemCategory: "main"
     - unlike default id, this is persisted
  - default "ID": "system_{systemCategory}"
  - when sorting result.categories, system sorted:
     - after runner, before variation
     - by CONF.system ARR order
  - result.systems[*].categories:
     - before merging: OBJ where key is category, value is id
     - after merging:
        - OBJ_ARR_ARR: id STR, title STR, category STR
        - second ARR is when merging combinations with same shared props
     - reported as comma-separated list of space-separated lists
     - remove result.systems[*].id|title
  - merging:
     - create shared systems in decreasing number of shared categories:
        - first look for properties shared on all system categories. On any match:
           - create a shared system OBJ for it, unless either:
              - no shared properties
                 - except top-level one, at systems[0], always defined
              - another shared system has exact same properties:
                 - just append categories ids to it instead
           - remove props from the matching combinations, so they are not in two different system OBJs
        - then same for all categories minus 1|2|...
           - using all possible sets of "categories minus 1|2|..."
     - undefined values:
        - like other values when grouping
        - but cleaned after grouping
           - i.e. not printed by reporters

Variations:
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation category"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - any CONF.runnerConfig.{runnerId}.PROP
        - cartesian product only to combinations with that runner
     - add comment why not CONF.duration:
        - no reasons why users would want to measure with different CONF.duration
        - complicates implementation
  - CATEGORY is "variation.{variationCategory}"
     - i.e. each variable property is a category
     - variationCategory is property VARR (dot-delimited)
     - getCombinationName() should remove "variation." prefix from CATEGORY
  - when sorting result.categories, variations sorted:
     - at end
     - input, then concurrency, then runnerConfig
     - input: sorted by CONF.inputs OBJ order
     - runnerConfig sorted by:
        - CONF.runner ARR order
        - then config prop name, alphabetically
  - add combination.config CONF_OBJ
     - not persisted in history nor used in report
     - contains config, with combination-specific variations
  - variation id is 'ID', not 'PROP.ID'
  - default id is "PROP_main"
     - PROP is only last property
     - exception: PROP is last two properties (_-separated) when either:
        - two variable properties have same last property
        - PROP is "system" (e.g. in runnerConfig), to avoid conflict with system default ids
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR STR (commaSpace-separated list)
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - should still set combination.categories.{variationCategory} on those combinations, but with id "", title "", titlePadded " ... "
     - when filtering unnecessary categories, should exclude ones with id ""
     - should all be in result.categories "CATEGORY"_ARR
  - result.variations.{variationCategory} VAL
     - persisted in history
        - used the most recent value
     - on report:
        - set combination.categories.{variationCategory}.value VAL
           - should only be reported by reporters able to show details
        - remove result.variations

--------------------

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each iteration, shallow copy that OBJ
     - i.e. each iteration has fresh copy
     - i.e. cannot communicate with next iteration
        - reason: might accidentally get previous iteration state, especially if property is set considitionally
        - other reason: ensure proper garbage collection
        - exception: top-level state, or in properties created during before()
  - passed as `this`
     - check whether FUNC.call(context) or FUNC.bind(context) is faster
        - probably bind() since it can be done once during init
     - should document that does not work with arrow function nor bound function, i.e. must be avoided
     - try to validate against binding step functions
        - only if multiple steps
  - should be passed to step function even when single step
     - reasons:
        - stats do not vary depending on whether there is a single step or not
        - more monomorphic
  - forbid inputId named `context`, to allow other runners to pass `context` alongside named arguments
  - add comments about:
     - advantages over using a named argument:
        - cannot re-assign `this`, i.e. no need to validate it
        - simpler syntax (no need to retrieve from arguments)
        - intuitive|common pattern in JavaScript
        - clearer separation between inputs and context
     - advantages over top-level scope (which can still be used)
        - not shared between tasks
        - not shared between iterations
        - does not require declaring a variable
     - problems with alternatives to single `context` OBJ:
        - separate `context` arguments for input and output (to next step)
           - information meant for later steps must be passed between several steps
        - `context` argument for input, return for output (to next step)
           - information meant for later steps must be passed between several steps
        - `{stepId}` argument for input, return for output
           - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
           - name conflict with any core argument
              - except inputs, which are validated against duplicates with steps
           - more complex to explain:
              - return vs context
              - `stepId` argument name
              - when repeating a step, only last iteration's return value is used

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - in documentation:
     - encourage `export const TASK = { STEP() {...}, ... };`
     - as opposed to `const TASK_STEP = function() {...}; export const TASK = { STEP: TASK_STEP, ... };`
  - validate against tasks with no steps (besides before|after)
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
        - i.e. must return `tasks` `{ taskId: ['main'] }` to parent
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - processes:
     - at benchmark start, when runner communicates available tasks to parent, it should also return available steps
        - returned as `tasks: { taskId: 'stepId'_ARR, ... }`
     - all steps of a given combination use same process
  - remove beforeEach|afterEach
     - rename beforeAll|afterAll to before|after
     - add comment that runners should avoid specific case for reserved exported names, since users might use different case convention for stepIds
  - each step is a combination category
  - implementation:
     - runner:
        - on start, returns steps to parent
           - ARR in execution order
        - on measure, gets OBJ:
           - maxLoops NUM
           - steps OBJ_ARR:
              - id "stepId", scale NUM, repeat NUM
              - ARR in execution order
        - do {
            const context = { ...beforeContext }
            const args = { context }

            for (const { id, scale, repeat } of steps) {
              while (scale--) {
                startTime()
                while (repeat--) {
                  steps[id](args)
                }
                endTime()
              }
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in steps execution order
     - parent:
        - `measureDuration`, `aggregationCountdown` are for whole sample (it is already the case)
        - `maxLoops` = 100ms / sum(steps.map((step) => step.median * step.repeat * step.scale))
        - `combination.steps` OBJ_ARR: id "stepId", ...
           - for all step-wise state: measures, bufferedMeasures, stats, loops, times, repeat, calibrated
           - not for: everything related to minLoopDuration, samples
        - total `benchmarkDuration` does not vary with number of steps
           - fix preview logic (at the moment, it uses combinations.length * CONF.duration)
           - also `measureDuration` measures sum of all steps
           - add comments why:
              - new steps are more likely to be due to splitting existing steps than adding new ones
              - adding steps does not increase `measureDuration`, i.e. decrease preview responsiveness
              - with CONF.select, all steps are still run.
                It is simpler to explain this by documenting that steps never influence total benchmark duration
  - result.steps OBJ_ARR: id 'STR'
     - for all steps, even if not measured
     - ARR is sorted by step execution order
        - two tasks might run steps in different order
           - including between different results
        - i.e. use the step mean order:
           - percentage of step index within its task's steps ARR
              - 0 for first step, 1 for last step
           - take the arithmetic mean between all tasks defining that step
           - if two steps equal, use stepId alphabetical order
        - step groups:
           - right before their earliest child
           - if two step groups have same earliest child, decide using:
              - if latest child is earlier, comes first
              - otherwise, use stepId alphabetical order
  - reporting:
     - report one separate table per step
        - step title should be in top-left corner
     - sorting between tables: by result.steps ARR order
  - excluding steps with CONF.select:
     - like any other combination categories:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
        - not taken into account to decide whether to stop combinations (based on rmoe)
           - including in the preview duration estimation
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one combination for that task exist
        - i.e. parent process measuring logic ignores steps:
           - at the beginning of measuring logic, combinations with same task but different steps are grouped
           - parent process does not pass any information to runner process about steps, and runner runs them all
           - at the end of measuring logic, combinations are ungrouped to different steps
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.select
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.select already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - step groups:
     - behave like steps except:
        - specified with CONF.stepsConfig.{stepId}.group 'stepId'_ARR
           - ignored if empty ARR
           - reasons for the syntax:
              - allow non-consecutive steps
              - not verbose (unlike using stepId, e.g. using stepId common prefixes)
        - stats are based on aggregation of other steps stats
           - use other steps stats, not `measures` because the number of `measures` might differ between steps when CONF.repeat true
              - add comment that could in principle use `measures` when CONF.repeat false, but does not because:
                 - it would make stats differ between CONF.repeat true|false
                    - this is confusing and might lead some users to use CONF.repeat false
                    - using CONF.repeat true|false should only change precision, not accuracy
                 - it would give better stats for step groups, discouraging CONF.repeat true
                 - it is slower
           - how:
              - samples|minLoopDuration: any
              - mean|quantiles|median|min|max|stdev|loops|times: add
              - repeat: Math.round(loops / times)
              - histogram:
                 - among all histograms first buckets, find one with smallest frequency:
                    - create a bucket with:
                       - frequency: smallestFrequency
                       - low|high: sum of all histograms first bucket's low|high
                    - for each first bucket:
                       - if smallestFrequency === firstBucketFrequency:
                          - discard bucket, i.e. next one becomes first bucket for that histogram
                          - also if >= (due to possible rounding error)
                       - otherwise, update:
                          - frequency: subtract smallestFrequency to it
                          - low|high: kept as is
                    - repeat
                 - re-distribute buckets:
                    - so buckets width is uniform, and so there are 1000 buckets
                    - do it by summing and interpolating
           - add comment that this assumes steps measures are positively corrolated
              - for: quantiles|median|min|max, stdev, histogram
              - if not, result is a bit inaccurate, but remains precise
     - persisted in history
        - as opposed to being dynamically computed during reporting
        - reason: allows not losing history when:
           - step group change which steps it includes
           - or their names
           - or whether it is a step group or a normal step
     - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
        - reason: users might want to see children only when need details
           - and vice-versa
     - can target another step group
        - error if cycle
     - special stepId "all":
        - only allowed in stepsConfig.{stepId}.group ARR
           - when present, do not allow other values in ARR
        - select all available steps
           - including ones not measured|included
        - forbid "all" as a stepId name, either normal step or step group
  - `maxLoops` should be divided by number of steps, for the current combination, since all steps measures are in-memory at once
  - add comments about:
     - complex step order:
        - problems:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allow reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as special kinds of steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.repeat error handling, etc.

Automatic repeat:
  - `repeat` vs `scale`:
     - both passed to runner.measure()
     - both are per step (not per task)
     - repeat is inside timestamp, scale outside
         while (scale) { start = now(); while (repeat) { stepFunc() }; end = now() }
     - goal:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: fast steps should be run more often than slow steps because:
           - they are less precise, i.e. each iteration brings more value
           - they take a smaller percentage of the overall CONF.duration
  - `repeat` NUM: keep current logic as is
  - `scale` NUM
     - always passed to runner.measure()
     - value:
        - Math.round(maxStepDuration / currentStepDuration)
           - maxStepDuration = for current task, median duration of slowest step
           - currentStepDuration = median duration of current step
        - i.e. always 1 if single step
  - CONF[.tasksConfig.{taskId}].repeat BOOL
     - def: false
     - if false and task has multiple steps, then:
        - for all steps of that task
        - `repeat` and `scale` are always 1
     - if true and task has multiple steps, then:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - including: cannot both read+write same property in neither arguments nor top-level scope
           - including:
              - stateful class instances like event emitters and streams
              - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.repeat true, increase step function complexity (including increasing input size)
           - split step into its own task
  - report imprecise steps
     - only if CONF.repeat false and multiple steps
        - reason: result might be slightly imprecise due to approximation of the repeat algorithm
     - when, if repeat had been used, it would have been >1
     - set combination.imprecise BOOL
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - reasons why CONF.repeat:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.repeat and `scale`:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over CONF.duration
           - problem: does not work with CONF.duration 0|1
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

Manual mode:
  - opt-in
     - ignore all of this unless CONF.[stepsConfig.{stepId}.]manual defined for that step
     - reasons:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
  - CONF.[stepsConfig.{stepId}.]manual "UNIT"
     - if no stepId: all steps
     - i.e. same step from different tasks have same unit
        - including if single step for all tasks
  - use hardcoded list of units:
     - list:
        - duration: fs ps ns us|μs ms s m h d
           - i.e. allow custom duration
              - could be useful when task file is measuring another process, e.g. time spent on a server
        - %
        - bytes: B KB|KiB MB|MiB GB|GiB TB|TiB PB|PiB
           - also ...b (bits not bytes)
        - counts: ops
     - enum validation:
        - reasons (as opposed to allow custom counts units):
           - simpler to explain
           - no need for case insensitivity
           - no need to validate max length
     - reasons why no empty string units:
        - ambiguous as user might either intend to use it to specify CONF.unit should not be used, or should be displayed with no units
        - forces distinguishing between different units
  - repeat loop still used
     - because automatic duration still measured, for CONF.rate
     - but do not set combination.imprecise
  - pass `steps[*].manual` true to runner:
     - each measure should then be an ARR of two values:
        - automatic duration NUM
        - step return VAL
  - must return NUM from step function
     - reasons, as opposed to set `measure` argument:
        - argument could be destructured, leading to assignment not working
        - argument would be used for too many things: inputs, message passing, manual measures
        - clear that return value has this type of semantics
     - reason why NUM instead of OBJ: works for every language, including cli runner
  - parent validates NUM:
     - for:
        - all tasks of a given step
        - all measures of a given combination
     - allow:
        - 0
        - floats
     - do not allow:
        - negative floats
        - not numbers
        - NaN and Infinity
        - undefined
  - combination.stats:
     - used for manual measures NUM
     - automatic durations are still:
        - measured (for CONF.rate) in combination.durationStats
        - used for calibration: maxLoops, scale
  - persisted at result.steps[*].manual "UNIT"
  - re-use existing unit-specific logic for:
     - automatic scaling
        - e.g. 'ns' -> 's' or 'B' -> 'MB'
     - significant digits|decimals
  - reporting sorting:
     - duration, %: asc
     - bytes, count: desc
     - do not allow configuring|overridding for the moment, to keep things simple, because most users won't need it
  - when merging combinations from different results with same stepId but different unit:
     - if same unit "kind" (duration, %, bytes|bits, count): allow comparing by normalizing stats during mergeResults():
        - find the lowest scale among all units, then multiply to it
        - if mixed manual + auto durations, turn all to manual durations
           - i.e. copy combination.stats to combination.durationStats
           - only if manual unit is duration
        - reason: not losing history when:
           - changing unit scale
           - switching from auto to manual duration
     - if different unit "kind":
        - only keep most recent unit, filtering out previous combinations with different unit kind
        - i.e. units are not a combination category

Rate:
  - CONF.[stepsConfig.{stepId}.]rate BOOL
     - def: false
  - reporting-only
     - not persisted in history
     - reporting flag
  - changes the reported value:
     - duration: 1/medianDuration, i.e. times per duration
     - %, bytes, count: value/medianDuration, i.e. scales the left side
  - sorting order:
     - duration: inverted
     - %, bytes, count: kept
  - reported unit:
     - duration, count: "ops/TIME_UNIT"
     - %, bytes: "UNIT/TIME_UNIT"
  - automatic scaling
     - duration, %, count: focused on TIME_UNIT
     - bytes: focused on UNIT, leaving TIME_UNIT as "s"

--------------------

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `dev` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - median measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for beforeEach, main and afterEach
  - always use await on beforeAll|afterAll, i.e. allow both sync and async
  - remove task.async BOOL

--------------------

`spyd` history branch:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - directory is "{gitRoot}/history/results/"
        - git root lookup is using same logic as other places which look for it, i.e. CONF.cwd
     - automatic commits should be prefixed with `[skip ci]`
     - individual results:
        - at /history/results/YYYY-MM-DD--HH-MM-SS--BRANCH--{result.id}.json
           - encode|decode characters in BRANCH:
              - [[:alnum:]-_] left as is
              - any others like percent encoding but using x instead of %
                 - including . and /
                 - including x if followed by two [0-9a-f]
                 - a-f lowercase only
              - extract to own library
        - one immutable FILE.json per result
           - i.e. single OBJ
        - format is JSON
           - reason: fast
  - add comments:
     - pros of using a separate git branch:
        - instead of:
           - using regular files in codebase
           - git hash-object + git cat-file
        - does not pollute git log
           - especially in CI where a single commit might have many CI jobs and results
        - does not require git push --force on the codebase
        - automatically handle committing
        - semi-automatically handle git push|pull
           - can update in CI without requiring developers to pull all the time
        - allows multiple files
        - easier to make it skip CI
        - does not create many tags
        - ensures all branches can always be used for branches comparison with CONF.since
        - easy to understand
        - no risk of pruning
        - versioned
     - only store history with git at the moment
        - however possible approach in the future, if there is a need for it: CONF.history STR:
           - "git"
           - "PATH"
           - CRUD stores plugins
     - why concatenated results are not cached:
        - simpler, i.e. no need to check the file, nor update it after each `spyd bench --save`, `spyd sync`, etc.
        - performance benefits not big enough since results are partially loaded in batches
        - manual edits of files would require some way to invalidate cache, e.g. a `spyd prune` command
        - takes space on the cache directory, i.e. might require automatic|manual pruning
     - why using individual results instead of single file for all results:
        - fast to create new results
        - does not create git conflicts
        - concurrent safe
        - small file size impact in git history
        - easier to edit
        - allow loading only few results instead of all

Require git:
  - only for:
     - CONF.save
     - `sync` command
     - CONF.since branch|commit|tag|"parent"
        - i.e. optional for CONF.since resultId
  - require:
     - `git` binary is executable (i.e. `git --version` has exit code 0)
     - minimum version of `git`
     - there is a `.git` in `[.../]{CONF.cwd}`
  - add comments:
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository

Loading results in batches:
  - done batch by batch, with size incrementing exponentially (2**n)
     - instead of all at once
     - in order, using timestamp in result's filename
  - performed iteratively until can determine no more results needed, in three stages:
     - target delta can be resolved
     - since delta can be resolved
     - beforeSinceResults can be results
        - sinceResult + beforeSinceResults and sinceResult + afterSinceResults contain same combinations (after `select` filtering)
        - i.e. sinceResult merging would be same even if more previous results were loaded
  - ensure that delta resolution:
     - apply `select` filtering first
        - i.e. for each batch, files must be JSON loaded
        - including delta formats: NUM, git branch
     - only use results from correct branch:
        - target delta: current branch
        - sinceResult's branch (if git reference of result id) or current branch (otherwise)
     - for since delta: only use results earlier than target result
  - figuring out if delta is resolved
     - by format:
        - NUM -> >= NUM results
        - "first" -> right away with all results
        - id|timestamp|duration -> right away with result index
        - git branch -> latest result in that branch
           - i.e. use BRANCH in filename but not in `systems[0].git.branch`
        - git commit|tag:
           - find `git` commit author date then use it like in timestamp delta format
           - i.e. does not use `systems[0].git.tag|commit`

`spyd sync` command:
  - on `spyd` git branch:
     - git pull --rebase
        - if merge conflict:
           - try to automatically solve by including additions from both branches
              - add comment that this might happen with history/renamed_branches.yml
              - then retry `git pull --rebase`
           - otherwise fail
     - then git push
        - not run if we know locally there is nothing to push
  - stdin|stdout|stderr "inherit" on git pull|push
     - reasons:
        - allow entering passwords
        - show any error message such as: authentication, git hooks, network, etc.
        - provide with progress
     - not on other git commands
     - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
     - only if CONF.quiet false
  - add comments:
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull

Multiple branches reporting:
  - only report one branch + targetResult
     - branch is:
        - if CONF.since is a git reference or a result id, use its branch
           - branch is resolved:
              - git branch: use it
              - git tag|commit: use the same git logic used to get the current branch, but with a git tag|commit
              - result id: get branch from the result's filename
        - otherwise, current branch
           - instead of using env-ci, should use the best library to guess the branch from a specific commit reference
              - ensure cwd can be specified
              - could also just inline the git calls made by the library
           - if no branch found, use the `undefined` branch
     - filter out any results from the branch later than targetResult, before applying sinceDelta
        - using timestamp + branch in result's filename
        - reasons:
           - ensure benchmark's history remains same as new results are being added
           - ensure history is sorted by timestamp, including the target result
           - parent branch's newer results are likely to contain performance improvements that were not merged in the current branch yet when the result was taken
     - all results without a git branch are treated as if part of a single `undefined` branch
     - result's branch is persisted when the benchmark is saved
  - CONF.since "parent":
     - same as CONF.since "branch" with parent branch
        - i.e. last commit of parent branch, since this is the one the child branch will merge to
     - if no parent branch, like "first" in current branch
  - add comments:
     - problems:
        - hard to know which is parent's branch parent commit using only the benchmarks data:
           - need to use commit ids, which might have changed since benchmark was run due to rebasing
           - user intent is ambiguous: even if branch has not been rebased to parent yet, might intend to do so
        - parent branch's benchmarks' timestamps might interleave with current branch's
        - current branch might have been rebased 0|1|n times onto parent branch
           - parent and current branch's performance improvements are mixed and hard to distinguish
        - grouping results per branch and showing several branches leads to:
           - more visually complex time series
           - harder to understand history since each branch has its own progression
        - should be easy to understand|explain
        - should preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - should not be impacted by rebasing
        - allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
     - discarded solutions:
        - report all results without taking branches into account
        - report all results, grouped by branches
        - report all results of parent branches, grouped by branches
           - variant: only show earlier results
        - let user decide with a CONF.branch configuration property

Renaming branches:
  - on results load, should also get a list of branch renames:
     - do it using the current git reflog
  - use the list of branch renames:
     - to normalize all results branches to their latest names, including when:
        - resolving CONF.since's branch
        - reporting
     - the result's filename and contents also contains the branch name but this name might be old, which is ok since it is fixed by branch renames
  - persist branch renames:
     - reason: git reflog lasts only for 90 days by default
     - done during CONF.save
        - reason: otherwise, user might not expect having to run `spyd sync` again
     - when saving, should automatically create its own git commit
        - using same logic as git commits for results saving (including `[skip ci]`), but as a separate commit
           - reason: cleaner, and limit impact of merge conflicts
     - only append new branch renames, not ones already persisted
        - reason for append-only: limit potential for merge conflicts
     - only add renames for branches used in at least one result
     - persisted to history/renamed_branched.yml
        - OBJ_ARR: from 'BRANCH', to 'BRANCH2'
        - reason for YAML array: limit potential for merge conflicts
     - always loaded and merged with current git reflog, with lower priority

--------------------

Find ways to improve precision even more???

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information

CONF.debug BOOL
  - meant as bug report attachment, not meant for users to debug themselves
  - saved debug information to "{process.cwd()}/spyd_debug_logs.yml"
     - not printed to stdout
     - no way to configure location
     - saved at end once, using try/finally wrapping the programmatic entry points
        - not streamed, so it does not impact benchmark
  - does not change the logic otherwise
     - including reporting and previews
  - file is YAML:
     - using document stream
     - of OBJ:
        - event "EVENT"
        - event-specific properties
  - include:
     - envinfo
     - resolved config
     - task files
     - runner.versions
     - combinations
     - samples
        - including: duration spent, estimated time left, progress bar percentage
  - for all commands
  - interface is debugLog(debug, "EVENT", EVENT_OBJ)
     - debug is undefined if CONF.debug false, mutable ARR otherwise
  - add to GitHub issue templates

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

Learn package 'simple-statistics' and|or 'jstat' and use it in spyd, where needed

--------------------

Add TypeScript support:
  - to:
     - spyd.ts
     - tasks.ts
  - export types of those too

Add formats:
  - report.reportMarkdown()->PROMISE_STR
     - for CONF.output "*.md|markdown|mdown|mkd|mkdn" or "README|readme"
     - def: reportTerminal() return value in ``` block
     - add default value for reportTerminal(): using markdown return as is
     - concatenation of reporters with same CONF.output: newline joined
     - footer: appended as Markdown
  - report.reportHtml()->PROMISE_STR
     - for CONF.output "*.html|htm"
     - file should be whole (i.e. have <html> tag)
     - reporter can save other files (imported by the main file) in the same directory or subdirectory
     - def: reportMarkdown() return value rendered to HTML
     - footer: inject to any element with id "spyd-footer"
  - report.reportSvg()->PROMISE_STR
     - for CONF.output "*.svg|png|jpg|jpeg|webp"
     - def: reportHtml() return value rendered to SVG
     - converts SVG to PNG|JPEG|WebP
     - footer: added with svg manipulation
  - error if same CONF.output and format external|html|svg (unlike terminal|markdown)
     - reasons:
        - hard to concatenate
        - can emulate concatenation using a parent HTML file

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
        - abscissa:
           - only show start|end
           - format should be date-only if different days, time-only otherwise
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - stacked bar graph for multiple stages benchmarks
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

GitHub action

GitHub PRs integration

Add other runners:
  - HTTP
  - JSON-RPC
  - graphQL
  - gRPC
  - WebSocket
  - TCP
  - chrome (maybe using puppetter)
  - firefox (maybe using puppetter-firefox)
  - selenium
  - bash
  - go

SaaS:
  - reporting dashboard
  - files:
     - either connect to GitHub repository
     - or edit file with online IDE, saved in the server
  - perform benchmark in-browser
  - public URLs, for sharing

--------------------

Learn the whole Terminal section in edl
  - including finishing ANSI sequences and terminal emulators
  - use cli table library
     - ensure this is still responsive (by creating multiple table if too wide)
        - if no library does this, create own library on top of another

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Terminal-histogram:
  - separate to own repository
  - add features:
     - color themable (using terminal-theme)
     - left bar with percentages
     - can specify number of abscissa ticks
        - or number of columns per tick
        - stack labels, i.e. might need to stack deeper than one level
    - allow minimum ordinate to be either 0 or minimum value
    - labelling columns
    - custom unit for ordinate
    - when too many columns:
       - if labeled: break into several histograms
       - otherwise: extrapolate

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

--------------------

Manually try all features with each Node.js version

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Positioning (in documentation), in priority order:
  - whole benchmarking flow, not just measuring:
     - reporting (pretty, configurable, live preview, multiple formats, insertion, hardware/software info)
     - combinations (variations, inputs, systems, steps, selection)
     - comparing
     - history (including time series)
     - debugging (spyd dev, nice error messages)
     - sharing for others to run
     - testing (limits)
     - concurrency benchmarking
     - CI
     - GitHub PRs
  - simplicity
     - no library|APIs, only export functions
     - no need to specify duration nor number of loops
     - can work with no|minimal configuration, including on-the-fly
  - precision
     - high precision
     - configurable precision
     - report statistical significance (including for diffs)
     - not only average, also: min|max, stdev, histogram, quantiles
  - platform|language-agnostic
     - including CLI, Node.js, TypeScript
     - including comparing Node.js versions

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both medians (for accuracy) and stdev (for precision)

--------------------

Add repo of spyd benchmarks:
  - contributors can add any
  - only for:
     - JavaScript
     - core Node.js or JavaScript, no modules
  - each benchmark is a directory with a single benchmark
     - optional spys.yml
  - README shows all results
     - as run in CI
     - each shows: title, tasks file content, reported result
     - a hardcoded list is maintained for sorting
     - created by a build task
  - binary for users to run any of the benchmarks on their machine
     - including with npx

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2020.stateofjs.com/en-US/resources/
  - https://javascriptkicks.com/submit

Ideas for articles about benchmarking:
  - choice between precision and accuracy
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes

------------------------
