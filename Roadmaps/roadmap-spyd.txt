
        
   SPYD  
        



TypeScript typing:
  - plugins:
     - `options` as:
        - second argument to subclass()
        - second argument to error constructors
     - plugin.staticMethods
        - merged to AnyError.*
           - excluding first argument
           - not to ErrorClass.*
     - plugin.instanceMethods
        - merged to error.*
           - excluding first argument
     - plugin.isOptions()
     - plugin.properties()
     - type each current plugin
        - including tests
     - `options` as last argument:
        - only if plugin.isOptions() is true
        - to:
           - plugin.staticMethods
           - plugin.instanceMethods
     - `errors` instance option
     - bind `options`:
        - to:
           - staticMethods
           - instanceMethods
           - plugin.properties()
        - after plugin.getOptions() applied
        - after globalOpts > parentClassOpts > classOpts > instanceOpts > methodsOpts merging applied
        - error.cause options are merged to parent
     - bind `error` instance to:
        - staticMethods
        - plugin.properties()
     - `props` is a core plugin
     - plugin.properties() can modify `error.*` by returning a value
        - `props`: re-use set-error-props TypeScript types to type the resulting error instance
           - including deep merging
  - merge-error-cause:
     - merge `cause` in ErrorClass constructors return value, like what is currently done with AnyError
     - make type recursive
  - allow more than 2 args in types of `custom` constructor
  - move `CustomError` type back to `error-custom-class`, and import it from `modern-errors`
  - add "Strict TypeScript types" to Features
  - add a TypeScript section at the end with an example of TypeScript usage:
     - importing AnyError type and assigning to caught error
     - type narrowing with instanceof
     - calling a function typed with a SpecificError type
     - type error based on some props for that error type
  - document typing plugins in the plugins creation doc
  - go through whole TypeScript doc

Plugins:
  - put each plugin in own repository:
     - not modern-errors-props: it is a core plugin
     - move each plugin's documentation to the plugin's repo
        - they should each include a short explanation on how to add plugin, and configure it
     - modern-errors-process's Install section should include warnings similar to log-process-errors
     - advertising modern-errors plugins:
       - in plugins: in "Other plugins" section
       - in other error-related projects: in "Related projects" section, as its own list mentioning the project is part of modern-errors, with list of plugins
  - create modern-errors-example, as a template repository

Merge "next" git branch, then release

Update in spyd:
  - use AnyError.parse()
     - use a second step to convert error classes, using new ({ OneError: TwoError, ... })[cause.name]("", { cause })
  - use AnyError.exit()
  - use OPTS.bugs() FUNC with plugins framework
     - test all errors, including:
        - invalid `plugin.bugs` shape
        - options.bugs
        - plugin.bugs

Add emoji to README:
  - like handle-cli-error and wild-wild-path
  - on:
     - Features section
     - Usage section main chapters, not sub chapters
        - fix Markdown anchors accordingly
  - ensure it displays correctly on different browsers

Universal JS:
  - enable "node:" linting rule for module names
  - ensure all my package.json have both `main` and `exports` fields, for backward compatiblity with many tools
  - figure out browsers minimum versions support
  - JavaScript new features
     - check which ones might be an issue
     - fix Babel so it covers them
  - for each project that is universal:
     - check each:
        - chromium-browser --disable-web-security --user-data-dir=chromtemp
        - load in console
     - add tags related to web, browsers, etc.
     - remove Node badge???
     - add badge:
        - left: logos of Node, browsers
        - right: universal
     - in Install section, specify versions
     - make a release

Separate error utilities to own packages
  - do not forget npm keywords
  - add each other as Related projects
  - promote modern-errors
  - then, write article about NUM common mistakes with error handling, based on what those packages solve

awesome-errors
  - JavaScript only

Document:
  - precise-now
     - add keywords
     - promote to https://github.com/parro-it/awesome-micro-npm-packages
  - time-resolution
     - add keywords

You-might-not-need-callbacks
  - similar repo as cross-platform-node-guid
  - sections:
     - benefits
        - simpler code
        - async stack trace
        - error propagation
        - easier parallelism
     - tips
        - including:
           - must use `return await`:
              - slower
              - but needed if in try|catch
              - and add stack trace line
     - examples
        - each has:
           - [redCross] Callbacks
           - [greenCheck] Async/await
     - list where callbacks are still needed
        - including:
           - user must provide some logic, not only data
     - tools
        - including linting

Config normalization:
  - `path` keyword:
     - allow 'file:' STR|URI (converted to normal STR path)
     - add keyword 'follow' to resolve symlinks
  - keywords:
     - plugin.before|after "pluginId"_ARR
        - used to specify order
           - also use OPTS.keywords ARR as a second order
        - build a DAG and traverse it
           - consumer error on cycles
           - check DAG logic in 080219d991c5e586e56df88d1412aae118c9277e
        - if plugins not loaded, ignored
           - i.e. does not mean requiredness
     - make some definition keywords not builtin: schema, glob
     - potential other keywords: specific validation libraries like Joi
  - unknown props:
     - unknown if both:
        - property itself does not match any rules
        - at least one sibling matches at least one rule
           - only direct parent, i.e. deep properties are not unknown unless they have a direct parent
           - i.e. user can opt out by adding a sibling dummy rule with rule.name ending with *
     - sibling check:
        - if a rule exists that matches property's parent after removing rule.name's last key
        - regardless of token type of any keys
           - including:
              - NUM and slices: e.g. for tuples
              - *: for dictionaries and ARRs
              - REGEXP
     - **
        - when checking property itself:
           - exclude rules which end with **
              - reason: catch-all "**" would disable unknown check
           - do not exclude rules with ** not at end
              - reason: "**.PROP" should mark any PROP as known
        - when listing siblings:
           - to know if there are any siblings
              - exclude if parent has any **
                 - reason: matches 0-n depth levels, i.e. does not work well here
                 - reason: rule "**.VARR" is meant for optional|additional props, not full shape definition
              - exclude if sibling key **
                 - reason: rule "**.PROP" should not mean that any top-level property is allowed
           - to list siblings
              - exclude if sibling key **
                 - reason: cannot name it
     - break down rules into each of their parent
        - e.g. 'a.b.c' -> ['', 'a', 'a.b', 'a.b.c']
        - used when matching both the property itself, and any possible sibling
        - reason: when 'a.b' is defined, it also implicitely defines 'a'
     - checked before any rules is applied
        - i.e. ignore transformations: default, transform, rename
     - ignore rules with rule.compute() defined, i.e.:
        - throw if specified in input
        - not shown in error message (list of possible props)
     - if OPTS.strict false (def):
        - unknown properties
           - do not throw
           - and are kept in return value
        - spyd should try to use strict true, where possible
        - reason default is false: logic avoids doing any action not explicitely configured through rule's keyword or option
     - show suggestions:
        - listing:
           - sibling rules' last key
           - excluding * or **
           - but including REGEXPs and slices
        - printing:
           - if one is similar enough, only print this one as "Did you mean"
           - otherwise, list all sorted alphabetically
     - possible implementation:
         - list of rules:
            - exclude ones with rule.compute
            - only get rule.name
               - after queryArrays normalization
               - split unions
            - break down rule.name into itself + parent + ... + root query
            - remove duplicates
            - turn into tree
               - root query on top
         - recurse over data
            - start at top level, with top query
            - get query's children
               - if none, skip this data prop
            - for each data prop:
               - find matching query's children
               - if none, error
                  - all query's children: suggestion
               - recurse, using only that data prop and its matching queries
                  - should keep track of each query's parent, for suggestions
  - consider whether using normalizeConfig() for runner task files is simpler
     - add comments that:
        - runners do not have to do so and can use own logic
        - reason: it runs in runner process, which might be different language with some data types non-serializable (e.g functions)
  - CLI flag parsing logic
     - uses same PROPs as normalizeLogic()
     - does not call normalizeLogic()
        - reason: separate CLI and programmatic logic
     - call flags parsing logic too
     - PROP.description STR:
        - shown after example if validation error
        - used in CLI --help output
        - if prop uses OBJ_ARR, default to previous OBJ's `description` (like PROP.example)
  - compilation step
     - compile(rules[, opts])->FUNC(inputs[, opts])
     - computes and memoizes any logic that can be without inputs, including:
        - calling each keyword.normalize() with non-FUNCs definitions
        - validation and normalization of keywords and rules
     - opts might be defined both at compile-time and run-time
        - shallow merged
        - opts.all at runtime:
           - will create new keywords and different rules ARR instance
           - compile logic must take into account, i.e. rely on memoization
     - to call once at load time, for:
        - performance
        - reliability: throw as soon as possible
     - no shortcut FUNC(inputs, rules, opts):
        - for simplicity|monomorphism
        - to encourage best practice
     - plugins logic should have similar feature, i.e. allow performing some logic at compile-time
  - do separate sync and async entry points
     - including compile()
     - see async helper from 667f207d3e4f8082d4f94cd4255fd29c9da67420
     - however, might consider code duplication with separate sync|async functions because:
        - might be simpler
        - better stack trace
     - internally, re-use existing `sync` boolean already implemented, ensuring:
        - keyword methods and definition functions called sync|async
        - throw if sync used and some of those functions are async
  - ensure normalizeConfig() is now fast:
     - it was previously quite slow inside spyd
  - way to use TypeScript types for validation:
     - e.g. PROP.types VAL, with VAL being a JavaScript representation of TypeScript types from another library
        - e.g. a library using a build-time step to do so
     - still allow JSON schemas as well
  - abstract to own library

Config loading, including shared configs, should be own library

--------------------

Variations:
  - git branch "variations"
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation dimension"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.inputs.{inputId}
     - CONF.runnerConfig.{runnerId}.PROP
  - DIMENSION is "variation.{variationDimension}"
     - i.e. each variable property is a dimension
     - add combination.dimensions['variation.{variationDimension}']
     - variationDimension is property VARR (dot-delimited)
     - getCombinationName() should remove "variation." prefix from DIMENSION
  - variation id is 'ID', not 'PROP.ID'
  - default id: main_{variationDimension.replaceAll(".", "_")}
     - e.g. "main_inputs_{inputId}"
     - defaultIdPrefix 'main_'
  - configuration normalization|validation should apply to each VAL
     - including yargs flags parsing
     - use same logic as combination-specific config
     - forbid empty OBJs as value
     - forbid "default" as variationId
        - reason: avoid ambiguity with config selectors
  - when sorting result.combinations per dimension, variations sorted:
     - at end
     - alphabetically:
        - propName
        - including recursively: inputs.*, runnerConfig.*, runnerConfig.*.*
  - use combination.config CONF_OBJ, like combination-specific config
  - compute one versions OBJ per runnerId -> per runnerId + runnerVariationIds
     - including when deduping version commands
     - including when [de]compressing rawResults
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - still set the dimension but with only one combination|id
     - done in listVariations()
     - use default id
        - i.e. "main_runnerConfig_{runnerId}_{propName}"
        - reasons:
           - keep history continuity with previous combinations of that runner
           - unlike fixed tokens like "" or undefined:
              - selectable
              - does not lead to duplicate ids between dimensions
  - some variations must be cartesianed with `tasks` (used during `init` stage), not `combinations` (after `init` stage)
     - noDimensions should still work
     - figure out which ones???
        - runnerConfig.*: allows validating each
  - add comments:
     - only works for configuration properties which can be combination-specific
     - why we only persist variation ids, not values
        - value might change but user intend the id to be constant, which is more relevant
        - value might be big to store, or have secrets
        - value might be not nice to report (e.g. big objects)
  - use in fast-cartsian

Omitted dimensions:
  - omitted dimensions should be added to footer
     - as metadata
  - merged like result.id|timestamp, i.e. always top-level in footer
     - before them in order
     - one line per dimension
  - values:
     - id, with CONF.titles applied (like in reporters)
     - even for runner, i.e. runner's package version not shown
  - keys:
     - dimension name: "task", "runner", "step", "system.{name}", "variation.{prop}"
     - capitalized
     - if another top-level property has same key, namespace by appending " (dimension)"
  - use cases:
     - when showing a previous result, the user probably does not remember the omitted combinations ids
     - showing which combinations ids got included with `select` on a new result, even if omitted

--------------------

Formats:
  - Lazy load formats
  - FORMAT.concat -> FORMAT.separator:
     - line separator STR instead of being BOOL
     - join() should only happen when FORMAT.separator is defined
     - FORMAT.footer should re-use that separator
  - add FORMAT.ansiColors BOOL: if false, PROP.colors is noop
  - add FORMAT.wrap BOOL: if false, no wrapping of contents

FORMAT.sanitize(STR)->STR
  - applied on:
     - CONF.titles
     - footer keys and values
        - deeply
        - before footerString is serialized
  - performance optimization:
     - applied only once per reporter, for all previews and for all history results + main result
  - formats:
     - terminal and markdown:
        - replace /\s+/gu by " "
        - trim whitespaces at start|end
        - remove ANSI sequences
     - external and programmatic: none
     - html: none needed since reporters should use textContent instead of innerHTML
  - goal: normalizing contents generated by either users or plugins (not system):
     - ensure correct formatting, e.g.:
        - single lines for each field in terminal
        - vertical alignment, i.e. no leading whitespaces
        - no ANSI sequences in terminal
     - security: avoid injection

--------------------

Task argument:
  - should be single argument OBJ: context OBJ2, inputs OBJ3
  - add comment about reasons:
     - clear distinction between context and inputs:
        - readonly vs read-write
        - set in config file vs in tasks file
        - combination-scoped vs iteration-scoped
     - avoids `this` since it:
        - does not work on tasks using arrow or bound functions
        - requires nested functions to use arrow or bound functions
     - extensible to more properties for future features
     - allows any type (unlike environment variables)
     - test-friendly and linting-friendly (unlike global variables)

Inputs immutability:
  - prevent mutations:
     - for:
        - arg.inputs = value
        - arg.inputs.inputId = value
     - reason:
        - could lead to iterations to influence each other by using shared state
           - encourage users to use context instead
           - although users can still bypass it by using top-level state or by setting context in before()
        - simpler for users to reason about inputs as constants
  - use a Proxy instead of Object.freeze()
     - show a nice error message
     - check performance impact
  - allow mutating arg.context[.VARR] = value
     - next iteration should see the mutation, i.e. whole `arg` object should be passed

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each main loop iteration, shallow clone context from before()
     - potential performance optimization: when this is an empty OBJ, create a new inline empty OBJ instead of shallow copying an existing one
        - empty OBJ condition should only happen once
        - see if worth it performance-wise
     - add comment: context properties set in before() are shared by all iterations
        - i.e. any mutation is kept in next iteration
        - this can be used as global changing state, e.g. incrementing id
           - however users should avoid when possible to keep iterations independent from each other
  - do not shallow nor deep clone context before each scale|repeat iteration
     - reasons:
        - deep cloning is not possible on some JavaScript types, e.g. classes
        - shallow cloning would mean top properties would be new on each iteration, but not deep ones
           - this might confuse users
           - for example, renaming context.var++ to context.deep.var++ would change semantics
        - cloning has a performance impact
     - add comment with CONF.repeat true that users should consider that context might have been set by previous iteration of current step
  - passed to step function even when single step
     - reasons:
        - stats do not vary depending on whether there is a single step or not
        - more monomorphic
  - forbid inputId named `context`, to allow other runners to pass `context` alongside named arguments
  - add comments about:
     - advantages over top-level scope (which can still be used)
        - not shared between tasks
        - not shared between iterations
        - does not require declaring a variable
     - problems with alternatives to single `context` OBJ:
        - separate `context` arguments for input and output (to next step)
           - information meant for later steps must be passed between several steps
        - `context` argument for input, return for output (to next step)
           - information meant for later steps must be passed between several steps
        - `{stepId}` argument for input, return for output
           - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
           - name conflict with any core argument
              - except inputs, which are validated against duplicates with steps
           - more complex to explain:
              - return vs context
              - `stepId` argument name
              - when repeating a step, only last iteration's return value is used

--------------------

before|after special steps:
  - on runner start:
     - runner does not know that some steps are special, i.e. return all to parent as tasks.taskId "stepId"_ARR
     - parent handles "before|after" as special steps:
        - separated from steps, i.e. tasks OBJ: id "taskId", steps "stepId"_ARR, hooks "hookId"_ARR
  - runner.hook()
     - runner.before|after() -> runner.hook()
     - parent only sends "hook" event if child defined the hook
     - event payload: step "hookId"
        - using hookId retrieved during start
     - unlike runner.measure():
        - run step function only once
        - any context change is persisted for future runner.hook|measure()

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - in documentation:
     - encourage `export const TASK = { STEP() {...}, ... };`
     - as opposed to `const TASK_STEP = function() {...}; export const TASK = { STEP: TASK_STEP, ... };`
  - validate against tasks with no steps (besides before|after)
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
        - i.e. must return `tasks` `{ taskId: ['main'] }` to parent
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - dimension:
     - always present, since runners enforce default id
     - fix dimension logic to allow dimension to have a default id, but no prefix nor dynamic|multiple possible dimensions
     - default id logic: validate that dimensions other than steps do not use "main" as id
  - before|after special steps:
     - remove beforeEach|afterEach
        - rename beforeAll|afterAll to before|after
     - alongside other steps but not considered a step:
        - run only once
        - not reported
        - not a dimension
        - cannot be excluded by CONF.select
        - no impact on CONF.repeat
        - end() is run even on error or cancel
     - can use `context`:
        - in before:
           - context is empty
           - context mutations are passed to any other steps
        - in after:
           - receives only the context mutations from `before`, not individual steps
     - validate that before|after is first|last in steps OBJ order
  - each non-special step is a combination dimension
     - add combination.dimensions.step
     - when sorting combinations: sorted first, before tasks
  - implementation:
     - runner:
        - on start:
           - return all `tasks: { taskId: 'stepId'_ARR, ... }`
              - ARR in execution order
        - on measure, gets OBJ:
           - maxLoops NUM
           - series OBJ_ARR:
              - id "stepId", repeat NUM
              - ARR in execution order
        - do {
            const arg = { context: { ...beforeContext }, inputs }
            for (const { id, repeat } of series) {
              const step= steps[id]
              startTime()
              while (repeat--) {
                step(arg)
              }
              endTime()
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in `series` execution order
     - all steps of a given combination use same process
     - measureEachCombination() [un]groups combinations with same dimensions except stepId into series, and iterates over those instead
        - rename "combination" to "series" for any group of steps with same other dimensions
     - getSampleState() and getStats() are iterated over sampleState.series[*]
     - sampleState:
        - measureDuration|totalDuration|sampleDurationMean: not-step-wise
        - allSamples: not-step-wise
        - all other properties: step-wise, at sampleState.series OBJ_ARR including: step 'stepId', repeat, calibration, stats, sampleMeasures|sampleMedian|sampleLoops
     - sample payload:
        - payload.series[*].repeat NUM: sampleState.repeat
        - payload.maxLoops NUM:
           - not step-wise
           - if first sample (measureDuration === 0): 1
           - otherwise: keep current logic except include steps in `repeatGrowth` computation
              - renamed to `growth`
              - sumEachStep(repeatGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - sampleMedianWeight = sampleMedian / sampleMediansSum
                    - sampleMediansSum = sum of each step sampleMedian
                    - if sampleMedianSum 0: sampleMedianWeight is 1 for each step
                    - if sampleMedianSum not 0 but sampleMedian 0: sampleMedianWeight is 0 for that step
              - maxMeasures threshold: divide (MAX_MEASURES - measures.length) by series.length
     - previewStats:
        - combinationEnd: keep most logic except:
           - compute samplesLeft for each step, and only use the Math.max()
        - skipping logic (uncalibrated or mean === 0) should be step-wise
           - preview should be skipped only if all steps are skipped
     - tweak isRemainingCombination logic for multiple steps:
        - stopped, `dev` stage: keep as is (not-step-wise)
        - mean === 0: series.some()
        - maxMeasures: use sum of all samplesState.step[*].measures.length
        - loops === 0 (CONF.precision 0): series.some()
        - rmoe: series.some()
     - minLoopDuration:
        - each step has its own minLoopDuration
        - multiply TARGET_DURATION by series.length
        - maxMeasures should use sum of series[*].measures.length
        - hasEnoughMeasures(): use series.every()
        - run steps normally otherwise, i.e. the runner executes them serially but with `repeat: 0`
     - truncateLogs: not-step-wise
     - dev command: run all steps at once, without headers in-between
  - separate steps in reporters:
     - reason: they might have different units or CONF.rate (i.e. min|max)
     - for:
        - tables (debug|history): each step should have its own table
        - boxplot: each step should have its own min|max
  - steps excluded by CONF.select:
     - like any other combination dimensions:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
        - not taken into account in:
           - isRemainingCombinationLogic logic
           - preview logic, including duration estimation
           - minLoopDuration
           - payload computation
              - except repeat always 1
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one step for that task is selected
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.select
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.select already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - error handling:
     - keep current logic i.e. exceptions are propagated
        - in:
           - `before` -> do not call later steps nor `after`
           - any other steps -> do not call later steps, but call `after`
              - exceptions in `after` itself are ignored
           - `after` -> nothing else to call
        - reasons:
           - ensures `after` does cleanup, but only if `before` completed
           - but assumes that exception leaves bad state, i.e. should not run additional steps, and `after` might fail
     - child should pass an optional stepId alongside error string, so that parent can print it
  - add comments about:
     - complex step order:
        - limitations of current solution:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allows reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as regular steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.repeat error handling, etc.
     - syntax alternatives for before|after not as good:
        - top-level before|after{taskId} function:
           - requires either picking a specific case or allowing multiple
           - top-level like task functions, although different, creating confusion
           - requires validating that it is not defined twice in multiple task files
           - less clear that context is shared with steps
        - before|after[.{taskId}], i.e. before|after is either a function or an object:
           - use same OBJ syntax as steps but with tasks instead, making before|after > task > step hierarchy unclear
     - why sampleTargetDuration does not increase with number of steps:
        - new steps are more likely to be due to splitting existing steps than adding new ones
        - keep preview responsive
     - we do not store steps execution order because not used for the moment

"each" special task
  - runner event payloads:
     - "start" event payload: never pass taskId
     - "hook" event payload: add task "taskId"
     - "measure" event payload: series[*].id "stepId" -> series[*].task "taskId", series[*].step "stepId"
  - "each" special task:
     - handled only by parent process
        - runner process does not know about special tasks
     - after runner start, parent adds each stepId from "each" task to other tasks' stepIds|hookIds
        - then remove "each" taskId
        - including during init
     - the taskId "each" is kept even when merged to another task, so it can be passed to "hook|measure" event payload
  - merging:
     - "each" task's steps can be both "before|after" and normal steps
     - lower priority, i.e. can be overridden by individual tasks
        - for both "before|after" and normal steps
     - prepended, i.e. first in steps order of each task, unless either:
        - overridden by task
        - stepId starts with "after", case-insensitively
     - only applies to tasks inside same file
        - reasons:
           - easier to reason about
           - allows scoping to a group of tasks
           - no possibility of redefining it in multiple files
           - required since different task files have different processes
  - add comments:
     - example use cases for regular steps in "all":
        - create temporary directory when all tasks write to filesystem
        - create temporary REST entity when all tasks modify REST entities
        - create temporary data when all tasks modify arguments
           - e.g. sorting arrays
     - reasons why shared steps are nested under "each" instead of being top-level:
        - keep top-level for tasks and nested level for steps
        - extensible if more properties were to be added to tasks

Step groups:
  - behave like steps except:
     - specified with CONF.groups.{stepId} 'stepId'_ARR
        - ignored if empty ARR
        - reasons for the syntax:
           - allow non-consecutive steps
           - not verbose (unlike using stepId, e.g. using stepId common prefixes)
     - stats are based on aggregation of other steps stats
        - use other steps stats, not `measures` because the number of `measures` might differ between steps when CONF.repeat true
           - add comment that could in principle use `measures` when CONF.repeat false, but does not because:
              - it would make stats differ between CONF.repeat true|false
                 - this is confusing and might lead some users to use CONF.repeat false
                 - using CONF.repeat true|false should only change precision, not accuracy
              - it would give better stats for step groups, discouraging CONF.repeat true
              - it is slower
        - how:
           - samples|minLoopDuration: any
           - mean|quantiles|median|min|max|stdev|loops|times: add
           - repeat: Math.round(loops / times)
           - histogram:
              - among all histograms first buckets, find one with smallest frequency:
                 - create a bucket with:
                    - frequency: smallestFrequency
                    - low|high: sum of all histograms first bucket's low|high
                 - for each first bucket:
                    - if smallestFrequency === firstBucketFrequency:
                       - discard bucket, i.e. next one becomes first bucket for that histogram
                       - also if >= (due to possible rounding error)
                    - otherwise, update:
                       - frequency: subtract smallestFrequency to it
                       - low|high: kept as is
                 - repeat
              - re-distribute buckets:
                 - so buckets width is uniform, and so there are 1000 buckets
                 - do it by summing and interpolating
        - add comment that this assumes steps measures are positively corrolated
           - for: quantiles|median|min|max, stdev, histogram
           - if not, result is a bit inaccurate, but remains precise
  - persisted in history
     - as opposed to being dynamically computed during reporting
     - reason: allows not losing history when:
        - step group change which steps it includes
        - or their names
        - or whether it is a step group or a normal step
  - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
     - reason: users might want to see children only when need details
        - and vice-versa
  - can target another step group
     - error if cycle
  - special stepId "all":
     - only allowed in CONF.groups.{stepId} ARR
        - when present, do not allow other values in ARR
     - select all available steps
        - including ones not measured|included
     - forbid "all" as a stepId name, either normal step or step group

Automatic repeat:
  - rename `scale` in prettifyStats() to `factor`
  - CONF.repeat BOOL
     - def: false
  - can use config selectors, e.g. CONF.repeat.{taskId} BOOL
  - runner:
     - params OBJ: maxLoops NUM, series OBJ_ARR: task 'taskId', step 'stepId', scale NUM, repeat NUM
     - do {
         const arg = { context: { ...beforeContext }, inputs }
         for (const { task, step, scale, repeat } of series) {
           const stepFunc = tasks[task][step]
           while (scale--) {
             startTime()
             while (repeat--) {
               stepFunc(arg)
             }
             endTime()
           }
         }
       } while (maxLoops--)
  - payload:
     - payload.series[*].repeat NUM:
        - if CONF.repeat false and task has multiple steps: 1
     - payload.series[*].scale NUM:
        - step-wise
        - also set at sampleState.series[*].scale[Last]
        - if:
           - CONF.repeat false: 1
           - first sample (sampleMedian === undefined): 1
           - at least one step is not ready: 1
              - "not ready": either stdev 0, stdev undefined or mean 0
              - i.e. always the case when uncalibrated
              - reason: ensure all steps have a `scale` as soon as possible
           - otherwise:
              - estimate total amount of loops for each step:
                 - using same logic as preview duration
                    - including logic for identical measures
                 - total amount, not loops left
                    - reason: more stable through the run
                 - excluding MAX_MEASURES
                 - excluding coldLoopsTarget
                    - reasons:
                       - the value is helpful for preview duration but does not reflect well the final value (it keeps changing), i.e. is inaccurate
                       - imprecise during the run, and also between runs, making `scale` imprecise too
              - leastLoopsStep: scale = 1
              - other steps:
                 - scale = Math.round(currentStep.estimatedLoops / leatLoopsStep.estimatedLoops)
                 - scale is Math.min()'d with both:
                    - upper limit to prevent `scale` from increasing sample duration too much:
                       - maxSampleDuration = Math.max(targetSampleDuration, slowestStepSampleMedian)
                       - try to keep step.scale as much as possible, until up to maxSampleDuration by doing:
                          - for each step, step.scaledMean = step.sampleMedian * step.scale
                          - if sum(steps[*].scaledMedian) <= maxSampleDuration: stop loop using all original steps[*].scale
                          - if fastestStep.scaledMedian > maxSampleDuration / steps.length
                             - modify each remaining step.scale so it fits maxSampleDuration / steps.length
                             - stop loop
                          - subtract fastestStep.scaledMedian * steps.length from maxSampleDuration
                          - remove fastestStep from list of steps
                             - if no more steps, stop loop using all original steps[*].scale
                          - repeat loop
                    - Math.floor((MAX_MEASURES - measures.length) / (series.length - 1))
                 - then scale is Math.max()'d with 1
              - reasons:
                 - repeat imprecise steps more so that all steps reach CONF.precision roughly at the same time
                 - includes all of:
                    - rstdev differences between combinations
                    - different CONF.precision per-combination
                    - logic when measures are identical
                    - logic specific to unit "boolean"
                 - produce same results regardless of CONF.precision
                 - keep scale stable across the run, since difference scales can lead to different engine optimization
     - payload.maxLoops NUM:
        - if:
           - first sample (measureDuration === 0): 1
           - CONF.repeat false: same as current behavior (i.e. use `growth`)
           - CONF.repeat true:
              - use growth = sumEachStep(repeatGrowth * scaleGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - scaleGrowth = scale / scaleLast
              - maxMeasures: divide (MAX_MEASURES - measures.length) by sum of all series[*].scale
                 - i.e. if all series[*].scale 1, divide by series.length
              - multiply each payload.series[*].scale by maxLoops
                 - including if some steps have no estimatedLoops
                 - but excluding if first sample
              - set payload.maxLoops 1
                 - reasons:
                    - each step has bigger loops, i.e. less cross-step optimization influence
                    - gives more flexibility to slow down some steps over others
     - steps excluded by CONF.select:
        - payload.series[*].scale: always 1
  - combination.imprecise BOOL
     - true when all of:
        - CONF.repeat false
        - multiple steps
        - if repeat had been used, it would have been >1
     - reporting:
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - repeat vs scale:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: make benchmark faster by ensuring each step reaches its rmoe target at same time
     - requirements for tasks with CONF.repeat true:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - i.e. cannot both read+write same property in neither `context` nor top-level scope, including:
           - stateful class instances like event emitters and streams
           - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.repeat true, increase step function complexity (including increasing input size)
           - split step into its own task
     - reasons why CONF.repeat:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.repeat and `scale`:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

--------------------

Add comment about preferred ways to loop, by preference:
  - remove manual loop and let core automatically loop
  - if has init|end (to measure separately): use steps
  - if init|end step slow: use CONF.repeat true
  - if init|end step also stateful: use CONF.repeat false, non-"auto" unit and return chunk ARR in main step
     - also preferred if manual looping happens inside library code
  - if main step does not have access to measures to return, and unit is duration: use CONF.repeat false, unit "count" and CONF.rate true in main step

combination.unit STR:
  - persisted in results
  - used and removed by prettifyStats() normalization
  - at the moment, hardcoded to "auto"

Split measures vs duration ARRs
  - runner: rename `measures` to `durations`
  - sample stage
     - assume that runner might return a `measures` ARR alongside the `durations` ARR
        - it is just forwarded as is to the transformation stage
        - renamed to `sampleMeasures`
     - uses `durations` ARR, not `measures` ARR nor stats
        - use durations, not measures for: repeat, sampleMedian
        - use sampleLoops, not stats.loops
        - except stats.rmoe (for `scale`)
     - `durations` ARR is only for current sample
        - no accumulation between samples
           - including no mergeSort()
     - validation of the response general shape:
        - check:
           - `durations`:
              - not undefined
              - is array
              - each element:
                 - forbid: undefined, not float, NaN, Infinity, negative
                 - allow: 0, float
           - `measures` (unless undefined):
              - is array
              - has same length as `durations`
        - those are plugin errors
        - does not inspect `measures` ARR individual elements
           - this is done by transformation stage instead
  - transformation stage:
     - transform sampleState to a sampleMeasures ARR
     - if not calibrated, skip stage and return empty sampleMeasures ARR
     - in order:
        - validation of each `sampleMeasures` ARR element
           - unit-specific
           - user errors (not plugin errors)
        - unit-specific normalization of each `sampleMeasures` ARR element
           - if unit "auto": none, but set from `durations` ARR
        - sort `sampleMeasures`
           - unless unit "auto": because already sorted
  - stats computation stage
     - does not use any sampleState
        - use sampleMeasures and measures, not `durations` ARR
           - merge sampleMeasures to measures with mergeSort()
        - use sampleMeasures.length, not sampleLoops
           - i.e. after chunk ARRs flattening
           - including for `times` calculation
        - except: calibrated, allSamples, durationState
  - stats|preview computation stage:
     - skipped when samplesMeasures is empty, which happens when either:
        - not calibrated
        - task has returned empty chunk ARRs as custom measures
  - maxMeasures check in isRemainingCombination() uses measures ARR, not durations ARR

Allow custom measures:
  - pass payload.sendMeasures BOOL to runner
     - if true, runner must return a `measures` VAL_ARR in response, using tasks return values
     - runner does not inspect nor validate measures, it just serializes and sends them to parent
        - serialization errors are not inspected by runner, just send to parent with "ipcSerialization type"
        - parent then assigns UserError if payload.sendMeasures was true, PluginError otherwise
  - step functions must then return NUM:
     - reasons, as opposed to mutating a `measure` argument:
        - argument could be destructured, leading to assignment not working
        - confusion with inputs also used as arguments
        - clear that return value has this semantics
     - reason why NUM instead of OBJ:
        - simple
        - works for every language, including cli runner

Allow tasks to return chunks, i.e. ARR of custom measures:
  - allow task to:
     - mix returning ARR and not ARR
        - returning VAL and [VAL] should behave the same
     - return empty chunk ARR
        - should behave similarly to not having measured
  - flatten chunk ARR of custom measures into a `sampleMeasures` single ARR:
     - done during transformation stage, after unit-specific normalization and before sorting
  - sample stage ignores chunk ARRs, but not later stages:
     - sampleLoops ignores chunk ARRS
        - but not sampleMeasures.length nor stats.loops, since those are done after chunk ARR flattening
     - sample response general shape validation ignores chunk ARRs (since it does not look into ARR items)
        - but ARR items validation recurse over chunk ARRs
  - sampleLoopSize:
     - mean length of step's returned chunk ARRs for the last sample
        - computed when flattening chunk ARRs
        - i.e. 1 when no chunk ARRs
        - Math.min() with 1 (in case the task only returns empty chunk ARRs)
     - used as part of maxMeasures in:
        - `scale` as part of the Math.min():
            - Math.floor((MAX_MEASURES - measures.length) / (series.length - 1))
           -> Math.floor((MAX_MEASURES - measures.length) / (series.length - 1) / currentStep.sampleLoopSize)
        - `maxLoop`:
            - (MAX_MEASURES - measures.length) / sum(series[*].scale)
           -> (MAX_MEASURES - measures.length) / sum(series[*].scale * series[*].sampleLoopSize)
  - add comments:
     - meant to be equivalent to having run the task several times returning each chunk ARR element

CONF.rate BOOL
  - def: false
  - applied on report time
     - not persisted in history
     - reporting flag
  - can use config selectors, e.g. CONF.rate.{stepId} BOOL
  - applyRate():
     - done:
        - after combination sorting
           - i.e. kept as is
           - add comment: do not allow configuring|overridding sorting for the moment, to keep things simple, because most users won't need it
        - before prettifyStats()
     - invert:
        - invert stats values (1/NUM)
           - of all stats of mainKind
           - for quantiles: each ARR element
           - for histogram: each start|end
        - swap min|max and meanMin|Max
        - reverse ARR order of quantiles|histogram
     - stats 0:
        - if a stat is 0, make it undefined
        - histogram:
           - if max 0: whole histogram undefined
           - if min 0: first bucket's min = bucketMax / 2
        - quantiles:
           - if max 0: whole quantiles undefined
           - if min 0:
              - find first non-0 quantile
              - replace all quantiles before it by its value divided by 2
        - fix reporters so they take into account those new possibilities of stats being undefined
  - with prettifyStats(): change factor|unitSuffix|unitPrefix

CONF.unit "UNIT"
  - if no stepId: all steps
  - i.e. same step from different tasks have same unit
     - including if single step for all tasks
  - can use config selectors, e.g. CONF.rate.{stepId} BOOL
  - persisted during `run` at combination.unit
     - cannot be changed during reporting

prettifyStats() factor|unit:
  - rename `unit` suffix to `unitSuffix`
  - add `unitPrefix`
     - empty string for most units
  - rename "duration|percentage|count" stat kinds to "duration|percentage|countKind"
  - split "durationKind" stat kind into:
     - "durationKind":
        - for minLoopDuration only
        - `factor` is based on itself (like other stat kinds except mainKind)
        - factor|unitPrefix|unitSuffix always use "durationUnit"
     - "mainKind":
        - for all others: mean[Min|Max]|median|min|max|quantiles|stdev|moe
        - `factor` is based on a single stat (mean[Min]|median)
        - factor|unitPrefix|unitSuffix depend on both combination.unit and CONF.rate
  - rename existing types for factor|unitSuffix|unitPrefix to: "durationUnit", "countUnit", "percentageUnit"
  - add new types for factor|unitSuffix|unitPrefix:
     - inverseCountUnit
        - factor: like countUnit
        - unitSuffix: like countUnit but inverse sign
        - unitPrefix: "1/"
     - durationRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: ops/s|ms|us|ns|ps|fs
     - bytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: B|KB|MB|GB|TB|PB/s
     - inverseBytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: s|ms|us|ns|ps|fs/B

Units:
  - Unit groups:
     - "autoOnly":
        - only use auto duration
        - set `sampleMeasures` using `durations` ARR
        - persisted unit: ns for 1 operation
        - payload.sendMeasures false
        - parent validates that `response.measures` is undefined
     - "mixed":
        - use both auto duration and custom measure
        - set measures to custom NUM / auto duration (in ns)
           - during measure normalization stage
           - chunk ARR elements use the same duration but divided by chunk ARR.length
           - if custom NUM is 0: result is 0 (i.e. same handling as any other NUM)
           - if auto duration 0: filter out the measure
              - even if custom NUM also 0
        - persisted unit: ops/ns
        - reason why we do not persist both the auto duration and measure:
           - would require whole logic to be duplicated for both: stats, preview duration estimation, isRemainingCombination(), saving, etc.
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - use durations.slice().sort() instead of durations.sort() when retrieving sampleMedian
           - reason: must keep its order since it correlates to `measures` order
     - "customOnly":
        - only use custom measure
        - persist task return value as a NUM
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - persisted unit: depends on unit
        - still compute durations
           - including repeat loop
           - reason: needs it for medianWeight (maxLoops)
           - but:
              - `calibrated` always true
                 - by setting it initially to true
                 - reason: uncalibrated durations impact sample durations but not custom measures precision nor accuracy
              - combination.imprecise always false
  - Available units:
     - "auto"
        - default value
        - group: "autoOnly"
        - no return
        - normalization: none, except one from group "autoOnly"
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "duration"
        - group: "customOnly"
        - return float (ns)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "count"
        - group: "mixed"
        - return float (ops)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: durationRate
           - CONF.rate true: durationUnit
        - add comment: possible intents depends on whether NUM is specified by:
           - code used by task: task does not know NUM and forwards it, i.e. check how many times something occured (e.g. number of writes)
           - task code: i.e. task performs a loop and returns loop size
              - in this case, returning chunk ARR of durations is better instead, if possible
     - "bytes"
        - group: "mixed"
        - return integer (bytes)
        - validation:
           - forbid: undefined, not integer, NaN, Infinity, negative
           - allow: 0, integer
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: bytesRate
           - CONF.rate true: inverseBytesRate
     - "boolean"
        - group: "customOnly"
        - return BOOL
        - validation:
           - forbid: undefined, not BOOL
        - normalization:
           - sortedMeasures:
              - set to ARR with two numbers: `true` count, total count
              - use BIGNUMs
           - unsortedMeasures: ARR of 0|1s
           - i.e. maxMeasures should use double of the limit
        - stat kind:
           - CONF.rate false: percentageUnit
           - CONF.rate true: inverseCountUnit
     - "number"
        - group: "customOnly"
        - return float
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: countUnit
           - CONF.rate true: inverseCountUnit
  - when merging combinations from different results with same stepId but different unit:
     - filter out older combinations with incompatible units
     - compatible units:
        - any unit with group "durationOnly" or "mixed", even if different unit
        - units with group "customOnly", only if same unit
  - add comments:
     - why CONF.unit enum values are generic words, not specific units, is to make it clear that:
        - this does not set the specific unit being reported
        - unit `factor` happens at reporting time, not measuring time
     - why default is "auto": makes returning values from task functions opt-in:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
     - we allow measures being 0
        - including for any stats, including mean
        - this is useful for most units:
           - a task might randomly not perform an action
              - measures being 0 remove the need for manually looping until the action is performed
           - a task might have a real mean of 0
              - e.g. task which never writes (0B/s), task which always return false (0%), etc.

CONF.unit "boolean":
  - no sorting
  - no outliers
  - mean: `true` count / total count
     - when unit is boolean, unlike other units, mean 0 has same behavior as other means
        - reason: handled correctly by Wilson score interval (`moe`) and binomial test (`diff`)
        - including:
           - do not skip `diff`
           - do not make `cold` 1
           - do not skip `variance|moe|...`
  - quantiles|min|max|median|histogram: undefined
     - i.e. reporters show it the same way as not measured yet combinations
  - MIN_STDEV_LOOPS:
     - not checked, i.e. computes variance|moe|... as soon as the first measure
     - reason:
        - `moe` does not change much based on the value of measures, it is more based on the number of measures
        - especially when the number of measures is low
  - variance: mean * (1 - mean)
     - can be 0 if mean 0|1
     - stdev: same as for other units, i.e. Math.sqrt(variance)
  - meanMin|Max:
     - use Wilson score interval with continuity correction
        - this makes it asymmetric
        - reason why continuity correction is used:
           - more accurate: significance level is a minimum instead of average
           - however, wider confidence interval:
              - 0-25%, i.e. not that much wider
              - wider when sample size low or `mean` very low|high
           - also, works better with sample size 1
        - implementation:
            function (length, mean) {
              const valueA = Z_VALUE_SQUARED - 1 / length + 4 * length * mean * (1 - mean)
              const valueB = 2 * length * mean + Z_VALUE_SQUARED
              const valueC = 4 * mean - 2
              const valueD = 2 * (length + Z_VALUE_SQUARED)
              const meanMin =
                mean === 0
                  ? 0
                  : Math.max(
                      (valueB - (Z_VALUE * Math.sqrt(valueA + valueC) + 1)) / valueD,
                      0,
                    )
              const meanMax =
                mean === 1
                  ? 1
                  : Math.min(
                      (valueB + (Z_VALUE * Math.sqrt(valueA - valueC) + 1)) / valueD,
                      1,
                    )
              return [meanMin, meanMax]
            }
            const Z_VALUE = 1.96
            const Z_VALUE_SQUARED = Z_VALUE ** 2
     - no need to Math.min|max() with:
        - 0|1 because Wilson score interval (with continuitt correction) already does it
        - stats.min|max because there are none
  - moe: (meanMax - meanMin) / 2
  - rstdev|rmoe: not computed
     - reasons:
        - stdev|moe is already a percentage
        - it is simpler DX that CONF.precision:
           - targets the absolute percentage (moe)
           - as opposed to the percentage of another percentage (rmoe)
        - otherwise, the benchmark would be much slower when `mean` is very low|high
           - instead, users should feel in control of the benchmark duration through CONF.precision alone
           - as opposed to `mean` changes significantly impacting the benchmark duration
        - does not work with mean 0
  - CONF.precision:
     - use moe instead of rmoe
     - compared against precisionTarget, like other units
     - use different CONF.precision values:
        - number of samples is linear, not quadratic, to moe
        - lower number of samples have higher moe than with normal distribution
        - maybe something like: 0, 20%, 10%, 5%, 2%, 1% (def), .1%, .01%, .001%, .0001%, .00001%
  - diff:
     - use binomial test instead of welch t-test
     - use absolute instead of relative diff
        - reasons:
           - consistent with moe, cold and CONF.precision
           - simpler to interpret
           - works the same for values below|above 0.5
              - i.e. inverting BOOL does not change it
     - add comments:
        - why we do not show mean|diff as 0-50% true|false:
           - confusing when a results includes percentages both above and below 50%
           - generally confusing
  - estimating number of loops left:
     - used for both preview duration and step `scale`
     - try to guess by inverting Wilson score interval (with continuity correction)
  - identical measures: do not use special logic
     - reason: Wilson score interval (for `moe`) and binomial test (for `diff`) handle identical measures correctly
  - cold:
     - do not divide by mean
        - i.e. absolute diff, not relative diff
     - reasons:
        - same reasons as for moe
        - more consistent with moe since they both use the same precisionTarget
     - implementation:
        - two entry points for boolean unit and other units
        - boolean unit:
           - pass incrementalMeanMin|Max as: mean +|- precisionTarget
           - do not divide cold by mean before returning it
        - other unit:
           - pass incrementalMeanMin|Max as: mean * (1 +|- precisionTarget)
           - divide cold by mean before returning it
  - envDev: try to handle booleans as 0|1s
     - i.e.:
        - variance is normal variance, not binomial variance
        - can use chi-squared distribution confidence interval
     - confirm this works well, i.e. correctly adjusts moe when there is environment variation

--------------------

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for each step
  - always use await on before|after, i.e. allow both sync and async
  - remove task.async BOOL

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `dev` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - make it a variable property
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - mean measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

--------------------

`spyd` history branch:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - directory is "{gitRoot}/history/results/"
        - git root lookup is using same logic as other places which look for it, i.e. CONF.cwd
     - automatic commits should be prefixed with `[skip ci]`
     - individual results:
        - at /history/results/YYYY-MM-DD--HH-MM-SS--BRANCH--{result.id}.json
           - encode|decode characters in BRANCH:
              - [[:alnum:]-_] left as is
              - any others like percent encoding but using x instead of %
                 - including . and /
                 - including x if followed by two [0-9a-f]
                 - a-f lowercase only
              - extract to own library
        - one immutable FILE.json per result
           - i.e. single OBJ
        - format is JSON
           - reason: fast
  - add comments:
     - pros of using a separate git branch:
        - instead of:
           - using regular files in codebase
           - git hash-object + git cat-file
        - does not pollute git log
           - especially in CI where a single commit might have many CI jobs and results
        - does not require git push --force on the codebase
        - automatically handle committing
        - semi-automatically handle git push|pull
           - can update in CI without requiring developers to pull all the time
        - allows multiple files
        - easier to make it skip CI
        - does not create many tags
        - ensures all branches can always be used for branches comparison with CONF.since
        - easy to understand
        - no risk of pruning
        - versioned
     - only store history with git at the moment
        - however possible approach in the future, if there is a need for it: CONF.history STR:
           - "git"
           - "PATH"
           - CRUD stores plugins
     - why concatenated results are not cached:
        - simpler, i.e. no need to check the file, nor update it after each `spyd bench --save`, `spyd sync`, etc.
        - performance benefits not big enough since only relevant results are loaded
        - manual edits of files would require some way to invalidate cache, e.g. a `spyd prune` command
        - takes space on the cache directory, i.e. might require automatic|manual pruning
     - why using individual results instead of single file for all results:
        - fast to create new results
        - does not create git conflicts
        - concurrent safe
        - small file size impact in git history
        - easier to edit
        - allow loading only few results instead of all

Require git:
  - only for:
     - CONF.save
     - `sync` command
     - CONF.since branch|commit|tag|"parent"
        - i.e. optional for CONF.since resultId
  - require:
     - `git` binary is executable (i.e. `git --version` has exit code 0)
     - minimum version of `git`
     - there is a `.git` in `[.../]{CONF.cwd}`
  - add comments:
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository

`spyd sync` command:
  - on `spyd` git branch:
     - git pull --rebase
        - if merge conflict:
           - try to automatically solve by including additions from both branches
              - add comment that this might happen with history/renamed_branches.yml
              - then retry `git pull --rebase`
           - otherwise fail
     - then git push
        - not run if we know locally there is nothing to push
  - stdin|stdout|stderr "inherit" on git pull|push
     - reasons:
        - allow entering passwords
        - show any error message such as: authentication, git hooks, network, etc.
        - provide with progress
     - not on other git commands
     - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
     - only if CONF.quiet false
  - add comments:
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull

Branches with delta resolution:
  - ensure that delta resolution only use results from correct branch:
     - target delta: current branch
     - sinceResult's branch (if git reference of result id) or current branch (otherwise)
  - resolving `git` delta format when it is a git branch:
     - latest result in the branch
     - then default to current behavior (using commit author date as timestamp)

Multiple branches reporting:
  - only report one branch + targetResult
     - branch is:
        - if CONF.since is a git reference or a result id, use its branch
           - branch is resolved:
              - git branch: use it
              - git tag|commit: use the same git logic used to get the current branch, but with a git tag|commit
              - result id: get branch from the result's filename
        - otherwise, current branch
           - instead of using env-ci, should use the best library to guess the branch from a specific commit reference
              - ensure cwd can be specified
              - could also just inline the git calls made by the library
           - if no branch found, use the `undefined` branch
     - filter out any results from the branch later than targetResult, before applying sinceDelta
        - using timestamp + branch in result's filename
        - reasons:
           - ensure benchmark's history remains same as new results are being added
           - ensure history is sorted by timestamp, including the target result
           - parent branch's newer results are likely to contain performance improvements that were not merged in the current branch yet when the result was taken
     - all results without a git branch are treated as if part of a single `undefined` branch
     - result's branch is persisted when the benchmark is saved
  - CONF.since "parent":
     - same as CONF.since "branch" with parent branch
        - i.e. last commit of parent branch, since this is the one the child branch will merge to
     - if no parent branch, like "first" in current branch
  - add comments:
     - problems:
        - hard to know which is parent's branch parent commit using only the benchmarks data:
           - need to use commit ids, which might have changed since benchmark was run due to rebasing
           - user intent is ambiguous: even if branch has not been rebased to parent yet, might intend to do so
        - parent branch's benchmarks' timestamps might interleave with current branch's
        - current branch might have been rebased 0|1|n times onto parent branch
           - parent and current branch's performance improvements are mixed and hard to distinguish
        - grouping results per branch and showing several branches leads to:
           - more visually complex time series
           - harder to understand history since each branch has its own progression
        - should be easy to understand|explain
        - should preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - should not be impacted by rebasing
        - allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
     - discarded solutions:
        - report all results without taking branches into account
        - report all results, grouped by branches
        - report all results of parent branches, grouped by branches
           - variant: only show earlier results
        - let user decide with a CONF.branch configuration property

Renaming branches:
  - on results load, should also get a list of branch renames:
     - do it using the current git reflog
  - use the list of branch renames:
     - to normalize all results branches to their latest names, including when:
        - resolving CONF.since's branch
        - reporting
     - the result's filename and contents also contains the branch name but this name might be old, which is ok since it is fixed by branch renames
  - persist branch renames:
     - reason: git reflog lasts only for 90 days by default
     - done during CONF.save
        - reason: otherwise, user might not expect having to run `spyd sync` again
     - when saving, should automatically create its own git commit
        - using same logic as git commits for results saving (including `[skip ci]`), but as a separate commit
           - reason: cleaner, and limit impact of merge conflicts
     - only append new branch renames, not ones already persisted
        - reason for append-only: limit potential for merge conflicts
     - only add renames for branches used in at least one result
     - persisted to history/renamed_branched.yml
        - OBJ_ARR: from 'BRANCH', to 'BRANCH2'
        - reason for YAML array: limit potential for merge conflicts
     - always loaded and merged with current git reflog, with lower priority

--------------------

Find ways to improve precision even more???

Plugin|core errors should print message to report GitHub issues to the plugin|core
  - it should include system information

CONF.debug BOOL
  - meant as bug report attachment, not meant for users to debug themselves
  - saved debug information to "{process.cwd()}/spyd_debug_logs.yml"
     - not printed to stdout
     - no way to configure location
     - saved at end once, using try/finally wrapping the programmatic entry points
        - not streamed, so it does not impact benchmark
  - does not change the logic otherwise
     - including reporting and previews
  - file is YAML:
     - using document stream
     - of OBJ:
        - event "EVENT"
        - event-specific properties
  - include:
     - envinfo
     - resolved config
     - task files
     - runner.versions
     - combinations
     - samples
        - including: duration spent, estimated time left, progress bar percentage
  - for all commands
  - interface is debugLog(debug, "EVENT", EVENT_OBJ)
     - debug is undefined if CONF.debug false, mutable ARR otherwise
  - add to GitHub issue templates

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

Learn package 'simple-statistics' and|or 'jstat' and use it in spyd, where needed

Find performance bottlenecks and optimize them

--------------------

Add TypeScript support:
  - to:
     - spyd.ts
     - tasks.ts
  - export types of those too

Add output formats:
  - report.reportMarkdown()->PROMISE_STR
     - for CONF.output "*.md|markdown|mdown|mkd|mkdn" or "README|readme"
     - def: reportTerminal() return value in ``` block
     - no padding
     - allows two reporters with same output
        - concatenated like terminal formats, i.e. with newlines
     - footer: appended as Markdown
  - report.reportHtml()->PROMISE_STR
     - for CONF.output "*.html|htm"
     - file should be whole (i.e. have <html> tag)
     - reporter can save other files (imported by the main file) in the same directory or subdirectory
     - def: reportMarkdown() return value rendered to HTML
     - no padding
     - does not allow two reporters with same output
        - reasons:
           - hard to concatenate
           - can emulate concatenation using a parent HTML file
     - footer: inject to any element with id "spyd-footer"
  - report.reportSvg()->PROMISE_STR
     - for CONF.output "*.svg|png|jpg|jpeg|webp"
     - def: reportHtml() return value rendered to SVG
     - converts SVG to PNG|JPEG|WebP
     - no padding
     - does not allow two reporters with same output
     - footer: added with svg manipulation
  - add default value for reportTerminal(): using markdown|html|svg return as is

Reporters:
  - types:
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for means
        - with full CLI width for slowest mean
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
        - abscissa:
           - only show start|end
           - format should be date-only if different days, time-only otherwise
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - stacked bar graph for multiple stages benchmarks
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - no JSON???
     - reasons:
        - might be misused in place of CONF.save (when intent is persisting)
        - might be misused in place of external reporters (when intent is external reporting)
        - increase importance of forward compatibility, because JSON consumers might not be versioned unlike reporters

GitHub action

GitHub PRs integration

Add other runners:
  - HTTP
  - JSON-RPC
  - graphQL
  - gRPC
  - WebSocket
  - TCP
  - chrome (maybe using puppetter)
  - firefox (maybe using puppetter-firefox)
  - selenium
  - bash
  - go

SaaS:
  - reporting dashboard
  - files:
     - either connect to GitHub repository
     - or edit file with online IDE, saved in the server
  - perform benchmark in-browser
  - public URLs, for sharing

--------------------

Learn the whole Terminal section in edl
  - including finishing ANSI sequences and terminal emulators
  - use cli table library
     - ensure this is still responsive (by creating multiple table if too wide)
        - if no library does this, create own library on top of another

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Terminal-histogram:
  - separate to own repository
  - add features:
     - color themable (using terminal-theme)
     - left bar with percentages
     - can specify number of abscissa ticks
        - or number of columns per tick
        - stack labels, i.e. might need to stack deeper than one level
    - allow minimum ordinate to be either 0 or minimum value
    - labelling columns
    - custom unit for ordinate
    - when too many columns:
       - if labeled: break into several histograms
       - otherwise: extrapolate

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

--------------------

Manually try all features with each Node.js version

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Positioning (in documentation), in priority order:
  - whole benchmarking flow, not just measuring:
     - reporting (pretty, configurable, live preview, multiple formats, insertion, hardware/software info)
     - combinations (variations, inputs, systems, steps, selection)
     - comparing
     - history (including time series)
     - debugging (spyd dev, nice error messages)
     - sharing for others to run
     - testing (limits)
     - concurrency benchmarking
     - CI
     - GitHub PRs
  - simplicity
     - no library|APIs, only export functions
     - no need to specify duration nor number of loops
     - can work with no|minimal configuration, including on-the-fly
  - precision
     - high precision
     - configurable precision
     - report statistical significance (including for diffs)
     - not only average, also: min|max, stdev, histogram, quantiles
  - platform|language-agnostic
     - including CLI, Node.js, TypeScript
     - including comparing Node.js versions

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both means (for accuracy) and stdev (for precision)

--------------------

Add repo of spyd benchmarks:
  - contributors can add any
  - only for:
     - JavaScript
     - core Node.js or JavaScript, no modules
  - each benchmark is a directory with a single benchmark
     - optional spys.yml
  - README shows all results
     - as run in CI
     - each shows: title, tasks file content, reported result
     - a hardcoded list is maintained for sorting
     - created by a build task
  - binary for users to run any of the benchmarks on their machine
     - including with npx

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2021.stateofjs.com/en-US/resources/
  - https://javascriptkicks.com/submit

Ideas for articles about benchmarking:
  - choice between accuracy vs precision
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes
