
        
   SPYD  
        



Validation:
  - used by:
     - main config
     - plugin config (CONF.runnerConfig|reporterConfig)
        - required to use any configuration property
        - handle errors to add plugin name
        - including common properties, e.g. CONF.reporterConfig.{id}.output (which must do path resolution)
     - not used to validate task files (runner-specific)
        - reason: it runs in runner process, which might be different language
        - runner should instead do the following on their own: default values, normalization, throwing errors
        - try to use shared library between runners of same language
           - including using the same validation code as main|plugin config for Node.js runners
  - validate(object, config, opts)
  - config.VARR.schema OBJ
     - JSON schema
  - config.VARR.validate(VAL, "VARR", OPTS)[->"ERROR"]
     - OPTS:
        - parent OBJ
        - root OBJ
  - validate against unknown properties
     - unless "*" in VARR
     - include automatic suggestion
  - config.VARR.example VAL
     - shown on validation error
  - config.VARR.normalize(VAL, "VARR", OPTS)->VAL
     - same OPTS as validate()
  - config.VARR.default VAL
     - applied if undefined
  - opts.cwd STR[(VAL, "VARR")]
     - used when resolving file paths
        - done when config.VARR.schema.pattern "path"
     - def: process.cwd()
     - when loading config:
        - must keep "CWD" of each VARR as an OBJ since this depends on how config was loaded, and this is known only before validation
        - replace current `PATH_CONFIG_PROPS` logic
        - including for CONF.tasks
     - FUNC can return empty STR to not do path resolution
        - used by CONF.config when prefixed with "npm:" for example
  - opts.validPropName REGEXP
     - use /^[a-z][a-zA-Z\d]*$/
        - i.e. use case (not _ - .) as delimiter
     - reason: when using "*" in VARR, property names are user-defined
  - VARR:
     - dot-delimited
     - can include "*" for either:
        - array indices
        - dynamic properties
  - add comments:
     - no warnings, only errors because:
        - simpler to specify
        - prefer failing hard
        - prefer semver major with breaking changes over deprecation period
        - does not work well with previews since that clears screen
        - simpler to implement
           - warnings would need to be returned from start() and passed to parent

--------------------

Dimensions:
  - combination.dimensions.DIMENSION "ID"
     - DIMENSION: "task", "runner", "system"
     - make logic depends on this instead of importing `COMBINATION_DIMENSIONS`
  - on report:
     - filter out unnecessary dimensions from combination.dimensions.DIMENSION
        - i.e. any dimension with only 1 id
        - exception: if all dimensions are unnecessary, keep task dimension
     - on report normalization, convert combination.dimensions.DIMENSION "ID" to OBJ: id STR, title STR, titlePadded STR
        - re-use then remove the current addTitles() logic
        - remove `combination.titles`: replace by a reporter util helper function
     - not persisted on stores:
        - reasons:
           - CONF.select might remove some ids
           - CONF.since might add some ids
  - getCombinationName(combination)->STR:
     - 'DIMENSION "ID", ...'
     - ids, not titles
     - for: `dev`, limit errors, measure errors
        - not reporters
     - should re-use some of the normalization happening for reporters:
        - result.combinations dimension ARR order
        - filter out unnecessary dimensions
  - getCombinationTitle[Padded](combination)->STR:
     - titles, not ids
        - using combination.dimensions.*.title[Padded]
     - for reporters

Cross-dimensions duplicate id validation:
  - for newly measured result with `run`:
     - throw error
     - only for the given result's ids, not including the previous results' ids
        - reasons:
           - would depend on CONF.since, making it fail or not depending on it
           - since CONF.since defaults to 0, it would not be useful most of the times
           - we would still need a separate way to handle cross-dimensions duplicate ids between results due to:
              - multiple branches results (since we do not load all results from all branches)
              - manual edits of result files
  - between different results:
     - prepend `DIMENSION_` to id
        - use _ instead of .
     - not done on the dimension with the most recent result

CONF.system "ID" -> CONF.system[.{systemDimension}] "ID"
  - DIMENSION is "system[.{systemDimension}]"
     - i.e. can use several dimensions
     - combination.dimensions.system -> combination.dimensions['system.{systemDimension}']
     - getCombinationName() should use only systemDimension
  - system id is "ID", without systemDimension
  - default systemDimension: "machine"
  - default "ID": "primary_{systemDimension}"
  - when sorting result.combinations per dimension, system sorted:
     - after runner, before variation
     - by CONF.system OBJ order
  - result.systems[*].dimensions:
     - before merging: OBJ where key is dimension, value is id
     - after merging:
        - OBJ_ARR_ARR: id STR, title STR, dimension STR
        - second ARR is when merging combinations with same shared props
     - reported as comma-separated list of space-separated lists
     - remove result.systems[*].id|title
  - merging:
     - create shared systems in decreasing number of shared dimensions:
        - first look for properties shared on all system dimensions. On any match:
           - create a shared system OBJ for it, unless either:
              - no shared properties
                 - except top-level one, at systems[0], always defined
              - another shared system has exact same properties:
                 - just append dimensions ids to it instead
           - remove props from the matching combinations, so they are not in two different system OBJs
        - then same for all dimensions minus 1|2|...
           - using all possible sets of "dimensions minus 1|2|..."
     - undefined values:
        - like other values when grouping
        - but cleaned after grouping
           - i.e. not printed by reporters
  - add comments:
     - reasons for allowing multiple system dimensions (e.g. CONF.system.os "linux") instead of a single one (e.g. CONF.system "os_linux")
        - separating those in reporting looks nicer
        - easier to configure titles
        - better dimension-wise sorting in reporting
        - easier to select (CONF.select|limit)

Variations:
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation dimension"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - any CONF.runnerConfig.{runnerId}.PROP
        - cartesian product only to combinations with that runner
  - DIMENSION is "variation.{variationDimension}"
     - i.e. each variable property is a dimension
     - add combination.dimensions['variation.{variationDimension}']
     - variationDimension is property VARR (dot-delimited)
     - getCombinationName() should remove "variation." prefix from DIMENSION
  - variation id is 'ID', not 'PROP.ID'
  - configuration normalization|validation should apply to each VAL
     - including yargs flags parsing
  - when sorting result.combinations per dimension, variations sorted:
     - at end
     - input, then concurrency, then runnerConfig
     - input: sorted by CONF.inputs OBJ order
     - runnerConfig sorted by:
        - CONF.runner ARR order
        - then config prop name, alphabetically
  - add combination.config CONF_OBJ
     - not persisted in history nor used in report
     - contains config, with combination-specific variations
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR STR (commaSpace-separated list)
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - should still set combination.dimensions.{variationDimension} on those combinations, but with id "", title "", titlePadded " ... "
     - when filtering unnecessary dimensions, should exclude ones with id ""
  - add comments:
     - why we only persist variation ids, not values
        - value might change but user intend the id to be constant, which is more relevant
        - value might be big to store, or have secrets
        - value might be not nice to report (e.g. big objects)

Default ids:
  - add default ids:
     - done:
        - right after results load
        - before CONF.since merge
     - for all possible dimensions with default ids
        - figure out all possible dimensions by using dimensions of all loaded results
           - add comment: do not also use currently measured runners' possible configuration properties
              - because this is not available in `show|remove`, which would make CONF.select|limit behave differently between commands
     - i.e. missing dimension should behave like the dimension with its default id, including in:
        - CONF.select|limit
        - diff
        - reporting (e.g. when mixed with other results not missing the dimension)
     - per dimension:
        - task|runner: never missing
        - step: "main"
           - assigned on result creation, i.e. saved
        - system: "primary_DIMENSION"
           - assigned on result creation, i.e. saved, except if multiple system dimensions
        - variation: "main_PROP"
           - not assigned on result creation, i.e. not saved
           - PROP is only last property
           - exception: PROP is last two properties (_-separated) when two variable properties have same last property
  - CONF.select:
     - when retrieving combination ids:
        - if CONF.select contains default ids
           - except for persisted default ids: "main" and "primary_machine"
           - should be checked case-insensitively
           - add comment: this guess might fail due to using partial substring matching, but this is not a problem because:
              - only impacts variations and systems (only if multiple system dimensions) since the other default ids are persisted
              - most substrings matching "primary|main_*" would match all of the other ids of the dimension, making the selectors not useful for users, i.e. unlikely
        - then add those default to the combination ids, if the combination is missing the relevant dimensions
     - reason: selection is applied before results are loaded, i.e. before all dimensions are known and default ids are applied
  - validate that new result ids do not match possible default ids (unless same dimension):
     - "main": step
     - "main_*": variations
     - "primary_*": system
  - add comments:
     - purpose is when a result is missing a dimension that other results have, that result should still:
        - be comparable in the history
           - e.g. only one step was used (undefined) but another step was added, forcing to rename the first step but it should still be comparable
        - be reported nicely
        - be selectable with CONF.select
     - history matching problem:
        - when:
           - a result misses a dimension
           - it must be compared with results with that dimension
           - those other results have multiple ids of that dimension
        - then it becomes difficult to know which id should be used to compare
        - possible solutions (others than default ids):
           - missing dimension never matches an id from a non-missing dimension
              - problem: creates discontinuity in history
           - missing dimension matches any ids from a non-missing dimension
              - problem: non-limear history and most likely wrong results

--------------------

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each iteration, shallow copy that OBJ
     - i.e. each iteration has fresh copy
     - i.e. cannot communicate with next iteration
        - reason: might accidentally get previous iteration state, especially if property is set considitionally
        - other reason: ensure proper garbage collection
        - exception: top-level state, or in properties created during before()
  - passed as `this`
     - check whether FUNC.call(context) or FUNC.bind(context) is faster
        - probably bind() since it can be done once during init
     - should document that does not work with arrow function nor bound function, i.e. must be avoided
     - try to validate against binding step functions
        - only if multiple steps
  - should be passed to step function even when single step
     - reasons:
        - stats do not vary depending on whether there is a single step or not
        - more monomorphic
  - forbid inputId named `context`, to allow other runners to pass `context` alongside named arguments
  - add comments about:
     - advantages over using a named argument:
        - cannot re-assign `this`, i.e. no need to validate it
        - simpler syntax (no need to retrieve from arguments)
        - intuitive|common pattern in JavaScript
        - clearer separation between inputs and context
     - advantages over top-level scope (which can still be used)
        - not shared between tasks
        - not shared between iterations
        - does not require declaring a variable
     - problems with alternatives to single `context` OBJ:
        - separate `context` arguments for input and output (to next step)
           - information meant for later steps must be passed between several steps
        - `context` argument for input, return for output (to next step)
           - information meant for later steps must be passed between several steps
        - `{stepId}` argument for input, return for output
           - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
           - name conflict with any core argument
              - except inputs, which are validated against duplicates with steps
           - more complex to explain:
              - return vs context
              - `stepId` argument name
              - when repeating a step, only last iteration's return value is used

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - in documentation:
     - encourage `export const TASK = { STEP() {...}, ... };`
     - as opposed to `const TASK_STEP = function() {...}; export const TASK = { STEP: TASK_STEP, ... };`
  - validate against tasks with no steps (besides before|after)
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
        - i.e. must return `tasks` `{ taskId: ['main'] }` to parent
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - processes:
     - at benchmark start, when runner communicates available tasks to parent, it should also return available steps
        - returned as `tasks: { taskId: 'stepId'_ARR, ... }`
     - all steps of a given combination use same process
  - remove beforeEach|afterEach
     - rename beforeAll|afterAll to before|after
     - add comment that runners should avoid specific case for reserved exported names, since users might use different case convention for stepIds
  - each step is a combination dimension
     - add combination.dimensions.step
     - when sorting combinations: sorted first, before tasks
  - implementation:
     - runner:
        - on start, returns steps to parent
           - ARR in execution order
        - on measure, gets OBJ:
           - maxLoops NUM
           - steps OBJ_ARR:
              - id "stepId", repeat NUM
              - ARR in execution order
        - do {
            const context = { ...beforeContext }
            for (const { id, repeat } of steps) {
              const step = steps[id]
              startTime()
              while (repeat--) {
                step.apply(context, args)
              }
              endTime()
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in steps execution order
     - measureEachCombination() [un]groups combinations with same dimensions except stepId into series, and iterates over those instead
        - rename "combination" to "series" for any group of steps with same other dimensions
     - getSampleState() and getStats() are iterated over sampleState.steps[*]
     - sampleState:
        - measureDuration|totalDuration|sampleDurationMean: not-step-wise
        - allSamples: not-step-wise
        - all other properties: step-wise, at sampleState.steps OBJ_ARR including: repeat, calibration, stats, sampleMeasures|sampleMedian|sampleLoops
     - sample payload:
        - payload.steps[*].repeat NUM: sampleState.repeat
        - payload.maxLoops NUM:
           - not step-wise
           - if first sample (measureDuration === 0): 1
           - otherwise: keep current logic except include steps in `repeatGrowth` computation
              - renamed to `growth`
              - sumEachStep(repeatGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - sampleMedianWeight = sampleMedian / sampleMediansSum
                    - sampleMediansSum = sum of each step sampleMedian
                    - if sampleMedianSum 0: sampleMedianWeight is 1 for each step
                    - if sampleMedianSum not 0 but sampleMedian 0: sampleMedianWeight is 0 for that step
              - maxMeasures threshold: divide (MAX_MEASURES - measures.length) by steps.length
     - previewStats:
        - combinationEnd: keep most logic except:
           - compute samplesLeft for each step, and only use the Math.max()
        - skipping logic (uncalibrated or mean === 0) should be step-wise
           - preview should be skipped only if all steps are skipped
     - tweak isRemainingCombination logic for multiple steps:
        - stopped, `dev` stage: keep as is (not-step-wise)
        - mean === 0: steps.some()
        - maxMeasures: use sum of all samplesState.step[*].measures.length
        - loops === 0 (CONF.precision 0): steps.some()
        - rmoe: steps.some()
     - minLoopDuration:
        - each step has its own minLoopDuration
        - multiply TARGET_DURATION by steps.length
        - maxMeasures should use sum of steps[*].measures.length
        - hasEnoughMeasures(): use steps.every()
        - run steps normally otherwise, i.e. the runner executes them serially but with `repeat: 0`
     - truncateLogs: not-step-wise
     - dev command: run all steps at once, without headers in-between
  - separate steps in reporters:
     - reason: they might have different units or CONF.rate (i.e. min|max)
     - for:
        - tables (debug|history): each steps should have its own table
        - boxplot: each step should have its own min|max
  - steps excluded by CONF.select:
     - like any other combination dimensions:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
        - not taken into account in:
           - isRemainingCombinationLogic logic
           - preview logic, including duration estimation
           - minLoopDuration
           - payload computation
              - except repeat always 1
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one step for that task is selected
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.select
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.select already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - error handling:
     - keep current logic i.e. exceptions are propagated
        - in:
           - `before` -> do not call later steps nor `after`
           - any other steps -> do not call later steps, but call `after`
              - exceptions in `after` itself are ignored
           - `after` -> nothing else to call
        - reasons:
           - ensures `after` does cleanup, but only if `before` completed
           - but assumes that exception leaves bad state, i.e. should not run additional steps, and `after` might fail
     - child should pass an optional stepId alongside error string, so that parent can print it
  - add comments about:
     - complex step order:
        - limitations of current solution:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allows reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as special kinds of steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.repeat error handling, etc.
     - why sampleTargetDuration does not increase with number of steps:
        - new steps are more likely to be due to splitting existing steps than adding new ones
        - keep preview responsive
     - we do not store steps execution order because not used for the moment

Step groups:
  - behave like steps except:
     - specified with CONF.stepsConfig.{stepId}.group 'stepId'_ARR
        - ignored if empty ARR
        - reasons for the syntax:
           - allow non-consecutive steps
           - not verbose (unlike using stepId, e.g. using stepId common prefixes)
     - stats are based on aggregation of other steps stats
        - use other steps stats, not `measures` because the number of `measures` might differ between steps when CONF.repeat true
           - add comment that could in principle use `measures` when CONF.repeat false, but does not because:
              - it would make stats differ between CONF.repeat true|false
                 - this is confusing and might lead some users to use CONF.repeat false
                 - using CONF.repeat true|false should only change precision, not accuracy
              - it would give better stats for step groups, discouraging CONF.repeat true
              - it is slower
        - how:
           - samples|minLoopDuration: any
           - mean|quantiles|median|min|max|stdev|loops|times: add
           - repeat: Math.round(loops / times)
           - histogram:
              - among all histograms first buckets, find one with smallest frequency:
                 - create a bucket with:
                    - frequency: smallestFrequency
                    - low|high: sum of all histograms first bucket's low|high
                 - for each first bucket:
                    - if smallestFrequency === firstBucketFrequency:
                       - discard bucket, i.e. next one becomes first bucket for that histogram
                       - also if >= (due to possible rounding error)
                    - otherwise, update:
                       - frequency: subtract smallestFrequency to it
                       - low|high: kept as is
                 - repeat
              - re-distribute buckets:
                 - so buckets width is uniform, and so there are 1000 buckets
                 - do it by summing and interpolating
        - add comment that this assumes steps measures are positively corrolated
           - for: quantiles|median|min|max, stdev, histogram
           - if not, result is a bit inaccurate, but remains precise
  - persisted in history
     - as opposed to being dynamically computed during reporting
     - reason: allows not losing history when:
        - step group change which steps it includes
        - or their names
        - or whether it is a step group or a normal step
  - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
     - reason: users might want to see children only when need details
        - and vice-versa
  - can target another step group
     - error if cycle
  - special stepId "all":
     - only allowed in stepsConfig.{stepId}.group ARR
        - when present, do not allow other values in ARR
     - select all available steps
        - including ones not measured|included
     - forbid "all" as a stepId name, either normal step or step group

Automatic repeat:
  - rename `scale` in prettifyStats() to `factor`
  - CONF[.tasksConfig.{taskId}].repeat BOOL
     - def: false
  - runner:
     - params OBJ: maxLoops NUM, steps OBJ_ARR: id 'stepId', scale NUM, repeat NUM
     - do {
         const context = { ...beforeContext }
         for (const { id, scale, repeat } of steps) {
           const step = steps[id]
           while (scale--) {
             startTime()
             while (repeat--) {
               step.apply(context, args)
             }
             endTime()
           }
         }
       } while (maxLoops--)
  - payload:
     - payload.steps[*].repeat NUM:
        - if CONF.repeat false and task has multiple steps: 1
     - payload.steps[*].scale NUM:
        - step-wise
        - also set at sampleState.steps[*].scale[Last]
        - if:
           - CONF.repeat false: 1
           - first sample (sampleMedian === undefined): 1
           - at least one step has no rstdev: 1
              - happens if either <5 measures or mean 0
                 - i.e. always the case when uncalibrated
              - reason: ensure all steps have a rstdev as soon as possible
           - otherwise:
              - steps with rstdev 0:
                 - scale = 1
                 - excluded from "lowest(rstdev**2)" below
              - step with lowest(rstdev**2): scale = 1
              - other steps:
                 - scale = Math.round(current(rstdev**2) / lowest(rstdev**2))
                 - scale is Math.min()'d with both:
                    - Math.round(lowestRstdevStep.sampleMedian / currentStep.sampleMedian)
                       - reason:
                          - prevent `scale` from increasing sample duration too much
                          - based on the fact that rstdev tends to be correlated with complexity, which is itself correlated with median duration
                       - if currentStep.sampleMedian 0: 1
                    - Math.floor((MAX_MEASURES - measures.length) / (steps.length - 1))
                 - then scale is Math.max()'d with 1
              - reasons:
                 - repeat imprecise steps more so that all steps reach CONF.precision roughly at the same time
                 - keep scale stable across the run, since difference scales can lead to different engine optimization
                    - which we use we don't use rmoe**2 instead of rstdev**2: although it would be more accurate, it would be less stable
                 - produce same results regardless of CONF.precision
     - payload.maxLoops NUM:
        - if:
           - first sample (measureDuration === 0): 1
           - CONF.repeat false: same as current behavior (i.e. use `growth`)
           - CONF.repeat true:
              - use growth = sumEachStep(repeatGrowth * scaleGrowth * sampleMedianWeight)
                 - repeatGrowth = repeat / repeatLast
                 - scaleGrowth = scale / scaleLast
              - maxMeasures: divide (MAX_MEASURES - measures.length) by sum of all steps[*].scale
                 - i.e. if all steps[*].scale 1, divide by steps.length
              - multiply each payload.steps[*].scale by maxLoops
                 - including if some steps have no rstdev
                 - but excluding if first sample
              - set payload.maxLoops 1
                 - reasons:
                    - each step has bigger loops, i.e. less cross-step optimization influence
                    - gives more flexibility to slow down some steps over others
     - steps excluded by CONF.select:
        - payload.steps[*].scale: always 1
  - combination.imprecise BOOL
     - true when all of:
        - CONF.repeat false
        - multiple steps
        - if repeat had been used, it would have been >1
     - reporting:
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - repeat vs scale:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: make benchmark faster by ensuring each step reaches its rmoe target at same time
     - requirements for tasks with CONF.repeat true:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - i.e. cannot both read+write same property in neither arguments nor top-level scope, including:
           - stateful class instances like event emitters and streams
           - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.repeat true, increase step function complexity (including increasing input size)
           - split step into its own task
     - reasons why CONF.repeat:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.repeat and `scale`:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

--------------------

Add comment about preferred ways to loop, by preference:
  - remove manual loop and let core automatically loop
  - if has init|end (to measure separately): use steps
  - if init|end step slow: use CONF.repeat true
  - if init|end step also stateful: use CONF.repeat false, non-"auto" unit and return chunk ARR in main step
     - also preferred if manual looping happens inside library code
  - if main step does not have access to measures to return, and unit is duration: use CONF.repeat false, unit "count" and CONF.rate true in main step

combination.unit STR:
  - persisted in results
  - used and removed by prettifyStats() normalization
  - at the moment, hardcoded to "auto"

Split measures vs duration ARRs
  - runner: rename `measures` to `durations`
  - sample stage
     - assume that runner might return a `measures` ARR alongside the `durations` ARR
        - it is just forwarded as is to the transformation stage
        - renamed to `sampleMeasures`
     - uses `durations` ARR, not `measures` ARR nor stats
        - use durations, not measures for: repeat, sampleMedian
        - use sampleLoops, not stats.loops
        - except stats.rmoe (for `scale`)
     - `durations` ARR is only for current sample
        - no accumulation between samples
           - including no mergeSort()
     - validation of the response general shape:
        - check:
           - `durations`:
              - not undefined
              - is array
              - each element:
                 - forbid: undefined, not float, NaN, Infinity, negative
                 - allow: 0, float
           - `measures` (unless undefined):
              - is array
              - has same length as `durations`
        - those are plugin errors
        - does not inspect `measures` ARR individual elements
           - this is done by transformation stage instead
  - transformation stage:
     - transform sampleState to a sampleMeasures ARR
     - if not calibrated, skip stage and return empty sampleMeasures ARR
     - in order:
        - validation of each `sampleMeasures` ARR element
           - unit-specific
           - user errors (not plugin errors)
        - unit-specific normalization of each `sampleMeasures` ARR element
           - if unit "auto": none, but set from `durations` ARR
        - sort `sampleMeasures`
           - unless unit "auto": because already sorted
  - stats computation stage
     - does not use any sampleState
        - use sampleMeasures and measures, not `durations` ARR
           - merge sampleMeasures to measures with mergeSort()
        - use sampleMeasures.length, not sampleLoops
           - i.e. after chunk ARRs flattening
           - including for `times` calculation
        - except: calibrated, allSamples, durationState
  - stats|preview computation stage:
     - skipped when samplesMeasures is empty, which happens when either:
        - not calibrated
        - task has returned empty chunk ARRs as custom measures
  - maxMeasures check in isRemainingCombination() uses measures ARR, not durations ARR

Allow custom measures:
  - pass payload.sendMeasures BOOL to runner
     - if true, runner must return a `measures` VAL_ARR in response, using tasks return values
     - runner does not inspect nor validate measures, it just serializes and sends them to parent
  - step functions must then return NUM:
     - reasons, as opposed to mutating a `measure` argument:
        - argument could be destructured, leading to assignment not working
        - confusion with inputs also used as arguments
        - clear that return value has this semantics
     - reason why NUM instead of OBJ:
        - simple
        - works for every language, including cli runner

Allow tasks to return chunks, i.e. ARR of custom measures:
  - allow task to:
     - mix returning ARR and not ARR
        - returning VAL and [VAL] should behave the same
     - return empty chunk ARR
        - should behave similarly to not having measured
  - flatten chunk ARR of custom measures into a `sampleMeasures` single ARR:
     - done during transformation stage, after unit-specific normalization and before sorting
  - sample stage ignores chunk ARRs, but not later stages:
     - sampleLoops ignores chunk ARRS
        - but not sampleMeasures.length nor stats.loops, since those are done after chunk ARR flattening
     - sample response general shape validation ignores chunk ARRs (since it does not look into ARR items)
        - but ARR items validation recurse over chunk ARRs
  - sampleLoopSize:
     - mean length of step's returned chunk ARRs for the last sample
        - computed when flattening chunk ARRs
        - i.e. 1 when no chunk ARRs
        - Math.min() with 1 (in case the task only returns empty chunk ARRs)
     - used as part of maxMeasures in:
        - `scale` as part of the Math.min():
            - Math.floor((MAX_MEASURES - measures.length) / (steps.length - 1))
           -> Math.floor((MAX_MEASURES - measures.length) / (steps.length - 1) / currentStep.sampleLoopSize)
        - `maxLoop`:
            - (MAX_MEASURES - measures.length) / sum(steps[*].scale)
           -> (MAX_MEASURES - measures.length) / sum(steps[*].scale * steps[*].sampleLoopSize)
  - add comments:
     - meant to be equivalent to having run the task several times returning each chunk ARR element

CONF.[stepsConfig.{stepId}.]rate BOOL
  - def: false
  - applied on report time
     - not persisted in history
     - reporting flag
  - applyRate():
     - done:
        - after combination sorting
           - i.e. kept as is
           - add comment: do not allow configuring|overridding sorting for the moment, to keep things simple, because most users won't need it
        - before prettifyStats()
     - invert:
        - invert stats values (1/NUM)
           - of all stats of mainKind
           - for quantiles: each ARR element
           - for histogram: each start|end
        - swap min|max and meanMin|Max
        - reverse ARR order of quantiles|histogram
     - stats 0:
        - if a stat is 0, make it undefined
        - histogram:
           - if max 0: whole histogram undefined
           - if min 0: first bucket's min = bucketMax / 2
        - quantiles:
           - if max 0: whole quantiles undefined
           - if min 0:
              - find first non-0 quantile
              - replace all quantiles before it by its value divided by 2
        - fix reporters so they take into account those new possibilities of stats being undefined
  - with prettifyStats(): change factor|unitSuffix|unitPrefix

CONF.[stepsConfig.{stepId}.]unit "UNIT"
  - if no stepId: all steps
  - i.e. same step from different tasks have same unit
     - including if single step for all tasks
  - persisted during `run` at combination.unit
     - cannot be changed during reporting

prettifyStats() factor|unit:
  - rename `unit` suffix to `unitSuffix`
  - add `unitPrefix`
     - empty string for most units
  - rename "duration|percentage|count" stat kinds to "duration|percentage|countKind"
  - split "durationKind" stat kind into:
     - "durationKind":
        - for minLoopDuration only
        - `factor` is based on itself (like other stat kinds except mainKind)
        - factor|unitPrefix|unitSuffix always use "durationUnit"
     - "mainKind":
        - for all others: mean[Min|Max]|median|min|max|quantiles|stdev|moe
        - `factor` is based on a single stat (mean[Min]|median)
        - factor|unitPrefix|unitSuffix depend on both combination.unit and CONF.rate
  - rename existing types for factor|unitSuffix|unitPrefix to: "durationUnit", "countUnit", "percentageUnit"
  - add new types for factor|unitSuffix|unitPrefix:
     - inverseCountUnit
        - factor: like countUnit
        - unitSuffix: like countUnit but inverse sign
        - unitPrefix: "1/"
     - durationRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: ops/s|ms|us|ns|ps|fs
     - bytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: B|KB|MB|GB|TB|PB/s
     - inverseBytesRate
        - factor: [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6]
        - unitSuffix: s|ms|us|ns|ps|fs/B

Units:
  - Unit groups:
     - "autoOnly":
        - only use auto duration
        - set `sampleMeasures` using `durations` ARR
        - persisted unit: ns for 1 operation
        - payload.sendMeasures false
        - parent validates that `response.measures` is undefined
     - "mixed":
        - use both auto duration and custom measure
        - set measures to custom NUM / auto duration (in ns)
           - during measure normalization stage
           - chunk ARR elements use the same duration but divided by chunk ARR.length
           - if custom NUM is 0: result is 0 (i.e. same handling as any other NUM)
           - if auto duration 0: filter out the measure
              - even if custom NUM also 0
        - persisted unit: ops/ns
        - reason why we do not persist both the auto duration and measure:
           - would require whole logic to be duplicated for both: stats, preview duration estimation, isRemainingCombination(), saving, etc.
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - use durations.slice().sort() instead of durations.sort() when retrieving sampleMedian
           - reason: must keep its order since it correlates to `measures` order
     - "customOnly":
        - only use custom measure
        - persist task return value as a NUM
        - payload.sendMeasures true
        - parent validates that `response.measures` is not undefined
        - persisted unit: depends on unit
        - still compute durations
           - including repeat loop
           - reason: needs it for medianWeight (maxLoops)
           - but:
              - `calibrated` always true
                 - by setting it initially to true
                 - reason: uncalibrated durations impact sample durations but not custom measures precision nor accuracy
              - combination.imprecise always false
  - Available units:
     - "auto"
        - default value
        - group: "autoOnly"
        - no return
        - normalization: none, except one from group "autoOnly"
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "duration"
        - group: "customOnly"
        - return float (ns)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: durationUnit
           - CONF.rate true: durationRate
     - "count"
        - group: "mixed"
        - return float (ops)
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: durationRate
           - CONF.rate true: durationUnit
        - add comment: possible intents depends on whether NUM is specified by:
           - code used by task: task does not know NUM and forwards it, i.e. check how many times something occured (e.g. number of writes)
           - task code: i.e. task performs a loop and returns loop size
              - in this case, returning chunk ARR of durations is better instead, if possible
     - "bytes"
        - group: "mixed"
        - return integer (bytes)
        - validation:
           - forbid: undefined, not integer, NaN, Infinity, negative
           - allow: 0, integer
        - normalization: none except one from group "mixed"
        - stat kind:
           - CONF.rate false: bytesRate
           - CONF.rate true: inverseBytesRate
     - "boolean"
        - group: "customOnly"
        - return BOOL
        - validation:
           - forbid: undefined, not BOOL
        - normalization:
           - set to ARR with two numbers: `true` count, total count
           - use BIGNUMs
        - stat kind:
           - CONF.rate false: percentageUnit
           - CONF.rate true: inverseCountUnit
     - "number"
        - group: "customOnly"
        - return float
        - validation:
           - forbid: undefined, not float, NaN, Infinity, negative
           - allow: 0, float
        - no normalization
        - stat kind:
           - CONF.rate false: countUnit
           - CONF.rate true: inverseCountUnit
  - when merging combinations from different results with same stepId but different unit:
     - filter out older combinations with incompatible units
     - compatible units:
        - any unit with group "durationOnly" or "mixed", even if different unit
        - units with group "customOnly", only if same unit
  - add comments:
     - why CONF.unit enum values are generic words, not specific units, is to make it clear that:
        - this does not set the specific unit being reported
        - unit `factor` happens at reporting time, not measuring time
     - why default is "auto": makes returning values from task functions opt-in:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
     - we allow measures being 0
        - including for any stats, including mean
        - this is useful for most units:
           - a task might randomly not perform an action
              - measures being 0 remove the need for manually looping until the action is performed
           - a task might have a real mean of 0
              - e.g. task which never writes (0B/s), task which always return false (0%), etc.

CONF.unit "boolean":
  - mean: `true` count / total count
     - mean 1 is handled like mean 0
        - including when checking whether to skip stdev-related stats
  - meanMin|Max: same as other units
  - quantiles|min|max|median|histogram: undefined
     - i.e. reporters show it the same way as not measured yet combinations
  - stdev|moe
     - estimated with a normal distribution approximation:
        - variance is mean * (1 - mean) instead
        - rest is same, including:
           - stdev = sqrt(variance)
           - standardError, tvalue, moe
     - add comments about statistical significance:
        - BOOLs require binomial confidence interval
        - normal distribution approximation is not as accurate as Wilson score interval
        - however:
           - most of the inaccuracy is when both percentage is very low|high and sample size is low
              - however CONF.precision threshold makes sample size always high enough that the difference is minimal
           - both `scale` computation and preview duration estimation rely on the assumption that rmoe is proportional to sqrt(loops)
  - rstdev|rmoe: divide by (1-mean) instead of mean if mean > 0.5
     - reason: inverting BOOL in task should not change them
  - CONF.precision thresholds are: 50-10-5-1% instead
     - i.e. 10 times higher
     - reason:
        - binomial distribution has much higher rstdev: minimum 100% (for mean 0.5), and higher as mean gets lower|higher
        - i.e. using same CONF.precision thresholds would be too slow
  - diff: same as other units
     - add comments:
        - why we do not make diff:
           - behave the same for mean and 1 - mean
              - e.g. diff(.2, .1) === diff(.8, .9)
              - i.e. inverting BOOL does not change it
              - reasons:
                 - not intuitive, e.g. diff(.8, .6) is -100%
                 - signedness would be ambiguous, e.g. diff(.8, .9) could be seen either as:
                    - positive: new value is higher
                    - negative: to make it behave like 1 - mean
           - absolute instead of relative to mean:
              - inconsistent with other units
              - inconsistent with rmoe and CONF.precision
              - harder for users when percentage is very low|high
           - show mean|diff as 0-50% true|false:
              - confusing when a results includes percentages both above and below 50%
              - generally confusing
        - users should invert BOOL or not so the mean is <.5, making the diff|limit more intuitive
           - e.g. .2 -> .1 is twice lower, i.e. -50%, but when inverted, .8 -> .9, i.e. +12.5%
  - steps with unit "boolean" are ignored by maxMeasures, both in isRemainingCombination() and in maxLoop computation

--------------------

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for beforeEach, main and afterEach
  - always use await on beforeAll|afterAll, i.e. allow both sync and async
  - remove task.async BOOL

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `dev` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - mean measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

--------------------

`spyd` history branch:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - directory is "{gitRoot}/history/results/"
        - git root lookup is using same logic as other places which look for it, i.e. CONF.cwd
     - automatic commits should be prefixed with `[skip ci]`
     - individual results:
        - at /history/results/YYYY-MM-DD--HH-MM-SS--BRANCH--{result.id}.json
           - encode|decode characters in BRANCH:
              - [[:alnum:]-_] left as is
              - any others like percent encoding but using x instead of %
                 - including . and /
                 - including x if followed by two [0-9a-f]
                 - a-f lowercase only
              - extract to own library
        - one immutable FILE.json per result
           - i.e. single OBJ
        - format is JSON
           - reason: fast
  - add comments:
     - pros of using a separate git branch:
        - instead of:
           - using regular files in codebase
           - git hash-object + git cat-file
        - does not pollute git log
           - especially in CI where a single commit might have many CI jobs and results
        - does not require git push --force on the codebase
        - automatically handle committing
        - semi-automatically handle git push|pull
           - can update in CI without requiring developers to pull all the time
        - allows multiple files
        - easier to make it skip CI
        - does not create many tags
        - ensures all branches can always be used for branches comparison with CONF.since
        - easy to understand
        - no risk of pruning
        - versioned
     - only store history with git at the moment
        - however possible approach in the future, if there is a need for it: CONF.history STR:
           - "git"
           - "PATH"
           - CRUD stores plugins
     - why concatenated results are not cached:
        - simpler, i.e. no need to check the file, nor update it after each `spyd bench --save`, `spyd sync`, etc.
        - performance benefits not big enough since results are partially loaded in batches
        - manual edits of files would require some way to invalidate cache, e.g. a `spyd prune` command
        - takes space on the cache directory, i.e. might require automatic|manual pruning
     - why using individual results instead of single file for all results:
        - fast to create new results
        - does not create git conflicts
        - concurrent safe
        - small file size impact in git history
        - easier to edit
        - allow loading only few results instead of all

Require git:
  - only for:
     - CONF.save
     - `sync` command
     - CONF.since branch|commit|tag|"parent"
        - i.e. optional for CONF.since resultId
  - require:
     - `git` binary is executable (i.e. `git --version` has exit code 0)
     - minimum version of `git`
     - there is a `.git` in `[.../]{CONF.cwd}`
  - add comments:
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository

Loading results in batches:
  - done batch by batch, with size incrementing exponentially (2**n)
     - instead of all at once
     - in order, using timestamp in result's filename
  - performed iteratively until can determine no more results needed, in three stages:
     - target delta can be resolved
     - since delta can be resolved
     - beforeSinceResults can be results
        - sinceResult + beforeSinceResults and sinceResult + afterSinceResults contain same combinations (after `select` filtering)
        - i.e. sinceResult merging would be same even if more previous results were loaded
  - ensure that delta resolution:
     - apply `select` filtering first
        - i.e. for each batch, files must be JSON loaded
        - including delta formats: NUM, git branch
     - only use results from correct branch:
        - target delta: current branch
        - sinceResult's branch (if git reference of result id) or current branch (otherwise)
     - for since delta: only use results earlier than target result
  - figuring out if delta is resolved
     - by format:
        - NUM -> >= NUM results
        - "first" -> right away with all results
        - id|timestamp|duration -> right away with result index
        - git branch -> latest result in that branch
           - i.e. use BRANCH in filename but not in `systems[0].git.branch`
        - git commit|tag:
           - find `git` commit author date then use it like in timestamp delta format
           - i.e. does not use `systems[0].git.tag|commit`

`spyd sync` command:
  - on `spyd` git branch:
     - git pull --rebase
        - if merge conflict:
           - try to automatically solve by including additions from both branches
              - add comment that this might happen with history/renamed_branches.yml
              - then retry `git pull --rebase`
           - otherwise fail
     - then git push
        - not run if we know locally there is nothing to push
  - stdin|stdout|stderr "inherit" on git pull|push
     - reasons:
        - allow entering passwords
        - show any error message such as: authentication, git hooks, network, etc.
        - provide with progress
     - not on other git commands
     - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
     - only if CONF.quiet false
  - add comments:
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull

Multiple branches reporting:
  - only report one branch + targetResult
     - branch is:
        - if CONF.since is a git reference or a result id, use its branch
           - branch is resolved:
              - git branch: use it
              - git tag|commit: use the same git logic used to get the current branch, but with a git tag|commit
              - result id: get branch from the result's filename
        - otherwise, current branch
           - instead of using env-ci, should use the best library to guess the branch from a specific commit reference
              - ensure cwd can be specified
              - could also just inline the git calls made by the library
           - if no branch found, use the `undefined` branch
     - filter out any results from the branch later than targetResult, before applying sinceDelta
        - using timestamp + branch in result's filename
        - reasons:
           - ensure benchmark's history remains same as new results are being added
           - ensure history is sorted by timestamp, including the target result
           - parent branch's newer results are likely to contain performance improvements that were not merged in the current branch yet when the result was taken
     - all results without a git branch are treated as if part of a single `undefined` branch
     - result's branch is persisted when the benchmark is saved
  - CONF.since "parent":
     - same as CONF.since "branch" with parent branch
        - i.e. last commit of parent branch, since this is the one the child branch will merge to
     - if no parent branch, like "first" in current branch
  - add comments:
     - problems:
        - hard to know which is parent's branch parent commit using only the benchmarks data:
           - need to use commit ids, which might have changed since benchmark was run due to rebasing
           - user intent is ambiguous: even if branch has not been rebased to parent yet, might intend to do so
        - parent branch's benchmarks' timestamps might interleave with current branch's
        - current branch might have been rebased 0|1|n times onto parent branch
           - parent and current branch's performance improvements are mixed and hard to distinguish
        - grouping results per branch and showing several branches leads to:
           - more visually complex time series
           - harder to understand history since each branch has its own progression
        - should be easy to understand|explain
        - should preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - should not be impacted by rebasing
        - allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
     - discarded solutions:
        - report all results without taking branches into account
        - report all results, grouped by branches
        - report all results of parent branches, grouped by branches
           - variant: only show earlier results
        - let user decide with a CONF.branch configuration property

Renaming branches:
  - on results load, should also get a list of branch renames:
     - do it using the current git reflog
  - use the list of branch renames:
     - to normalize all results branches to their latest names, including when:
        - resolving CONF.since's branch
        - reporting
     - the result's filename and contents also contains the branch name but this name might be old, which is ok since it is fixed by branch renames
  - persist branch renames:
     - reason: git reflog lasts only for 90 days by default
     - done during CONF.save
        - reason: otherwise, user might not expect having to run `spyd sync` again
     - when saving, should automatically create its own git commit
        - using same logic as git commits for results saving (including `[skip ci]`), but as a separate commit
           - reason: cleaner, and limit impact of merge conflicts
     - only append new branch renames, not ones already persisted
        - reason for append-only: limit potential for merge conflicts
     - only add renames for branches used in at least one result
     - persisted to history/renamed_branched.yml
        - OBJ_ARR: from 'BRANCH', to 'BRANCH2'
        - reason for YAML array: limit potential for merge conflicts
     - always loaded and merged with current git reflog, with lower priority

--------------------

Find ways to improve precision even more???

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information
  - in runners:
     - send: errorMessage STR, errorType STR
     - errorType:
        - default to "plugin"
        - unless "plugin": user error
        - parent process prepends a specific string based on the error type
           - i.e. runner does not need to
     - specific error types:
        - "ipcSerialization"
           - when serializing payload
           - if CONF.manual, parent handles it as a user error, i.e. a task returned a non-JSON-serializable value
           - otherwise, plugin error
        - "tasksLoad": tasks file load
        - "tasksSyntax": tasks file syntax error
        - "stepRun": when running a step function
        - "config": runnerConfig validation

CONF.debug BOOL
  - meant as bug report attachment, not meant for users to debug themselves
  - saved debug information to "{process.cwd()}/spyd_debug_logs.yml"
     - not printed to stdout
     - no way to configure location
     - saved at end once, using try/finally wrapping the programmatic entry points
        - not streamed, so it does not impact benchmark
  - does not change the logic otherwise
     - including reporting and previews
  - file is YAML:
     - using document stream
     - of OBJ:
        - event "EVENT"
        - event-specific properties
  - include:
     - envinfo
     - resolved config
     - task files
     - runner.versions
     - combinations
     - samples
        - including: duration spent, estimated time left, progress bar percentage
  - for all commands
  - interface is debugLog(debug, "EVENT", EVENT_OBJ)
     - debug is undefined if CONF.debug false, mutable ARR otherwise
  - add to GitHub issue templates

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

Fix --help CLI flag output: it has odd newlines breaks

Learn package 'simple-statistics' and|or 'jstat' and use it in spyd, where needed

Find performance bottlenecks and optimize them

--------------------

Add TypeScript support:
  - to:
     - spyd.ts
     - tasks.ts
  - export types of those too

Add output formats:
  - report.reportMarkdown()->PROMISE_STR
     - for CONF.output "*.md|markdown|mdown|mkd|mkdn" or "README|readme"
     - def: reportTerminal() return value in ``` block
     - no padding
     - allows two reporters with same output
        - concatenated like terminal formats, i.e. with newlines
     - footer: appended as Markdown
  - report.reportHtml()->PROMISE_STR
     - for CONF.output "*.html|htm"
     - file should be whole (i.e. have <html> tag)
     - reporter can save other files (imported by the main file) in the same directory or subdirectory
     - def: reportMarkdown() return value rendered to HTML
     - no padding
     - does not allow two reporters with same output
        - reasons:
           - hard to concatenate
           - can emulate concatenation using a parent HTML file
     - footer: inject to any element with id "spyd-footer"
  - report.reportSvg()->PROMISE_STR
     - for CONF.output "*.svg|png|jpg|jpeg|webp"
     - def: reportHtml() return value rendered to SVG
     - converts SVG to PNG|JPEG|WebP
     - no padding
     - does not allow two reporters with same output
     - footer: added with svg manipulation
  - add default value for reportTerminal(): using markdown|html|svg return as is

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for means
        - with full CLI width for slowest mean
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
        - abscissa:
           - only show start|end
           - format should be date-only if different days, time-only otherwise
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - stacked bar graph for multiple stages benchmarks
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

GitHub action

GitHub PRs integration

Add other runners:
  - HTTP
  - JSON-RPC
  - graphQL
  - gRPC
  - WebSocket
  - TCP
  - chrome (maybe using puppetter)
  - firefox (maybe using puppetter-firefox)
  - selenium
  - bash
  - go

SaaS:
  - reporting dashboard
  - files:
     - either connect to GitHub repository
     - or edit file with online IDE, saved in the server
  - perform benchmark in-browser
  - public URLs, for sharing

--------------------

Learn the whole Terminal section in edl
  - including finishing ANSI sequences and terminal emulators
  - use cli table library
     - ensure this is still responsive (by creating multiple table if too wide)
        - if no library does this, create own library on top of another

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Terminal-histogram:
  - separate to own repository
  - add features:
     - color themable (using terminal-theme)
     - left bar with percentages
     - can specify number of abscissa ticks
        - or number of columns per tick
        - stack labels, i.e. might need to stack deeper than one level
    - allow minimum ordinate to be either 0 or minimum value
    - labelling columns
    - custom unit for ordinate
    - when too many columns:
       - if labeled: break into several histograms
       - otherwise: extrapolate

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

--------------------

Manually try all features with each Node.js version

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Positioning (in documentation), in priority order:
  - whole benchmarking flow, not just measuring:
     - reporting (pretty, configurable, live preview, multiple formats, insertion, hardware/software info)
     - combinations (variations, inputs, systems, steps, selection)
     - comparing
     - history (including time series)
     - debugging (spyd dev, nice error messages)
     - sharing for others to run
     - testing (limits)
     - concurrency benchmarking
     - CI
     - GitHub PRs
  - simplicity
     - no library|APIs, only export functions
     - no need to specify duration nor number of loops
     - can work with no|minimal configuration, including on-the-fly
  - precision
     - high precision
     - configurable precision
     - report statistical significance (including for diffs)
     - not only average, also: min|max, stdev, histogram, quantiles
  - platform|language-agnostic
     - including CLI, Node.js, TypeScript
     - including comparing Node.js versions

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both means (for accuracy) and stdev (for precision)

--------------------

Add repo of spyd benchmarks:
  - contributors can add any
  - only for:
     - JavaScript
     - core Node.js or JavaScript, no modules
  - each benchmark is a directory with a single benchmark
     - optional spys.yml
  - README shows all results
     - as run in CI
     - each shows: title, tasks file content, reported result
     - a hardcoded list is maintained for sorting
     - created by a build task
  - binary for users to run any of the benchmarks on their machine
     - including with npx

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2020.stateofjs.com/en-US/resources/
  - https://javascriptkicks.com/submit

Ideas for articles about benchmarking:
  - choice between accuracy vs precision
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes

------------------------

stdev threshold:
  - main problem is preventing too low threshold (making run stop too early) not too high (making run slower)
  - potential problems:
     - sample stdev is much lower than real stdev
     - sample stdev is 0 (always the same)
        - only a problem due to rounding either:
           - due to float epsilon (2e-16)
           - due to integer (1)
        - need to take into account real stdev might be 0
        - including when mean is 0
           - or 0|1 with unit "boolean"
  - findings:
     - current mean estimation with t-test: t-value includes stdev uncertainty
     - variance confidence interval:
        - chi-squared test:
           - variance * (n-1) / chi-value
           - i.e. stdev * Math.sqrt(n - 1) / Math.sqrt(chi-value)
           - chi-value:
              - degrees of freedom: n - 1
              - asymetric (biggest on the bigger side)
              - must use half on each side, e.g. 95% -> 0.025|0.975
              - Math.sqrt(n - 1) / Math.sqrt(chi-value) for n 2-... and 50-51:
                 - 0.025 (inverted): 2.24, 1.92, 1.76, 1.67, 1.60, 1.55, ..., 1.1971, 1.1951
                 - 0.975: 31.9, 6.26, 3.73, 2.87, 2.45, 2.20, 2.04, 1.91, 1.83, 1.75, 1.69, ..., 1.2461, 1.2430
  - use different significance than 95%???
     - values:
        - 95%: 0% longer runs in average, and rstdev is multiplied by (for 2,3,4,5 samples): 42, 4.8, 2.6, 2
        - 99%: 40% longer and: 260, 13, 5.4, 3.7
        - 99.5%: 75% longer and: 10000, 26, 8.9, 5.5
        - 99.9%: 150% longer and: 26000, 130, 27, 13
     - make significance depend on sample size, since magnitude of error is bigger for smaller sample size???
  - with unit "auto|duration": hardcoded 5 is not great??? Use better formula???
  - with unit "boolean": what should it be???
  - with unit "count|bytes|number" and small integer measures???
     - proportional to how small integers are???
     - deciding the threshold:
        - formula for:
           - given n measures
           - from 0 to max
           - normally distributed
           - stdev s without rounding
           - stdev t with integer nearest rounding
           - ratio r = s/t
           - threshold u
           - what's the probability of r <= u?
        - then compute the minimum n for 95% probability for a given r
        - then decide on the best r, and use the minimum n as threshold
     - add comments:
        - applies even with CONF.rate true since the ratio is multiplied by the returned integer
           - unless the integer is meant to be constant, but we cannot know this from a series of samples, since they might appear to be constant but not really be

Problems:
  - with a somewhat accurate stdev and mean, 95% of times it will be within moe
     - moe is as narrow as defined by CONF.precision
     - but 5% can still be outside
  - the higher a mean outlier is, the more precise it looks
     - because it decreases rmoe, for a given moe
     - amplitude:
        - proportional to (sampleMean / realMean) / sqrt(n)
        - in 97.5% times, max 1.96 / sqrt(n)
        - but 2.5% times includes much higher values
           - following normal distribution
     - mitigated by higher sample size due to:
        - outliers removal
        - each outlier having less weight in the total mean
           - proportional to sqrt(n) (like moe)
     - this problem is specific to using rmoe as key logic
        - moe alone does not have this problem, i.e. does not compensate for it

Solutions:
  - increase significance rate from 95%???
  - have outliers removal start earlier (while keeping overall removal ratio)???
