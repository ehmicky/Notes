
                                  ┏━━━━━━━━━━━━━━━━┓
                                  ┃   STATISTICS   ┃
                                  ┗━━━━━━━━━━━━━━━━┛

Ma définition des statistiques :
  - statistiques inférentielles/inductive :
      -> étude de l'imprévisibilité
        -  science attribuant des probablités aussi fiables que possibles à des phénomènes qu'un individu ou groupe n'arrive pas à prédire
      -> manière de rationaliser/chiffrer et rendre prévisible un monde vivant et aléatoire
      -> par définition, jamais de 100% de chance, sinon pas besoin de statistiques
  - statistiques descriptives/explanatory :
      -> étude de la variabilité
        - différentes valeures (i.e. étude de la différence entre les valeurs) que peut avoir une population pour un factor donné

POSITIVE/NEGATIVE ==>             #"Condition positive|negative" (CP|CN):
                                  #  - number of positive|negative possible solutions
                                  #  - "prevalence": CP/(CP+CN)
                                  #"Predicted condition positive|negative":
                                  #  - number of positive|negative results
                                  #  - can be:
                                  #     - "true positive" (TP)/"statistical power"
                                  #     - "true negative" (TN)
                                  #     - "false positive" (FP)/"type I error"/"false alarm"
                                  #     - "false negative" (FN)/"type II error"/"miss"
                                  #Predicted condition:
                                  #  - % of positives|negatives results that are true|false
                                  #  - ratios:
                                  #     - "precision": TP/(TP+FP)
                                  #        - also called "positive predictive value" (PPV)
                                  #        - % of positives that are true
                                  #        - inverse: "false discovery rate" (FDR), FP/(TP+FP)
                                  #     - "negative predictive value" (NPV): TN/(TN+FN)
                                  #        - % of negatives that are true
                                  #        - inverse: "false omission rate" (FOR), FN/(TN+FN)
                                  #True condition:
                                  #  - % of positive|negatives results among all possible solutions
                                  #  - ratios:
                                  #     - "sensitivy": TP/CP
                                  #        - % of positive results among all possible solutions
                                  #        - also called "recall"/"probability of detection"/
                                  #          "true positive rate" (TPR)/"statistical power"
                                  #        - inverse: "miss rate"/"false negative rate" (FNR)/"false alarm ratio" / β, FN/CP
                                  #     - "specificity": TN/CN
                                  #        - % of negative results among all possible solutions
                                  #        - also called "true negative rate" (TNR)
                                  #        - inverse: "fall-out"/"false positive rate" (FPR) / "significance level" / α, FP/CN
                                  #     - "positive likelihood ratio" (LR+): TPR/FPR, i.e. (TP*CN)/(FP*CP)
                                  #     - "negative likelihood ratio" (LR-): FNR/TNR, i.e. (FN*CN)/(TN*CP)
                                  #     - "diagnostic odds ratio" (DOR): LR+/LR-, i.e. (TP*TN)/(FP*FN)
                                  #     - "F₁ score": 2/(1/PPV + 1/TPR), i.e. combine PPV and TPR

Statistiques :
  - inferentiale/inductive :
    - utilise probablités
    - plus par rapport à la sampling distribution du sample que la distribution du sample
    - implique des conclusions et hypothèses
    - exemples :
      - statistical independence, conditional probablity
      - souvent font une estimation, avec un interval de confidence
  - descriptives :
    - description d'une collection of data, de manière chiffrée ou visuelle (plots)
    - plus par rapport à la distribution du sample que la sampling distribution du sample
    - summary statistics :
      - description synthétique
    - types :
      - location :
        - measures of central tendency / "expectation":
          - arithmetic mean : ∑xₙ / n, noté ̄x (ou μ pour une population)
          - median :
            - x tel que P(xₙ <= x) = 0.5
            - si pair, prend la moyenne des deux éléments du milieu
          - mode :
            - x tel que P(P(xₙ) > P(x)) = 0
            - most occuring value
          - tail :
            - least occuring region value de la distribution
          - geometric mean :
            - ₙ√ ∏S (taille n)
            - utilisé à la place d'arithmetic mean pour comparer des sets avec des ranges différents mais pondérés pareil
            - ex :
              - set dont premier nombre est entre 0 et 1, et deuxième entre 0 et 100
              - comparaison de tels sets, mais 1er et 2nd nombre doivent avoir même importance
                - avec arithmetic mean, 1er aurait 100 fois d'importance
          - harmonic mean :
            - n / ∑ xₙ⁻¹
          - weighted mean :
            - par exemple notes avec coefficients
          - mid-range :
            - (max+min)/2
            - rarement utilisé, très peu robuste ni fiable
          - mid-hinge :
            - (Q₃+Q₁)/2
            - plus robuste que mid-range
          - trimean (TM) :
            - (Q₁+2*median+Q₃)/4
      - Spread :
        - [average/mean] [absolute] deviation :
          - ( ∑sₙ∈ S dev(xₙ) ) / (n-1)
            - où dev(xₙ) = |μ(xₙ) - f(X)|
              - c'est la [non-average] absolute deviation
              - f(x) mesure la central tendency. Peut être :
                - mean   -> deviation from the mean
                - median -> deviation from the median
                - mode   -> deviation from the mode
            - mean deviation <= standard deviation
            - comme variance pour division par n
            - attention [mean] [absolute] deviation from the median != median absolute deviation
        - median absolute deviation (MAD) :
          - comme mean deviation, sauf :
            - qu'effectue une median, non une somme/mean
            - utilise toujours mean comme dev()
        - variance :
          - ( ∑sₙ∈ S dev(xₙ)² ) / (n-1)
            - où dev est la mean deviation
            - division par n si S est la population entière :
              - n s'approchant d'∞, /(n-1) ou /n ne fait plus de différence
              - mais ̊σ² pour évaluer ̅σ² est un meilleur estimator si (n-1) que si n
              - donc n-1 pour la sample variance, et n pour la population entière
          - souvent noté σ²
            - parfois noté s² si σ²₁, par opposition à ̅σ²
          - σ²(X)         = (μ(X²) - μ(X)²)*(n/(n-1))
                          = (∑X² - (∑X)²/n)/(n-1)
            σ²(X)*n*(n-1) = n∑X² - (∑X)²
        - standard deviation :
          - √variance
          - souvent noté σ
          - moins utilise si fort skewness
          - Chebyshev's inequality :
            - P(dev(E(xₙ), μ) >= t) <= σ²/t²
              - soit : si P a une variance x, alors la possibilité qu'un xₙ ait un écart à la moyenne d'au moins t est inférieur à x/t²
            - autre formulation : P(dev(E(xₙ), μ) >= tσ ) <= 1/t²
              - soit : la possibilité d'un écart d'au moins t fois la standard deviation est inférieur à 1/t² :
                - écart >= 1.4σ : <= 50%
                - écart >= 2σ   : <= 25%
                - écart >= 4σ   : <= 6.25%
        - coefficient of variation (CV)/united risk :
          - σ/μ
          - "pourcentage" de σ par rapport à moyenne -> "dimensionless" quantity (cad ratio,pourcentage)
          - relative standard deviation (RSD) :
            - |CV|
        - index/coefficient of dispersion/variance-to-mean ratio(VMR) :
          - σ²/μ
          - relative variance :
            - |VMR|
        - Quartiles :
          - valeurs séparant en quatre S
          - IQR (InterQuartile Range) / H-spread : Range entre Q₁ et Q₃
          - lower-hinge et upper-hinge : autre nom de Q₁ et Q₃
          - Tukey's five numbers summary : { min, Q₁, Q², Q₃, max }
        - quantile function / inverse distribution function :
          - pour un pourcentage p "percentile" donné, donne la valeur x tel que p% des valeurs sont derrière x
          - par exemple f(0.25) donne Q₁
          - qdf = cdf⁻¹
            - par ex., si cdf = x², qdf = √x
        - Range :
          - longueur du plus petit interval comprenant l'ensemble de S
          - toujours >= 2σ
      - Shape :
        - skewness :
          - asymetry d'une distribution
          - si négatif tail est plus grande à gauche, sinon contraire
            - median - mean ou mode - mean peut donner une idée
        - kurtosis :
          - peackness d'une distribution
            - si low peak, fat tails
          - type :
            - mesokurtic : 0 (ex: normal distribution)
            - leptokurtic : > 0 (ex: Cauchy, Student's t, Poisson, exponential)
            - platykurtic : < 0

Standard score :
  - si x ~ Dist(̅μ,̅σ), standard score(x) = (x - ̅μ)/̅σ
  - si Dist() est N(), "z-score"
    - t() : "T-score"
    - χ²() : "Chi-score"
    - F() : "F-score"

Comparaisons de deux Dist() :
  - raisons :
    - entre une Dist() de data et une théorique : D₁ vs D₀
      - ex. voir si D₁ ~ N()
    - ou entre deux Dist() de data : D₁ vs D₂
  - PP Plot :
    - soit cdf(D₁)~cdf(D₂)
    - suit ligne y = x si cdf(D₁) = cdf(D₂)
  - QQ Plot :
    - soit quantile.f(D₁)~quantile.f(D₂)
    - il est typique d'avoir les valeurs extrêmes moins sur la ligne

Probability simple (Bernoulli) :
  - essai isolé :
    - Chance de succès : p (en %, par exemple 0.5)
    - Chance de non-succès : ~p, soit 1 - p
  - n tentatives :
    - Chance de que des succès    : pⁿ
    - Chance d'aucun succès       : (~p)ⁿ
    - Chance d'au moins un succès : ~((~p)ⁿ)
    - Chance d'au moins un échec  : ~(pⁿ)
    - Chance de m succès/échecs parmi n : cf Binomial

Experiment :
  - Probability space est un triplet (Ω,𝓕,P) :
    - Sample space, noté S ou Ω, est ensemble des simple events possibles
      - une random variable est une fonction qui assigne un outcome à chaque simple event : X: Ω -> S, où S est appelé "state
        space"
        - noté X, Y, ...
        - peut être :
          - discrète et infinie : notamment si S == N
          - discrète et finie   : notamment si S est factorial (subset de N)
          - continue et infinie : notamment si S == R
          - continue et finie   : notamment si S est subset de R
      - les simple events sont notés ωₙ
      - plusieurs random variables peuvent assignées au même Ω
    - Le σ-algebra 𝓕 comprend ensemble des events Eₙ possibles à partir des simples events
      - events sont noté Eₙ ou A,B,C,...
      - un Eₙ est une combinaison de simple events, 2^Ω
      - 𝓕 \ E : complementary event de E, noté Ē
    - Probability function, P :
      - fonction assignant prob. à chaque Eₙ
      - donc P: 𝓕 -> [0,1]
      - si :
        - continu: Probability density function, pdf
        - discret:
          - Probability mass function, pmf, ou frequency function
          - si fini, Categorical distribution
      - P(Ω) = 1
    - ex., one-coin flip :
      - Ω = {H,T}
        𝓕 = 2^Ω = { ∅, {H}, {T}, {H,T} }
        P(∅) = 0, P({H}) = 0.5, P({T}) = 0.5, P({H,T}) = 1
  - représentation graphique :
    - ex. pour deux random variables X et Y :
      - X et Y sont axes x et z
      - P() est axe y, assigne prob. aux variables X et Y
      - chaque simple event (Ω) peut être représenté comme une zone sur le place (X,Y) comprenant les outcomes xₙ et yₙ
        produits par ce simple event
      - 𝓕 est ensemble des combinaisons de zones
  - Probability space (et donc events et variables) est toujours "random", au sens statistique du terme :
    - cad aucun P(Eₙ) = 1
    - c'est-à-dire imprévisible de manière certaine par le sujet
    - si 100% de chance, pas de probablité ni de statistiques sur ce phénomène.

Events probablities :
  - P(𝓕) = 1, donc :
    - 0 <= P(E) <= 1
    - P(Ē) = 1 - P(E)
  - si E₁ ⊆ E₂, alors P(E₁) <= P(E₂)
  - Joint probablity :
    - probablity qu'un résultat donné satisfasse à la fois E₁ et E₂
    - noté E₁ ∩ E₂
  - Conditional probablity :
    - probablité, lors d'un event E₁, que celui-ci soit aussi un event E₂
    - noté P(E₂|E₁), soit P(E₁ ∩ E₂) / P(E₁) (pour P(E₁) > 0)
    - par conséquent (multiplication axiom) : P(E₁ ∩ E₂) = P(E₂|E₁) * P(E₁)
    - Bayes' theorem : P(E₁|E₂) = P(E₂|E₁) * P(E₁) / P(E₂) (pour P(E₂) > 0)
  - Law of total probability :
    - Si E₁...Eₙ sont des events disjoints et que leur union est Ω
    - alors P(A) = ∑ P(A|Eₙ) * P(Eₙ)
    - Bayes' rule (raisonnement inverse) :
      - P(Eₖ|A) = P(A|Eₖ) * P(Eₖ) / ∑ P(A|Eₙ)*P(Eₙ)
  - P(E₁) + P(E₂) = P(E₁ ∪ E₂) + P(E₁ ∩ E₂)
    - P(E₁ ∪ E₂) = P(E₁) + P(E₂) - P(E₁ ∩ E₂)
    - si E₁ et E₂ disjoints, P(E₁) + P(E₂) = P(E₁ ∪ E₂)
  - Statical independence :
    - fait que si l'on sait qu'un résultat satisfait E₁, cela ne modifie pas chances que cela satisfasse E₂
    - donc P(E₂|E₁) = P(E₂), soit "probablité de E₂ en sachant que E₁ intervient" = "probablité de E₂ sans savoir si E₁ intervient"
      - est symétrique car implique aussi que P(E₁|E₂) = P(E₁)
      - implique aussi que P(E₁ ∩ E₂) = P(E₁) * P(E₂) (définition à préférer)
      - si :
         - P(E₁ ∩ E₂) < P(E₁) * P(E₂), alors occurence d'E₁ ou E₂ diminue chance d'occurence de l'autre
            - cad P(E₁|E₂) > P(E₁), et P(E₂|E₁) > P(E₂)
         - inverse
    - "iid" (idependant and identically distributed) variables :
      - signifient des variates :
        - statistically independent entre elles (génération d'une ne modifie pas prob. des autres)
        - ayant la même distribution
      - en général, PRNG, PRNS, et théorème statistiques supposent des iid. variables
  - Multiple independence :
    - indépendance d'un event par rapport à l'occurence non d'un event, mais de plusieurs events :
      - P(E₀|E₁ ∩ E₂) = P(E₀)
    - si un seul event, "pairwise independence"
    - Si tous events sont pairwise independents, ne garantie pas les multiple independences
    - mutual independence :
      - si P(E₁ ∩ ... ∩ Eₙ) = P(E₁) * ... * P(Eₙ)
        - ainsi que pour tout regroupement dans E₁...Eₙ
      - donc pour chaque Eₙ, P(Eₙ|E... ∩ E...) = P(Eₙ)
  - P(E₁) =, < ou > P(E₂) équivaut à P(E₁|E₂) =, < ou > P(E₂|E₁)
      - cad : si probabilité de E₁ est faible et que E₂ est fort, il est plus courant pour un individu de E₁ de faire E₂, que
              l'inverse.
              Si probabilité identiques, aussi courant pour l'un que pour l'autre.
  - Selection bias :
    - considérer que P(E₁|E₂) = P(E₁), et oublier impact de condition E₂
    - par exemple, personne ayant souvent E₁ car E₂ arrive souvent pour elle, et que P(E₁|E₂) est élevé, pourtant P(E₁) peut
      etre faible de manière générale
        - ex: demander assistance Linux : E₁ = 0.01%, avoir barbe de geek : E₂ = 5%, demander assistance Linux quand barbe de
          geek : 20%. Donc personne avec barbe de geek pense que demander assistance Linux = 20%.
  - autre fallacy :
    - penser que P(E₁|E₂) = P(E₂|E₁)
       - cela n'est vrai que si (et est vrai quand) P(E₁) = P(E₂) (cf dessus)
    - ex: hard drug users smoke majiruana, so (wrong ->) marijuana use hard drug
       - en effet marijuana population est bien plus grande que hard drug users

Causalité et correlation :
  - hétérogénéité :
    - plusieurs facteurs possibles : P(X|Y,Z), "confounding factors"
    - il faut pouvoir les dissocier pour étudier P(X|Y)
  - correlation : P(X|Y)/P(X), soit P(X ∩ Y)/(P(X)*P(Y))
    - si correlation = 1, alors statistical independence
  - causalité et bien plus compliquée, et implique plus que correlation

Joint probability distribution :
  - comme une probability distribution normale, sauf que pour réunion de plusieurs random variables
    - donc P(X ∩ Y) pour bivariate
  - lié à un même Ω : s'il y a un xₙ, il y a aussi un yₙ
  - appelé aussi multivariate distribution (bivariate si 2)
  - graphiquement, un graph de points (xₙ,yₙ), avec éventuellement axe z nombre d'occurences
    - par ex. graphiquement pour X et Y, P() en ordonnée, et X et Y autres axes
  - ne pas confondre avec des P() combinant avec d'autres opérateurs que ∩ :
    - ex. : P(X ∩ Y) != P(X + Y)
  - P(X ∩ Y) dépend de P(X|Y) ou P(Y|X) :
    - P(X ∩ Y) = P(X) * P(Y|X)
               = P(Y) * P(X|Y)
    - P(X ∩ Y ∩ Z) = P(X) * P(Y|X) * P(Z|Y,X) ("chain rule of probability")
    - si X et Y indépendants (P(Y|X) = P(Y) et P(X|Y) = P(X)), alors P(X ∩ Y) = P(X) * P(Y)
    - ∑P(X ∩ Y) = 1
  - On peut penser à P(X ∩ Y) comme une matrice, avec X et Y comme dimensions, et P(X) et P(Y) des vecteurs
  - si continu, appelé joint distribution fonction (jdf)

Conditional probability distribution :
  - comme jdf, sauf que non P(X ∩ Y) mais P(Y|X)
  - P(Y|X) n'est pas la même chose que P(X|Y)
  - si P(Y|X) = X, alors X et Y indépendants
    - graphiquement, axe x ne modifie pas axe y,z
  - P(Y|X) = P(X ∩ Y) / P(X)
  - si continu, appelé conditional distribution fonction

Terminology :
  - data/outcomes :
    - ensemble des données récoltées sur un sample space
  - Parameter :
    - une statistique est déduite d'un sample space à partir de data, un parameter d'une population
    - une statistique estime un parameter, de manière plus ou moins fiable

Randomness :
  - variable désigne le x d'un graph (abscisse), pas seulement un event donné
    - contrairement à une variable classique, qui prend une valeur f(x) donné pour une x donné
      - une random/schochastic variable a un ensemble de f(x) avec chacun une P(f(x))
      - une random variable est donc toute variable associée à une probability distribution
    - donc, pour qu'un variable soit random, il faut juste qu'il n'y ait aucun P() == 1
    - c'est pourquoi on peut par exemple parler de "Cauchy random variable", etc.
    - une random variable X ayant une pdf f() est notée X ~ f()
  - difference entre entropy et distribution
    - les P() de chaque xₙ peuvent différer : une distribution n'est pas forcément uniforme
    - ex : lancer de deux dés est random, mais n'a pas une distribution uniforme
    - différence :
      - distribution est l'axe y, la P()
      - entropy est l'axe x, le fait qu'il y ait plusieurs xₙ randomly possibles
  - random variate :
    - particular outcome x d'une random variable X, généré suivant la probability distribution
    - génération est appelé PRNS (pseudo-random number sampling)
      - contrairement à PRNG, est soumis à une probability distribution pas forcément uniforme
      - peut être généré via l'inverse distribution avec comme argument U(0,1) :
        - ex : rnorm(n) -> qnorm(runif(n))
      - mais il existe des méthodes plus efficientes implémentées peut-être par rnorm, etc.
  - différence entre résultat d'une experiment passé et prévision :
    - relative frequency vs probabilité
    - "observed values" vs random variates
    - discret vs discret ou continu
    - cependant peut utiliser même raisonnement

Philosophie de randomness :
  - vues :
    - Bayesian/classical (Bayes/Laplace/Bernoulli, XVIIème) :
      - experiment cherche à prouver une prob. théorique
      - probabilité d'1/n est lorsque n différents mutually exclusive outcomes d'une experiment sont possibles, et que aucun
        n'est plus favorisé que l'autre (principe d'indifférence)
      - pb : ne marche que pour variable avec une prob. distr. uniforme
      - types :
        - objectiviste/physical:
          - unpredictable est subjectif, random objectif
          - probabilité est une tendance réelle
        - subjectiviste/evidential:
          - random est l'absence de connaissance
          - probabilité est une confiance subjective donnée
    - fréquentistes (Neyman/Pearson/Venn/Fischer):
      - experiment cherche à déterminer une prob. inconnue
      - probability est limit (n=∞) de la fréquence relative
    - propensistes (Popper/Miller)
 - chaotic vs quantique :
   - vue non-déterministe :
     - quantique est random
     - système chaotique est déterministe, mais affecté par un nombre énorme de petits facteurs (butterfly effect), qu'il est
       infeasible pour un être humain de connaître
   - vue déterministe :
     - pareil pour chaos
     - mais quantique est aussi chaotique, seulement nous ignorons les causes pour l'état actuel des sciences

Expected values :
  - expected value/mean, E(E₁) ou X(E₁) :
    - si simple event, outcome de X lorsque E₁
    - sinon, pondéré ensemble des simple events comprenant E₁ par leur probabilité :
      - E(E₁) = (∑Eₙ∈ E₁ E(Eₙ) * P(Eₙ) ) / P(E₁)
      - pour l'ensemble de 𝓕 :
        - E(X) = ∑x∈ R x*P(X = x)

Distribution :
  - pdf, cf plus haut
  - cumulative distribution function (cdf) :
    - non chance que x = xₙ, mais que x >= xₙ
    - il s'agit donc de ∫ de la distribution
  - types :
    - discrete, continue ou mix entre disrete et continuous distribution :
      - par ex., si pile, E(X) = 1, si face, E(X) = random entre 0 et 1

Robustness :
  - outliers :
    - xₙ étant très éloignée du reste des xₙ
    - doivent être écartés car fausse statistiques
  - cause :
    - erreur de mesure
    - distribution "heavy-tailed"
  - statistiques les éloignant sont dites "robust"/resistant :
    - breakdown point :
      - nombre de xₙ outliers pouvant être présents sans fausser calcul
      - maximum 0.5
      - mean a breakdown point de 0, median 0.5 => median est bien plus robuste
      - median absolute deviation plus robuste que standard deviation
    - range a breakdown point de 0, IQR 0.25
    - standard deviation est pas robuste, expected median variation si
  - truncated mean :
    - discard une part du sample avant de faire le mean
      - souvent un même pourcentage sur la tail basse et haute
      - interquartile mean (IQM) : ne garde que l'IQR
    - ensuite, il faut remultiplier par 1 / pourcentage perdu
      - ex : par 2 pour l'IQM
  - conflit entre :
    - + precision, mais - signifiance :
      - si pas assez robuste, résultats faux
      - mais si trop d'outliers, résultat proche de la normalité pour le phénomène, et pas de prédiction particulière
    - + precision, mais - accuracy :
      - estimator robuste vs estimator optimal

Plots :
  - point plot :
    - pour indiquer valeur discrète d'un factor donné
  - bar plot :
    - pour indiquer quantité/compte d'un factor donné
  - histogramme :
    - comme bar plot, mais en utilisant non des valeurs discrètes mais des ranges (pour valeurs real, ou un grand nombre de
      valeurs discrètes)
  - box plot :
    - centre est Q₁-median-Q₃
    - extrémités (appelée "whiskers") peut être :
      - minimum et maximum, dans le range :
        - tout
        - [Q₁-1.5*IQR, Q₃+1.5*IQR]
          - 1.5*IQR = 2σ pour Normal, soit range de 95% des résultats
        - [Q₁-σ, Q₃+σ]
      - premier et dernier décile
    - parfois outliers (events en dehors des extrémités) sont indiqués par des points

Pondération pour random variable :
  - il faut :
    - pondérer chaque Eₙ par P(Eₙ) (les E plus probables ont plus de poids)
      - le tout divisé par ∑ P(Eₙ) si pmf et ∫ P(Eₙ) si pdf (si sur tout S, alors division par 1)
    - ne pas diviser par nombre d'events (déjà mis en moyenne par multiplication par P(Eₙ))
  - ex : σ² = ∑/∫ dev(Eₙ)² * P(Eₙ)

pdf et pmf :
  - P(xₙ) :
    - Pour une pmf, probabilité de xₙ
    - Pour une pdf, P(xₙ) est la densité, non la probabilité. Ne signifie pas probabilité de xₙ (toujour infinitésimale)
      - signifie : densité moyenne sur 1 unité x = probabilité de cet unité, densité sur 1/2 unité = prob.unité*2, etc.
      - conséquence :
        - permet de faire intégrale pour cdf
          - la densité P(x) dépend de range de x, de sorte que ∫ P(X) == 1
          - e.g., P(x) de U(0,2) est 0.5 et U(0,4) est 0.25
        - peut comparer pdf et pmf, si on prend que valeur discrète de pdf
          - ex: pdf de N(np,np(1-p)) approxime B(n,p)
        - pour une cdf et cmf, il faut rajouter 0.5 à la valeur discrète de la cdf ("continuity correction"), car sinon
          dernière valeur ne prend pas l'unité entière autour d'elle mais que la moitié.
  - probabilité cumulée Pc(xₙ) :
    - Pour une pmf, ∑min(S),xₙ P()
    - Pour une pdf, ∫min(S),xₙ P()
  - P(Eₙ)
    - Pc(max(Eₙ)) - Pc(min(Eₙ))

Erreur lors d'inférence :
  - 2 stats correlées n'implique pas relation de causalité.
  - études fondées à des fins commerciales
  - manière incorrecte de présenter graphs ou data de manière à appuyer un discours
  - "confounding" : difficulté de séparer facteurs les uns des autres

Measures :
  - variables sont ce qui est mesuré dans le sample
  - types :
    - quantitatif/numeric
      - discrète (integral)
      - continue (real)
    - qualitatif/factor/category
      - on manipule alors leur nombre d'occurences/frequency
  - un individu d'un sample est un "sujet"
  - un ensemble de data pour un ensemble de variables sur un sujet est une "observation"
  - accurate vs precise :
    - accuracy :
      - différence entre valeur estimée et valeur réelle
      - graphiquement, différence entre "centre" de la pdf de l'estimation et valeur réelle
    - precision :
      - différence entre valeurs estimées sur plusieurs experiments identiques
      - aussi appelé degré de reproductibility(instruments différents)/répétabilité(mêmes instruments)
      - graphiquement, étendue de SampleMean
    - également tiers : measurement resolution
      - différence minimale entre deux estimation

Distributions :
  - parameters :
    - types :
      - shape : change la shape
      - scale : change le spread
        - rate : change le spread, de manière inverse (1/scale)
      - location : change la mean
  - notations :
    - Distribution(Var...)
    - X ~ Distribution(Var...) signifie random variable X ayant une distribution Distribution
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Distribution | Binomial      | Poisson            | Uniform cont.| Exponential| Neg.Binomial | Normal/Gaussian/Z            |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Type         | Discrète      | Discrète           | Continue     | Continue   | Discrète     | Continue                     |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Aspect       | Bell-curve    | Bell-curve         | Rectangle    | Descend    | Comme B()    | Bell-curve                   |
|              | sauf n ou p   | Ecrasé à gauche    |              | rapidement |              |                              |
|              | petit.        | si λ faible.       |              | puis longue|              |                              |
|              |               | A droite tail ∞    |              | tail       |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Notation     | B(n,p)        | Pois(λ)            | U(a,b)       |            | NB(r,p)      | N(μ,σ²)                      |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Paramètres   | n: nb essais  | λ: mean            | a: min       | λ: exp rate| r: nb de suc.| μ: mean                      |
|              | p: % de succès| (n:[0,∞],p:[0,1])  | b: max       |            | p: % échecs  | σ²                           |
|              |               |                    |              |            |              | Standard normal : N(0,1)     |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| x            | nb de succès  | nb de succès       |              | Temps      | nb d'échecs  |                              |
|              | (essais sont  |                    |              |            | avant que r  |                              |
|              |  indépendants)|                    |              |            | succès arriv |                              |
|              |               |                    |              |            | (essais sont |                              |
|              |               |                    |              |            | indépendants)|                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| range        | [0,n]         | [0,∞]              | [a,b]        | [0,∞]      | [0,∞]        | R                            |
| mean         | n*p           | λ                  | (a+b)/2      | 1/λ        | p*r/(1-p)    | μ                            |
| median       | ⎣n*p⎦         | ⎣λ + 1/3 - 1/50λ⎦  | (a+b)/2      | ln(2)/λ    |              | μ                            |
| variance     | np(1-p)       | λ                  | (b-a)²/12    | 1/λ²       | p*r/(1-p)²   | σ²                           |
| mode         |               |                    | a            |            |              | μ                            |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| P(x)         | (n) * p^x     | λ^x / x! / ℮^λ     | 1/(b-a)      | λ℮^(-λx)   | (x+r-1) *    | Gaussian function:           |
| (density f())| (x)           |                    | si x∈ [a,b]  | (P(0) = λ) | (  x  )      | ℮^(-(x-μ)²/2σ²) / σ√(2π)     |
|              | * (1-p)^(n-x) | ℮^λ et λ sont des  | 0 sinon      |            | (1-p)^r * p^x| ou ϕ((x-μ)/σ)/σ              |
|              |               | scaling factor.    |              | λ sont des |              | Standard normal ϕ/φ:         |
|              |               | L'essence est      |              | scaling    |              | ℮^(-x²/2) / √(2π)            |
|              |               | dans ℮^x/x!        |              | factor.    |              | Explication :                |
|              |               |                    |              | Essence est|              |  n^-x² : bell curve autour   |
|              |               |                    |              | dans ℮^-x  |              |          de [-2,2]           |
|              |               |                    |              |            |              |  /2    : ramène à [-1,1]     |
|              |               |                    |              |            |              |  /√2π,℮: pour que ∫P() = 1   |
|              |               |                    |              |            |              |  x-μ   : ramène à [μ-1,μ+1]  |
|              |               |                    |              |            |              |  σ     : ramène à [μ-σ,μ+σ]  |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Pc(x)        |               |                    | (x-a)/(b-a)  | 1 - ℮^(-λx)|              |                              |
|              |               |                    | si x∈ [a,b]  |            |              |                              |
|              |               |                    | 0 si x < a   |            |              |                              |
|              |               |                    | 1 si x > b   |            |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Examples     | Pile ou face  | Nb d'évén. sur une | Tout nombre  | Temps entre| Nb de pile   | ̊μ pour toute ̅s (si σ finite).|
|              | successifs    | période donnée, où:| entre a et b | 2 Pois(λ)  | avant de     | Donc dist. de beaucoup de    |
|              | (p = 0.5)     |  - p et n sont     | avec chances | events ou  | faire r faces| measurements.                |
|              | Oui ou non à  |    inconnus, n     | égales,      | entre 1    |              | Permet également d'approximer|
|              | un sondage    |    très grand, et  | continu.     | instant et |              | autre distributions.         |
|              |               |    p très faible   |              | 1 Pois(λ)  |              |                              |
|              |               |  - mais average    |              | event.     |              |                              |
|              |               |    rate λ connu    |              | Temps entre|              |                              |
|              |               | Equivaut à         |              | deux coups |              |                              |
|              |               | B(n,λ/p) dans ces  |              | de tel.    |              |                              |
|              |               | conditions.        |              | Période    |              |                              |
|              |               | Coups de tel. sur  |              | fonctionmt |              |                              |
|              |               | période donnée.    |              | d'une mach.|              |                              |
|              |               | Photons arrivant   |              | Ex: si     |              |                              |
|              |               | sur un télescope.  |              | 8 appels/h,|              |                              |
|              |               | Voitures feu rouge.|              | prochain   |              |                              |
|              |               | Queue de clients.  |              | appel prob.|              |                              |
|              |               | Nb buts au foot.   |              | == P(8)    |              |                              |
|              |               |                    |              | heures     |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| R functions  | *binom        | *pois              | *unif        | *exp       | *nbinom      | *norm                        |
|              |               |                    |              |            | *geom (infra)|                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre        | B(1,p) est    | Cf plus bas,       | U(0,1)ⁿ      |Geom(λ/(λ+1)| Si r∈ N,     |                              |
|              | appelé        | poisson process.   |  = Be(1/n,1) | est la     | appelée      |                              |
|              | Bernoulli dst.|                    | pour n>0     | version    |"Pascal dist."|                              |
|              | dont un event |                    |              | discrète de|Sinon "Polya  |                              |
|              | est un        |                    |              | Exp(λ)     | dist."       |                              |
|              |Bernoulli trial|                    |              |            |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)| Attention,    |                    | |U(a,b) -    | Memoryless:| Alternative: |                              |
|              | pour avoir %  |                    |  U(a,b)| ~   | Exp(x+a)/  | nb d'essais, |                              |
|              | d'occurences  |                    |Trian(0,b-a,0)| Exp(x) =   | non d'échecs.|                              |
|              | et non nb,    |                    | μ(U(a,b)+    | Exp(y+a)/  | Ajouter alors|                              |
|              | utiliser      |                    |   U(a,b)) ~  | Exp(y) pour| r au résultat|                              |
|              | B(n,p)/n, ce  |                    |Trian(a,b,    | tout x,y,a.|              |                              |
|              | qui est difer.|                    | (b+a)/2)     | Donc mêmes |              |                              |
|              | B(1,p), mais a|                    |              |courbes pour|              |                              |
|              | mêmes σ et μ  |                    |              |tout [x,x+a]|              |                              |
|              |               |                    |              |Décroissance|              |                              |
|              |               |                    |              |rate toujour|              |                              |
|              |               |                    |              |même donc.  |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)|B(1,p) + ... + |                    |              |Décroissance| Geometric(p) |                              |
|              |B(1,p) = B(n,p)|                    |              |rate entre  | == NB(1,p).  |                              |
|              |si indépendants|                    |              |x et x+1    | A une allure |                              |
|              |               |                    |              |pour pdf ou | exponentielle|                              |
|              |               |                    |              |cdf est     |              |                              |
|              |               |                    |              | ℮^-λ       |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)| Bernoulli     |                    |              | Si X et Y  | NB(1,p) + ...|                              |
|              | process =     |                    |              | ~ Exp(),   | + NB(1,p)    |                              |
|              | suite de      |                    |              | X/(X+Y)    | = NB(r,p) si |                              |
|              | Bern. trials. |                    |              | ~ U(0,1)   | indépendants |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+

+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Distribution | Uniform di.| Student's t      | Chi-squared       | F                        | Hypergéométrique              |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Type         | Discrete   | Continue         | Continue          | Continue                 | Discrète                      |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Aspect       | Bar plot   | Comme N(0,1)     | Si n faible,      | Gamma-form               | Comme B()                     |
|              | avec pas de| plus plat.       | exponenti.,       |                          |                               |
|              | 1 et même  | + df, + proche   | + df, + proche    |                          |                               |
|              | hauteur    | de N(0,1)        | de N(0,1)         |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Notation     |            | t(df)            | χ²(df)            | F(df)                    |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Paramètres   | a: min     | df: degrés of    | df: degrés of     | df1 et df2:              | m: subpop. du groupe          |
|              | b: max     |     freedom, soit|     freedom       | degrés of                | n: subpop. du non-groupe      |
|              |            |     n-1 (n > 1)  |                   | freedom                  | k: taille du sample           |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Note perso   | n: b-a+1   | Similair à N(0,1)| Similaire à N(0,1)|                          | Similaire à B(k,m/N)          |
|              |            | pour n > 30      | pour n > 30       |                          | pour k/N petit                |
|              |            |                  |                   |                          | Soit N = m+n                  |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| x            |            |                  | ∑ de df variates  | μ(df1 variates ~ N(0,1)²)| Prob. que x individus du      |
|              |            |                  | suivant N(0,1)²   |/μ(df2 variates ~ N(0,1)²)| groupe soient tirés sur un    |
|              |            |                  |                   |                          | sample de taille k            |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| range        | [a,b]      | [-∞,∞]           |                   |                          | [max(0,k-n),min(m,k)]         |
| mean         | (a+b)/2    | 0                | df                | df2/(df2-2)              | k*m/N                         |
|              |            |                  |                   | undef. si df2 < 2        |                               |
| median       | (a+b)/2    | 0                |                   |                          |                               |
| variance     | (n²-1)/12  | df/(df-2)        | 2*df              | 2*df2²*(df1+df2-2)       | kmn/N² * (N-k)/(N-1)          |
|              |            | undef. si df < 2 |                   | /(df1*(df2-2)²*(df2-4))  |                               |
|              |            |                  |                   | pour df2 > 4             |                               |
| mode         | Chacune    | 0                | df-2 (0 si df<2)  |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| P(x)         | 0 si:      |                  |                   |                          | (m) * ( n )  / (N)            |
| (density f())|  x pas int.|                  |                   |                          | (x)   (k-x)    (k)            |
|              |  x < a     |                  |                   |                          |                               |
|              |  ou x > b  |                  |                   |                          |                               |
|              | sinon 1/n  |                  |                   |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Pc(x)        | 0 si x < a |                  |                   |                          |                               |
|              | 1 si x > b |                  |                   |                          |                               |
|              | sinon:     |                  |                   |                          |                               |
|              | (⎣x⎦-a+1)/n|                  |                   |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Examples     | Chances    |̊μ*√n si ̅σ inconnu,|σ₁/̊σ*(n-1) si ̅σ    | Equivaut à               | Test de proportion avec un    |
|              | égales,    |que ̅s suit N(0,1).|inconnu, et que    | (df1/χ²(df1))            | sample sans remplacement      |
|              | entre a et |                  | ̅s ~ N()           | /(df2/χ²(df2))           |                               |
|              | b, entier. |                  |                   | Utilisé pour             |                               |
|              | Roulette.  |                  |                   | comparer deux            |                               |
|              | Dé.        |                  |                   | σₙ (F-test)              |                               |
|              |            |                  |                   | Distribution de          |                               |
|              |            |                  |                   | (σ²₁/σ²₂) /              |                               |
|              |            |                  |                   | (̅σ²₁/̅σ²₂), pour df1      |                               |
|              |            |                  |                   | == n₁-1, et df2 == n₂-1  |                               |
|              |            |                  |                   | F(1,df2) = t(df2)^2      |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| R functions  | *unifd     | *t               | *chisq            | *f                       | *hyper                        |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Autre        |            | t(df) = N(0,1)/  | χ²(2) = Exp(1/2)  |                          |                               |
|              |            |  √(χ²(df)/df)    | χ²(2n)            |                          |                               |
|              |            |                  |     = Gamma(n,1/2)|                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+

+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Distribution | Gamma                 | Beta               | Triangular        | Cauchy         | Weibull                    |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Type         | Continue              | Continue           | Continue          | Continue       | Continue                   |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Aspect       | Exp. si k ou α faible | α&β>1: bell        | Triangle          | Bell           | Gamma-like                 |
|              | Bell sinon            | α&β=1: U(0,1)      |                   |                |                            |
|              |                       | α&β<1: inverse-bell|                   |                |                            |
|              |                       | α>1&β<1: curve     |                   |                |                            |
|              |                       |          croissante|                   |                |                            |
|              |                       | α<1&β>1: decroisnte|                   |                |                            |
|              |                       | Si α>β, penche     |                   |                |                            |
|              |                       | vers droite,       |                   |                |                            |
|              |                       | sinon inverse.     |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Notation     |                       | Be(α,β)            |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Paramètres   | α: shape              | α: shape du 1er    | a: min            | x₀: median     | k: shape                   |
|              | β/λ: rate             | β: shape du 2ème   | c: mode           | ϒ: scale       | λ: scale                   |
|              |   ou                  |                    | b: max            |                |                            |
|              | k: comme α            |                    |                   | Standard:      |                            |
|              | θ: comme 1/β (scale)  |                    |                   |   (0,1)        |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Note perso   | Converge vers N() pour|                    |                   |                | Comme ₖ√Exp(), avec        |
|              | un grand k            |                    |                   |                | scale adjustement.         |
|              |                       |                    |                   |                | Donc contrairement à Exp() |
|              |                       |                    |                   |                | decreate rate augmente     |
|              |                       |                    |                   |                | (k>1) ou decrease (k<1)    |
|              |                       |                    |                   |                | constamment                |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| x            | Si α∈ N, somme de α   | Si X ~ Gamma(α,λ)  |                   |                |                            |
|              | iid. Exp. variables   | et Y ~ Gamma(β,λ)  |                   |                |                            |
|              |                       | alors X/(X+Y)      |                   |                |                            |
|              |                       | ~ Beta(α,β)        |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| range        | [0,∞)                 | [0,1]              | [a,b]             | (-∞,+∞)        | [0,∞)                      |
| mean         | α/β                   | α/(α+β)            | (a+b+c)/3         | Aucune         | λ*Γ(1+1/k)                 |
| median       |                       | αβ/(α+β)²/(α+β+1)  | Dist. au point    | x₀             | λ*ₖ√(ln(2))                |
|              |                       |                    | a ou b le plus    |                |                            |
|              |                       |                    | proche:           |                |                            |
|              |                       |                    | √((b-a)(c-a)/2))  |                |                            |
| mode         | (α-1)/β, pour α>=1    | (α-1)/(α+β-2), pour| c                 | x₀             | λ*ₖ√((k-1)/k)              |
|              |                       | α>1 et β>1         |                   |                | (0 si k <= 1)              |
| variance     | α/β²                  |                    | (a²+b²+c²-ab      | Aucune         | λ²*Γ(1+2/k) - μ²           |
|              |                       |                    | -ac-bc)/18        |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| P(x)         | β^α/Γ(α) * x^(α-1)    | x^(α-1)/B(α,β)     | 0 si hors range   | 1 / (πϒ *      | k/λ*(x/λ)^(k-1)            |
| (density f())|  / ℮^(βx)             | * (1-x)^(β-1)      | Si x <= c,        |(1+((x-x₀)/ϒ)²))| / ℮^((x/λ)^k)              |
|              |                       |                    | 2(x-a)/(b-a)/(c-a)|                |                            |
|              |                       |                    | Sinon,            |                |                            |
|              |                       |                    | 2(b-x)/(b-a)/(b-c)|                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Pc(x)        |                       |                    | 0 ou 1 si hors    |                | 1-℮^(-(x/λ)^k)             |
|              |                       |                    | range. Si x <= c, |                |                            |
|              |                       |                    |                   |                |                            |
|              |                       |                    | (x-a)²/(b-a)/(c-a)|                |                            |
|              |                       |                    | Sinon, 1 - ((b-x)²|                |                            |
|              |                       |                    | / (b-a) / (b-c)   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Examples     |                       |                    |                   |                | Espérance de vie, car      |
|              |                       |                    |                   |                | mort plus likely plus      |
|              |                       |                    |                   |                | x est grand.               |
|              |                       |                    |                   |                | Taux de défection produit. |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| R functions  | *gamma                | *beta              | *trngl            | *cauchy        | *weibull                   |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Autre        | Si α∈ N, appelé       | 1-Be(α,β) = Be(β,α)|                   | N(0,a)/N(0,b)  | Weibull(k,λ) =             |
|              | "Erlang dist."        |                    |                   | = Cauchy(0,a/b)|  ₖ√Exp(λ^-k)               |
|              | avec paramètres       |                    |                   |                |Donc Weibull(k,1) = ₖ√Exp(1)|
|              | α,β -> k,λ.           |                    |                   | Cauchy(0,1) =  | et Weibull(1,λ) = Exp(1/λ) |
|              | Gamma(1,β) = Exp(β)   |                    |                   |  t(1)          |                            |
|              | Analogue continue de  |                    |                   |                |                            |
|              | NB()                  |                    |                   | ̊μ(Cauchy(a,b) =|                            |
|              |                       |                    |                   |   Cauchy(a,b), |                            |
|              |                       |                    |                   |   non CLT.     |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+

+--------------+---------------+----------------------------------------------------------------------------------------------+
| Distribution | Log-normal    | (Fisher's) noncentral                                                                        |
|              |               | hypergeometric                                                                               |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Type         | Continue      | Discrète                                                                                     |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Aspect       | Bell penchée  | Comme hypergeometric                                                                         |
|              | vers gauche   | mais tirée vers                                                                              |
|              |               | gauche ou droite                                                                             |
|              |               | selon ω                                                                                      |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Notation     | lnN(μ,σ²)     |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Paramètres   | μ: log.mean   | m,n,k: comme hypergeom.                                                                      |
|              | σ²: log.var   | ω: odds ratio, soit ω₁/ω₂                                                                    |
|              |               | ωₙ est la prob. de tirer                                                                     |
|              |               | un individu de ce groupe                                                                     |
|              |               | (en dehors de sa prop.                                                                       |
|              |               | m ou n)                                                                                      |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Note perso   | ℮^N(μ,σ²)     | Si ω=1, égal à hypergeom.                                                                    |
|              |               | Soit N = m+n                                                                                 |
|              |               | xmin = max(0,k-n)                                                                            |
|              |               | xmax = min(k,m)                                                                              |
|              |               | Pₑ = ∑xmin,xmax (m)*( n )                                                                    |
|              |               |                 (i) (k-i)                                                                    |
|              |               |      * ω^i * i^e                                                                             |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| x            |               | Nb d'individus du groupe                                                                     |
|              |               | 1 sur un sample de taille k                                                                  |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| range        | (0,∞)         | [xmin,xmax]                                                                                  |
| mean         | ℮^(μ+σ²/2)    | P₁/P₀                                                                                        |
| median       | ℮^μ           |                                                                                              |
| mode         | ℮^(μ-σ²)      |                                                                                              |
| variance     | (℮^σ²-1) *    | P₂/P₀-(P₁/P₀)²                                                                               |
|              | ℮^(2μ+σ²)     |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| P(x)         | 1/(x√(2πσ²)   | (m)*( n )*ω^x / P₀                                                                           |
| (density f())|*℮^((ln(x)-μ)²)| (x) (k-x)                                                                                    |
|              |      /2σ²)    |                                                                                              |
|              |               |                                                                                              |
|              |               |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Pc(x)        |               |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Examples     | Phénomène dont|                                                                                              |
|              | le logarithme |                                                                                              |
|              | ~ N().        |                                                                                              |
|              | Beaucoup de   |                                                                                              |
|              | tissus vivants|                                                                                              |
|              | (poids,taille)|                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| R functions  | *lnorm        | *hypr                                                                                        |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Autre        | ∏ lnN(μ,σ²) = |                                                                                              |
|              |   lnN(∑μ,∑σ²) |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+

Multivariate distribution :
  - distribution sur plusieurs variables
  - représentée graphiquement par un graph à n-dimensions si 2 ou 3 variables
  - types :
    - joint distribution de variables ayant toutes la même ̅s
    - plusieurs variables aux rôles différents venant de paramètres sériels de ̅s (ex: multinomial dist.)
  - ne semblent pas avoir de cdf ??

+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Distribution | Multinomial     | Categorical      | Multivariate         | Multivariate (Fisher's)                          |
|              |                 | /"Discrète"      | Hypergéométq.        | noncentral hypergeom.                            |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Type         | Discrète        | Discrète         | Discrète             | Discrète                                         |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Aspect       | Ressemble à B() | Comme Bernoulli  | Comme                |                                                  |
|              | pour chaque var | pour chaque var  | Multinomial()        |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Notation     |                 |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Paramètres   | n: sample size  | p₁,...,pₖ: prob. | m...mₙ: sous-        | Comme central,                                   |
|              | p₁,...,pₖ: prob.|                  | groupe size          | avec en plus:                                    |
|              | des sous-groupes|                  | k: sample size       | ω...ωₙ: odds ratio                               |
|              | (∑pₖ=1)         |                  | (∑mₙ=pop.size)       |   du groupe n                                    |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Note perso   |                 | = Multinomial    | Comme Multinomial    | Si tous ωₙ = 1,                                  |
|              |                 |   (1,...)        | sans remplacement    | = central hypergeom.                             |
|              |                 |                  | Soit N = ∑mₙ         |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| x            | Chaque xₖ =     | Si xₖ = 1, prob. |                      |                                                  |
|              | prob. que sample| qu'event soit du |                      |                                                  |
|              | contienne xₖ    |groupe k, soit pₖ |                      |                                                  |
|              | individus du    | Si 0, inverse.   |                      |                                                  |
|              | sous-groupe k   |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
|POUR CHAQUE xₙ|                 |                  |                      |                                                  |
| range        | [0,n] (∑xₖ=n)   | [0,1]            | [0,mₙ] (∑xₙ=k)       |                                                  |
| mean         | n*pₖ            | pₖ               | mₙ*k/N               |                                                  |
| median       |                 | median(p₁,...pₖ) |                      |                                                  |
| mode         |                 | max(p₁,...,pₖ)   |                      |                                                  |
| variance     | n*pₖ*(1-pₖ)     | pₖ*(1-pₖ)        | mₙ/N*(1-mₙ/N) * k    |                                                  |
|              |                 |                  | * (N-k)/(N-1)        |                                                  |
| cov(x₁,x₂)   | -np₁p₂          | -p₁p₂            | -km₁m₂/N²*(N-k)/(N-1)|                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| P(x)         | (    n    )     | pₖ, où k est     | (∏(mₙ))/(N)          |                                                  |
| (density f())| (x₁,...,xₖ)     | l'index du xₖ    |   (xₙ)  (k)          |                                                  |
|              | * ∏pₖ^xₖ        | = 1              |                      |                                                  |
|              |                 |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Examples     | Chances que n   | Jeu vidéo, prob. | Prob. tirer deux     |                                                  |
|              | prochains coups | qu'ennemi fasse  | carreaux, 1 pique    |                                                  |
|              | de roulette     | attaque 1,2 ou 3 | et 3 trèfle sur      |                                                  |
|              | soient x₀ rouge,|                  | 6 cartes piochées.   |                                                  |
|              | x₁ noir et x₂   |                  |                      |                                                  |
|              | zeros.          |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| R functions  | *multinom       | *multinom(size=1)| *mhypr               | *mhypr                                           |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Autre        | Si k = 1 ou 2 ->|                  | Si n = 1 ou 2 ->     |                                                  |
|              | Binomial        |                  | Hypergéométrique     |                                                  |
|              | Chaque xₖ ~     |                  | univariate.          |                                                  |
|              | B(n,pₖ)         |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+

Approximation par distribution normale :
  - on prend moyenne et σ de la distribution à approximer, et on la donne à N()
  - notamment :
     - B(n,p)  -> N(np,√(np(1-p))) (quand n est grand, p proche de 0.5)
       - lorsque p *très* éloigné de 0.5, préférer Poisson approximation : B(n,p) -> Pois(np)
     - Pois(λ) -> N(λ,λ)           (quand λ est grand)
     - t(v)    -> N(0,1)           (quand v est grand)
  - faire la "continuity correction" lors de cdf, mais pas pour pdf

Confidence :
  - confidence :
    - Pc(x ∈ "confidence interval") = confidence level
    - margin of error: length(confidence interval)/2, noté E
  - confidence level pour N() :
    - "three-sigma"/"empirical"/"68-95-99.7 rule"
    - P(μ-σ,μ+σ)   = 68%
    - P(μ-1.96σ,μ+1.96σ) = 95%
      - également P(μ-2.58σ,μ+2.58σ) = 99%
    - P(μ-3σ,μ+3σ) = 99.7%
  - calcul :
    - pour confidence interval de ̊m estimant ̅m :
        - résoudre équation, de sorte d'isoler m̅, en remplaçant m̊ par m
          - right-hand donne le confidence interval
        - si ̊m = Dist() * m̅, alors conf.int. pour ̅m est m/Dist()
        - ex:
            ̊μ₁ = N(0,1) * ̅σ / √n + ̅μ
            ̅μ (conf.int.) = μ₁ - N(0,1) * ̅σ / √n
  - souvent, on :
    - calcule confidence interval, pour un n et confidence level donné
    - calcule un n, pour un confidence interval et confidence level donné
      - par ex., si P() suit N(), et que l'on veut confidence level de 95%, calculer n de sorte que σ = confidence interval/4
    - dans les deux cas, dépend du résultat donné
      - ex: ampleur de confidence interval de s ~ B(n,p) dépend de p, mais p est lui-même l'objet de l'estimation (estimé via
            s) et du confidence interval
    - solution :
      - utilise une équation de n/confidence interval requis en fonction du paramètre
        - ex: pour s ~ B(n,p)/n, en utilisant CLT :
          - on cherche 95% de confidence dans interval p±a
            - donc on cherche un n tel que σ = a/2
      - ou on résout cette équation pour sa valeur donnant le n/confidence interval maximum :
        - ex: pour s ~ B(n,p), p=0.5 donne σ² maximum n/4
              On peut estimer via calcul du saddle point de p*(1-p) :
                (p*(1-p))' = 0
                (p-p²)'    = 0
                1-2p       = 0
                p          = 1/2
          - donc √(n*p*(1-p))/n = a/2
                 √n/n = a
                 √n = 1/a
                 n = 1/a²
            Donc pour un interval de confiance de ±5%, n = 1/0.05² = 400

Sampling :
  - sample :
    - subset d'une population
    - peut être avec ou sans replacement (doublons)
    - peut être ordered, alors position de l'élément importe
    - ensemble des samples possibles, "sample space du random sample" :
      - pour une population de taille ̅n et un sample de taille n :
        - avec replacement : ̅nⁿ
        - sans replacement, unordered : (̅n)
                                        (n)
        - sans replacement, ordered : (̅n)ₙ
  - notation :
    - s dénote un random sample
    - ̊s est la distribution de ce sample
      - (s)̊  est équivalent de ̊s
      - (statistic)̊  signifie distribution de tel statistic
    - ̅s est la distribution de la population
  - sampling distribution :
    - si sample random, alors P() que le sample choisi, de taille n, ait la valeur x
    - sample space of the sample average/median/etc. : quand f(x) est average/mean/etc.
      - ici, je me concentre sur SampleMean, mais cela est parfois valable pour les autres
      - ex:
          +------------+--------------------+--------------------------+
          |  SOURCE    |   SAMPLE,taille n  |       SAMPLEMEAN         |
          +------------+--------------------+--------------------------+
          |   Data     |   sample(data,n)   | ∞ fois μ(sample(data,n)) |
          | Ex nihilo  |   r*(n,param)      |  ∞ fois μ(r*(n,param))   |
          +------------+--------------------+--------------------------+
    - SampleMean est un sample de sample, donc :
      - les unités observées sont des samples (groupe de n outcomes) et non des outcomes isolés
      - le x observé est une statistic (ex: height mean) et non une measure (ex: height)
    - tout cela suppose que sample soit random
    - notation :
      - m pour statistic d'un sample donné, ou m₁, ou mₙ
      - ̊m pour sampling dist. d'un statistic, ou sampleM
      - ̅m pour parameter d'une population, ou m(̅s)
      - ex: μ₁ est μ(s₁), ̊μ est sampleMean, ̅μ est mean(̅s)
            σ²₁ est var(s₁), ̊σ² est sampleVar, ̅σ² est var(̅s)
      - ̊μ(n) = probability qu'un sample de taille n ait une mean x
    - différence entre une statistic, e.g. μ₁, et un parameter, e.g. ̅μ, est la sampling error
      - la distribution de la sampling error d'une statistic donné est ̊m-̅m
    - standard error d'une statistic est le σ de sa sampling dist., noté SE
      - SE(μ) = σ(̊μ)
  - ̊μ(1) == ̅s

CLT (Central Limit Theorem) :
  - Somme/mean de n iid. variables, (suivant ̅s(̅μ,̅σ²) tend vers N(̅μ,̅σ²/n) quand n augmente, quelque soit ̅s
    - en d'autre terme ̊μ tend vers N(̅μ,̅σ²/n), pour tout ̅s
    - seulement pour ̊μ, pas forcément pour autres statistics
  - n minimum pour avoir un bon match dépend de ̅s :
    - selon mes estimations, pour un match de 95% :
      - Exponential() : n > 135
      - U() : n > 50
      - Pois() : n > assez grand (encore plus lorsque λ faible)
  - attention, si n est trop petit, alors ne marche pas
  - conséquences :
    - μ(̊μ) == ̅μ
    - σ²(̊μ) == ̅σ²/n (Law of Large numbers)
      - donc σ(̊μ) == ̅σ/√n
  - plus n est grand :
    - plus σ(̊μ) est faible
    - donc plus μ a des chances d'être proche de ̅μ (selon facteur √n)
      - par exemple, multiplier par 100 n, divise par 10 écart moyen de μ à ̅μ
  - en conséquences, beaucoup de measurements suivent N(), car ils mesurent la μ d'un ensemble de phénomènes plus petits sur
    un interval de temps ou d'espace
  - Ne marche pas pour les distributions n'ayant pas de finite variances, par ex. Cauchy

Estimation :
  - prédiction :
    - d'un paramètre inconnu ̅y d'une population Y
    - à partir d'un sample s
      - dont sa statistique y
      - mais pas nécessairement. Peut par ex. utiliser max₁/2 pour estimer ̅μ
    - utilise une fonction f ("estimator")
    - produit une prédiction "estimate" f₁, ayant une distribution (f)̊
      - f₁ parfois noté θ^ (^ au dessus) et ̅y noté θ
      - estimate est un "point estimate" si valeur discrète
    - exemple μ(sample) est un estimator pour prédire paramètre ̅μ, produisant un estimate μ₁ avec une distribution ̊μ₁
  - implique que l'on connaisse/suppose la distribution de ̊y
    - on peut utiliser N() comme première approximation (CLT)
  - biases :
    - estimator peut être biased ou unbiased :
      - [mean-]bias((f)̊ ,̅y) : |μ((f)̊ )-̅y|
      - median-bias((f)̊ ,̅y) : |median((f)̊ )-̅y|
        - un estimator peut être median-unbiased mais mean-biased
    - un estimator peut être biased, mais avoir une faible MSE car bonne précision, et donc être préféré
      - ex: max₁/2 pour estimer μ(U(0,b))
    - biases peuvent être corrigés en multipliant μ((fbias)̊ ) :
      - ex:
        - f = max₁/2, estimator de ̅μ pour U(0,b)
          - samplef suit b-Exponential(2n/b)
          - donc (fbias)̊  suit Exponential(2n/b) * -1
          - donc μ((fbias)̊ ) = -b/2n
          - donc ̅μ = f + b/2n
                   = f + ̅μ/n
                   = f * n/(n-1)
        - donc multiplier f par n/(n-1) pour corriger bias
  - comparaison d'estimators :
    - comparer (f)̊  et (g)̊  par rapport à ̅y
      - peut simuler pour comparer
    - peut comparer :
      - le bias entre mean des (f)̊  et ̅y pour voir l'accuracy
      - le σ des (f)̊  pour voir la precision
      - MSE/RMSD permet de prendre un peu des deux :
        - MSE (mean-squared error) :
          - variance de (f)̊ , mais par rapport à ̅y, non μ((f)̊ ) :
            - ∑(dev(chaque x, ̅y)²) / (n-1)
          - égal également à :
            - σ²((f)̊ ) + bias((f)̊ ,̅y)²
        - RMSD (root mean-squared deviation)/RMSE (...error):
          - √MSE
        - on peut par exemple diviser MSE/RMSE entre eux pour voir de combien un estimator est meilleur qu'un autre
          - si MSE(f)/MSE(g) = 1/4, f est quatre fois mieux que g
    - ex:
      - ̅y = ̅μ ; ̅s suit U(0,b)
      - f = max₁/2 ; g = μ₁
      - (f)̊  = b-Exponential(2n/b)
        - soit σ((f)̊ ) = b/2n et μ((f)̊ ) = ̅y - b/2n
      - (g)̊  = N(̅y,̅σ²/n)
        - soit μ((g)̊ ) = ̅y et σ((g)̊ ) = b/√(12n)
      - σ(g) > σ(f) (sauf n < 2), selon ratio √n et non n, donc bien moins precise
      - mais bias(g,̅y) < bias(f,̅y), donc plus accurate
      - MSE(f) = b²/4n² + b/2n, MSE(g) = b²/12n
        - MSE(f) / MSE(g) = 3/n + 6/b, préférer donc f, pour n >= 3/(1-6/b)

Estimation en ignorant des paramètres :
  - lors d'une estimation, on dispose souvent du sample mais ignore tout paramètre de ̅s
  - hors on a souvent besoin :
    - par ex., pour estimer ̊μ, il faut connaître ̅σ²
  - solutions :
    - utiliser m₁ au lieu de ̅m
      - cependant, cela change la distribution du paramètre à évaluer
    - example :
      - ignore ̅σ², mais sait que ̅s suit N() :
        - estimer ̅μ :
          - utiliser σ₁ au lieu de ̅σ
          - cependant, en conséquence, suit t(n-1), non N(0,1)
    - lorsque n grandit, m₁ s'approche de ̅m, donc moins besoin de cela
      - en conséquence, définition de ̊m en ignorant ̅m se rapproche de celle en connaissant ̅m
        - ex:
          - t() équivaut presque à N(0,1), si n > 30
          - χ²(n-1)/(n-1) équivaut presque à N(0,1), si n > 100

Estimators :
  - lorsque suit N(), souvent CLT
    - donc marche pourvu que n soit assez grand (dépend de ̅s)
  - ̅μ :
    - μ₁ :
      - si ̅σ connu, ̊μ₁ = N(0,1) * ̅σ / √n + ̅μ
      - si ̅σ inconnu et que ̅s ~ N(), ̊μ₁ = t(n-1) * σ₁ /√n + ̅μ
    - midrange:
      - pour U() seulement
      - (midrange)̊  = { si < ̅μ, ̅μ - Exponential(2n/ran̅ge) }
                      { si > ̅μ, μ + Exponential(2n/ran̅ge) }
      - donc :
        - μ(((midrange)̊ ) = ̅μ, et σ = ran̅ge/(√2*n)
        - meilleur que ̊μ, qui a σ = ran̅ge/√(12n)
        - RMSE(̊μ)/RMSE((midrange)̊ ) = √(n/6)
  - som̅me : utiliser ̅μ*n
  - ̅μ₁-̅μ₂ :
    - μ₁-μ₂ :
      - ̅σ²₁ et ̅σ²₂ connus :
        - (μ₁-μ₂)̊  = N(0,1) * √(̅σ²(̊μ₁) + ̅σ²(̊μ₂)) + (̅μ₁-̅μ₂)
        - en fait :
          - μ(μ₁±μ₂)̊  = μ(̊μ₁) ± μ(̊μ₂)
          - σ²(μ₁±μ₂)̊  = ̅σ²(̊μ₁) + ̅σ²(̊μ₂)
            - où σ²(̊μₙ) = σ²ₙ/nₙ
              et ̅σ²(̊μₙ) = ̅σ²ₙ/nₙ
      - ̅σ²₁ et ̅σ²₂ inconnus :
        - ̅σ²₁ == ̅σ²₂ connu, et vrai :
          - (μ₁-μ₂)̊  = t(n₁+n₂-2) * √(((n₁-1)*σ²₁ + (n₂-1)*σ²₂) / (n₁+n₂-2) * (1/n₁+1/n₂)) + (̅μ₁-̅μ₂)
        - ̅σ²₁ == ̅σ²₂ inconnu, ou faux :
          - (μ₁-μ₂)̊  = t(df) * √(σ²(̊μ₁) + σ²(̊μ₂)) + (̅μ₁-̅μ₂)
            - où df = (σ²(̊μ₁)+σ²(̊μ₂))² / (σ²(̊μ₁)²/(n₁-1) + σ²(̊μ₂)²/(n₂-1))) (Welch-Satterthwaite equation)
  - ̅σ :
    - σ₁ :
      - si s̅ ~ N(), ̊σ₁ = √(χ²(n-1)/(n-1)) * ̅σ
  - ̅σ² :
    - σ²₁ :
      - si ̅s ~ N(), ̊σ²₁ = χ²(n-1)/(n-1) * ̅σ²
        - car ((n-1)*σ²₁/̅σ²)̊  = χ²(n-1)
  - ̅σ²₁/̅σ²₂ :
    - σ²₁/σ²₂*(n₂-3)/(n₂-1) :
      - si ̅s suit N()
      - (σ²₁/σ²₂*(n₂-3)/(n₂-1))̊  = F(n₁-1,n₂-1)*(n₂-3)/(n₂-1) * ̅σ²₁/̅σ²₂
      - raison :
        - ((σ²₁/σ²₂) / (̅σ²₁/̅σ²₂))̊  = F(n₁-1,n₂-1)
          - F() est en fait une division de deux χ²(nₙ-1)/(nₙ-1)
        - (σ²₁/σ²₂)̊  = F(n₁-1,n₂-1) * ̅σ²₁/̅σ²₂
          - or, F(...) n'a pas une mean de 1, donc il y a un bias. (n₂-3)/(n₂-1) corrige ce bias.
        - utilisé pour un two samples variance F-test ((σ²₁/σ²₂)̊  = F() si ̅σ²₁/̅σ²₂ == 1)

Noncentral distribution :
  - généralisation d'un ̅s, avec un ou deux non-central parameters (en général "ncp")
    - généralisation, car une valeur de ces paramètres == version normale, "centrale" de ̅s
  - noté χ'², F', t', etc.
  - Signification possible de ncp :
    - μ de la dist. sur laquelle ̅s est basée (souvent N()). Si == 0, version centrale
      - noncentral N() n'existe donc pas, il suffit de modifier μ
    - odds ratio ω pour Hypergeom()
  - Doubly non-central :
    - quand ̅s basée sur deux dist. (par ex. ratio), alors ncp pour chaque de ces dist.
  - Ex:
    - t' :
      - basé sur N(ncp,1), soit N(ncp,1)/√(χ²(df)/df)
    - χ'² :
      - basé sur un ensemble de N(μₙ,σ²ₙ), où ncp = ∑ (μₙ/σₙ)²
        - si tous N() ont mêmes μ et σ, alors ncp = (μ/σ)² * df
          - soit √(ncp/df) = μ/σ
    - F' :
      - F'(df1,df2,ncp) = (χ'²(df1,ncp)/df1)/(χ²(df2)/df2)

Hypothesis testing :
  - null hypothesis H₀ vs alternative hypothesis H₁
    - H₁ : affirmation qu'un phénomène suit telle distribution. Correspond à ce que l'on veut "prouver".
    - H₀ : négation de H₁. Correspond à l'état des faits ou à un état "conservateur" en cas de dangers.
    - hypothèse "prouvée" est dite "statistically significant" pour un α donné
  - errors :
    - Type I error  :
      - false positive, accepting H₁, alors que H₁ est faux
      - prob. est le significance level, α
    - Type II error :
      - false negative, rejecting H₁, alors que H₁ est vrai
      - prob. est β
      - statistical power / sensitivity : 1-β
      - je crois qu'il se calcule via la dist. "générale" que suit le test statistic (la dist. que suit t que H₀ soit vrai ou
        non), non une dist. centrale
      - power analysis :
        - calcul du power
        - permet de déterminer n minimum pour avoir un 1-β raisonnable et avoir chances de détecter quelque chose
    - plus grave de faire Type I error que Type II error
      - test for differences sont faits pour tendre vers des Type II error plutôt que Type I error, en favorisant H₀
      - il faut prouver que H₀ est faux, non que H₁ est vrai (réfutabilité)
  - principes :
    - un H₀ est posé
      - en général un ==, <= ou >= entre un statistic et un parameter, ou entre deux statistics
      - one-sided >= ou <= vs two-sided == (directionality)
    - un "test statistic" t est calculé à partir de ces derniers
    - la distribution du t, ̊t, lorsque H₀ est vraie, est connu
      - les 1-α premiers/centraux/derniers % (selon directionality) de ̊t sont la rejection region C
        - si Op de H₀ est >=, derniers %
        - si <=, premiers %
        - si ==, centraux %
      - ainsi pour un α donné, on peut juger si H₀ est vraie, si t ne tombe pas dans C, il y a 1-α% de chances que H₀ soit faux
    - en général ̊t est une version normalisée des écarts possibles entre les deux statistics/parameters pour une H₀ donnée
      - donc par exemple, pour un one sample location t-test, il s'agit principalement d'un ̊μ normalisé
  - la distribution test statistic, la notation de la variable du test statistic et le nom de la catégorie du test varient :
    - z-test           : ̊z suit N() quand H₀ est vrai
    - t-test           : ̊t suit t() quand H₀ est vrai
    - chi-squared test : ̊χ suit χ²() quand H₀ est vrai
    - F-test           : ̊f suit F(df1,df2) quand H₀ est vrai
  - p-value :
    - correspond à α maximum pour avoir un C acceptant H₀
    - donc prob. que H₀ soit vraie
    - aussi moyen rapide de juger H₀ est vraie :
      - au lieu de regarder si t ne tombe pas dans C
      - on regarde si p-value >= α
    - p-value de >= est 1 - p-value de <=
  - différentiel :
    - hypothèses peuvent être exprimées en terme de différentiel
      - ex:
        - H₀ : μ₁ == μ₀   -> μ₁-μ₀ == 0
        - H₀ : σ²₁ == σ²₀ -> σ²₁/σ²₀ == 1
    - un différentiel d₀ est parfois ajouté au deuxième terme de l'hypothèse :
      - ajout si μ
      - multiplication si σ
    - cela permet de changer termes de la différence/ratio :
        - H₀ : μ₁ == μ₀ + d₀   -> μ₁-μ₀ == d₀
        - H₀ : σ²₁ == σ²₀ + d₀ -> σ²₁/σ²₀ == d₀
  - paired tests :
    - lorsqu'il y a deux samples, s₁ et s₂ peuvent être :
      - unpaired, c'est-à-dire choisis indépendemment l'un de l'autre
      - paired, c'est-à-dire que l'individu n de s₁ et de s₂ est le même, après une transformation
        - ex: même personne après une expérience
        - considérés meilleurs qu'unpaired
        - sᵈ dénote s₁-s₂
        - revient en fait à un test one sample, où s₁ est sᵈ, et parametre est 0
  - fonctionnement général :
    - si ̊m = Dist() * σ(̊m) + ̅m
      - alors pour H₀: m₁ Op m₀
        - si m = (m₁-m₂), alors H₀: m₁ Op m₂
      - t = (m₁-m₂) / σ(̊m)
      - et t suit Dist() si H₀ vraie
        - et que H₀ soit vraie ou non :
          - [+(m₁-m₀)] si N()
          - [ncp parameter] si autre Dist.
    - si ̊m = Dist() * σ(̊m) * ̅m
      - pareil sauf que (m₁/m₀) au lieu de (m₁-m₀) (ou m₁-m₂)
  - types de tests :
    - notation :
      - Op signifie, >, < ou != selon le test, pour H₁
      - μ₀ signifie paramètre auquel on compare si un seul sample
      - μᵈ signifie μ(sᵈ)
      - pareil pour σₙ et σ²ₙ
    - quand ̅s ~ N() :
           +-------------------------------------------+----------------------------------------------------------------------+
           |                                           |                       Two samples                                    |
           |                One sample                 +---------------+----------+-------------------------------------------+
           |                                           |   Unpaired    |  Paired  |               Unpaired                    |
           +---------------------+---------------------+---------------+----------+-------------------------------------------+
           |        ̅σ connu      |       ̅σ inconnu     |     ̅σ connu   | ̅σ inconnu| ̅σ inconnu, σ₁ == σ₂ | ̅σ inconnu, σ₁ != σ₂ |
           |                     |                     |               |          |      "pooled"       |      "unpooled"     |
+----------+---------------------+---------------------+---------------+----------+---------------------+---------------------+
|          | μ₁ Op μ₀            | Pareil              | μ₁ Op μ₂      | Pareil   | Pareil              | Pareil              |
| Location | z = (μ₁-μ₀) / (̅σ/√n)| t = (μ₁-μ₀) /(σ₁/√n)| z = (μ₁-μ₂) / | t = μᵈ / | t = (μ₁-μ₂) /       | t = (μ₁-μ₂) /       |
|    test  |                     |                     |     √(̅σ²₁/n₁+ |   (σᵈ/√n)|    √(((n₁-1) * σ²₁ +|     √(σ²₁/n₁+σ²₂/n₂)|
|          | ̊z suit N(0,1)       | ̊t suit t(n-1)       |     σ²₂/n₂)   |          |     (n₂-1) * σ²₂) / | ̊t suit t(df), où    |
|          | [+(μ₁-μ₀)]          |                     | ̊z suit N(0,1) | ̊t suit   |     (n₁ + n₂ - 2) * | df= (σ²₁/n₁+σ²₂/n₂)²|
|          |                     |                     | [+(μ₁-μ₂)]    | t(n-1)   |     (1/n₁ + 1/n₂))  |  / ((σ²₁/n₁)²/(n₁-1)|
|          |                     |                     |               |          | ̊t suit t(n₁+n₂-2)   |  + (σ²₂/n₂)²/(n₂-1))|
|          |                     |                     |               |          |                     | "Welch/Satterthwaite|
|          |                     |                     |               |          |                     |  test"              |
+----------+---------------------+---------------------+---------------+----------+---------------------+---------------------+
|Proportion| Comme location test avec ̅σ connus, sauf qu'il s'agit de pourcentages (B(n,p)/n), donc σₙ = √(pₙ*(1-pₙ))          |
|   test   | Si multi-sample, Chi-squared test (cf dessous)                                                                   |
+----------+---------------------+---------------------+---------------+----------+-------------------------------------------+
|          |                     | σ²₁ Op σ²₀          |               |          | σ²₁ Op σ²₂                                |
| Variance |                     | χ² = (n-1) * σ²₁/σ²₀|               |          | F = σ²₁/σ²₂                               |
|   test   |                     | ̊χ² suit χ²(n-1)     |               |          | ̊F suit F(n₁-1,n₂-1)                       |
+----------+---------------------+---------------------+---------------+----------+-------------------------------------------+
      - "̊* suit *()" signifie "quand H₀ est vrai, ̊* suit *()", quand rien n'est précisé
        - le test statistic suit en fait une certaine distribution "générale" que H₀ soit vraie ou non
          - comprenant notamment m₁ - m₂ ou m₁/m₂ si test m₁ == m₂
        - mais lorsque H₀ est vraie, on peut réduire cette distribution
          - par exemple m₁/m₂ devient 1 si m₁ == m₂
        - souvent distribution "générale" est une version "noncentral" de la distribution de base
          - il s'agit de même dist., mais avec un paramètre ncp indiquant le μ(dist.)
          - il s'agit d'une généralisation de la dist. commune, "centrale", laquelle à ncp = 0
          - lorsque H₀ est vrai, ncp = 0, donc dist. centrale
      - "̊* suit *()[...]" signifie "quand H₀ est vrai, inclusion de [...] inutile"
      - attention, variance two samples F-test est très sensible à non normalité de ̅s
    - Proportion test multi samples Chi-squared test / Pearson's chi-squared test :
      - Soit A = { a₁,...,aₙ } et B = { b₁,...,bₙ }, et P = A/(A+B)
      - H₀ : tous pₙ == p₀, où p₀ est ∑A/(∑A+∑B)
        - cad si l'un des aₙ/bₙ est différent des autres
        - test donc si catégorie (A,B) est indépendants de catégorie (1,...,n)
          - si tous pₙ == p₀, catégorie (A,B) n'a aucune influence sur (1,...,n)
          - utilisé donc pour l'indépendance entre deux facteurs
      - calcul :
        - soit observed quantités A et B, noté xₙ
        - soit expected quantités de A = (A+B)*p₀, et de B = (A+B)*(1-p₀), noté xₑ
        - alors χ² = ∑ (xₙ-xₑ)²/xₑ
        - χ² suit χ²(n-1) si H₀ est vrai
      - χ² est ici utilisé pour une approximation de ce qui devrait être B()
      - Yate's continuity correction :
        - alors χ² = ∑ (xₙ-xₑ-0.5)²/xₑ
        - à utiliser quand + de 20% des xₑ < 5
      - seul == est possible pour H₀, et C est one-sided (premiers % de χ²(n-1))
      - marche aussi s'il y a un C, etc.

Hypothèse fallacies :
  - poser une H₁ suggérée par un dataset, et le tester avec ce dataset (post hoc theorizing)
    - en effet, ce dataset, ayant suggéré H₁, a de forte chance de le conforter.
    - Il faut donc utiliser un autre dataset
    - souvent quand collection des data est coûteuse
    - type I error

Explanatory/independent/regressor vs response/dependent variables :
  - il s'agit de voir les conséquences que l'explanatory variable X a sur la response variable Y
    - en d'autres termes, pour chaque variate xₙ possible, il y a une yₙ donné
    - on peut se représenter visuellement avec un graph avec l'ensemble des des points (xₙ,yₙ)
    - il s'agit en fait de représenter X et Y par rapport à leur P(X ∩ Y)
    - noté Y ~ X, appelé joint distribution de X et Y
  - taille n de X == taille de Y
    - n est le nombre d'observations. Il peut y avoir des doublons numériques (ex. deux xₙ différents, mais même réponse yₙ)
      ou factorials (ex. plusieurs réponse yₙ pour un même facteur xₙ)
    - X peut être factorial
      - ex. présence ou non de tel médicament ; et yₙ moyenne d'obésité d'une population
    - Y peut être factorial si X l'est. Cependant, Y factorial et X numérique me semble un problème de conception.
  - xₙ est toujours une valeur discrète (1-dimension), mais yₙ peut être une valeur n-dimensions, dont une random variable ̊yₙ
    - cependant si X n'est pas un facteur, observable seulement si plusieurs experiments avec mêmes xₙ sont faits
    - distinguer réelle variation (̊yₙ) de variation due au measurement error
    - ̊y lui-même peut être vu comme un Y ~ X, où Y est la P()
  - étude de la relation X et Y :
    - si tous les yₙ identiques, alors Y est indépendant de X
      - cependant, les différents measurement de yₙ sont unlikely to give exactly identical observations (measurement error)
        - il faut donc savoir si les observations y₁,...,yₙ suivent la distribution normale de Y, ̊Y
      - pour des ̊yₙ, il faut les comparer ensemble -> location n-samples test, par exemple sur μ(̊yₙ)
        - si ̊yₙ, on prend par exemple leur mean
    - si dépendant, étude de la relation :
      - si X numérique :
        - régression polynomiale

Covariance :
  - cov(X,Y) = (∑ (xₙ-μ(X))*(yₙ-μ(Y))) / (n-1)
    - comme variance, sauf qu'au lieu de dev(x...)^2, dev(x...) * dev(y...)
  - si proche de 0, variables indépendantes
    - si loin de 0, Y tend à diminuer (si < 0) ou augmenter (si > 0) quand X augmente
    - pourquoi :
      - si valeurs positives ensemble, et valeurs négatives ensemble => produits sont positifs -> cov > 0
      - si valeurs positives de l'un avec valeurs négatives de l'autre => produits sont négatifs -> cov < 0
      - si valeurs positives de l'un sont parfois avec valeurs positives, parfois négatifs => produits sont parfois positifs,
        parfois négatifs -> cov proche de 0
  - graphiquement, graph avec (X,Y) :
    - si proche de 0, nuage de points
    - si loin de 0, nuage tendant vers une ligne, ascendante (>0) ou descendante (<0)
  - cas particuliers :
    - cov(X,Y) = cov(Y,X)
    - cov(X,X) = σ²(X)
    - cov(X,Y), où Y = { a,...,a }, = 0
    - cov(X,Y), où Y ~ N() et indépendant de X, = 0

Regression model/analysis :
  - il s'agit de grapher une courbe montrant tendance de P(Y|X)
    - si plus de deux variables, multiple regression, sinon simple regression
  - linear [regression] model :
    - lorsque coefficient polynomial est 1
    - observed values (data), noté Y, vs predicted values, noté Z, (regression point sur même x pour chaque observed value)
      - residual: εₙ = yₙ - zₙ
      - SSE (Sum-squared of errors)/RSS(residual sum of squares) = ∑ εₙ²
        - peut aussi être calculé ainsi:
          - SSE = (σ²(Y) - b²*σ²(X)) * (n-1)
        - residual standard error:
          - σ(̊εₙ) = √(SSE/(n-2))
    - "best fit/regression/least-square" line :
      - propriétés :
        - min. SSE
        - ∑εₙ = 0
        - ∑ xₙ*εₙ = 0
      - bx + a :
        - a et b sont les "regression coefficients"
        - b = cov(X,Y) / σ²(X)
            = cor(X,Y) / σ(X) * σ(Y)
        - a = μ(Y) - a*μ(X)
          - car regression line passe à travers (μ(X),μ(Y)), "center of mass", et a une slope b
    - [Pearson's] [coefficient de] correlation (r, cor(X,Y))
      - [-1,1] : plus éloigné de 0, meilleure fit
      - r = cor(X,Y) = cov(X,Y)/(σ(X)*σ(Y))
      - donc comme la covariance, mais ajusté par la grandeur de X et Y (dimensionless)
    - coefficient de détermination :
      - [0,1] : plus éloigné de 0, meilleure fit
      - r²
      - aussi égal à 1 - SSE / SST
        - où SST = σ²(Y)*(n-1)
        - donc ratio entre variance normale et "explained variance"
      - r² (et r) augmente mécaniquement avec nombre d'explanatory variables (regressors)
        - comprend donc ajout de regressors au même coefficient ou non : Y = bX + a -> Y = cZ + bX + a, ou -> Y = cZ² + bX + a
        - adjusted r², noté souvent ̅r² (mais noté dans ma doc. adj.r² pour ne pas confondre avec ̅r):
          - corrige cela
          - soit p nombre de regressors (1 pour une simple regression Y = bX + a) :
            - adj.r² = 1 - (1 - r²)*(n-1)/(n-1-p)
        - sert à comparer deux modèles au nombre de regressors différent. Sinon utiliser r²
          - r² ne dépend pas d'une régression donnée, adj.r² si
  - degré of freedom :
    - degré polynomial de la regression - 1
    - augmenter nombre de degré de freedom a tendance a améliorer r
  - régression linéaire, cad regression coefficients, d'un sample sont une estimation de ceux de la population
    - on peut donc :
      - calculer un ̊a et b̊, et interval de confiance
      - tester H₀: a == 0 ou b == 0
    - l'estimation de a et b cherche à minimiser SSE
      - dépend donc de ̊εₙ
      - par conséquent sampling error de ̊εₙ induit sampling error de ̊a et ̊b
      - calculs suivants supposent que ̊εₙ ~ N(), ou que n est grand, auquel cas il le suit (CLT)
    - Calcul de ̊a et b̊ :
      - b̊ = t(n-2) * σ(̊εₙ) / σ(X) / √(n-1) + ̅b
        - donc t = b₁ / σ(̊εₙ) * σ(X) * √(n-1), et suit t(n-2)[+(b₁-b₀)]
      - b̊² = F(1,n-2) * σ²(̊εₙ) / σ²(X) / (n-1) + ̅b²
        - donc F = b²₁ / σ²(̊εₙ) * σ²(X) * (n-1), et suit F(1,n-2)[+(b²₁-b²₀)]
      - ̊a = t(n-2) * σ(b̊) * √(μ(X²))) + ̅a
        - donc t = a₁ / σ(b̊) * √(μ(X²)), et suit t(n-2)[+(a₁-a₀)]
    - tests :
      - H₀: b == 0 :
        - si b == 0, alors Y est indépendant de X
          - les σ²(Y) peut être grande (yₙ varient) mais σ²(Y) n'est pas expliquée par X


Simulation :
  - utile par exemple :
    - connaissant une taille n, et supposant une ̅s
    - mais ignorant Distribution de sampleStat
    - pour :
      - vérifier qu'un sampleStat suit bien une loi normale
      - sinon, tenter de l'estimer
      - ou pour l'utiliser comme pdf/cdf/intervals de confiance sur un cas donné
  - comment :
    - génération de m sample(n) que possible suivant ̅s, ce qui produit un sampleStat
    - plus m est grand, plus proche du vrai/théorique sampleStat
  - simulation inutile si l'on connaît propriétés de sampleStat à partir de ̅s et n

Computation sur ̅s :
  - ̅s + a :
    - conserve shape et σ de ̅s
    - ̅μ += a
  - ̅s * a :
    - conserve shape de ̅s
    - ̅μ et ̅σ *= a
    - densité max. /= a (pour pdf)
  - ̅s₁ + ̅s₂ :
    - mix des shapes :
      - si même ̅s, CLT
    - ̅μ = ̅μ₁ + ̅μ₂
    - ̅σ² = ̅σ²₁ + ̅σ²₂
  - ̅s₁ * s̅₂ :
    - mix des shapes :
      - N() * N() -> N()
    - ̅μ = ̅μ₁ * ̅μ₂
    - pour N() * Exponential() :
      - ̅σ² = 2 * ̅σ²₁ * ̅σ²₂
    - pour N() * N() :
      - ̅σ² = 2 * (̅σ²₁ * ̅σ²₂)²

NA :
  - si NA, réduire n à sa vraie valeur
  - parfois NA contient des infos par lui-même
    - dépend du contexte de l'experiment
    - dans ce cas, inclure NA dans le calcul et les transtyper vers une valeur

Propositions suivantes sont équivalentes (si l'un, alors les autres) :
  - discret :
    - Pour un set de n éléments, où N = (b-a), et n' = n-1 :
      - et pas 2 éléments avec même valeur
    - alors :
      - éléments ~ Unif.disc.(a,b)
      - distance entre un élément et le suivant ~ Geom(n'/N)
        - équivaut à: distance entre élément i et élément i+α, selon ordre croissant ~ NB(α,n'/N)
        - seulement pour n suffisamment grand, n > 30 pour 95% d'approx.
      - Nb éléments dans un subset de taille m ~ B(m,n/N)
        - marche pas pour un subset trop important
          - par ex. sur un subset de taille N, nb. éléments est connu (n), non une suite de m trials avec prob. n/N
          - cela est dû au fait que prob. est induite du nombre connu d'éléments. Sinon, subset important marche aussi
  - continu :
    - Pour un set de n éléments de longueur N, où N = b-a :
      - éléments ~ U(a,b)
        - distance absolue entre deux éléments (dont élément et élément suivant, selon ordre de sélection)
          ~ Triangular(0,b-a,0)
      - distance entre un élément et le suivant ~ Exp(n'/N)
        - car n'/N devenant infinitésimal, Geom(n'/N) =~ Geom((n'/N)/((n'/N)+1)) = Exp(n'/N)
        - équivaut à: distance entre élément i et élément i+α ~ Erlang(α,n'/N)
        - seulement pour n suffisamment grand, n > 30 pour 95% d'approx.
        - distance(i,i+α)/distance(i,i+α+1) ~ U(0,1)
          - car si X et Y ~ Exp(), alors X/(X+Y) ~ U(0,1)
      - Nb éléments dans un subset de taille m ~ Pois(n*m/N)

Poisson process :
  - cas continu des propositions précédentes :
    - ou le set est le temps, ou l'espace (plane, volume, etc.)
  - Assumptions suffisantes pour entrer dans cas précédent :
    - pas 2 succès simultanés (car espace infinitésimal)
    - prob. des events sont indépendants les unes des autres
  - plutôt que d'utiliser b-a et n (n/N), définit seulement λ, le nb de succès moyen pour une unité temporelle donnée (ex. : secondes)
  - Résumé :
    - Nb. succès pour ω units temporelles ~ Poisson(ω*λ)
    - Interv.temps (en units temportelles) entre deux succès ~ Exp(λ) (interval moyen = 1/λ)
  - ex:
    - si 10 succès pour 2 secondes, alors Nb.succès suit Poisson(2*λ) = 10 (n est en secondes), donc λ = 10/2 = 5,
      et Interval.temps ~ Exp(5)
  - en général représenté par un graph avec temps t en abscisse et N(t) (nb.succès) en ordonnée
    - N(t) est un counting process, avec des incréments égaux et entiers donc
  - non-homogeneous : quand λ change avec le temps

Faire ses propres distributions :
  - cdf = ∫pdf (ou pdf = antider.(cdf))
  - qdf = cdf⁻¹
  - PRNS = qdf prenant U(0,1) comme input

Time series :
  - Y ~ X, où X est le temps, en général mesuré à interval régulier
    - par conséquent, X est ordered (ce qui n'est pas le cas pour tout Y ~ X)
  - en général line charts
  - étude par exemple des cycles
  - analysis (past) vs forecasting

Empirical probablity :
  - équivaut à relative frequency
  - empirical cdf :
    - step function où chaque step == chaque valeur (sorted), augmentant prob. de 1/n
    - but est d'estimer la cdf réelle à partir d'un sample
  - empirical pdf :
    - pareil pour pdf
    - KDE (Kernel-density Estimation) :
      - non step-function
      - smoothed en fonction d'un paramètre "bw" (bandwidth)
        - bandwidth selector sélectionne le bw
          - ex. connu est nrd : min(σ,IRQ/1.34)*0.9(ou 1.06) / ⁵√n
