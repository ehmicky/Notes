
                                  ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
                                  ‚îÉ   STATISTICS   ‚îÉ
                                  ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ

Ma d√©finition des statistiques :
  - statistiques inf√©rentielles/inductive :
      -> √©tude de l'impr√©visibilit√©
        -  science attribuant des probablit√©s aussi fiables que possibles √† des ph√©nom√®nes qu'un individu ou groupe n'arrive pas √† pr√©dire
      -> mani√®re de rationaliser/chiffrer et rendre pr√©visible un monde vivant et al√©atoire
      -> par d√©finition, jamais de 100% de chance, sinon pas besoin de statistiques
  - statistiques descriptives/explanatory :
      -> √©tude de la variabilit√©
        - diff√©rentes valeures (i.e. √©tude de la diff√©rence entre les valeurs) que peut avoir une population pour un factor donn√©

POSITIVE/NEGATIVE ==>             #"Condition positive|negative" (CP|CN):
                                  #  - number of positive|negative possible solutions
                                  #  - "prevalence": CP/(CP+CN)
                                  #"Predicted condition positive|negative":
                                  #  - number of positive|negative results
                                  #  - can be:
                                  #     - "true positive" (TP)/"statistical power"
                                  #     - "true negative" (TN)
                                  #     - "false positive" (FP)/"type I error"/"false alarm"
                                  #     - "false negative" (FN)/"type II error"/"miss"
                                  #Predicted condition:
                                  #  - % of positives|negatives results that are true|false
                                  #  - ratios:
                                  #     - "precision": TP/(TP+FP)
                                  #        - also called "positive predictive value" (PPV)
                                  #        - % of positives that are true
                                  #        - inverse: "false discovery rate" (FDR), FP/(TP+FP)
                                  #     - "negative predictive value" (NPV): TN/(TN+FN)
                                  #        - % of negatives that are true
                                  #        - inverse: "false omission rate" (FOR), FN/(TN+FN)
                                  #True condition:
                                  #  - % of positive|negatives results among all possible solutions
                                  #  - ratios:
                                  #     - "sensitivy": TP/CP
                                  #        - % of positive results among all possible solutions
                                  #        - also called "recall"/"probability of detection"/
                                  #          "true positive rate" (TPR)/"statistical power"
                                  #        - inverse: "miss rate"/"false negative rate" (FNR)/"false alarm ratio" / Œ≤, FN/CP
                                  #     - "specificity": TN/CN
                                  #        - % of negative results among all possible solutions
                                  #        - also called "true negative rate" (TNR)
                                  #        - inverse: "fall-out"/"false positive rate" (FPR) / "significance level" / Œ±, FP/CN
                                  #     - "positive likelihood ratio" (LR+): TPR/FPR, i.e. (TP*CN)/(FP*CP)
                                  #     - "negative likelihood ratio" (LR-): FNR/TNR, i.e. (FN*CN)/(TN*CP)
                                  #     - "diagnostic odds ratio" (DOR): LR+/LR-, i.e. (TP*TN)/(FP*FN)
                                  #     - "F‚ÇÅ score": 2/(1/PPV + 1/TPR), i.e. combine PPV and TPR

Statistiques :
  - inferentiale/inductive :
    - utilise probablit√©s
    - plus par rapport √† la sampling distribution du sample que la distribution du sample
    - implique des conclusions et hypoth√®ses
    - exemples :
      - statistical independence, conditional probablity
      - souvent font une estimation, avec un interval de confidence
  - descriptives :
    - description d'une collection of data, de mani√®re chiffr√©e ou visuelle (plots)
    - plus par rapport √† la distribution du sample que la sampling distribution du sample
    - summary statistics :
      - description synth√©tique
    - types :
      - location :
        - measures of central tendency / "expectation":
          - arithmetic mean : ‚àëx‚Çô / n, not√© ÃÑx (ou Œº pour une population)
          - median :
            - x tel que P(x‚Çô <= x) = 0.5
            - si pair, prend la moyenne des deux √©l√©ments du milieu
          - mode :
            - x tel que P(P(x‚Çô) > P(x)) = 0
            - most occuring value
          - tail :
            - least occuring region value de la distribution
          - geometric mean :
            - ‚Çô‚àö ‚àèS (taille n)
            - utilis√© √† la place d'arithmetic mean pour comparer des sets avec des ranges diff√©rents mais pond√©r√©s pareil
            - ex :
              - set dont premier nombre est entre 0 et 1, et deuxi√®me entre 0 et 100
              - comparaison de tels sets, mais 1er et 2nd nombre doivent avoir m√™me importance
                - avec arithmetic mean, 1er aurait 100 fois d'importance
          - harmonic mean :
            - n / ‚àë x‚Çô‚Åª¬π
          - weighted mean :
            - par exemple notes avec coefficients
          - mid-range :
            - (max+min)/2
            - rarement utilis√©, tr√®s peu robuste ni fiable
          - mid-hinge :
            - (Q‚ÇÉ+Q‚ÇÅ)/2
            - plus robuste que mid-range
          - trimean (TM) :
            - (Q‚ÇÅ+2*median+Q‚ÇÉ)/4
      - Spread :
        - [average/mean] [absolute] deviation :
          - ( ‚àës‚Çô‚àà S dev(x‚Çô) ) / (n-1)
            - o√π dev(x‚Çô) = |Œº(x‚Çô) - f(X)|
              - c'est la [non-average] absolute deviation
              - f(x) mesure la central tendency. Peut √™tre :
                - mean   -> deviation from the mean
                - median -> deviation from the median
                - mode   -> deviation from the mode
            - mean deviation <= standard deviation
            - comme variance pour division par n
            - attention [mean] [absolute] deviation from the median != median absolute deviation
        - median absolute deviation (MAD) :
          - comme mean deviation, sauf :
            - qu'effectue une median, non une somme/mean
            - utilise toujours mean comme dev()
        - variance :
          - ( ‚àës‚Çô‚àà S dev(x‚Çô)¬≤ ) / (n-1)
            - o√π dev est la mean deviation
            - division par n si S est la population enti√®re :
              - n s'approchant d'‚àû, /(n-1) ou /n ne fait plus de diff√©rence
              - mais ÃäœÉ¬≤ pour √©valuer ÃÖœÉ¬≤ est un meilleur estimator si (n-1) que si n
              - donc n-1 pour la sample variance, et n pour la population enti√®re
          - souvent not√© œÉ¬≤
            - parfois not√© s¬≤ si œÉ¬≤‚ÇÅ, par opposition √† ÃÖœÉ¬≤
          - œÉ¬≤(X)         = (Œº(X¬≤) - Œº(X)¬≤)*(n/(n-1))
                          = (‚àëX¬≤ - (‚àëX)¬≤/n)/(n-1)
            œÉ¬≤(X)*n*(n-1) = n‚àëX¬≤ - (‚àëX)¬≤
        - standard deviation :
          - ‚àövariance
          - souvent not√© œÉ
          - moins utilise si fort skewness
          - Chebyshev's inequality :
            - P(dev(E(x‚Çô), Œº) >= t) <= œÉ¬≤/t¬≤
              - soit : si P a une variance x, alors la possibilit√© qu'un x‚Çô ait un √©cart √† la moyenne d'au moins t est inf√©rieur √† x/t¬≤
            - autre formulation : P(dev(E(x‚Çô), Œº) >= tœÉ ) <= 1/t¬≤
              - soit : la possibilit√© d'un √©cart d'au moins t fois la standard deviation est inf√©rieur √† 1/t¬≤ :
                - √©cart >= 1.4œÉ : <= 50%
                - √©cart >= 2œÉ   : <= 25%
                - √©cart >= 4œÉ   : <= 6.25%
        - coefficient of variation (CV)/united risk :
          - œÉ/Œº
          - "pourcentage" de œÉ par rapport √† moyenne -> "dimensionless" quantity (cad ratio,pourcentage)
          - relative standard deviation (RSD) :
            - |CV|
        - index/coefficient of dispersion/variance-to-mean ratio(VMR) :
          - œÉ¬≤/Œº
          - relative variance :
            - |VMR|
        - Quartiles :
          - valeurs s√©parant en quatre S
          - IQR (InterQuartile Range) / H-spread : Range entre Q‚ÇÅ et Q‚ÇÉ
          - lower-hinge et upper-hinge : autre nom de Q‚ÇÅ et Q‚ÇÉ
          - Tukey's five numbers summary : { min, Q‚ÇÅ, Q¬≤, Q‚ÇÉ, max }
        - quantile function / inverse distribution function :
          - pour un pourcentage p "percentile" donn√©, donne la valeur x tel que p% des valeurs sont derri√®re x
          - par exemple f(0.25) donne Q‚ÇÅ
          - qdf = cdf‚Åª¬π
            - par ex., si cdf = x¬≤, qdf = ‚àöx
        - Range :
          - longueur du plus petit interval comprenant l'ensemble de S
          - toujours >= 2œÉ
      - Shape :
        - skewness :
          - asymetry d'une distribution
          - si n√©gatif tail est plus grande √† gauche, sinon contraire
            - median - mean ou mode - mean peut donner une id√©e
        - kurtosis :
          - peackness d'une distribution
            - si low peak, fat tails
          - type :
            - mesokurtic : 0 (ex: normal distribution)
            - leptokurtic : > 0 (ex: Cauchy, Student's t, Poisson, exponential)
            - platykurtic : < 0

Standard score :
  - si x ~ Dist(ÃÖŒº,ÃÖœÉ), standard score(x) = (x - ÃÖŒº)/ÃÖœÉ
  - si Dist() est N(), "z-score"
    - t() : "T-score"
    - œá¬≤() : "Chi-score"
    - F() : "F-score"

Comparaisons de deux Dist() :
  - raisons :
    - entre une Dist() de data et une th√©orique : D‚ÇÅ vs D‚ÇÄ
      - ex. voir si D‚ÇÅ ~ N()
    - ou entre deux Dist() de data : D‚ÇÅ vs D‚ÇÇ
  - PP Plot :
    - soit cdf(D‚ÇÅ)~cdf(D‚ÇÇ)
    - suit ligne y = x si cdf(D‚ÇÅ) = cdf(D‚ÇÇ)
  - QQ Plot :
    - soit quantile.f(D‚ÇÅ)~quantile.f(D‚ÇÇ)
    - il est typique d'avoir les valeurs extr√™mes moins sur la ligne

Probability simple (Bernoulli) :
  - essai isol√© :
    - Chance de succ√®s : p (en %, par exemple 0.5)
    - Chance de non-succ√®s : ~p, soit 1 - p
  - n tentatives :
    - Chance de que des succ√®s    : p‚Åø
    - Chance d'aucun succ√®s       : (~p)‚Åø
    - Chance d'au moins un succ√®s : ~((~p)‚Åø)
    - Chance d'au moins un √©chec  : ~(p‚Åø)
    - Chance de m succ√®s/√©checs parmi n : cf Binomial

Experiment :
  - Probability space est un triplet (Œ©,ùìï,P) :
    - Sample space, not√© S ou Œ©, est ensemble des simple events possibles
      - une random variable est une fonction qui assigne un outcome √† chaque simple event : X: Œ© -> S, o√π S est appel√© "state
        space"
        - not√© X, Y, ...
        - peut √™tre :
          - discr√®te et infinie : notamment si S == N
          - discr√®te et finie   : notamment si S est factorial (subset de N)
          - continue et infinie : notamment si S == R
          - continue et finie   : notamment si S est subset de R
      - les simple events sont not√©s œâ‚Çô
      - plusieurs random variables peuvent assign√©es au m√™me Œ©
    - Le œÉ-algebra ùìï comprend ensemble des events E‚Çô possibles √† partir des simples events
      - events sont not√© E‚Çô ou A,B,C,...
      - un E‚Çô est une combinaison de simple events, 2^Œ©
      - ùìï \ E : complementary event de E, not√© ƒí
    - Probability function, P :
      - fonction assignant prob. √† chaque E‚Çô
      - donc P: ùìï -> [0,1]
      - si :
        - continu: Probability density function, pdf
        - discret:
          - Probability mass function, pmf, ou frequency function
          - si fini, Categorical distribution
      - P(Œ©) = 1
    - ex., one-coin flip :
      - Œ© = {H,T}
        ùìï = 2^Œ© = { ‚àÖ, {H}, {T}, {H,T} }
        P(‚àÖ) = 0, P({H}) = 0.5, P({T}) = 0.5, P({H,T}) = 1
  - repr√©sentation graphique :
    - ex. pour deux random variables X et Y :
      - X et Y sont axes x et z
      - P() est axe y, assigne prob. aux variables X et Y
      - chaque simple event (Œ©) peut √™tre repr√©sent√© comme une zone sur le place (X,Y) comprenant les outcomes x‚Çô et y‚Çô
        produits par ce simple event
      - ùìï est ensemble des combinaisons de zones
  - Probability space (et donc events et variables) est toujours "random", au sens statistique du terme :
    - cad aucun P(E‚Çô) = 1
    - c'est-√†-dire impr√©visible de mani√®re certaine par le sujet
    - si 100% de chance, pas de probablit√© ni de statistiques sur ce ph√©nom√®ne.

Events probablities :
  - P(ùìï) = 1, donc :
    - 0 <= P(E) <= 1
    - P(ƒí) = 1 - P(E)
  - si E‚ÇÅ ‚äÜ E‚ÇÇ, alors P(E‚ÇÅ) <= P(E‚ÇÇ)
  - Joint probablity :
    - probablity qu'un r√©sultat donn√© satisfasse √† la fois E‚ÇÅ et E‚ÇÇ
    - not√© E‚ÇÅ ‚à© E‚ÇÇ
  - Conditional probablity :
    - probablit√©, lors d'un event E‚ÇÅ, que celui-ci soit aussi un event E‚ÇÇ
    - not√© P(E‚ÇÇ|E‚ÇÅ), soit P(E‚ÇÅ ‚à© E‚ÇÇ) / P(E‚ÇÅ) (pour P(E‚ÇÅ) > 0)
    - par cons√©quent (multiplication axiom) : P(E‚ÇÅ ‚à© E‚ÇÇ) = P(E‚ÇÇ|E‚ÇÅ) * P(E‚ÇÅ)
    - Bayes' theorem : P(E‚ÇÅ|E‚ÇÇ) = P(E‚ÇÇ|E‚ÇÅ) * P(E‚ÇÅ) / P(E‚ÇÇ) (pour P(E‚ÇÇ) > 0)
  - Law of total probability :
    - Si E‚ÇÅ...E‚Çô sont des events disjoints et que leur union est Œ©
    - alors P(A) = ‚àë P(A|E‚Çô) * P(E‚Çô)
    - Bayes' rule (raisonnement inverse) :
      - P(E‚Çñ|A) = P(A|E‚Çñ) * P(E‚Çñ) / ‚àë P(A|E‚Çô)*P(E‚Çô)
  - P(E‚ÇÅ) + P(E‚ÇÇ) = P(E‚ÇÅ ‚à™ E‚ÇÇ) + P(E‚ÇÅ ‚à© E‚ÇÇ)
    - P(E‚ÇÅ ‚à™ E‚ÇÇ) = P(E‚ÇÅ) + P(E‚ÇÇ) - P(E‚ÇÅ ‚à© E‚ÇÇ)
    - si E‚ÇÅ et E‚ÇÇ disjoints, P(E‚ÇÅ) + P(E‚ÇÇ) = P(E‚ÇÅ ‚à™ E‚ÇÇ)
  - Statical independence :
    - fait que si l'on sait qu'un r√©sultat satisfait E‚ÇÅ, cela ne modifie pas chances que cela satisfasse E‚ÇÇ
    - donc P(E‚ÇÇ|E‚ÇÅ) = P(E‚ÇÇ), soit "probablit√© de E‚ÇÇ en sachant que E‚ÇÅ intervient" = "probablit√© de E‚ÇÇ sans savoir si E‚ÇÅ intervient"
      - est sym√©trique car implique aussi que P(E‚ÇÅ|E‚ÇÇ) = P(E‚ÇÅ)
      - implique aussi que P(E‚ÇÅ ‚à© E‚ÇÇ) = P(E‚ÇÅ) * P(E‚ÇÇ) (d√©finition √† pr√©f√©rer)
      - si :
         - P(E‚ÇÅ ‚à© E‚ÇÇ) < P(E‚ÇÅ) * P(E‚ÇÇ), alors occurence d'E‚ÇÅ ou E‚ÇÇ diminue chance d'occurence de l'autre
            - cad P(E‚ÇÅ|E‚ÇÇ) > P(E‚ÇÅ), et P(E‚ÇÇ|E‚ÇÅ) > P(E‚ÇÇ)
         - inverse
    - "iid" (idependant and identically distributed) variables :
      - signifient des variates :
        - statistically independent entre elles (g√©n√©ration d'une ne modifie pas prob. des autres)
        - ayant la m√™me distribution
      - en g√©n√©ral, PRNG, PRNS, et th√©or√®me statistiques supposent des iid. variables
  - Multiple independence :
    - ind√©pendance d'un event par rapport √† l'occurence non d'un event, mais de plusieurs events :
      - P(E‚ÇÄ|E‚ÇÅ ‚à© E‚ÇÇ) = P(E‚ÇÄ)
    - si un seul event, "pairwise independence"
    - Si tous events sont pairwise independents, ne garantie pas les multiple independences
    - mutual independence :
      - si P(E‚ÇÅ ‚à© ... ‚à© E‚Çô) = P(E‚ÇÅ) * ... * P(E‚Çô)
        - ainsi que pour tout regroupement dans E‚ÇÅ...E‚Çô
      - donc pour chaque E‚Çô, P(E‚Çô|E... ‚à© E...) = P(E‚Çô)
  - P(E‚ÇÅ) =, < ou > P(E‚ÇÇ) √©quivaut √† P(E‚ÇÅ|E‚ÇÇ) =, < ou > P(E‚ÇÇ|E‚ÇÅ)
      - cad : si probabilit√© de E‚ÇÅ est faible et que E‚ÇÇ est fort, il est plus courant pour un individu de E‚ÇÅ de faire E‚ÇÇ, que
              l'inverse.
              Si probabilit√© identiques, aussi courant pour l'un que pour l'autre.
  - Selection bias :
    - consid√©rer que P(E‚ÇÅ|E‚ÇÇ) = P(E‚ÇÅ), et oublier impact de condition E‚ÇÇ
    - par exemple, personne ayant souvent E‚ÇÅ car E‚ÇÇ arrive souvent pour elle, et que P(E‚ÇÅ|E‚ÇÇ) est √©lev√©, pourtant P(E‚ÇÅ) peut
      etre faible de mani√®re g√©n√©rale
        - ex: demander assistance Linux : E‚ÇÅ = 0.01%, avoir barbe de geek : E‚ÇÇ = 5%, demander assistance Linux quand barbe de
          geek : 20%. Donc personne avec barbe de geek pense que demander assistance Linux = 20%.
  - autre fallacy :
    - penser que P(E‚ÇÅ|E‚ÇÇ) = P(E‚ÇÇ|E‚ÇÅ)
       - cela n'est vrai que si (et est vrai quand) P(E‚ÇÅ) = P(E‚ÇÇ) (cf dessus)
    - ex: hard drug users smoke majiruana, so (wrong ->) marijuana use hard drug
       - en effet marijuana population est bien plus grande que hard drug users

Causalit√© et correlation :
  - h√©t√©rog√©n√©it√© :
    - plusieurs facteurs possibles : P(X|Y,Z), "confounding factors"
    - il faut pouvoir les dissocier pour √©tudier P(X|Y)
  - correlation : P(X|Y)/P(X), soit P(X ‚à© Y)/(P(X)*P(Y))
    - si correlation = 1, alors statistical independence
  - causalit√© et bien plus compliqu√©e, et implique plus que correlation

Joint probability distribution :
  - comme une probability distribution normale, sauf que pour r√©union de plusieurs random variables
    - donc P(X ‚à© Y) pour bivariate
  - li√© √† un m√™me Œ© : s'il y a un x‚Çô, il y a aussi un y‚Çô
  - appel√© aussi multivariate distribution (bivariate si 2)
  - graphiquement, un graph de points (x‚Çô,y‚Çô), avec √©ventuellement axe z nombre d'occurences
    - par ex. graphiquement pour X et Y, P() en ordonn√©e, et X et Y autres axes
  - ne pas confondre avec des P() combinant avec d'autres op√©rateurs que ‚à© :
    - ex. : P(X ‚à© Y) != P(X + Y)
  - P(X ‚à© Y) d√©pend de P(X|Y) ou P(Y|X) :
    - P(X ‚à© Y) = P(X) * P(Y|X)
               = P(Y) * P(X|Y)
    - P(X ‚à© Y ‚à© Z) = P(X) * P(Y|X) * P(Z|Y,X) ("chain rule of probability")
    - si X et Y ind√©pendants (P(Y|X) = P(Y) et P(X|Y) = P(X)), alors P(X ‚à© Y) = P(X) * P(Y)
    - ‚àëP(X ‚à© Y) = 1
  - On peut penser √† P(X ‚à© Y) comme une matrice, avec X et Y comme dimensions, et P(X) et P(Y) des vecteurs
  - si continu, appel√© joint distribution fonction (jdf)

Conditional probability distribution :
  - comme jdf, sauf que non P(X ‚à© Y) mais P(Y|X)
  - P(Y|X) n'est pas la m√™me chose que P(X|Y)
  - si P(Y|X) = X, alors X et Y ind√©pendants
    - graphiquement, axe x ne modifie pas axe y,z
  - P(Y|X) = P(X ‚à© Y) / P(X)
  - si continu, appel√© conditional distribution fonction

Terminology :
  - data/outcomes :
    - ensemble des donn√©es r√©colt√©es sur un sample space
  - Parameter :
    - une statistique est d√©duite d'un sample space √† partir de data, un parameter d'une population
    - une statistique estime un parameter, de mani√®re plus ou moins fiable

Randomness :
  - variable d√©signe le x d'un graph (abscisse), pas seulement un event donn√©
    - contrairement √† une variable classique, qui prend une valeur f(x) donn√© pour une x donn√©
      - une random/schochastic variable a un ensemble de f(x) avec chacun une P(f(x))
      - une random variable est donc toute variable associ√©e √† une probability distribution
    - donc, pour qu'un variable soit random, il faut juste qu'il n'y ait aucun P() == 1
    - c'est pourquoi on peut par exemple parler de "Cauchy random variable", etc.
    - une random variable X ayant une pdf f() est not√©e X ~ f()
  - difference entre entropy et distribution
    - les P() de chaque x‚Çô peuvent diff√©rer : une distribution n'est pas forc√©ment uniforme
    - ex : lancer de deux d√©s est random, mais n'a pas une distribution uniforme
    - diff√©rence :
      - distribution est l'axe y, la P()
      - entropy est l'axe x, le fait qu'il y ait plusieurs x‚Çô randomly possibles
  - random variate :
    - particular outcome x d'une random variable X, g√©n√©r√© suivant la probability distribution
    - g√©n√©ration est appel√© PRNS (pseudo-random number sampling)
      - contrairement √† PRNG, est soumis √† une probability distribution pas forc√©ment uniforme
      - peut √™tre g√©n√©r√© via l'inverse distribution avec comme argument U(0,1) :
        - ex : rnorm(n) -> qnorm(runif(n))
      - mais il existe des m√©thodes plus efficientes impl√©ment√©es peut-√™tre par rnorm, etc.
  - diff√©rence entre r√©sultat d'une experiment pass√© et pr√©vision :
    - relative frequency vs probabilit√©
    - "observed values" vs random variates
    - discret vs discret ou continu
    - cependant peut utiliser m√™me raisonnement

Philosophie de randomness :
  - vues :
    - Bayesian/classical (Bayes/Laplace/Bernoulli, XVII√®me) :
      - experiment cherche √† prouver une prob. th√©orique
      - probabilit√© d'1/n est lorsque n diff√©rents mutually exclusive outcomes d'une experiment sont possibles, et que aucun
        n'est plus favoris√© que l'autre (principe d'indiff√©rence)
      - pb : ne marche que pour variable avec une prob. distr. uniforme
      - types :
        - objectiviste/physical:
          - unpredictable est subjectif, random objectif
          - probabilit√© est une tendance r√©elle
        - subjectiviste/evidential:
          - random est l'absence de connaissance
          - probabilit√© est une confiance subjective donn√©e
    - fr√©quentistes (Neyman/Pearson/Venn/Fischer):
      - experiment cherche √† d√©terminer une prob. inconnue
      - probability est limit (n=‚àû) de la fr√©quence relative
    - propensistes (Popper/Miller)
 - chaotic vs quantique :
   - vue non-d√©terministe :
     - quantique est random
     - syst√®me chaotique est d√©terministe, mais affect√© par un nombre √©norme de petits facteurs (butterfly effect), qu'il est
       infeasible pour un √™tre humain de conna√Ætre
   - vue d√©terministe :
     - pareil pour chaos
     - mais quantique est aussi chaotique, seulement nous ignorons les causes pour l'√©tat actuel des sciences

Expected values :
  - expected value/mean, E(E‚ÇÅ) ou X(E‚ÇÅ) :
    - si simple event, outcome de X lorsque E‚ÇÅ
    - sinon, pond√©r√© ensemble des simple events comprenant E‚ÇÅ par leur probabilit√© :
      - E(E‚ÇÅ) = (‚àëE‚Çô‚àà E‚ÇÅ E(E‚Çô) * P(E‚Çô) ) / P(E‚ÇÅ)
      - pour l'ensemble de ùìï :
        - E(X) = ‚àëx‚àà R x*P(X = x)

Distribution :
  - pdf, cf plus haut
  - cumulative distribution function (cdf) :
    - non chance que x = x‚Çô, mais que x >= x‚Çô
    - il s'agit donc de ‚à´ de la distribution
  - types :
    - discrete, continue ou mix entre disrete et continuous distribution :
      - par ex., si pile, E(X) = 1, si face, E(X) = random entre 0 et 1

Robustness :
  - outliers :
    - x‚Çô √©tant tr√®s √©loign√©e du reste des x‚Çô
    - doivent √™tre √©cart√©s car fausse statistiques
  - cause :
    - erreur de mesure
    - distribution "heavy-tailed"
  - statistiques les √©loignant sont dites "robust"/resistant :
    - breakdown point :
      - nombre de x‚Çô outliers pouvant √™tre pr√©sents sans fausser calcul
      - maximum 0.5
      - mean a breakdown point de 0, median 0.5 => median est bien plus robuste
      - median absolute deviation plus robuste que standard deviation
    - range a breakdown point de 0, IQR 0.25
    - standard deviation est pas robuste, expected median variation si
  - truncated mean :
    - discard une part du sample avant de faire le mean
      - souvent un m√™me pourcentage sur la tail basse et haute
      - interquartile mean (IQM) : ne garde que l'IQR
    - ensuite, il faut remultiplier par 1 / pourcentage perdu
      - ex : par 2 pour l'IQM
  - conflit entre :
    - + precision, mais - signifiance :
      - si pas assez robuste, r√©sultats faux
      - mais si trop d'outliers, r√©sultat proche de la normalit√© pour le ph√©nom√®ne, et pas de pr√©diction particuli√®re
    - + precision, mais - accuracy :
      - estimator robuste vs estimator optimal

Plots :
  - point plot :
    - pour indiquer valeur discr√®te d'un factor donn√©
  - bar plot :
    - pour indiquer quantit√©/compte d'un factor donn√©
  - histogramme :
    - comme bar plot, mais en utilisant non des valeurs discr√®tes mais des ranges (pour valeurs real, ou un grand nombre de
      valeurs discr√®tes)
  - box plot :
    - centre est Q‚ÇÅ-median-Q‚ÇÉ
    - extr√©mit√©s (appel√©e "whiskers") peut √™tre :
      - minimum et maximum, dans le range :
        - tout
        - [Q‚ÇÅ-1.5*IQR, Q‚ÇÉ+1.5*IQR]
          - 1.5*IQR = 2œÉ pour Normal, soit range de 95% des r√©sultats
        - [Q‚ÇÅ-œÉ, Q‚ÇÉ+œÉ]
      - premier et dernier d√©cile
    - parfois outliers (events en dehors des extr√©mit√©s) sont indiqu√©s par des points

Pond√©ration pour random variable :
  - il faut :
    - pond√©rer chaque E‚Çô par P(E‚Çô) (les E plus probables ont plus de poids)
      - le tout divis√© par ‚àë P(E‚Çô) si pmf et ‚à´ P(E‚Çô) si pdf (si sur tout S, alors division par 1)
    - ne pas diviser par nombre d'events (d√©j√† mis en moyenne par multiplication par P(E‚Çô))
  - ex : œÉ¬≤ = ‚àë/‚à´ dev(E‚Çô)¬≤ * P(E‚Çô)

pdf et pmf :
  - P(x‚Çô) :
    - Pour une pmf, probabilit√© de x‚Çô
    - Pour une pdf, P(x‚Çô) est la densit√©, non la probabilit√©. Ne signifie pas probabilit√© de x‚Çô (toujour infinit√©simale)
      - signifie : densit√© moyenne sur 1 unit√© x = probabilit√© de cet unit√©, densit√© sur 1/2 unit√© = prob.unit√©*2, etc.
      - cons√©quence :
        - permet de faire int√©grale pour cdf
          - la densit√© P(x) d√©pend de range de x, de sorte que ‚à´ P(X) == 1
          - e.g., P(x) de U(0,2) est 0.5 et U(0,4) est 0.25
        - peut comparer pdf et pmf, si on prend que valeur discr√®te de pdf
          - ex: pdf de N(np,np(1-p)) approxime B(n,p)
        - pour une cdf et cmf, il faut rajouter 0.5 √† la valeur discr√®te de la cdf ("continuity correction"), car sinon
          derni√®re valeur ne prend pas l'unit√© enti√®re autour d'elle mais que la moiti√©.
  - probabilit√© cumul√©e Pc(x‚Çô) :
    - Pour une pmf, ‚àëmin(S),x‚Çô P()
    - Pour une pdf, ‚à´min(S),x‚Çô P()
  - P(E‚Çô)
    - Pc(max(E‚Çô)) - Pc(min(E‚Çô))

Erreur lors d'inf√©rence :
  - 2 stats correl√©es n'implique pas relation de causalit√©.
  - √©tudes fond√©es √† des fins commerciales
  - mani√®re incorrecte de pr√©senter graphs ou data de mani√®re √† appuyer un discours
  - "confounding" : difficult√© de s√©parer facteurs les uns des autres

Measures :
  - variables sont ce qui est mesur√© dans le sample
  - types :
    - quantitatif/numeric
      - discr√®te (integral)
      - continue (real)
    - qualitatif/factor/category
      - on manipule alors leur nombre d'occurences/frequency
  - un individu d'un sample est un "sujet"
  - un ensemble de data pour un ensemble de variables sur un sujet est une "observation"
  - accurate vs precise :
    - accuracy :
      - diff√©rence entre valeur estim√©e et valeur r√©elle
      - graphiquement, diff√©rence entre "centre" de la pdf de l'estimation et valeur r√©elle
    - precision :
      - diff√©rence entre valeurs estim√©es sur plusieurs experiments identiques
      - aussi appel√© degr√© de reproductibility(instruments diff√©rents)/r√©p√©tabilit√©(m√™mes instruments)
      - graphiquement, √©tendue de SampleMean
    - √©galement tiers : measurement resolution
      - diff√©rence minimale entre deux estimation

Distributions :
  - parameters :
    - types :
      - shape : change la shape
      - scale : change le spread
        - rate : change le spread, de mani√®re inverse (1/scale)
      - location : change la mean
  - notations :
    - Distribution(Var...)
    - X ~ Distribution(Var...) signifie random variable X ayant une distribution Distribution
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Distribution | Binomial      | Poisson            | Uniform cont.| Exponential| Neg.Binomial | Normal/Gaussian/Z            |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Type         | Discr√®te      | Discr√®te           | Continue     | Continue   | Discr√®te     | Continue                     |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Aspect       | Bell-curve    | Bell-curve         | Rectangle    | Descend    | Comme B()    | Bell-curve                   |
|              | sauf n ou p   | Ecras√© √† gauche    |              | rapidement |              |                              |
|              | petit.        | si Œª faible.       |              | puis longue|              |                              |
|              |               | A droite tail ‚àû    |              | tail       |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Notation     | B(n,p)        | Pois(Œª)            | U(a,b)       |            | NB(r,p)      | N(Œº,œÉ¬≤)                      |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Param√®tres   | n: nb essais  | Œª: mean            | a: min       | Œª: exp rate| r: nb de suc.| Œº: mean                      |
|              | p: % de succ√®s| (n:[0,‚àû],p:[0,1])  | b: max       |            | p: % √©checs  | œÉ¬≤                           |
|              |               |                    |              |            |              | Standard normal : N(0,1)     |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| x            | nb de succ√®s  | nb de succ√®s       |              | Temps      | nb d'√©checs  |                              |
|              | (essais sont  |                    |              |            | avant que r  |                              |
|              |  ind√©pendants)|                    |              |            | succ√®s arriv |                              |
|              |               |                    |              |            | (essais sont |                              |
|              |               |                    |              |            | ind√©pendants)|                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| range        | [0,n]         | [0,‚àû]              | [a,b]        | [0,‚àû]      | [0,‚àû]        | R                            |
| mean         | n*p           | Œª                  | (a+b)/2      | 1/Œª        | p*r/(1-p)    | Œº                            |
| median       | ‚é£n*p‚é¶         | ‚é£Œª + 1/3 - 1/50Œª‚é¶  | (a+b)/2      | ln(2)/Œª    |              | Œº                            |
| variance     | np(1-p)       | Œª                  | (b-a)¬≤/12    | 1/Œª¬≤       | p*r/(1-p)¬≤   | œÉ¬≤                           |
| mode         |               |                    | a            |            |              | Œº                            |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| P(x)         | (n) * p^x     | Œª^x / x! / ‚ÑÆ^Œª     | 1/(b-a)      | Œª‚ÑÆ^(-Œªx)   | (x+r-1) *    | Gaussian function:           |
| (density f())| (x)           |                    | si x‚àà [a,b]  | (P(0) = Œª) | (  x  )      | ‚ÑÆ^(-(x-Œº)¬≤/2œÉ¬≤) / œÉ‚àö(2œÄ)     |
|              | * (1-p)^(n-x) | ‚ÑÆ^Œª et Œª sont des  | 0 sinon      |            | (1-p)^r * p^x| ou œï((x-Œº)/œÉ)/œÉ              |
|              |               | scaling factor.    |              | Œª sont des |              | Standard normal œï/œÜ:         |
|              |               | L'essence est      |              | scaling    |              | ‚ÑÆ^(-x¬≤/2) / ‚àö(2œÄ)            |
|              |               | dans ‚ÑÆ^x/x!        |              | factor.    |              | Explication :                |
|              |               |                    |              | Essence est|              |  n^-x¬≤ : bell curve autour   |
|              |               |                    |              | dans ‚ÑÆ^-x  |              |          de [-2,2]           |
|              |               |                    |              |            |              |  /2    : ram√®ne √† [-1,1]     |
|              |               |                    |              |            |              |  /‚àö2œÄ,‚ÑÆ: pour que ‚à´P() = 1   |
|              |               |                    |              |            |              |  x-Œº   : ram√®ne √† [Œº-1,Œº+1]  |
|              |               |                    |              |            |              |  œÉ     : ram√®ne √† [Œº-œÉ,Œº+œÉ]  |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Pc(x)        |               |                    | (x-a)/(b-a)  | 1 - ‚ÑÆ^(-Œªx)|              |                              |
|              |               |                    | si x‚àà [a,b]  |            |              |                              |
|              |               |                    | 0 si x < a   |            |              |                              |
|              |               |                    | 1 si x > b   |            |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Examples     | Pile ou face  | Nb d'√©v√©n. sur une | Tout nombre  | Temps entre| Nb de pile   | ÃäŒº pour toute ÃÖs (si œÉ finite).|
|              | successifs    | p√©riode donn√©e, o√π:| entre a et b | 2 Pois(Œª)  | avant de     | Donc dist. de beaucoup de    |
|              | (p = 0.5)     |  - p et n sont     | avec chances | events ou  | faire r faces| measurements.                |
|              | Oui ou non √†  |    inconnus, n     | √©gales,      | entre 1    |              | Permet √©galement d'approximer|
|              | un sondage    |    tr√®s grand, et  | continu.     | instant et |              | autre distributions.         |
|              |               |    p tr√®s faible   |              | 1 Pois(Œª)  |              |                              |
|              |               |  - mais average    |              | event.     |              |                              |
|              |               |    rate Œª connu    |              | Temps entre|              |                              |
|              |               | Equivaut √†         |              | deux coups |              |                              |
|              |               | B(n,Œª/p) dans ces  |              | de tel.    |              |                              |
|              |               | conditions.        |              | P√©riode    |              |                              |
|              |               | Coups de tel. sur  |              | fonctionmt |              |                              |
|              |               | p√©riode donn√©e.    |              | d'une mach.|              |                              |
|              |               | Photons arrivant   |              | Ex: si     |              |                              |
|              |               | sur un t√©lescope.  |              | 8 appels/h,|              |                              |
|              |               | Voitures feu rouge.|              | prochain   |              |                              |
|              |               | Queue de clients.  |              | appel prob.|              |                              |
|              |               | Nb buts au foot.   |              | == P(8)    |              |                              |
|              |               |                    |              | heures     |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| R functions  | *binom        | *pois              | *unif        | *exp       | *nbinom      | *norm                        |
|              |               |                    |              |            | *geom (infra)|                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre        | B(1,p) est    | Cf plus bas,       | U(0,1)‚Åø      |Geom(Œª/(Œª+1)| Si r‚àà N,     |                              |
|              | appel√©        | poisson process.   |  = Be(1/n,1) | est la     | appel√©e      |                              |
|              | Bernoulli dst.|                    | pour n>0     | version    |"Pascal dist."|                              |
|              | dont un event |                    |              | discr√®te de|Sinon "Polya  |                              |
|              | est un        |                    |              | Exp(Œª)     | dist."       |                              |
|              |Bernoulli trial|                    |              |            |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)| Attention,    |                    | |U(a,b) -    | Memoryless:| Alternative: |                              |
|              | pour avoir %  |                    |  U(a,b)| ~   | Exp(x+a)/  | nb d'essais, |                              |
|              | d'occurences  |                    |Trian(0,b-a,0)| Exp(x) =   | non d'√©checs.|                              |
|              | et non nb,    |                    | Œº(U(a,b)+    | Exp(y+a)/  | Ajouter alors|                              |
|              | utiliser      |                    |   U(a,b)) ~  | Exp(y) pour| r au r√©sultat|                              |
|              | B(n,p)/n, ce  |                    |Trian(a,b,    | tout x,y,a.|              |                              |
|              | qui est difer.|                    | (b+a)/2)     | Donc m√™mes |              |                              |
|              | B(1,p), mais a|                    |              |courbes pour|              |                              |
|              | m√™mes œÉ et Œº  |                    |              |tout [x,x+a]|              |                              |
|              |               |                    |              |D√©croissance|              |                              |
|              |               |                    |              |rate toujour|              |                              |
|              |               |                    |              |m√™me donc.  |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)|B(1,p) + ... + |                    |              |D√©croissance| Geometric(p) |                              |
|              |B(1,p) = B(n,p)|                    |              |rate entre  | == NB(1,p).  |                              |
|              |si ind√©pendants|                    |              |x et x+1    | A une allure |                              |
|              |               |                    |              |pour pdf ou | exponentielle|                              |
|              |               |                    |              |cdf est     |              |                              |
|              |               |                    |              | ‚ÑÆ^-Œª       |              |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+
| Autre (suite)| Bernoulli     |                    |              | Si X et Y  | NB(1,p) + ...|                              |
|              | process =     |                    |              | ~ Exp(),   | + NB(1,p)    |                              |
|              | suite de      |                    |              | X/(X+Y)    | = NB(r,p) si |                              |
|              | Bern. trials. |                    |              | ~ U(0,1)   | ind√©pendants |                              |
+--------------+---------------+--------------------+--------------+------------+--------------+------------------------------+

+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Distribution | Uniform di.| Student's t      | Chi-squared       | F                        | Hyperg√©om√©trique              |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Type         | Discrete   | Continue         | Continue          | Continue                 | Discr√®te                      |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Aspect       | Bar plot   | Comme N(0,1)     | Si n faible,      | Gamma-form               | Comme B()                     |
|              | avec pas de| plus plat.       | exponenti.,       |                          |                               |
|              | 1 et m√™me  | + df, + proche   | + df, + proche    |                          |                               |
|              | hauteur    | de N(0,1)        | de N(0,1)         |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Notation     |            | t(df)            | œá¬≤(df)            | F(df)                    |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Param√®tres   | a: min     | df: degr√©s of    | df: degr√©s of     | df1 et df2:              | m: subpop. du groupe          |
|              | b: max     |     freedom, soit|     freedom       | degr√©s of                | n: subpop. du non-groupe      |
|              |            |     n-1 (n > 1)  |                   | freedom                  | k: taille du sample           |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Note perso   | n: b-a+1   | Similair √† N(0,1)| Similaire √† N(0,1)|                          | Similaire √† B(k,m/N)          |
|              |            | pour n > 30      | pour n > 30       |                          | pour k/N petit                |
|              |            |                  |                   |                          | Soit N = m+n                  |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| x            |            |                  | ‚àë de df variates  | Œº(df1 variates ~ N(0,1)¬≤)| Prob. que x individus du      |
|              |            |                  | suivant N(0,1)¬≤   |/Œº(df2 variates ~ N(0,1)¬≤)| groupe soient tir√©s sur un    |
|              |            |                  |                   |                          | sample de taille k            |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| range        | [a,b]      | [-‚àû,‚àû]           |                   |                          | [max(0,k-n),min(m,k)]         |
| mean         | (a+b)/2    | 0                | df                | df2/(df2-2)              | k*m/N                         |
|              |            |                  |                   | undef. si df2 < 2        |                               |
| median       | (a+b)/2    | 0                |                   |                          |                               |
| variance     | (n¬≤-1)/12  | df/(df-2)        | 2*df              | 2*df2¬≤*(df1+df2-2)       | kmn/N¬≤ * (N-k)/(N-1)          |
|              |            | undef. si df < 2 |                   | /(df1*(df2-2)¬≤*(df2-4))  |                               |
|              |            |                  |                   | pour df2 > 4             |                               |
| mode         | Chacune    | 0                | df-2 (0 si df<2)  |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| P(x)         | 0 si:      |                  |                   |                          | (m) * ( n )  / (N)            |
| (density f())|  x pas int.|                  |                   |                          | (x)   (k-x)    (k)            |
|              |  x < a     |                  |                   |                          |                               |
|              |  ou x > b  |                  |                   |                          |                               |
|              | sinon 1/n  |                  |                   |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Pc(x)        | 0 si x < a |                  |                   |                          |                               |
|              | 1 si x > b |                  |                   |                          |                               |
|              | sinon:     |                  |                   |                          |                               |
|              | (‚é£x‚é¶-a+1)/n|                  |                   |                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Examples     | Chances    |ÃäŒº*‚àön si ÃÖœÉ inconnu,|œÉ‚ÇÅ/ÃäœÉ*(n-1) si ÃÖœÉ    | Equivaut √†               | Test de proportion avec un    |
|              | √©gales,    |que ÃÖs suit N(0,1).|inconnu, et que    | (df1/œá¬≤(df1))            | sample sans remplacement      |
|              | entre a et |                  | ÃÖs ~ N()           | /(df2/œá¬≤(df2))           |                               |
|              | b, entier. |                  |                   | Utilis√© pour             |                               |
|              | Roulette.  |                  |                   | comparer deux            |                               |
|              | D√©.        |                  |                   | œÉ‚Çô (F-test)              |                               |
|              |            |                  |                   | Distribution de          |                               |
|              |            |                  |                   | (œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ) /              |                               |
|              |            |                  |                   | (ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ), pour df1      |                               |
|              |            |                  |                   | == n‚ÇÅ-1, et df2 == n‚ÇÇ-1  |                               |
|              |            |                  |                   | F(1,df2) = t(df2)^2      |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| R functions  | *unifd     | *t               | *chisq            | *f                       | *hyper                        |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+
| Autre        |            | t(df) = N(0,1)/  | œá¬≤(2) = Exp(1/2)  |                          |                               |
|              |            |  ‚àö(œá¬≤(df)/df)    | œá¬≤(2n)            |                          |                               |
|              |            |                  |     = Gamma(n,1/2)|                          |                               |
+--------------+------------+------------------+-------------------+--------------------------+-------------------------------+

+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Distribution | Gamma                 | Beta               | Triangular        | Cauchy         | Weibull                    |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Type         | Continue              | Continue           | Continue          | Continue       | Continue                   |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Aspect       | Exp. si k ou Œ± faible | Œ±&Œ≤>1: bell        | Triangle          | Bell           | Gamma-like                 |
|              | Bell sinon            | Œ±&Œ≤=1: U(0,1)      |                   |                |                            |
|              |                       | Œ±&Œ≤<1: inverse-bell|                   |                |                            |
|              |                       | Œ±>1&Œ≤<1: curve     |                   |                |                            |
|              |                       |          croissante|                   |                |                            |
|              |                       | Œ±<1&Œ≤>1: decroisnte|                   |                |                            |
|              |                       | Si Œ±>Œ≤, penche     |                   |                |                            |
|              |                       | vers droite,       |                   |                |                            |
|              |                       | sinon inverse.     |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Notation     |                       | Be(Œ±,Œ≤)            |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Param√®tres   | Œ±: shape              | Œ±: shape du 1er    | a: min            | x‚ÇÄ: median     | k: shape                   |
|              | Œ≤/Œª: rate             | Œ≤: shape du 2√®me   | c: mode           | œí: scale       | Œª: scale                   |
|              |   ou                  |                    | b: max            |                |                            |
|              | k: comme Œ±            |                    |                   | Standard:      |                            |
|              | Œ∏: comme 1/Œ≤ (scale)  |                    |                   |   (0,1)        |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Note perso   | Converge vers N() pour|                    |                   |                | Comme ‚Çñ‚àöExp(), avec        |
|              | un grand k            |                    |                   |                | scale adjustement.         |
|              |                       |                    |                   |                | Donc contrairement √† Exp() |
|              |                       |                    |                   |                | decreate rate augmente     |
|              |                       |                    |                   |                | (k>1) ou decrease (k<1)    |
|              |                       |                    |                   |                | constamment                |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| x            | Si Œ±‚àà N, somme de Œ±   | Si X ~ Gamma(Œ±,Œª)  |                   |                |                            |
|              | iid. Exp. variables   | et Y ~ Gamma(Œ≤,Œª)  |                   |                |                            |
|              |                       | alors X/(X+Y)      |                   |                |                            |
|              |                       | ~ Beta(Œ±,Œ≤)        |                   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| range        | [0,‚àû)                 | [0,1]              | [a,b]             | (-‚àû,+‚àû)        | [0,‚àû)                      |
| mean         | Œ±/Œ≤                   | Œ±/(Œ±+Œ≤)            | (a+b+c)/3         | Aucune         | Œª*Œì(1+1/k)                 |
| median       |                       | Œ±Œ≤/(Œ±+Œ≤)¬≤/(Œ±+Œ≤+1)  | Dist. au point    | x‚ÇÄ             | Œª*‚Çñ‚àö(ln(2))                |
|              |                       |                    | a ou b le plus    |                |                            |
|              |                       |                    | proche:           |                |                            |
|              |                       |                    | ‚àö((b-a)(c-a)/2))  |                |                            |
| mode         | (Œ±-1)/Œ≤, pour Œ±>=1    | (Œ±-1)/(Œ±+Œ≤-2), pour| c                 | x‚ÇÄ             | Œª*‚Çñ‚àö((k-1)/k)              |
|              |                       | Œ±>1 et Œ≤>1         |                   |                | (0 si k <= 1)              |
| variance     | Œ±/Œ≤¬≤                  |                    | (a¬≤+b¬≤+c¬≤-ab      | Aucune         | Œª¬≤*Œì(1+2/k) - Œº¬≤           |
|              |                       |                    | -ac-bc)/18        |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| P(x)         | Œ≤^Œ±/Œì(Œ±) * x^(Œ±-1)    | x^(Œ±-1)/B(Œ±,Œ≤)     | 0 si hors range   | 1 / (œÄœí *      | k/Œª*(x/Œª)^(k-1)            |
| (density f())|  / ‚ÑÆ^(Œ≤x)             | * (1-x)^(Œ≤-1)      | Si x <= c,        |(1+((x-x‚ÇÄ)/œí)¬≤))| / ‚ÑÆ^((x/Œª)^k)              |
|              |                       |                    | 2(x-a)/(b-a)/(c-a)|                |                            |
|              |                       |                    | Sinon,            |                |                            |
|              |                       |                    | 2(b-x)/(b-a)/(b-c)|                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Pc(x)        |                       |                    | 0 ou 1 si hors    |                | 1-‚ÑÆ^(-(x/Œª)^k)             |
|              |                       |                    | range. Si x <= c, |                |                            |
|              |                       |                    |                   |                |                            |
|              |                       |                    | (x-a)¬≤/(b-a)/(c-a)|                |                            |
|              |                       |                    | Sinon, 1 - ((b-x)¬≤|                |                            |
|              |                       |                    | / (b-a) / (b-c)   |                |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Examples     |                       |                    |                   |                | Esp√©rance de vie, car      |
|              |                       |                    |                   |                | mort plus likely plus      |
|              |                       |                    |                   |                | x est grand.               |
|              |                       |                    |                   |                | Taux de d√©fection produit. |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| R functions  | *gamma                | *beta              | *trngl            | *cauchy        | *weibull                   |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+
| Autre        | Si Œ±‚àà N, appel√©       | 1-Be(Œ±,Œ≤) = Be(Œ≤,Œ±)|                   | N(0,a)/N(0,b)  | Weibull(k,Œª) =             |
|              | "Erlang dist."        |                    |                   | = Cauchy(0,a/b)|  ‚Çñ‚àöExp(Œª^-k)               |
|              | avec param√®tres       |                    |                   |                |Donc Weibull(k,1) = ‚Çñ‚àöExp(1)|
|              | Œ±,Œ≤ -> k,Œª.           |                    |                   | Cauchy(0,1) =  | et Weibull(1,Œª) = Exp(1/Œª) |
|              | Gamma(1,Œ≤) = Exp(Œ≤)   |                    |                   |  t(1)          |                            |
|              | Analogue continue de  |                    |                   |                |                            |
|              | NB()                  |                    |                   | ÃäŒº(Cauchy(a,b) =|                            |
|              |                       |                    |                   |   Cauchy(a,b), |                            |
|              |                       |                    |                   |   non CLT.     |                            |
+--------------+-----------------------+--------------------+-------------------+----------------+----------------------------+

+--------------+---------------+----------------------------------------------------------------------------------------------+
| Distribution | Log-normal    | (Fisher's) noncentral                                                                        |
|              |               | hypergeometric                                                                               |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Type         | Continue      | Discr√®te                                                                                     |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Aspect       | Bell pench√©e  | Comme hypergeometric                                                                         |
|              | vers gauche   | mais tir√©e vers                                                                              |
|              |               | gauche ou droite                                                                             |
|              |               | selon œâ                                                                                      |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Notation     | lnN(Œº,œÉ¬≤)     |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Param√®tres   | Œº: log.mean   | m,n,k: comme hypergeom.                                                                      |
|              | œÉ¬≤: log.var   | œâ: odds ratio, soit œâ‚ÇÅ/œâ‚ÇÇ                                                                    |
|              |               | œâ‚Çô est la prob. de tirer                                                                     |
|              |               | un individu de ce groupe                                                                     |
|              |               | (en dehors de sa prop.                                                                       |
|              |               | m ou n)                                                                                      |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Note perso   | ‚ÑÆ^N(Œº,œÉ¬≤)     | Si œâ=1, √©gal √† hypergeom.                                                                    |
|              |               | Soit N = m+n                                                                                 |
|              |               | xmin = max(0,k-n)                                                                            |
|              |               | xmax = min(k,m)                                                                              |
|              |               | P‚Çë = ‚àëxmin,xmax (m)*( n )                                                                    |
|              |               |                 (i) (k-i)                                                                    |
|              |               |      * œâ^i * i^e                                                                             |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| x            |               | Nb d'individus du groupe                                                                     |
|              |               | 1 sur un sample de taille k                                                                  |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| range        | (0,‚àû)         | [xmin,xmax]                                                                                  |
| mean         | ‚ÑÆ^(Œº+œÉ¬≤/2)    | P‚ÇÅ/P‚ÇÄ                                                                                        |
| median       | ‚ÑÆ^Œº           |                                                                                              |
| mode         | ‚ÑÆ^(Œº-œÉ¬≤)      |                                                                                              |
| variance     | (‚ÑÆ^œÉ¬≤-1) *    | P‚ÇÇ/P‚ÇÄ-(P‚ÇÅ/P‚ÇÄ)¬≤                                                                               |
|              | ‚ÑÆ^(2Œº+œÉ¬≤)     |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| P(x)         | 1/(x‚àö(2œÄœÉ¬≤)   | (m)*( n )*œâ^x / P‚ÇÄ                                                                           |
| (density f())|*‚ÑÆ^((ln(x)-Œº)¬≤)| (x) (k-x)                                                                                    |
|              |      /2œÉ¬≤)    |                                                                                              |
|              |               |                                                                                              |
|              |               |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Pc(x)        |               |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Examples     | Ph√©nom√®ne dont|                                                                                              |
|              | le logarithme |                                                                                              |
|              | ~ N().        |                                                                                              |
|              | Beaucoup de   |                                                                                              |
|              | tissus vivants|                                                                                              |
|              | (poids,taille)|                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| R functions  | *lnorm        | *hypr                                                                                        |
+--------------+---------------+----------------------------------------------------------------------------------------------+
| Autre        | ‚àè lnN(Œº,œÉ¬≤) = |                                                                                              |
|              |   lnN(‚àëŒº,‚àëœÉ¬≤) |                                                                                              |
+--------------+---------------+----------------------------------------------------------------------------------------------+

Multivariate distribution :
  - distribution sur plusieurs variables
  - repr√©sent√©e graphiquement par un graph √† n-dimensions si 2 ou 3 variables
  - types :
    - joint distribution de variables ayant toutes la m√™me ÃÖs
    - plusieurs variables aux r√¥les diff√©rents venant de param√®tres s√©riels de ÃÖs (ex: multinomial dist.)
  - ne semblent pas avoir de cdf ??

+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Distribution | Multinomial     | Categorical      | Multivariate         | Multivariate (Fisher's)                          |
|              |                 | /"Discr√®te"      | Hyperg√©om√©tq.        | noncentral hypergeom.                            |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Type         | Discr√®te        | Discr√®te         | Discr√®te             | Discr√®te                                         |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Aspect       | Ressemble √† B() | Comme Bernoulli  | Comme                |                                                  |
|              | pour chaque var | pour chaque var  | Multinomial()        |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Notation     |                 |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Param√®tres   | n: sample size  | p‚ÇÅ,...,p‚Çñ: prob. | m...m‚Çô: sous-        | Comme central,                                   |
|              | p‚ÇÅ,...,p‚Çñ: prob.|                  | groupe size          | avec en plus:                                    |
|              | des sous-groupes|                  | k: sample size       | œâ...œâ‚Çô: odds ratio                               |
|              | (‚àëp‚Çñ=1)         |                  | (‚àëm‚Çô=pop.size)       |   du groupe n                                    |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Note perso   |                 | = Multinomial    | Comme Multinomial    | Si tous œâ‚Çô = 1,                                  |
|              |                 |   (1,...)        | sans remplacement    | = central hypergeom.                             |
|              |                 |                  | Soit N = ‚àëm‚Çô         |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| x            | Chaque x‚Çñ =     | Si x‚Çñ = 1, prob. |                      |                                                  |
|              | prob. que sample| qu'event soit du |                      |                                                  |
|              | contienne x‚Çñ    |groupe k, soit p‚Çñ |                      |                                                  |
|              | individus du    | Si 0, inverse.   |                      |                                                  |
|              | sous-groupe k   |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
|POUR CHAQUE x‚Çô|                 |                  |                      |                                                  |
| range        | [0,n] (‚àëx‚Çñ=n)   | [0,1]            | [0,m‚Çô] (‚àëx‚Çô=k)       |                                                  |
| mean         | n*p‚Çñ            | p‚Çñ               | m‚Çô*k/N               |                                                  |
| median       |                 | median(p‚ÇÅ,...p‚Çñ) |                      |                                                  |
| mode         |                 | max(p‚ÇÅ,...,p‚Çñ)   |                      |                                                  |
| variance     | n*p‚Çñ*(1-p‚Çñ)     | p‚Çñ*(1-p‚Çñ)        | m‚Çô/N*(1-m‚Çô/N) * k    |                                                  |
|              |                 |                  | * (N-k)/(N-1)        |                                                  |
| cov(x‚ÇÅ,x‚ÇÇ)   | -np‚ÇÅp‚ÇÇ          | -p‚ÇÅp‚ÇÇ            | -km‚ÇÅm‚ÇÇ/N¬≤*(N-k)/(N-1)|                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| P(x)         | (    n    )     | p‚Çñ, o√π k est     | (‚àè(m‚Çô))/(N)          |                                                  |
| (density f())| (x‚ÇÅ,...,x‚Çñ)     | l'index du x‚Çñ    |   (x‚Çô)  (k)          |                                                  |
|              | * ‚àèp‚Çñ^x‚Çñ        | = 1              |                      |                                                  |
|              |                 |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Examples     | Chances que n   | Jeu vid√©o, prob. | Prob. tirer deux     |                                                  |
|              | prochains coups | qu'ennemi fasse  | carreaux, 1 pique    |                                                  |
|              | de roulette     | attaque 1,2 ou 3 | et 3 tr√®fle sur      |                                                  |
|              | soient x‚ÇÄ rouge,|                  | 6 cartes pioch√©es.   |                                                  |
|              | x‚ÇÅ noir et x‚ÇÇ   |                  |                      |                                                  |
|              | zeros.          |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| R functions  | *multinom       | *multinom(size=1)| *mhypr               | *mhypr                                           |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+
| Autre        | Si k = 1 ou 2 ->|                  | Si n = 1 ou 2 ->     |                                                  |
|              | Binomial        |                  | Hyperg√©om√©trique     |                                                  |
|              | Chaque x‚Çñ ~     |                  | univariate.          |                                                  |
|              | B(n,p‚Çñ)         |                  |                      |                                                  |
+--------------+-----------------+------------------+----------------------+--------------------------------------------------+

Approximation par distribution normale :
  - on prend moyenne et œÉ de la distribution √† approximer, et on la donne √† N()
  - notamment :
     - B(n,p)  -> N(np,‚àö(np(1-p))) (quand n est grand, p proche de 0.5)
       - lorsque p *tr√®s* √©loign√© de 0.5, pr√©f√©rer Poisson approximation : B(n,p) -> Pois(np)
     - Pois(Œª) -> N(Œª,Œª)           (quand Œª est grand)
     - t(v)    -> N(0,1)           (quand v est grand)
  - faire la "continuity correction" lors de cdf, mais pas pour pdf

Confidence :
  - confidence :
    - Pc(x ‚àà "confidence interval") = confidence level
    - margin of error: length(confidence interval)/2, not√© E
  - confidence level pour N() :
    - "three-sigma"/"empirical"/"68-95-99.7 rule"
    - P(Œº-œÉ,Œº+œÉ)   = 68%
    - P(Œº-1.96œÉ,Œº+1.96œÉ) = 95%
      - √©galement P(Œº-2.58œÉ,Œº+2.58œÉ) = 99%
    - P(Œº-3œÉ,Œº+3œÉ) = 99.7%
  - calcul :
    - pour confidence interval de Ãäm estimant ÃÖm :
        - r√©soudre √©quation, de sorte d'isoler mÃÖ, en rempla√ßant mÃä par m
          - right-hand donne le confidence interval
        - si Ãäm = Dist() * mÃÖ, alors conf.int. pour ÃÖm est m/Dist()
        - ex:
            ÃäŒº‚ÇÅ = N(0,1) * ÃÖœÉ / ‚àön + ÃÖŒº
            ÃÖŒº (conf.int.) = Œº‚ÇÅ - N(0,1) * ÃÖœÉ / ‚àön
  - souvent, on :
    - calcule confidence interval, pour un n et confidence level donn√©
    - calcule un n, pour un confidence interval et confidence level donn√©
      - par ex., si P() suit N(), et que l'on veut confidence level de 95%, calculer n de sorte que œÉ = confidence interval/4
    - dans les deux cas, d√©pend du r√©sultat donn√©
      - ex: ampleur de confidence interval de s ~ B(n,p) d√©pend de p, mais p est lui-m√™me l'objet de l'estimation (estim√© via
            s) et du confidence interval
    - solution :
      - utilise une √©quation de n/confidence interval requis en fonction du param√®tre
        - ex: pour s ~ B(n,p)/n, en utilisant CLT :
          - on cherche 95% de confidence dans interval p¬±a
            - donc on cherche un n tel que œÉ = a/2
      - ou on r√©sout cette √©quation pour sa valeur donnant le n/confidence interval maximum :
        - ex: pour s ~ B(n,p), p=0.5 donne œÉ¬≤ maximum n/4
              On peut estimer via calcul du saddle point de p*(1-p) :
                (p*(1-p))' = 0
                (p-p¬≤)'    = 0
                1-2p       = 0
                p          = 1/2
          - donc ‚àö(n*p*(1-p))/n = a/2
                 ‚àön/n = a
                 ‚àön = 1/a
                 n = 1/a¬≤
            Donc pour un interval de confiance de ¬±5%, n = 1/0.05¬≤ = 400

Sampling :
  - sample :
    - subset d'une population
    - peut √™tre avec ou sans replacement (doublons)
    - peut √™tre ordered, alors position de l'√©l√©ment importe
    - ensemble des samples possibles, "sample space du random sample" :
      - pour une population de taille ÃÖn et un sample de taille n :
        - avec replacement : ÃÖn‚Åø
        - sans replacement, unordered : (ÃÖn)
                                        (n)
        - sans replacement, ordered : (ÃÖn)‚Çô
  - notation :
    - s d√©note un random sample
    - Ãäs est la distribution de ce sample
      - (s)Ãä  est √©quivalent de Ãäs
      - (statistic)Ãä  signifie distribution de tel statistic
    - ÃÖs est la distribution de la population
  - sampling distribution :
    - si sample random, alors P() que le sample choisi, de taille n, ait la valeur x
    - sample space of the sample average/median/etc. : quand f(x) est average/mean/etc.
      - ici, je me concentre sur SampleMean, mais cela est parfois valable pour les autres
      - ex:
          +------------+--------------------+--------------------------+
          |  SOURCE    |   SAMPLE,taille n  |       SAMPLEMEAN         |
          +------------+--------------------+--------------------------+
          |   Data     |   sample(data,n)   | ‚àû fois Œº(sample(data,n)) |
          | Ex nihilo  |   r*(n,param)      |  ‚àû fois Œº(r*(n,param))   |
          +------------+--------------------+--------------------------+
    - SampleMean est un sample de sample, donc :
      - les unit√©s observ√©es sont des samples (groupe de n outcomes) et non des outcomes isol√©s
      - le x observ√© est une statistic (ex: height mean) et non une measure (ex: height)
    - tout cela suppose que sample soit random
    - notation :
      - m pour statistic d'un sample donn√©, ou m‚ÇÅ, ou m‚Çô
      - Ãäm pour sampling dist. d'un statistic, ou sampleM
      - ÃÖm pour parameter d'une population, ou m(ÃÖs)
      - ex: Œº‚ÇÅ est Œº(s‚ÇÅ), ÃäŒº est sampleMean, ÃÖŒº est mean(ÃÖs)
            œÉ¬≤‚ÇÅ est var(s‚ÇÅ), ÃäœÉ¬≤ est sampleVar, ÃÖœÉ¬≤ est var(ÃÖs)
      - ÃäŒº(n) = probability qu'un sample de taille n ait une mean x
    - diff√©rence entre une statistic, e.g. Œº‚ÇÅ, et un parameter, e.g. ÃÖŒº, est la sampling error
      - la distribution de la sampling error d'une statistic donn√© est Ãäm-ÃÖm
    - standard error d'une statistic est le œÉ de sa sampling dist., not√© SE
      - SE(Œº) = œÉ(ÃäŒº)
  - ÃäŒº(1) == ÃÖs

CLT (Central Limit Theorem) :
  - Somme/mean de n iid. variables, (suivant ÃÖs(ÃÖŒº,ÃÖœÉ¬≤) tend vers N(ÃÖŒº,ÃÖœÉ¬≤/n) quand n augmente, quelque soit ÃÖs
    - en d'autre terme ÃäŒº tend vers N(ÃÖŒº,ÃÖœÉ¬≤/n), pour tout ÃÖs
    - seulement pour ÃäŒº, pas forc√©ment pour autres statistics
  - n minimum pour avoir un bon match d√©pend de ÃÖs :
    - selon mes estimations, pour un match de 95% :
      - Exponential() : n > 135
      - U() : n > 50
      - Pois() : n > assez grand (encore plus lorsque Œª faible)
  - attention, si n est trop petit, alors ne marche pas
  - cons√©quences :
    - Œº(ÃäŒº) == ÃÖŒº
    - œÉ¬≤(ÃäŒº) == ÃÖœÉ¬≤/n (Law of Large numbers)
      - donc œÉ(ÃäŒº) == ÃÖœÉ/‚àön
  - plus n est grand :
    - plus œÉ(ÃäŒº) est faible
    - donc plus Œº a des chances d'√™tre proche de ÃÖŒº (selon facteur ‚àön)
      - par exemple, multiplier par 100 n, divise par 10 √©cart moyen de Œº √† ÃÖŒº
  - en cons√©quences, beaucoup de measurements suivent N(), car ils mesurent la Œº d'un ensemble de ph√©nom√®nes plus petits sur
    un interval de temps ou d'espace
  - Ne marche pas pour les distributions n'ayant pas de finite variances, par ex. Cauchy

Estimation :
  - pr√©diction :
    - d'un param√®tre inconnu ÃÖy d'une population Y
    - √† partir d'un sample s
      - dont sa statistique y
      - mais pas n√©cessairement. Peut par ex. utiliser max‚ÇÅ/2 pour estimer ÃÖŒº
    - utilise une fonction f ("estimator")
    - produit une pr√©diction "estimate" f‚ÇÅ, ayant une distribution (f)Ãä
      - f‚ÇÅ parfois not√© Œ∏^ (^ au dessus) et ÃÖy not√© Œ∏
      - estimate est un "point estimate" si valeur discr√®te
    - exemple Œº(sample) est un estimator pour pr√©dire param√®tre ÃÖŒº, produisant un estimate Œº‚ÇÅ avec une distribution ÃäŒº‚ÇÅ
  - implique que l'on connaisse/suppose la distribution de Ãäy
    - on peut utiliser N() comme premi√®re approximation (CLT)
  - biases :
    - estimator peut √™tre biased ou unbiased :
      - [mean-]bias((f)Ãä ,ÃÖy) : |Œº((f)Ãä )-ÃÖy|
      - median-bias((f)Ãä ,ÃÖy) : |median((f)Ãä )-ÃÖy|
        - un estimator peut √™tre median-unbiased mais mean-biased
    - un estimator peut √™tre biased, mais avoir une faible MSE car bonne pr√©cision, et donc √™tre pr√©f√©r√©
      - ex: max‚ÇÅ/2 pour estimer Œº(U(0,b))
    - biases peuvent √™tre corrig√©s en multipliant Œº((fbias)Ãä ) :
      - ex:
        - f = max‚ÇÅ/2, estimator de ÃÖŒº pour U(0,b)
          - samplef suit b-Exponential(2n/b)
          - donc (fbias)Ãä  suit Exponential(2n/b) * -1
          - donc Œº((fbias)Ãä ) = -b/2n
          - donc ÃÖŒº = f + b/2n
                   = f + ÃÖŒº/n
                   = f * n/(n-1)
        - donc multiplier f par n/(n-1) pour corriger bias
  - comparaison d'estimators :
    - comparer (f)Ãä  et (g)Ãä  par rapport √† ÃÖy
      - peut simuler pour comparer
    - peut comparer :
      - le bias entre mean des (f)Ãä  et ÃÖy pour voir l'accuracy
      - le œÉ des (f)Ãä  pour voir la precision
      - MSE/RMSD permet de prendre un peu des deux :
        - MSE (mean-squared error) :
          - variance de (f)Ãä , mais par rapport √† ÃÖy, non Œº((f)Ãä ) :
            - ‚àë(dev(chaque x, ÃÖy)¬≤) / (n-1)
          - √©gal √©galement √† :
            - œÉ¬≤((f)Ãä ) + bias((f)Ãä ,ÃÖy)¬≤
        - RMSD (root mean-squared deviation)/RMSE (...error):
          - ‚àöMSE
        - on peut par exemple diviser MSE/RMSE entre eux pour voir de combien un estimator est meilleur qu'un autre
          - si MSE(f)/MSE(g) = 1/4, f est quatre fois mieux que g
    - ex:
      - ÃÖy = ÃÖŒº ; ÃÖs suit U(0,b)
      - f = max‚ÇÅ/2 ; g = Œº‚ÇÅ
      - (f)Ãä  = b-Exponential(2n/b)
        - soit œÉ((f)Ãä ) = b/2n et Œº((f)Ãä ) = ÃÖy - b/2n
      - (g)Ãä  = N(ÃÖy,ÃÖœÉ¬≤/n)
        - soit Œº((g)Ãä ) = ÃÖy et œÉ((g)Ãä ) = b/‚àö(12n)
      - œÉ(g) > œÉ(f) (sauf n < 2), selon ratio ‚àön et non n, donc bien moins precise
      - mais bias(g,ÃÖy) < bias(f,ÃÖy), donc plus accurate
      - MSE(f) = b¬≤/4n¬≤ + b/2n, MSE(g) = b¬≤/12n
        - MSE(f) / MSE(g) = 3/n + 6/b, pr√©f√©rer donc f, pour n >= 3/(1-6/b)

Estimation en ignorant des param√®tres :
  - lors d'une estimation, on dispose souvent du sample mais ignore tout param√®tre de ÃÖs
  - hors on a souvent besoin :
    - par ex., pour estimer ÃäŒº, il faut conna√Ætre ÃÖœÉ¬≤
  - solutions :
    - utiliser m‚ÇÅ au lieu de ÃÖm
      - cependant, cela change la distribution du param√®tre √† √©valuer
    - example :
      - ignore ÃÖœÉ¬≤, mais sait que ÃÖs suit N() :
        - estimer ÃÖŒº :
          - utiliser œÉ‚ÇÅ au lieu de ÃÖœÉ
          - cependant, en cons√©quence, suit t(n-1), non N(0,1)
    - lorsque n grandit, m‚ÇÅ s'approche de ÃÖm, donc moins besoin de cela
      - en cons√©quence, d√©finition de Ãäm en ignorant ÃÖm se rapproche de celle en connaissant ÃÖm
        - ex:
          - t() √©quivaut presque √† N(0,1), si n > 30
          - œá¬≤(n-1)/(n-1) √©quivaut presque √† N(0,1), si n > 100

Estimators :
  - lorsque suit N(), souvent CLT
    - donc marche pourvu que n soit assez grand (d√©pend de ÃÖs)
  - ÃÖŒº :
    - Œº‚ÇÅ :
      - si ÃÖœÉ connu, ÃäŒº‚ÇÅ = N(0,1) * ÃÖœÉ / ‚àön + ÃÖŒº
      - si ÃÖœÉ inconnu et que ÃÖs ~ N(), ÃäŒº‚ÇÅ = t(n-1) * œÉ‚ÇÅ /‚àön + ÃÖŒº
    - midrange:
      - pour U() seulement
      - (midrange)Ãä  = { si < ÃÖŒº, ÃÖŒº - Exponential(2n/ranÃÖge) }
                      { si > ÃÖŒº, Œº + Exponential(2n/ranÃÖge) }
      - donc :
        - Œº(((midrange)Ãä ) = ÃÖŒº, et œÉ = ranÃÖge/(‚àö2*n)
        - meilleur que ÃäŒº, qui a œÉ = ranÃÖge/‚àö(12n)
        - RMSE(ÃäŒº)/RMSE((midrange)Ãä ) = ‚àö(n/6)
  - somÃÖme : utiliser ÃÖŒº*n
  - ÃÖŒº‚ÇÅ-ÃÖŒº‚ÇÇ :
    - Œº‚ÇÅ-Œº‚ÇÇ :
      - ÃÖœÉ¬≤‚ÇÅ et ÃÖœÉ¬≤‚ÇÇ connus :
        - (Œº‚ÇÅ-Œº‚ÇÇ)Ãä  = N(0,1) * ‚àö(ÃÖœÉ¬≤(ÃäŒº‚ÇÅ) + ÃÖœÉ¬≤(ÃäŒº‚ÇÇ)) + (ÃÖŒº‚ÇÅ-ÃÖŒº‚ÇÇ)
        - en fait :
          - Œº(Œº‚ÇÅ¬±Œº‚ÇÇ)Ãä  = Œº(ÃäŒº‚ÇÅ) ¬± Œº(ÃäŒº‚ÇÇ)
          - œÉ¬≤(Œº‚ÇÅ¬±Œº‚ÇÇ)Ãä  = ÃÖœÉ¬≤(ÃäŒº‚ÇÅ) + ÃÖœÉ¬≤(ÃäŒº‚ÇÇ)
            - o√π œÉ¬≤(ÃäŒº‚Çô) = œÉ¬≤‚Çô/n‚Çô
              et ÃÖœÉ¬≤(ÃäŒº‚Çô) = ÃÖœÉ¬≤‚Çô/n‚Çô
      - ÃÖœÉ¬≤‚ÇÅ et ÃÖœÉ¬≤‚ÇÇ inconnus :
        - ÃÖœÉ¬≤‚ÇÅ == ÃÖœÉ¬≤‚ÇÇ connu, et vrai :
          - (Œº‚ÇÅ-Œº‚ÇÇ)Ãä  = t(n‚ÇÅ+n‚ÇÇ-2) * ‚àö(((n‚ÇÅ-1)*œÉ¬≤‚ÇÅ + (n‚ÇÇ-1)*œÉ¬≤‚ÇÇ) / (n‚ÇÅ+n‚ÇÇ-2) * (1/n‚ÇÅ+1/n‚ÇÇ)) + (ÃÖŒº‚ÇÅ-ÃÖŒº‚ÇÇ)
        - ÃÖœÉ¬≤‚ÇÅ == ÃÖœÉ¬≤‚ÇÇ inconnu, ou faux :
          - (Œº‚ÇÅ-Œº‚ÇÇ)Ãä  = t(df) * ‚àö(œÉ¬≤(ÃäŒº‚ÇÅ) + œÉ¬≤(ÃäŒº‚ÇÇ)) + (ÃÖŒº‚ÇÅ-ÃÖŒº‚ÇÇ)
            - o√π df = (œÉ¬≤(ÃäŒº‚ÇÅ)+œÉ¬≤(ÃäŒº‚ÇÇ))¬≤ / (œÉ¬≤(ÃäŒº‚ÇÅ)¬≤/(n‚ÇÅ-1) + œÉ¬≤(ÃäŒº‚ÇÇ)¬≤/(n‚ÇÇ-1))) (Welch-Satterthwaite equation)
  - ÃÖœÉ :
    - œÉ‚ÇÅ :
      - si sÃÖ ~ N(), ÃäœÉ‚ÇÅ = ‚àö(œá¬≤(n-1)/(n-1)) * ÃÖœÉ
  - ÃÖœÉ¬≤ :
    - œÉ¬≤‚ÇÅ :
      - si ÃÖs ~ N(), ÃäœÉ¬≤‚ÇÅ = œá¬≤(n-1)/(n-1) * ÃÖœÉ¬≤
        - car ((n-1)*œÉ¬≤‚ÇÅ/ÃÖœÉ¬≤)Ãä  = œá¬≤(n-1)
  - ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ :
    - œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ*(n‚ÇÇ-3)/(n‚ÇÇ-1) :
      - si ÃÖs suit N()
      - (œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ*(n‚ÇÇ-3)/(n‚ÇÇ-1))Ãä  = F(n‚ÇÅ-1,n‚ÇÇ-1)*(n‚ÇÇ-3)/(n‚ÇÇ-1) * ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ
      - raison :
        - ((œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ) / (ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ))Ãä  = F(n‚ÇÅ-1,n‚ÇÇ-1)
          - F() est en fait une division de deux œá¬≤(n‚Çô-1)/(n‚Çô-1)
        - (œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ)Ãä  = F(n‚ÇÅ-1,n‚ÇÇ-1) * ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ
          - or, F(...) n'a pas une mean de 1, donc il y a un bias. (n‚ÇÇ-3)/(n‚ÇÇ-1) corrige ce bias.
        - utilis√© pour un two samples variance F-test ((œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ)Ãä  = F() si ÃÖœÉ¬≤‚ÇÅ/ÃÖœÉ¬≤‚ÇÇ == 1)

Noncentral distribution :
  - g√©n√©ralisation d'un ÃÖs, avec un ou deux non-central parameters (en g√©n√©ral "ncp")
    - g√©n√©ralisation, car une valeur de ces param√®tres == version normale, "centrale" de ÃÖs
  - not√© œá'¬≤, F', t', etc.
  - Signification possible de ncp :
    - Œº de la dist. sur laquelle ÃÖs est bas√©e (souvent N()). Si == 0, version centrale
      - noncentral N() n'existe donc pas, il suffit de modifier Œº
    - odds ratio œâ pour Hypergeom()
  - Doubly non-central :
    - quand ÃÖs bas√©e sur deux dist. (par ex. ratio), alors ncp pour chaque de ces dist.
  - Ex:
    - t' :
      - bas√© sur N(ncp,1), soit N(ncp,1)/‚àö(œá¬≤(df)/df)
    - œá'¬≤ :
      - bas√© sur un ensemble de N(Œº‚Çô,œÉ¬≤‚Çô), o√π ncp = ‚àë (Œº‚Çô/œÉ‚Çô)¬≤
        - si tous N() ont m√™mes Œº et œÉ, alors ncp = (Œº/œÉ)¬≤ * df
          - soit ‚àö(ncp/df) = Œº/œÉ
    - F' :
      - F'(df1,df2,ncp) = (œá'¬≤(df1,ncp)/df1)/(œá¬≤(df2)/df2)

Hypothesis testing :
  - null hypothesis H‚ÇÄ vs alternative hypothesis H‚ÇÅ
    - H‚ÇÅ : affirmation qu'un ph√©nom√®ne suit telle distribution. Correspond √† ce que l'on veut "prouver".
    - H‚ÇÄ : n√©gation de H‚ÇÅ. Correspond √† l'√©tat des faits ou √† un √©tat "conservateur" en cas de dangers.
    - hypoth√®se "prouv√©e" est dite "statistically significant" pour un Œ± donn√©
  - errors :
    - Type I error  :
      - false positive, accepting H‚ÇÅ, alors que H‚ÇÅ est faux
      - prob. est le significance level, Œ±
    - Type II error :
      - false negative, rejecting H‚ÇÅ, alors que H‚ÇÅ est vrai
      - prob. est Œ≤
      - statistical power / sensitivity : 1-Œ≤
      - je crois qu'il se calcule via la dist. "g√©n√©rale" que suit le test statistic (la dist. que suit t que H‚ÇÄ soit vrai ou
        non), non une dist. centrale
      - power analysis :
        - calcul du power
        - permet de d√©terminer n minimum pour avoir un 1-Œ≤ raisonnable et avoir chances de d√©tecter quelque chose
    - plus grave de faire Type I error que Type II error
      - test for differences sont faits pour tendre vers des Type II error plut√¥t que Type I error, en favorisant H‚ÇÄ
      - il faut prouver que H‚ÇÄ est faux, non que H‚ÇÅ est vrai (r√©futabilit√©)
  - principes :
    - un H‚ÇÄ est pos√©
      - en g√©n√©ral un ==, <= ou >= entre un statistic et un parameter, ou entre deux statistics
      - one-sided >= ou <= vs two-sided == (directionality)
    - un "test statistic" t est calcul√© √† partir de ces derniers
    - la distribution du t, Ãät, lorsque H‚ÇÄ est vraie, est connu
      - les 1-Œ± premiers/centraux/derniers % (selon directionality) de Ãät sont la rejection region C
        - si Op de H‚ÇÄ est >=, derniers %
        - si <=, premiers %
        - si ==, centraux %
      - ainsi pour un Œ± donn√©, on peut juger si H‚ÇÄ est vraie, si t ne tombe pas dans C, il y a 1-Œ±% de chances que H‚ÇÄ soit faux
    - en g√©n√©ral Ãät est une version normalis√©e des √©carts possibles entre les deux statistics/parameters pour une H‚ÇÄ donn√©e
      - donc par exemple, pour un one sample location t-test, il s'agit principalement d'un ÃäŒº normalis√©
  - la distribution test statistic, la notation de la variable du test statistic et le nom de la cat√©gorie du test varient :
    - z-test           : Ãäz suit N() quand H‚ÇÄ est vrai
    - t-test           : Ãät suit t() quand H‚ÇÄ est vrai
    - chi-squared test : Ãäœá suit œá¬≤() quand H‚ÇÄ est vrai
    - F-test           : Ãäf suit F(df1,df2) quand H‚ÇÄ est vrai
  - p-value :
    - correspond √† Œ± maximum pour avoir un C acceptant H‚ÇÄ
    - donc prob. que H‚ÇÄ soit vraie
    - aussi moyen rapide de juger H‚ÇÄ est vraie :
      - au lieu de regarder si t ne tombe pas dans C
      - on regarde si p-value >= Œ±
    - p-value de >= est 1 - p-value de <=
  - diff√©rentiel :
    - hypoth√®ses peuvent √™tre exprim√©es en terme de diff√©rentiel
      - ex:
        - H‚ÇÄ : Œº‚ÇÅ == Œº‚ÇÄ   -> Œº‚ÇÅ-Œº‚ÇÄ == 0
        - H‚ÇÄ : œÉ¬≤‚ÇÅ == œÉ¬≤‚ÇÄ -> œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÄ == 1
    - un diff√©rentiel d‚ÇÄ est parfois ajout√© au deuxi√®me terme de l'hypoth√®se :
      - ajout si Œº
      - multiplication si œÉ
    - cela permet de changer termes de la diff√©rence/ratio :
        - H‚ÇÄ : Œº‚ÇÅ == Œº‚ÇÄ + d‚ÇÄ   -> Œº‚ÇÅ-Œº‚ÇÄ == d‚ÇÄ
        - H‚ÇÄ : œÉ¬≤‚ÇÅ == œÉ¬≤‚ÇÄ + d‚ÇÄ -> œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÄ == d‚ÇÄ
  - paired tests :
    - lorsqu'il y a deux samples, s‚ÇÅ et s‚ÇÇ peuvent √™tre :
      - unpaired, c'est-√†-dire choisis ind√©pendemment l'un de l'autre
      - paired, c'est-√†-dire que l'individu n de s‚ÇÅ et de s‚ÇÇ est le m√™me, apr√®s une transformation
        - ex: m√™me personne apr√®s une exp√©rience
        - consid√©r√©s meilleurs qu'unpaired
        - s·µà d√©note s‚ÇÅ-s‚ÇÇ
        - revient en fait √† un test one sample, o√π s‚ÇÅ est s·µà, et parametre est 0
  - fonctionnement g√©n√©ral :
    - si Ãäm = Dist() * œÉ(Ãäm) + ÃÖm
      - alors pour H‚ÇÄ: m‚ÇÅ Op m‚ÇÄ
        - si m = (m‚ÇÅ-m‚ÇÇ), alors H‚ÇÄ: m‚ÇÅ Op m‚ÇÇ
      - t = (m‚ÇÅ-m‚ÇÇ) / œÉ(Ãäm)
      - et t suit Dist() si H‚ÇÄ vraie
        - et que H‚ÇÄ soit vraie ou non :
          - [+(m‚ÇÅ-m‚ÇÄ)] si N()
          - [ncp parameter] si autre Dist.
    - si Ãäm = Dist() * œÉ(Ãäm) * ÃÖm
      - pareil sauf que (m‚ÇÅ/m‚ÇÄ) au lieu de (m‚ÇÅ-m‚ÇÄ) (ou m‚ÇÅ-m‚ÇÇ)
  - types de tests :
    - notation :
      - Op signifie, >, < ou != selon le test, pour H‚ÇÅ
      - Œº‚ÇÄ signifie param√®tre auquel on compare si un seul sample
      - Œº·µà signifie Œº(s·µà)
      - pareil pour œÉ‚Çô et œÉ¬≤‚Çô
    - quand ÃÖs ~ N() :
           +-------------------------------------------+----------------------------------------------------------------------+
           |                                           |                       Two samples                                    |
           |                One sample                 +---------------+----------+-------------------------------------------+
           |                                           |   Unpaired    |  Paired  |               Unpaired                    |
           +---------------------+---------------------+---------------+----------+-------------------------------------------+
           |        ÃÖœÉ connu      |       ÃÖœÉ inconnu     |     ÃÖœÉ connu   | ÃÖœÉ inconnu| ÃÖœÉ inconnu, œÉ‚ÇÅ == œÉ‚ÇÇ | ÃÖœÉ inconnu, œÉ‚ÇÅ != œÉ‚ÇÇ |
           |                     |                     |               |          |      "pooled"       |      "unpooled"     |
+----------+---------------------+---------------------+---------------+----------+---------------------+---------------------+
|          | Œº‚ÇÅ Op Œº‚ÇÄ            | Pareil              | Œº‚ÇÅ Op Œº‚ÇÇ      | Pareil   | Pareil              | Pareil              |
| Location | z = (Œº‚ÇÅ-Œº‚ÇÄ) / (ÃÖœÉ/‚àön)| t = (Œº‚ÇÅ-Œº‚ÇÄ) /(œÉ‚ÇÅ/‚àön)| z = (Œº‚ÇÅ-Œº‚ÇÇ) / | t = Œº·µà / | t = (Œº‚ÇÅ-Œº‚ÇÇ) /       | t = (Œº‚ÇÅ-Œº‚ÇÇ) /       |
|    test  |                     |                     |     ‚àö(ÃÖœÉ¬≤‚ÇÅ/n‚ÇÅ+ |   (œÉ·µà/‚àön)|    ‚àö(((n‚ÇÅ-1) * œÉ¬≤‚ÇÅ +|     ‚àö(œÉ¬≤‚ÇÅ/n‚ÇÅ+œÉ¬≤‚ÇÇ/n‚ÇÇ)|
|          | Ãäz suit N(0,1)       | Ãät suit t(n-1)       |     œÉ¬≤‚ÇÇ/n‚ÇÇ)   |          |     (n‚ÇÇ-1) * œÉ¬≤‚ÇÇ) / | Ãät suit t(df), o√π    |
|          | [+(Œº‚ÇÅ-Œº‚ÇÄ)]          |                     | Ãäz suit N(0,1) | Ãät suit   |     (n‚ÇÅ + n‚ÇÇ - 2) * | df= (œÉ¬≤‚ÇÅ/n‚ÇÅ+œÉ¬≤‚ÇÇ/n‚ÇÇ)¬≤|
|          |                     |                     | [+(Œº‚ÇÅ-Œº‚ÇÇ)]    | t(n-1)   |     (1/n‚ÇÅ + 1/n‚ÇÇ))  |  / ((œÉ¬≤‚ÇÅ/n‚ÇÅ)¬≤/(n‚ÇÅ-1)|
|          |                     |                     |               |          | Ãät suit t(n‚ÇÅ+n‚ÇÇ-2)   |  + (œÉ¬≤‚ÇÇ/n‚ÇÇ)¬≤/(n‚ÇÇ-1))|
|          |                     |                     |               |          |                     | "Welch/Satterthwaite|
|          |                     |                     |               |          |                     |  test"              |
+----------+---------------------+---------------------+---------------+----------+---------------------+---------------------+
|Proportion| Comme location test avec ÃÖœÉ connus, sauf qu'il s'agit de pourcentages (B(n,p)/n), donc œÉ‚Çô = ‚àö(p‚Çô*(1-p‚Çô))          |
|   test   | Si multi-sample, Chi-squared test (cf dessous)                                                                   |
+----------+---------------------+---------------------+---------------+----------+-------------------------------------------+
|          |                     | œÉ¬≤‚ÇÅ Op œÉ¬≤‚ÇÄ          |               |          | œÉ¬≤‚ÇÅ Op œÉ¬≤‚ÇÇ                                |
| Variance |                     | œá¬≤ = (n-1) * œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÄ|               |          | F = œÉ¬≤‚ÇÅ/œÉ¬≤‚ÇÇ                               |
|   test   |                     | Ãäœá¬≤ suit œá¬≤(n-1)     |               |          | ÃäF suit F(n‚ÇÅ-1,n‚ÇÇ-1)                       |
+----------+---------------------+---------------------+---------------+----------+-------------------------------------------+
      - "Ãä* suit *()" signifie "quand H‚ÇÄ est vrai, Ãä* suit *()", quand rien n'est pr√©cis√©
        - le test statistic suit en fait une certaine distribution "g√©n√©rale" que H‚ÇÄ soit vraie ou non
          - comprenant notamment m‚ÇÅ - m‚ÇÇ ou m‚ÇÅ/m‚ÇÇ si test m‚ÇÅ == m‚ÇÇ
        - mais lorsque H‚ÇÄ est vraie, on peut r√©duire cette distribution
          - par exemple m‚ÇÅ/m‚ÇÇ devient 1 si m‚ÇÅ == m‚ÇÇ
        - souvent distribution "g√©n√©rale" est une version "noncentral" de la distribution de base
          - il s'agit de m√™me dist., mais avec un param√®tre ncp indiquant le Œº(dist.)
          - il s'agit d'une g√©n√©ralisation de la dist. commune, "centrale", laquelle √† ncp = 0
          - lorsque H‚ÇÄ est vrai, ncp = 0, donc dist. centrale
      - "Ãä* suit *()[...]" signifie "quand H‚ÇÄ est vrai, inclusion de [...] inutile"
      - attention, variance two samples F-test est tr√®s sensible √† non normalit√© de ÃÖs
    - Proportion test multi samples Chi-squared test / Pearson's chi-squared test :
      - Soit A = { a‚ÇÅ,...,a‚Çô } et B = { b‚ÇÅ,...,b‚Çô }, et P = A/(A+B)
      - H‚ÇÄ : tous p‚Çô == p‚ÇÄ, o√π p‚ÇÄ est ‚àëA/(‚àëA+‚àëB)
        - cad si l'un des a‚Çô/b‚Çô est diff√©rent des autres
        - test donc si cat√©gorie (A,B) est ind√©pendants de cat√©gorie (1,...,n)
          - si tous p‚Çô == p‚ÇÄ, cat√©gorie (A,B) n'a aucune influence sur (1,...,n)
          - utilis√© donc pour l'ind√©pendance entre deux facteurs
      - calcul :
        - soit observed quantit√©s A et B, not√© x‚Çô
        - soit expected quantit√©s de A = (A+B)*p‚ÇÄ, et de B = (A+B)*(1-p‚ÇÄ), not√© x‚Çë
        - alors œá¬≤ = ‚àë (x‚Çô-x‚Çë)¬≤/x‚Çë
        - œá¬≤ suit œá¬≤(n-1) si H‚ÇÄ est vrai
      - œá¬≤ est ici utilis√© pour une approximation de ce qui devrait √™tre B()
      - Yate's continuity correction :
        - alors œá¬≤ = ‚àë (x‚Çô-x‚Çë-0.5)¬≤/x‚Çë
        - √† utiliser quand + de 20% des x‚Çë < 5
      - seul == est possible pour H‚ÇÄ, et C est one-sided (premiers % de œá¬≤(n-1))
      - marche aussi s'il y a un C, etc.

Hypoth√®se fallacies :
  - poser une H‚ÇÅ sugg√©r√©e par un dataset, et le tester avec ce dataset (post hoc theorizing)
    - en effet, ce dataset, ayant sugg√©r√© H‚ÇÅ, a de forte chance de le conforter.
    - Il faut donc utiliser un autre dataset
    - souvent quand collection des data est co√ªteuse
    - type I error

Explanatory/independent/regressor vs response/dependent variables :
  - il s'agit de voir les cons√©quences que l'explanatory variable X a sur la response variable Y
    - en d'autres termes, pour chaque variate x‚Çô possible, il y a une y‚Çô donn√©
    - on peut se repr√©senter visuellement avec un graph avec l'ensemble des des points (x‚Çô,y‚Çô)
    - il s'agit en fait de repr√©senter X et Y par rapport √† leur P(X ‚à© Y)
    - not√© Y ~ X, appel√© joint distribution de X et Y
  - taille n de X == taille de Y
    - n est le nombre d'observations. Il peut y avoir des doublons num√©riques (ex. deux x‚Çô diff√©rents, mais m√™me r√©ponse y‚Çô)
      ou factorials (ex. plusieurs r√©ponse y‚Çô pour un m√™me facteur x‚Çô)
    - X peut √™tre factorial
      - ex. pr√©sence ou non de tel m√©dicament ; et y‚Çô moyenne d'ob√©sit√© d'une population
    - Y peut √™tre factorial si X l'est. Cependant, Y factorial et X num√©rique me semble un probl√®me de conception.
  - x‚Çô est toujours une valeur discr√®te (1-dimension), mais y‚Çô peut √™tre une valeur n-dimensions, dont une random variable Ãäy‚Çô
    - cependant si X n'est pas un facteur, observable seulement si plusieurs experiments avec m√™mes x‚Çô sont faits
    - distinguer r√©elle variation (Ãäy‚Çô) de variation due au measurement error
    - Ãäy lui-m√™me peut √™tre vu comme un Y ~ X, o√π Y est la P()
  - √©tude de la relation X et Y :
    - si tous les y‚Çô identiques, alors Y est ind√©pendant de X
      - cependant, les diff√©rents measurement de y‚Çô sont unlikely to give exactly identical observations (measurement error)
        - il faut donc savoir si les observations y‚ÇÅ,...,y‚Çô suivent la distribution normale de Y, ÃäY
      - pour des Ãäy‚Çô, il faut les comparer ensemble -> location n-samples test, par exemple sur Œº(Ãäy‚Çô)
        - si Ãäy‚Çô, on prend par exemple leur mean
    - si d√©pendant, √©tude de la relation :
      - si X num√©rique :
        - r√©gression polynomiale

Covariance :
  - cov(X,Y) = (‚àë (x‚Çô-Œº(X))*(y‚Çô-Œº(Y))) / (n-1)
    - comme variance, sauf qu'au lieu de dev(x...)^2, dev(x...) * dev(y...)
  - si proche de 0, variables ind√©pendantes
    - si loin de 0, Y tend √† diminuer (si < 0) ou augmenter (si > 0) quand X augmente
    - pourquoi :
      - si valeurs positives ensemble, et valeurs n√©gatives ensemble => produits sont positifs -> cov > 0
      - si valeurs positives de l'un avec valeurs n√©gatives de l'autre => produits sont n√©gatifs -> cov < 0
      - si valeurs positives de l'un sont parfois avec valeurs positives, parfois n√©gatifs => produits sont parfois positifs,
        parfois n√©gatifs -> cov proche de 0
  - graphiquement, graph avec (X,Y) :
    - si proche de 0, nuage de points
    - si loin de 0, nuage tendant vers une ligne, ascendante (>0) ou descendante (<0)
  - cas particuliers :
    - cov(X,Y) = cov(Y,X)
    - cov(X,X) = œÉ¬≤(X)
    - cov(X,Y), o√π Y = { a,...,a }, = 0
    - cov(X,Y), o√π Y ~ N() et ind√©pendant de X, = 0

Regression model/analysis :
  - il s'agit de grapher une courbe montrant tendance de P(Y|X)
    - si plus de deux variables, multiple regression, sinon simple regression
  - linear [regression] model :
    - lorsque coefficient polynomial est 1
    - observed values (data), not√© Y, vs predicted values, not√© Z, (regression point sur m√™me x pour chaque observed value)
      - residual: Œµ‚Çô = y‚Çô - z‚Çô
      - SSE (Sum-squared of errors)/RSS(residual sum of squares) = ‚àë Œµ‚Çô¬≤
        - peut aussi √™tre calcul√© ainsi:
          - SSE = (œÉ¬≤(Y) - b¬≤*œÉ¬≤(X)) * (n-1)
        - residual standard error:
          - œÉ(ÃäŒµ‚Çô) = ‚àö(SSE/(n-2))
    - "best fit/regression/least-square" line :
      - propri√©t√©s :
        - min. SSE
        - ‚àëŒµ‚Çô = 0
        - ‚àë x‚Çô*Œµ‚Çô = 0
      - bx + a :
        - a et b sont les "regression coefficients"
        - b = cov(X,Y) / œÉ¬≤(X)
            = cor(X,Y) / œÉ(X) * œÉ(Y)
        - a = Œº(Y) - a*Œº(X)
          - car regression line passe √† travers (Œº(X),Œº(Y)), "center of mass", et a une slope b
    - [Pearson's] [coefficient de] correlation (r, cor(X,Y))
      - [-1,1] : plus √©loign√© de 0, meilleure fit
      - r = cor(X,Y) = cov(X,Y)/(œÉ(X)*œÉ(Y))
      - donc comme la covariance, mais ajust√© par la grandeur de X et Y (dimensionless)
    - coefficient de d√©termination :
      - [0,1] : plus √©loign√© de 0, meilleure fit
      - r¬≤
      - aussi √©gal √† 1 - SSE / SST
        - o√π SST = œÉ¬≤(Y)*(n-1)
        - donc ratio entre variance normale et "explained variance"
      - r¬≤ (et r) augmente m√©caniquement avec nombre d'explanatory variables (regressors)
        - comprend donc ajout de regressors au m√™me coefficient ou non : Y = bX + a -> Y = cZ + bX + a, ou -> Y = cZ¬≤ + bX + a
        - adjusted r¬≤, not√© souvent ÃÖr¬≤ (mais not√© dans ma doc. adj.r¬≤ pour ne pas confondre avec ÃÖr):
          - corrige cela
          - soit p nombre de regressors (1 pour une simple regression Y = bX + a) :
            - adj.r¬≤ = 1 - (1 - r¬≤)*(n-1)/(n-1-p)
        - sert √† comparer deux mod√®les au nombre de regressors diff√©rent. Sinon utiliser r¬≤
          - r¬≤ ne d√©pend pas d'une r√©gression donn√©e, adj.r¬≤ si
  - degr√© of freedom :
    - degr√© polynomial de la regression - 1
    - augmenter nombre de degr√© de freedom a tendance a am√©liorer r
  - r√©gression lin√©aire, cad regression coefficients, d'un sample sont une estimation de ceux de la population
    - on peut donc :
      - calculer un Ãäa et bÃä, et interval de confiance
      - tester H‚ÇÄ: a == 0 ou b == 0
    - l'estimation de a et b cherche √† minimiser SSE
      - d√©pend donc de ÃäŒµ‚Çô
      - par cons√©quent sampling error de ÃäŒµ‚Çô induit sampling error de Ãäa et Ãäb
      - calculs suivants supposent que ÃäŒµ‚Çô ~ N(), ou que n est grand, auquel cas il le suit (CLT)
    - Calcul de Ãäa et bÃä :
      - bÃä = t(n-2) * œÉ(ÃäŒµ‚Çô) / œÉ(X) / ‚àö(n-1) + ÃÖb
        - donc t = b‚ÇÅ / œÉ(ÃäŒµ‚Çô) * œÉ(X) * ‚àö(n-1), et suit t(n-2)[+(b‚ÇÅ-b‚ÇÄ)]
      - bÃä¬≤ = F(1,n-2) * œÉ¬≤(ÃäŒµ‚Çô) / œÉ¬≤(X) / (n-1) + ÃÖb¬≤
        - donc F = b¬≤‚ÇÅ / œÉ¬≤(ÃäŒµ‚Çô) * œÉ¬≤(X) * (n-1), et suit F(1,n-2)[+(b¬≤‚ÇÅ-b¬≤‚ÇÄ)]
      - Ãäa = t(n-2) * œÉ(bÃä) * ‚àö(Œº(X¬≤))) + ÃÖa
        - donc t = a‚ÇÅ / œÉ(bÃä) * ‚àö(Œº(X¬≤)), et suit t(n-2)[+(a‚ÇÅ-a‚ÇÄ)]
    - tests :
      - H‚ÇÄ: b == 0 :
        - si b == 0, alors Y est ind√©pendant de X
          - les œÉ¬≤(Y) peut √™tre grande (y‚Çô varient) mais œÉ¬≤(Y) n'est pas expliqu√©e par X


Simulation :
  - utile par exemple :
    - connaissant une taille n, et supposant une ÃÖs
    - mais ignorant Distribution de sampleStat
    - pour :
      - v√©rifier qu'un sampleStat suit bien une loi normale
      - sinon, tenter de l'estimer
      - ou pour l'utiliser comme pdf/cdf/intervals de confiance sur un cas donn√©
  - comment :
    - g√©n√©ration de m sample(n) que possible suivant ÃÖs, ce qui produit un sampleStat
    - plus m est grand, plus proche du vrai/th√©orique sampleStat
  - simulation inutile si l'on conna√Æt propri√©t√©s de sampleStat √† partir de ÃÖs et n

Computation sur ÃÖs :
  - ÃÖs + a :
    - conserve shape et œÉ de ÃÖs
    - ÃÖŒº += a
  - ÃÖs * a :
    - conserve shape de ÃÖs
    - ÃÖŒº et ÃÖœÉ *= a
    - densit√© max. /= a (pour pdf)
  - ÃÖs‚ÇÅ + ÃÖs‚ÇÇ :
    - mix des shapes :
      - si m√™me ÃÖs, CLT
    - ÃÖŒº = ÃÖŒº‚ÇÅ + ÃÖŒº‚ÇÇ
    - ÃÖœÉ¬≤ = ÃÖœÉ¬≤‚ÇÅ + ÃÖœÉ¬≤‚ÇÇ
  - ÃÖs‚ÇÅ * sÃÖ‚ÇÇ :
    - mix des shapes :
      - N() * N() -> N()
    - ÃÖŒº = ÃÖŒº‚ÇÅ * ÃÖŒº‚ÇÇ
    - pour N() * Exponential() :
      - ÃÖœÉ¬≤ = 2 * ÃÖœÉ¬≤‚ÇÅ * ÃÖœÉ¬≤‚ÇÇ
    - pour N() * N() :
      - ÃÖœÉ¬≤ = 2 * (ÃÖœÉ¬≤‚ÇÅ * ÃÖœÉ¬≤‚ÇÇ)¬≤

NA :
  - si NA, r√©duire n √† sa vraie valeur
  - parfois NA contient des infos par lui-m√™me
    - d√©pend du contexte de l'experiment
    - dans ce cas, inclure NA dans le calcul et les transtyper vers une valeur

Propositions suivantes sont √©quivalentes (si l'un, alors les autres) :
  - discret :
    - Pour un set de n √©l√©ments, o√π N = (b-a), et n' = n-1 :
      - et pas 2 √©l√©ments avec m√™me valeur
    - alors :
      - √©l√©ments ~ Unif.disc.(a,b)
      - distance entre un √©l√©ment et le suivant ~ Geom(n'/N)
        - √©quivaut √†: distance entre √©l√©ment i et √©l√©ment i+Œ±, selon ordre croissant ~ NB(Œ±,n'/N)
        - seulement pour n suffisamment grand, n > 30 pour 95% d'approx.
      - Nb √©l√©ments dans un subset de taille m ~ B(m,n/N)
        - marche pas pour un subset trop important
          - par ex. sur un subset de taille N, nb. √©l√©ments est connu (n), non une suite de m trials avec prob. n/N
          - cela est d√ª au fait que prob. est induite du nombre connu d'√©l√©ments. Sinon, subset important marche aussi
  - continu :
    - Pour un set de n √©l√©ments de longueur N, o√π N = b-a :
      - √©l√©ments ~ U(a,b)
        - distance absolue entre deux √©l√©ments (dont √©l√©ment et √©l√©ment suivant, selon ordre de s√©lection)
          ~ Triangular(0,b-a,0)
      - distance entre un √©l√©ment et le suivant ~ Exp(n'/N)
        - car n'/N devenant infinit√©simal, Geom(n'/N) =~ Geom((n'/N)/((n'/N)+1)) = Exp(n'/N)
        - √©quivaut √†: distance entre √©l√©ment i et √©l√©ment i+Œ± ~ Erlang(Œ±,n'/N)
        - seulement pour n suffisamment grand, n > 30 pour 95% d'approx.
        - distance(i,i+Œ±)/distance(i,i+Œ±+1) ~ U(0,1)
          - car si X et Y ~ Exp(), alors X/(X+Y) ~ U(0,1)
      - Nb √©l√©ments dans un subset de taille m ~ Pois(n*m/N)

Poisson process :
  - cas continu des propositions pr√©c√©dentes :
    - ou le set est le temps, ou l'espace (plane, volume, etc.)
  - Assumptions suffisantes pour entrer dans cas pr√©c√©dent :
    - pas 2 succ√®s simultan√©s (car espace infinit√©simal)
    - prob. des events sont ind√©pendants les unes des autres
  - plut√¥t que d'utiliser b-a et n (n/N), d√©finit seulement Œª, le nb de succ√®s moyen pour une unit√© temporelle donn√©e (ex. : secondes)
  - R√©sum√© :
    - Nb. succ√®s pour œâ units temporelles ~ Poisson(œâ*Œª)
    - Interv.temps (en units temportelles) entre deux succ√®s ~ Exp(Œª) (interval moyen = 1/Œª)
  - ex:
    - si 10 succ√®s pour 2 secondes, alors Nb.succ√®s suit Poisson(2*Œª) = 10 (n est en secondes), donc Œª = 10/2 = 5,
      et Interval.temps ~ Exp(5)
  - en g√©n√©ral repr√©sent√© par un graph avec temps t en abscisse et N(t) (nb.succ√®s) en ordonn√©e
    - N(t) est un counting process, avec des incr√©ments √©gaux et entiers donc
  - non-homogeneous : quand Œª change avec le temps

Faire ses propres distributions :
  - cdf = ‚à´pdf (ou pdf = antider.(cdf))
  - qdf = cdf‚Åª¬π
  - PRNS = qdf prenant U(0,1) comme input

Time series :
  - Y ~ X, o√π X est le temps, en g√©n√©ral mesur√© √† interval r√©gulier
    - par cons√©quent, X est ordered (ce qui n'est pas le cas pour tout Y ~ X)
  - en g√©n√©ral line charts
  - √©tude par exemple des cycles
  - analysis (past) vs forecasting

Empirical probablity :
  - √©quivaut √† relative frequency
  - empirical cdf :
    - step function o√π chaque step == chaque valeur (sorted), augmentant prob. de 1/n
    - but est d'estimer la cdf r√©elle √† partir d'un sample
  - empirical pdf :
    - pareil pour pdf
    - KDE (Kernel-density Estimation) :
      - non step-function
      - smoothed en fonction d'un param√®tre "bw" (bandwidth)
        - bandwidth selector s√©lectionne le bw
          - ex. connu est nrd : min(œÉ,IRQ/1.34)*0.9(ou 1.06) / ‚Åµ‚àön
