
            
   POSTGRES  
            



Relationships:
  - can be 0, 0|1, many or 0|many on each side
  - 1-to-1: foreign key
  - all others: third table with foreign keys for both sides
Enum:
  - prefer small tables when possible
How to check that foreign key cover all the primary key values?
Primary key:
  - should always create surrogate key as primary key, even when natural keys are candidate keys
  - should then put non-primary candidate keys with unique constraint
  - check article stackoverflow answered in StackOverflow

TO FINISH ==>                     #  - SSH, GPG
                                  #  - repmgr
                                  #  - pgcrypto
                                  #  - procedural languages

VERSION ==>                       #15.3

ARCHITECTURE ==>                  #RDBMS with focus on standard compliance and extensibility.
                                  #Conform to SQL:2016 for most of it
                                  #Client (psql, pgadmin, etc.) / server (postgres) architecture


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SUMMARY            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DATABASE DESIGN ==>               #Modelling (pgModeler)
                                  #  - create tables with right columns, constraints (pkey, fkey, default, check, not null, unique, exclude) and properties (inherits)
                                  #  - using:
                                  #     - normal types (NUM, BOOL, STR, BSTR, BYTEA, DATE|TIME, ENUM) and arrays, ranges
                                  #     - special types (net, dictionary, xml, json, hstore, ltree, geometry) and created types (ctype, domain)
                                  #     - id vs uuid
                                  #     - views for encapsulation, or security restriction per column
                                  #     - [event] triggers
                                  #     - sequences
                                  #     - schemas
                                  #     - foreign tables (including file_fdw for CSV files)
                                  #     - listen|notify for clients communication
                                  #Security:
                                  #  - roles and privileges
                                  #  - authentication
                                  #  - unix_socket_directories
                                  #  - FUNC definition (leakproof, security definer)
                                  #Multithread-safety (transactions, locks)
                                  #Watch out for:
                                  #  - null possibility in queries
                                  #  - SQL injection when concatenating 'SQL' (use quote_*() or format())

PERFORMANCE ==>                   #  - on design, check using:
                                  #     - materialized views instead of views
                                  #     - rules instead of triggers
                                  #     - index
                                  #     - cursors
                                  #     - partitions
                                  #     - prepared statements
                                  #     - large objects
                                  #     - tablespaces
                                  #  - ENVVAR tunning:
                                  #     - disabling durability, decreasing checkpoints frequency, using RAM disks
                                  #     - setting right resources needed for [maintenance_]work_mem, effective_cache_size, wal_buffers, max_stack_depth, temp_file_limit,
                                  #       max_files_per_processes, effective_io_concurrency, shared_buffers, max_connections, statement_timeout
                                  #     - using pgtune
                                  #  - optimizing queries with explain
                                  #  - using pgbench
                                  #  - for big data write, see below best practices
                                  #  - use connection pooling (pgBouncer)
                                  #  - upgrading hardware
                                  #  - FUNC definition (volatility, cost, rows)
                                  #  - TABLE fillfactor, fastupdate
                                  #  - autovacuum tunning

SETUP FOR END USERS               #  - create [A|W]FUNC (possibily from PL/* languages), prepared statements, comments
 AND FUTURE MAINTENANCE ==>       #  - logging
                                  #  - [hot] standby with [a]sync. streaming replication
                                  #  - use pgagent for regular tasks:
                                  #     - pgbadger and pgcluu reports creation
                                  #     - check_postgres
                                  #        - good idea to merge pgbadger, pgcluu and check_postgres into one HTML file with a script
                                  #     - pg_dumpall
                                  #  - if durability, check proper cache usage (wal_sync_method, HDD|filesystem cache)

TESTING ==>                       #  - random filling (datafiller.py)
                                  #  - unit testing (pgTap)
                                  #  - load testing (Tsung)

MAINTENANCE ==>                   #  - monitoring:
                                  #     - use pgadmin, with Server status window, and opening a psql within pgadmin (create proper .psqlrc), or use teamPostgreSQL
                                  #     - resource (should not exceed max_connections and work_mem), space usage or other problems:
                                  #        - pgbadger and pgcluu
                                  #        - pg_top
                                  #        - check_postgres
                                  #  - data update (should create functions):
                                  #     - partitionning
                                  #     - copy "TABLE" from
                                  #  - cluster/reindex (ask for exclusive lock)
                                  #  - check PostgreSQL upgrades, and use pg_upgrade
                                  #  - create restore points with pg_create_restore_point(STR) after critical operations


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SYNTAX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COMMAND;                          #"Statement"
CLAUSE ==>                        #Part of a COMMAND attached to keywords
                                  #E.g. WHERE BOOL

WHITESPACES ==>                   #Ignored

CASE-SENSITIVITY ==>              #Case-insensitive for:
                                  #  - keywords (usually uppercase)
                                  #  - VARs (usually lowercase)
"VAR"
U&"VAR2"                          #Make VAR case-sensitive

-- COMMENT ...
/* COMMENT */                     #

(VAR [= VAL], ...)                #OPTS. Used in many SQL statements for named parameters.
                                  #VAL is only optional if either:
                                  #  - it has a default value
                                  #  - it has no value, i.e. it is implicitly a BOOL
                                  #Some commands allow specifying OPTS both in (...) and as keywords
                                  #  - prefer (...) as keywords tend to be deprecated then
(VAR [VAL], ...)                  #ZOPTS. Same without = sign

DDL                               #"Data Definition Language"
                                  #Statements operating on entities themselves
                                  #E.g. create, alter, drop, truncate, comment, etc.
DML                               #"Data Manipulation Language"
                                  #Statements operating on entities contents
                                  #E.g. select, insert, update, delete, explain, etc.
DCL                               #"Data Control Language"
                                  #Statements operating on authorization
                                  #E.g. grant, revoke, etc.
TCL                               #"Transaction Control Language"
                                  #Statements operating on transactions
                                  #E.g. commit, rollback, savepoint, etc.

OUTPUT ==>                        #Some statements return the NUM of ROWs manipulated.
                                  #This is separate from the ROW_SET returned by a SUBQUERY, and is not printed by default
                                  #For: select|insert|update|delete|merge|move|fetch|copy|create table as


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             NAME              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


name                              #TYPE of a "VAR", used internally. 64 bytes string.
NAME[NUM]                         #'CHAR'

VAR                               #Any identifier
                                  #[[:alnum:]_]+
                                  #Max 63 chars
"VAR"                             #Allows reserved keywords, case-sensitive, and do not trim whitespaces
                                  #"" to escape "
as VAR                            #Some COMMANDs allows optional `as`.
                                  #This allows reserved keywords, but remains case-insensitive unless quoted

"..."                             #Notation to means a VAR (with|without quoting)
                                  #Including "TABLE", "COL", etc.

TEXT <-> NAME
VARCHAR|BPCHAR <=-> NAME          #Type casting


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SCHEMA             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create schema "SCHEMA"            #Creates a SCHEMA, i.e. namespace inside a DATABASE
                                  #Dependency parent of the ENTITYs it contains

create schema "SCHEMA"            #Combine create schema + create ENTITYs within that SCHEMA
 create ENTITY ... [,...]         #ENTITY: table, view, index, sequence, trigger
                                  #Can also use `grant ...` instead of `create ENTITY`
alter ENTITY "ENTITY"
 set schema "SCHEMA"              #For all ENTITYs that can have a SCHEMA

[SCHEMA.]"NAME"                   #"NAME"s are namespaced by SCHEMA

ENVVAR search_path                #'SCHEMA,...' used as default. Leftmost has priority.
                                  #Def: '"$user", public'
current_schemas(BOOL)             #Show search_path
 ->'SCHEMA'_ARR                   #If false, do not include pg_catalog and pg_temp*

DEFAULT CREATE SCHEMA ==>         #When creating a new ENTITY, use the leftmost SCHEMA of search_path, except "$user"
current_schema
current_schema()->'SCHEMA'        #Default write schema. null if none

pg_ENTITY_is_visible(OID)->BOOL   #True if ENTITY's SCHEMA is within ENVVAR search_path

"$user"                           #Can be used in ENVVAR search_path, replaced by current "ROLE" name
                                  #This SCHEMA must be manually created
                                  #Meant for user-specific data

public                            #SCHEMA automatically created, initially empty
                                  #Meant for non-user-specific data

pg_namespace                      #TABLE with all SCHEMAs
pg_namespace.oid                  #OID
pg_namespace.nspname              #NAME of "SCHEMA"

regnamespace                      #TYPE alias to OID, to cast pg_namespace.oid as "SCHEMA" name

pg_type.typnamespace              #REGNAMESPACE of TYPE
pg_class.relnamespace             #REGNAMESPACE of RELATION
pg_constraint.connamespace        #REGNAMESPACE of CONSTRAINT
pg_tables.schemaname              #pg_namespace.nspname of TABLE
pg_indexes.schemaname             #pg_namespace.nspname of INDEX
pg_views.schemaname               #pg_namespace.nspname of VIEW
pg_matviews.schemaname            #pg_namespace.nspname of MVIEW
pg_sequences.schemaname           #pg_namespace.nspname of SEQUENCE
pg_ts_parser.prsnamespace         #REGNAMESPACE of PARSER
pg_ts_template.tmplnamespace      #REGNAMESPACE of TEMPLATE
pg_ts_dict.dictnamespace          #REGNAMESPACE of DICTIONARY
pg_ts_config.cfgnamespace         #REGNAMESPACE of REGCONF
pg_conversion.connamespace        #REGNAMESPACE of CONVERSION
pg_collation.collnamespace        #REGNAMESPACE of COLLATION
pg_opfamily.opfnamespace          #REGNAMESPACE of OPFAMILY
pg_opclass.opcnamespace           #REGNAMESPACE of OPCLASS


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          PG_CATALOG           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_catalog                        #SCHEMA for system data. "System catalogs"
                                  #Contains all pg_* TABLE|VIEWs and builtins FUNC|TYPEs
                                  #Prepended to ENVVAR search_path
                                  #  - can be explicitly added to ENVVAR search_path to override priority order
                                  #Sometimes readonly. Should avoid modifying.
                                  #DATABASE-specific unless on cluster-specific ENTITY
                                  #  - dependency child of DATABASE

pg_node_tree                      #Internal serialized information TYPE, specific to pg_catalog.*
pg_get_expr(PG_NODE_TREE,         #Converts PG_NODE_TREE to STR
 TABLE.oid[, BOOL])->STR          #BOOL (def: false) is prettify


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      INFORMATION SCHEMA       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


information_schema                #SCHEMA for system data
                                  #Unlike pg_catalog, is standard SQL
                                  #Does not contain Postgres-specific information
                                  #Are all VIEWs over pg_catalog.*
                                  #To document ???

DOMAIN TYPES ==>                  #The following are DOMAIN_TYPEs used only by information_schema
                                  #All use not null
sql_identifier                    #NAME_TYPE, COLLATION "C"
yes_or_no                         #VARCHAR(3)_TYPE, 'YES|NO', COLLATION "C"
character_data                    #VARCHAR_TYPE, COLLATION "C"
cardinal_number                   #INT4_TYPE, >= 0
time_stamp                        #TIMESTAMPTZ_TYPE, default current_timestamp


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            ENTITY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENTITY                            #Entity that can be created with `create ENTITY ...`
                                  #E.g. table, collation, function, etc.

"ENTITY"                          #Notation depends on entity. It can be one of the following.
VAR,...                           #For most ENTITYs
VAR                               #For access method, collation, conversion, database, event trigger, language,
                                  #subscription, tablespace, text search *
VAR(...),...                      #For aggregate, function, procedure
VAR(TYPE|none, TYPE2|none),...    #For operator
(TYPE as TYPE2)                   #For cast
VAR on "TABLE"                    #For policy, trigger, rule
VAR using ACCESS_METHOD           #For operator class|family
for TYPE language LANG            #For transform
for ROLE server SERVER            #For user mapping
                                  #ROLE can be user, current_role, current_user, public

alter ENTITY ... "ENTITY" ...     #Sets ENTITY options after creation
                                  #Not for ENTITY: access method, cast, transform
                                  #Several ENTITYs allow combining several `alter ...` into a single statement
                                  #  - faster than doing individual statements serially
                                  #  - for most actions of:
                                  #     - alter [foreign] table, materialized view, type "ROW"
                                  #     - alter function, routine, procedure

alter ENTITY "ENTITY"             #Rename an ENTITY.
 rename to "VAR"                  #Not for ENTITY: extension, operator, user mapping

create or replace ENTITY ...      #If "ENTITY" already exists, drop it first.
                                  #For ENTITY:
                                  #  - view, rule, trigger
                                  #  - aggregate, function, procedure
                                  #  - language, transform
create ENTITY if not exists ...   #If "ENTITY" already exists, noop but no error
                                  #For ENTITY: collation, extension, foreign table, index, materialized view, schema,
                                  #sequence, server, statistics, table, user mapping
alter ENTITY if exists ...        #Fail if "ENTITY" does not exist
                                  #For ENTITY: foreign table, index, materialized view, sequence, table, view

create ENTITY ... with (OPTS)     #Common syntax found in multiple ENTITYs
                                  #If OPTS.VAR VAL optional, then also optional in alter ...
                                  #When present the following are available too
alter ENTITY ... set (OPTS)       #Sets OPTS after creation
alter ENTITY ...                  #Sets OPTS to default value
 reset (OPTS.VAR,...)             #Not with ENTITY: publication|subscription

drop ENTITY "ENTITY"              #Delete an ENTITY
drop ENTITY if exists "ENTITY"    #Unless set, fails if exists


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         DEPENDENCIES          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


drop ENTITY ... restrict|cascade  #If there are dependency child objects
                                  #  - 'restrict' (def): fail
                                  #  - 'cascade': drop them, recursively
                                  #     - not allowed for database|tablespace|role|user mapping

pg_[sh]depend                     #TABLE with child|parent dependencies between [cluster-wide] ENTITYs
pg_[sh]depend.classid             #REGCLASS of a pg_catalog.* "TABLE". Entity type, e.g. pg_type|proc|class|...
pg_[sh]depend.objid               #OID of the ENTITY within its pg_catalog.* "TABLE"
pg_[sh]depend.objsubid            #INT4. "COL" number. 0 if classid is not pg_class
pg_[sh]depend.refclassid
pg_[sh]depend.refobjid
pg_[sh]depend.refobjsubid         #Same for the ENTITY it depends on
pg_shdepend.dbid                  #pg_database.oid of DATABASE
pg_depend.deptype                 #'CHAR' for the type of dependence among:
                                  #  - 'n' ("normal"): delete child with `cascade`
                                  #  - 'a' ("auto"): delete child with `cascade|restrict`
                                  #  - 'i' ("internal"): like 'a', except if child is dropped due to another parent being
                                  #    deleted, its initial parent is deleted too
                                  #  - 'P|S' ("partition primary|secondary"): partition CHILD_TABLE
                                  #  - 'e' ("extension"): EXTENSION direct child
                                  #  - 'x' ("auto extension"): EXTENSION indirect child
pg_shdepend.deptype               #'CHAR' for the type of dependence among:
                                  #  - 'o' ("owner"): ROLE owner child
                                  #  - 'a' ("ACL"): ROLE mentioned in ACL
                                  #  - 'r' ("policy"): ROLE mentioned in POLICY
                                  #  - 't' ("tablespace"): TABLESPACE parent of a TABLE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        TYPE DEFINITION        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


BASE TYPE ==>                     #Non-user-defined TYPE
                                  #Includes any "_TYPE"

create type TYPE(YOPTS)           #Creates user-defined TYPE, based on C functions
                                  #Not fully documented yet ???

YOPTS.category                    #'CHAR' among:
                                  #  - 'B': BOOL
                                  #  - 'N': NUM (or OID-like)
                                  #  - 'S': STR|NAME
                                  #  - 'Z': char, pg_node_tree
                                  #  - 'V': BSTR
                                  #  - 'D': date|time[stamp][tz]
                                  #  - 'T': INTERVAL
                                  #  - 'A': ARR_TYPE
                                  #  - 'G': point|line|lseg|box|polygon|path|circle
                                  #  - 'I': inet|cidr
                                  #  - 'U': user-defined (including tid|cid|xid|xid8, bytea, EXTENSION TYPEs like json, etc.)
                                  #  - 'E': ENUM_TYPE
                                  #  - 'R': [MULTI]RANGE_TYPE
                                  #  - 'C': ROW_TYPE
                                  #  - 'P': pseudo-type
                                  #  - 'X': unknown
                                  #Def: 'U'

YOPTS.default                     #VAL assigned as default value

YOPTS.like                        #TYPE2

YOPTS.input                       #FUNC
YOPTS.ouput                       #FUNC
YOPTS.receive                     #FUNC
YOPTS.send                        #FUNC
YOPTS.typemod_in                  #FUNC
YOPTS.typemod_out                 #FUNC
YOPTS.analyze                     #FUNC

pg_type                           #TABLE with all TYPEs
pg_type.oid                       #OID
pg_type.typname                   #"TYPE" name
pg_type.typtype                   #'CHAR' among:
                                  #  - 'b': base TYPE (typcategory 'B|N|S|Z|V|D|T|A|G|I|U')
                                  #  - 'e': ENUM_TYPE (typcategory 'E')
                                  #  - 'r': RANGE_TYPE (typcategory 'R')
                                  #  - 'm': MULTIRANGE_TYPE (typcategory 'R')
                                  #  - 'd': DOMAIN_TYPE (uses underlying TYPE's typcategory)
                                  #  - 'c': ROW_TYPE (typcategory 'C')
                                  #  - 'p': pseudo-type (typcategory 'P|X')
pg_type.typcategory               #'CHAR' of YOPTS.category
pg_type.typdefault                #STR of YOPTS.default. Can be:
                                  #  - null: none
                                  #  - VAL: if `create type` was used
                                  #  - 'EXPR': with DOMAIN_TYPE
pg_type.typinput                  #REGPROC of YOPTS.input
pg_type.typoutput                 #REGPROC of YOPTS.output
pg_type.typreceive                #REGPROC of YOPTS.receive. 0 if none.
pg_type.typsend                   #REGPROC of YOPTS.send. 0 if none.
pg_type.typmodin                  #REGPROC of YOPTS.typemod_in. 0 if none.
pg_type.typmodout                 #REGPROC of YOPTS.typemod_out. 0 if none.
pg_type.typanalyze                #REGPROC of YOPTS.analyze. 0 if none.
pg_type.typisdefined              #BOOL. Temporarily set to false while `create type` is ongoing.

regtype                           #TYPE alias to OID, to cast pg_type.oid as "TYPE" name


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            DOMAIN             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create domain "TYPE" [as] TYPE2   #Creates a TYPE based on TYPE2 but with additional CONSTRAINTs

create domain ... default VAL     #Default VAL (def: null)
                                  #Can be `default`
alter domain "TYPE"
 set default VAL                  #
alter domain "TYPE" drop default  #

create domain ... [not] null      #
alter domain "TYPE"
 set|drop not null                #

create domain ...
 check(BOOL_REXPR)                #Can use `value` to refer to the new value

create domain ...
 constraint "CONSTRAINT" ...      #Set name of "CONSTRAINT" underlying [not] null or check()
alter domain "TYPE"
 rename constraint
 "CONSTRAINT" to "CONSTRAINT2"    #

alter domain "TYPE"
 drop constraint
 [if exists] "CONSTRAINT"
 [restrict|cascade]               #

pg_type.typbasetype               #REGTYPE of underlying TYPE of DOMAIN_TYPE. 0 if not DOMAIN_TYPE
pg_type.typtypmod                 #INT4 passed to underlying TYPE(INT4) of DOMAIN_TYPE
                                  #0 if not DOMAIN_TYPE, or if underlying TYPE is not variable bounded
pg_attribute.atttypmod            #Same for COL
pg_type.typndims                  #INT4. Number of dimensions of underlying ARR_TYPE of DOMAIN_TYPE
                                  #0 if not DOMAIN_TYPE, or if underlying TYPE is not ARR_TYPE
pg_attribute.attndims             #Same for COL
pg_type.typdefaultbin             #PG_NODE_TREE of the default value, if using DOMAIN_TYPE
pg_type.typnotnull                #BOOL. `not null` on DOMAIN_TYPE. 0 if not DOMAIN_TYPE

pg_constraint.contypid            #REGTYPRE of CONSTRAINT's DOMAIN.
                                  #0 if CONSTRAINT is on a TABLE instead


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         TYPE CASTING          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_typeof(VAL)->'TYPE'            #TYPE as a STR

cast(VAL as TYPE)
VAL::TYPE                         #Explicit casting
TYPE '...'                        #Explicit casting, but UNKNOWN '...' only

ASSIGNMENT CASTING ==>            #Automatic casting when insert value in a COL
                                  #Always implies explicit casting too

IMPLICIT CASTING ==>              #Automatic casting when passing arguments to a FUNC
                                  #Always implies explicit + assignment casting too

TYPE -> TYPE2                     #Means can do implicit casting
TYPE => TYPE2                     #Means can do assignment casting
TYPE ~> TYPE2                     #Means can do explicit casting
TYPE <=-> TYPE2                   #Means TYPE2 => TYPE, TYPE -> TYPE2

pg_cast                           #TABLE with implicit|explicit type casting
pg_cast.oid                       #OID
pg_cast.castsource|casttarget     #REGTYPE of source|target TYPE
pg_cast.castfunc                  #REGPROC of FUNC used for conversion
                                  #FUNC(TYPE1_VAL[, INT[, BOOL]])->TYPE2_VAL
                                  #  - INT is the pg_attribute.ATTTYPEMOD
                                  #  - BOOL is true if explicit cast
pg_cast.castcontext               #Casting:
                                  #  - 'e': explicit
                                  #  - 'a': assignment
                                  #  - 'i': implicit
pg_cast.castmethod                #How to cast, among:
                                  #  - 'f': castfunc
                                  #  - 'i': C-level I/O functions
                                  #  - 'b': nothing (TYPEs are binary-equivalent)

create cast (TYPE as TYPE2)
 with function FUNC(...)          #Creates a type casting FUNC
 [as assignment|implicit]         #assignment|implicit is 'a|i' castcontext (def: 'e')
create cast ... with inout ...
create cast ...
 without function ...             #Same but with a 'i|b' castmethod (def: 'f')

YOPTS.preferred                   #BOOL (def: false). Each YOPTS.category has a single true, which is the preferred type cast.
pg_type.typispreferred            #Also used as the preferred TYPE in overloaded FUNCs
                                  #For:
                                  #  - NUM -> FLOAT8
                                  #  - STR -> TEXT
                                  #  - BSTR -> VARBIT
                                  #  - date|time[stamp][tz] -> TIMESTAMPTZ
                                  #  - REG* -> OID
                                  #  - inet|cidr -> INET


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          LARGE TYPES          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


FIXED LENGTH TYPES ==>            #Types that do not have a variable length

VARIABLE BOUNDED LENGTH TYPES     #One of:
 ==>                              #  - numeric(...)
                                  #  - varchar|bpchar
                                  #  - bit(...)
                                  #  - time[stamp][tz]|interval
VARIABLE UNBOUNDED LENGTH TYPES   #One of:
 ==>                              #  - numeric
                                  #  - text
                                  #  - bit varying, bytea
                                  #  - pg_node_tree
                                  #  - [MULTI]RANGE_TYPE
                                  #  - ARR_TYPE
                                  #  - ROW_TYPE
                                  #  - any other TYPE built from variable '...': json[b]|jsonpath, xml, hstore,
                                  #    ltree|l[txt]query, query_int, path|polygon, inet|cidr, tsvector|tsquery
                                  #Max length: 1GB

SIZING CAST ==>                   #Type cast to same type
                                  #Used to enforce size of variable bounded length TYPEs
                                  #  - failing if too large
                                  #  - padding or truncating
                                  #Usually implicit casting

YOPTS.internallength              #INT2. Number of bytes of a TYPE.
                                  #Can be `variable` for variable bounded|unbounded TYPE
pg_type.typlen                    #INT2 of YOPTS.internallength. Can be:
                                  #  - -2: cstring|unknown
                                  #  - -1: variable bounded|unbounded TYPE
pg_attribute.attlen               #Same for COL

YOPTS.alignment                   #'CHAR' specifying how many bytes to align the values when stored, i.e. padding them if necessary.
pg_type.typalign                  #Can be:
                                  #  - 'c' (1 byte, i.e. no alignment)
                                  #  - 's' (2 bytes)
                                  #  - 'i' (4 bytes)
                                  #  - 'd' (8 bytes, usually)
                                  #Usually same as YOPTS.internallength for fixed-length TYPEs except:
                                  #  - no alignment: cstring|unknown, name, uuid
                                  #  - 2 bytes: tidd
                                  #For variable length TYPEs:
                                  #  - ARR|RANGE_TYPE: uses underlying TYPE's alignment
                                  #  - 8 bytes: ROW_TYPE, PATH|POLYGON
                                  #  - 4 bytes: all others
pg_attribute.attalign             #Same for COL

YOPTS.passedbyvalue               #BOOL. Whether the TYPE is passed by value or by reference.
pg_type.typbyval                  #Builtin TYPEs: true if fixed length of 1-8 bytes, except macaddr[8] and tid
pg_attribute.attbyval             #Same for COL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             TOAST             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TOAST ==>                         #Large variable length TYPE values are split them in several ~2KB chunks
                                  #Goals:
                                  #  - fit in a single 8KB heap page, so that computation can happen in-memory only, which is faster
                                  #  - compression, to save space
IN-LINE TOAST ==>                 #Keeps data as is but possibly compress it
                                  #Requires 1|4 additional bytes
                                  #If <127 bytes
OUT-OF-LINE TOAST ==>             #Replace data with 18 bytes pointer to pg_toast_OID TABLE
                                  #If >127 bytes
pg_class.reltoastrelid            #REGCLASS of the RELATION's pg_toast_OID "TABLE". 0 if none
pg_toast.*                        #SCHEMA with out-of-line data
pg_toast.pg_toast_OID             #"TABLE" with TOAST out-of-line data
pg_toast.pg_toast_OID.chunk_id    #Data ID
pg_toast.pg_toast_OID.chunk_seq   #Data chunk serial NUM
pg_toast.pg_toast_OID.chunk_data  #BYTEA with the data
pg_toast_temp_NUM.*               #Same with TEMP TABLEs
                                  #Same special behavior as pg_temp_NUM

ENVVAR default_toast_compression  #Compress TOASTed values (in-line or out-of-line) among:
                                  #  - 'pglz' (def): Postgres-specific LZMA-like, optimized for speed (not space)
                                  #  - 'lz4': optimized for space
                                  #     - requires compiling with --with-lz4 (is the case with Ubuntu)
create table "TABLE"
 ("COL" TYPE compression ENUM,...)
alter table "TABLE"
 alter [column] "COL"
 set compression ENUM             #Sets compression, like ENVVAR default_toast_compression
pg_attribute.attcompression       #'CHAR' of compression among '\0' (default_toast_compression), 'p' (pglz), 'l' (lz4)

alter table "TABLE"               #Sets TOAST behavior among:
 alter [column] "COL"             #  - plain
 set storage ENUM                 #     - no out-of-line nor compression
                                  #     - only possible value for fixed length TYPEs
                                  #  - extended:
                                  #     - both out-of-line and compression
                                  #     - built-in TYPEs: for variable length TYPEs
                                  #  - external:
                                  #     - out-of-line but not compression
                                  #     - i.e. faster computation but more space
                                  #  - main:
                                  #     - no out-of-line but compression
                                  #     - might still be out-of-line, but only as last resort after extended|external ones
                                  #     - built-in TYPEs: numeric, inet|cidr
pg_attribute.attstorage           #'CHAR' for TOAST behavior for the COL
                                  #Can be 'p|x|e|m' for plain|extended|external|main
YOPTS.storage
pg_type.typstorage                #Same for a TYPE, i.e. setting default value for COLs using it

create table "TABLE"              #Do not use out-of-line nor compression for values < UINT bytes
 set (toast_tuple_target = UINT)  #Min: 128, max: 8160, def: 2040 (which is usually good)

OPTS.process_toast                #BOOL (def: true). Whether `vacuum` applies to TOAST_TABLEs too
                                  #Always `on` if `vacuum full`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         LARGE OBJECTS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LARGEOBJ                          #VAL that is persisted as its own file
                                  #Meant for large values
                                  #  - always on-file, not in-memory
                                  #  - conceptually like a file path or file descriptor
                                  #  - i.e. low memory but slow read|write
                                  #Split into 2KB chunks
LARGE OBJECT                      #"ENTITY" name

OID                               #When specifying as argument of LARGEOBJ creation, allows specifying its OID
                                  #Can be 0|-1 to ask for a new one

lo_import('PATH'[, OID])->OID     #Creates a LARGEOBJ from a file
lo_export(OID, 'PATH')            #Creates a file from a LARGEOBJ

lo_creat(OID)->OID                #Creates an empty LARGEOBJ
lo_from_bytea(OID, BYTEA)->OID    #Creates a LARGEOBJ from BYTEA
lo_unlink(OID)                    #Delete LARGEOBJ from database. Must be done after use
                                  #Does not remove from files, if imported from files

lo_get(OID[, INT8, INT4])->BYTEA  #Reads LARGEOBJ
                                  #If INT8, only from that byte, with a length of INT4 bytes
lo_put(OID, INT8, BYTEA)          #Writes LARGEOBJ

pg_largeobject_metadata           #TABLE with all LARGEOBJ metadata
pg_largeobject_metadata.oid       #OID of LARGEOBJ

pg_largeobject                    #TABLE with all LARGEOBJ data
                                  #Must be superuser
pg_largeobject.loid               #pg_largeobject_metadata.oid of LARGEOBJ
pg_largeobject.data               #BYTEA with contents
pg_largeobject.pageno             #INT 0-based index, if BYTEA split into several chunks

log_manage("COL")                 #Calls lo_unlink() on all LARGEOBJ OIDs that have been removed|changed since last call
                                  #Should be done as a TFUNC: before update or delete, for each row
                                  #Must `delete * from TABLE` before `drop|truncate table` to ensure TFUNC is called
                                  #Trusted postgres extension 'lo'

lo                                #TYPE that abstracts LARGEOBJ
                                  #Must be used as log_manage() "COL" TYPE

vacuumlo DATABASE...              #CLI. Remove LARGEOBJs which OIDs does not exist in DATABASE anymore
--limit|-l NUM                    #Max of LARGEOBJs to remove (def: 1000). Meant for performance
                                  #Can be 0
--dry-run|-n                      #
--host|-h HOST
--port|-p PORT
--username|-U USER
--no-password|-w
--password|-W                     #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             EQUAL             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


operator([SCHEMA.]OP)             #Another way to write OP
                                  #Default SCHEMA: pg_catalog

OPERATORS ==>                     #Operators shared by all TYPEs, except a few documented as such

(VAL)                             #Parenthesis to override operator order

VAL = VAL2
VAL <> VAL2                       #BOOL

VAL [not] in (VAL2,...)           #BOOL. Whether VAL = <> any VAL2

case [LVAL]
  when TVAL then RVAL
  [...]                           #Switch statement. Substitutes to RVAL where LVAL = TVAL
  [else RVAL]                     #Def LVAL is true, i.e. can use TVAL BOOLs, like an if statement
end                               #Def RVAL: null


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            COMPARE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


VAL > >= < <= VAL2                #BOOL

VAL between [symmetric]           #BOOL. Same as VAL >= VAL2 and VAL <= VAL3
 VAL2 and VAL3                    #If VAL3 < VAL2:
                                  #  - if symmetric: swap them
                                  #  - otherwise: returns false

greatest|least(VAL...)->BOOL      #Using > <

min|max(SET)->VAL                 #AFUNC


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            UNKNOWN            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


unknown                           #Unknown TYPE
                                  #Has no operators, but is usually implicitly transtyped based on operators
unknown -> VAL                    #Possible with any TYPE, except any*
VAL ~> unknown                    #Never possible, except with unknown itself

'...'                             #UNKNOWN
                                  #'' to escape a '
                                  #Otherwise can include any character, including \, newlines and Unicode codepoints.
                                  #Cannot include \0
                                  #  - must use BYTEA with '\x00'
                                  #Internally encoded using server ENCODING
                                  #Used for the literal value of all TYPEs, meant to be cast
                                  #  - noted as TYPE_UNKNOWN, meaning UNKNOWN that can be cast to TYPE
                                  #  - type casting is TYPE-specific, and not specified in pg_cast
$[TAG]$...$[TAG]$                 #Like '...' but can include '
U&'...'                           #Like '...' but can also include \NNNN or \+NNNNNN codepoint
E'...'                            #Like '...' but can use backslash escape sequences
                                  #\b, \f, \n, \r, \t, \v, \\, \NNN, \xNN, \uNNNN, \UNNNNNNNN

any*                              #TYPE. "Polymorphic type", i.e. abstract
                                  #Used in FUNCs to represent generic types
unknown -> any*                   #Not possible

cstring                           #TYPE. null-terminated STR
                                  #Similar to unknown, but meant to use inside FUNCs
                                  #Does not have any OPs (not even =), not used in FUNCs|COLs, but can be cast
CSTR <~=> STR                     #

PSEUDO-TYPE ==>                   #Any of: any*, unknown, record, cstring, void, [event_]trigger, internal, *_handler, pg_ddl_command


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             NULL              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


null                              #Missing data
                                  #Can be any TYPE (def: unknown)
                                  #Often avoided
                                  #  - e.g. with not null CONSTRAINTs

VAL OP null
FUNC(..., null, ...)              #Always return null, except statements below

null = <> < <= > >= null          #null
null [not] in (null)              #null
VAL is [not] distinct from VAL2   #Like VAL = <> VAL2 but null = null
VAL is [not] null                 #Same as VAL is [not] distinct from null
BOOL is [not] BOOL2               #Same as BOOL is [not] distinct from BOOL2

null or false                     #null
null and true                     #null
null or true                      #true
null and false                    #false
not null                          #null

case null when null else VAL end  #VAL

nullif(VAL, VAL2)->VAL|null       #null if VAL = VAL2. Otherwise VAL
coalesce(VAL,...)->VAL            #First VAL not null

concat(STR,...)->STR2             #See below
quote_nullable(VAL)->STR          #See below

ARR = <> ARR2                     #null = null, i.e. can be compared
                                  #null is also distinct from missing item
ARR < <= > >= ARR2                #Missing item < present item < null
ARR @> <@ && ARR2                 #Fail if contains null

ROW = <> < <= > >= ROW2           #null if contains any null

[not] null                        #COL_CONSTRAINT. See below

unique nulls [not] distinct
create unique index ...           #If `nulls distinct` (def), ignore nulls
 nulls [not] distinct             #I.e. can have multiple nulls
exclude()                         #Ignore nulls

references "TABLE2"("COL2")       #null ROWs are ignored in both COL and COL2
 match simple|full                #For multicolumn foreign keys:
                                  #  - match simple (def): ignore ROW if at least one COL null
                                  #  - match full:
                                  #     - ignore ROW if all COLs null
                                  #     - fail if some COLs null but not others

check (BOOL_REXPR)                #null -> true

where BOOL_REXPR                  #null -> false
                                  #For: select, insert on conflict, update, delete, exclude, copy, AFUNC filter, etc.
having BOOL_REXPR                 #null -> false
join ... on BOOL_REXPR            #null -> false
                                  #I.e. ROWs with nulls are usually:
                                  #  - ignored by inner join
                                  #  - kept as single ROWs by outer join

distinct
union|intersect|except distinct
AFUNC(distinct)                   #null = null
group by
over (partition by)               #null = null
BTREE DEDUPLICATION ==>           #null = null

order by ... nulls first|last
create index ...
 (...  nulls first|last,...)      #Def: nulls first for desc, nulls last for asc

AFUNC(...)                        #Ignores nulls
                                  #  - except: *agg(...), count(*), *rank(), cume_dist()
                                  #Returns null if empty SET
                                  #  - except: count()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            BOOLEAN            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


boolean                           #TYPE

BOOL => STR                       #Cast as 'true|false'
BOOL <~> INT4                     #Cast as 0|1

true|false|null                   #BOOL

BOOL or BOOL2                     #
BOOL and BOOL2                    #
not BOOL                          #

bool_or(BOOL_SET)->BOOL           #AFUNC. BOOL or
bool_and|every(BOOL_SET)->BOOL    #AFUNC. BOOL and

BOOL > >= < <= BOOL2              #true > false


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            NUMBER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


NUM                               #INT|FLOAT|money
INT                               #int2|4|8
FLOAT                             #float4|8|numeric

int2
smallint                          #TYPE. Signed 2 bytes integer
int4
integer                           #TYPE. Signed 4 bytes integer
int8
bigint                            #TYPE. Signed 8 bytes integer

float4
real                              #TYPE. 4 bytes float
float8
double precision                  #TYPE. 8 bytes float

numeric[(NUM[, NUM2])]            #TYPE. Fixed precision integer with NUM digits including NUM2 decimals
decimal[(NUM[, NUM2])]            #NUM2 can be negative: NUM2 last integer digits are zeros
                                  #Def NUM,NUM2: unbounded
                                  #Def NUM2: 0
                                  #Max NUM[2]: 1e3
                                  #Min NUM2: -1e3
                                  #Max value: 1e130000, min epsilon: 1e-16000
                                  #Rounded nearest, then towards [-]Infinity

INT2 <=-> INT4 <=-> INT8
 <=-> NUMERIC                     #When cast as smaller TYPE, fails if beyond max size.
 <=-> FLOAT4 <=-> FLOAT8          #When cast from FLOAT to INT, rounds nearest, then towards 0

money                             #TYPE. Like numeric(Infinity, 2). 8-byte long
                                  #Max value: 1e17
MONEY <=> NUMERIC
INT4|8 => MONEY                   #Type cast

NUM                               #Literal NUM
NUMe...                           #TYPE:
                                  #  - numeric: decimals or NUMe...
                                  #  - int8: >= 2**31
                                  #  - int4: otherwise
-0                                #Same as +0

'NaN'
'[-]Infinity'                     #FLOAT_UNKNOWN

generate_series(NUM, NUM2[, NUM3])
 ->NUM4_SET                       #From NUM to NUM2, with step NUM3 (def: 1)

width_bucket
 (NUM, NUM2, NUM3, INT)->INT2     #Bucket index INT2 of value NUM in an histogram from NUM2 to NUM3 with INT buckets.

to_hex(NUM)->STR                  #Hexadecimal, lowercase, without 0x prefix

to_char(NUM, STR)->STR2           #Use format STR, e.g. '999D9' (3 integer digits, 1 decimal digit)
                                  #Uses lc_time, lc_monetary, lc_numeric
                                  #Not documented yet


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             MATH              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


NUM + - / * NUM2                  #
-NUM                              #

sum(NUM_SET)->NUM                 #AFUNC
avg(NUM_SET)->NUM                 #AFUNC

INT & | # << >> INT2
~INT                              #Bitwise operations
INT # INT2                        #xor

power(NUM, NUM2)->FLOAT
NUM ^ NUM2                        #

mod(NUM, NUM2)->NUM3
NUM % NUM2                        #

div(NUM, NUM2)->INT               #trunc(NUM/NUM2)

|/ NUM                            #Square root
cbrt(NUM)->NUM
||/ NUM                           #Cube root

factorial(INT)->FLOAT             #

exp(NUM)->FLOAT                   #
ln(NUM)->FLOAT                    #
log10(NUM)->FLOAT                 #
log(NUM[, NUM2])->FLOAT           #Def: 10

abs(NUM)->NUM
@ NUM                             #
sign(NUM)                         #-1, 0 or 1

ceil(NUM)->FLOAT                  #
floor(NUM)->FLOAT                 #
trunc(NUM[, INT])->FLOAT          #Def INT: 0
round(NUM[, INT])->FLOAT          #Def INT: 0

[a]cos|sin|tan(NUM)->FLOAT        #
cot(NUM)->FLOAT                   #
atan2(NUM, NUM2)->FLOAT           #
degrees|radians(NUM)->FLOAT       #Conversion
pi()->FLOAT                       #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            STRINGS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


STR                               #varchar|bpchar|text (not char|name)
'...'                             #Personal notation meaning a STR
                                  #Including 'TABLE', 'COL', etc.
'STR'                             #Notation meaning a literal STR value

STRINGS ==>                       #Unless specified otherwise, operations are done Unicode codepoint-wise
                                  #  - including astral
                                  #  - "character" usually means "Unicode codepoint"
                                  #Unicode-aware, e.g. case
                                  #1-based indices

varchar[(NUM)]                    #TYPE. STR with NUM max Unicode codepoints (def: unlim)
                                  #Max NUM: 1e7
bpchar[(NUM)]
char(NUM)                         #TYPE. Like varchar, but pads with spaces (slower and takes more space)
text                              #TYPE. Like varchar, but variable length

BPCHAR -> VARCHAR|TEXT            #Type cast removes padded spaces
VAL -> STR                        #For all TYPEs
                                  #Usually looks like '...' TYPE_UNKNOWN
                                  #  - e.g. row(VAL,...) -> '(VAL,...)'

'...'                             #STR_UNKNOWN

string_agg(STR_SET, 'DELIM')->STR #AFUNC. Converts to STR

ascii('CHAR')->UINT               #Unicode codepoint
chr(UINT)->'CHAR'                 #Inverse

[char_]length(STR)->INT           #In Unicode codepoints
bit|octet_length(STR)->INT        #In server ENCODING bits|bytes

position(STR in STR2)->UINT       #Index of STR inside STR2. 0 if not found.

STR ^@ STR2
starts_with(STR, STR2)->BOOL      #

left|right(STR, UINT)->STR2       #UINT first|last characters
substring
 (STR from UINT [for UINT2])->STR2#UINT2 characters (def: all) from index UINT
overlay(STR placing STR2
 from UINT [for UINT2])->STR3     #Replace substring(STR from UINT [for UINT2]) by STR2

STR || STR2                       #Concatenation
concat(STR,...)->STR2             #Same as STR || ... but ignores nulls
concat_ws(STR, STR2...)           #Same but with separator STR

repeat(STR, UINT)->STR2           #
reverse(STR)->STR2                #

replace(STR, STR2, STR3)->STR4    #Replace each occurence of STR2 by STR3 in STR
translate
 (STR, 'CHAR...', 'CHAR2...')
 ->STR2                           #Replace each CHAR in STR by CHAR2
split_part(STR, STR2, UINT)
 ->STR3|null                      #Split STR by delimiter STR2 and returns item number UINT

lower|upper(STR)->STR2            #
initcap(STR)->STR2                #Titleize. upper() to first character, and characters after a non-letter|digit
                                  #lower() otherwise

trim([trailing|leading|both]
 [STR from] STR2)->STR3           #Remove (def: both) characters among STR (def: ' ') from STR2
l|rpad(STR, UINT[, STR2])->STR3   #Pads STR with STR2 (def: ' ') to length UINT.
                                  #If lower length, truncates.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             CHAR              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


char                              #TYPE. Like char(1)

'CHAR'                            #CHAR_UNKNOWN

CHAR <~> INT4                     #Type cast from first digit, e.g. '2' <~> 2
CHAR <=-> TEXT
CHAR <=> VARCHAR|BPCHAR           #Type cast. Truncate first Unicode codepoint


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           ESCAPING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


quote_ident(STR)->STR             #Escape STR to use as SQL "VAR"
                                  #Wrap in "" if needed, and escape "
quote_literal(VAL)->STR           #Escape STR to use as SQL STR
                                  #Wrap in '', and escape ' \
                                  #If not STR, stringify
quote_nullable(VAL)->STR          #Same as quote_literal(VAL), but if VAL is null, returns 'NULL', not null

format(STR, VAL...)->STR2         #Similar to sprintf(). Can use:
                                  #  - %I: quote_ident()
                                  #  - %s: quote_literal() without the outside ''
                                  #  - %L: quote_nullable()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           GLOBBING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


'GLOB'                            #Only two special characters: % (like *) and _ (like ?)
                                  #No file expansion

STR [not] [i]like 'GLOB'          #BOOL. True if matches
 [escape 'CHAR']                  #i if case-insensitive
                                  #'CHAR' is escape character (def: '\').
STR [!]~~[*] 'GLOB'               #Alternative syntax for [not] [i]like


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            REGEXP             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


'REGEXP'                          #Also with some different syntaxes:
                                  #  - \< \> -> \m \M
                                  #  - \b \B -> \y \Y
                                  #Also does not have:
                                  #  - greediness
                                  #  - n|p|w flags
                                  #  - modifiers (\u \l ...)
                                  #  - [...&&--...]
                                  #  - (?<GROUP>)
                                  #  - \p{PROP}
                                  #Use locales|Unicode for case and [[:...:]]

STR [!]~[*] 'REGEXP'              #BOOL. True if matches
                                  #* if case-insensitive

substring(STR, 'REGEXP')          #Returns first match
 ->STR2|null                      #If there is an outer set of parenthesis, return that part only
regexp_matches                    #Returns first match (or all if FLAG 'g')
 (STR, STR2[, 'FLAGS'])->ARR_SET  #If no match, empty SET
                                  #ARR are parenthesis matches (if none, single item)

regexp_replace
 (STR, 'REGEXP', STR2[, 'FLAGS'])
 ->STR3                           #Replace matches by STR2

regexp_split_to_array
 (STR, 'REGEXP'[, 'FLAGS'])       #Split STR using REGEXP delimiter.
 ->STR_ARR                        #If delimiter not found, returns {STR}
regexp_split_to_table(...)
 ->STR_SET                        #Same but as a SET


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ENUM              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENUM_TYPE                         #Each enum has its own TYPE
anyenum                           #TYPE. Any ENUM_TYPE

'...'                             #ENUM_VAL_UNKNOWN
ENUM_VAL                          #4-byte long

create type "ENUM_TYPE"
 as enum(ENUM_VAL...)             #Arguments list decides ordering

alter type "ENUM_TYPE" add value
 [if not exists] ENUM_VAL
 [before|after ENUM_VAL2]         #
alter type "ENUM_TYPE"
 rename ENUM_VAL to ENUM_VAL2     #

ENUM_VAL < <= > >= ENUM_VAL2      #BOOL. Using ordering

enum_first|last(ENUM_VAL)         #First|last ENUM_VAL of the same type
 ->ENUM_VAL2                      #null::ENUM_TYPE can be used as argument
enum_range(ENUM_VAL)              #All ENUM_VALs of the same type
 ->ENUM_VAL_ARR                   #null::ENUM_TYPE can be used as argument
enum_range(ENUM_VAL, ENUM_VAL2)   #From ENUM_VAL to ENUM_VAL2
 ->ENUM_VAL_ARR                   #null::ENUM_TYPE can be used to represent start|end

pg_enum                           #TABLE with all ENUM_VALs
pg_enum.oid                       #OID
pg_enum.enumlabel                 #NAME of ENUM_VAL
pg_enum.enumsortorder             #UINT. Sort position
pg_enum.enumtypid                 #REGTYPE of ENUM_TYPE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             BITS              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


BSTR                              #BIT|VARBIT

bit[(NUM)]                        #TYPE. BIT, i.e. like STR, but bit-wise
                                  #Def NUM: 1
                                  #Max NUM: 1e8
                                  #0-based indices
varbit[(NUM)]
bit varying[(NUM)]                #TYPE. VARBIT, i.e. same but variable length (unless NUM)

BIT <-> VARBIT                    #Type cast. Truncates last bits if needed.
BIT <~> INT4|8                    #Type cast. Fails if larger than max INT

B'...'                            #BIT with 0|1s
X'...'                            #BIT with hex chars

BSTR & | # << >> BSTR2            #
~BSTR                             #

bit_and|or|xor(INT|BIT_SET)
 ->INT|BIT                        #AFUNC. Bitwise and|or|xor

get_bit(BSTR|BYTEA, UINT)->0|1    #
set_bit(BSTR|BYTEA, UINT, 0|1)
 ->BSTR|BYTEA                     #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             BYTES             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


bytea                             #TYPE. Like BSTR but byte-wise
                                  #0-based indices

'...'                             #BYTEA_UNKNOWN, using server ENCODING
'\x...'                           #If starting with \x, ... is hex only
                                  #Only way to include \x00

encode(BYTEA, STR)->STR2          #Converts to format STR:
                                  #  - 'hex': hex only
                                  #  - 'escape': octal \NNN only if not [[:print:]]
                                  #  - 'base64'
decode(STR, STR2)->BYTEA          #Inverse

string_agg
 (BYTEA_SET, DELIM_BYTEA)->BYTEA  #AFUNC. Converts to BYTEA

get_byte(BYTEA, UINT)->0-255      #
set_byte(BYTEA, UINT, 0-255)
 ->BYTEA                          #

trim(...)                         #Same as STR, but for BYTEA

md5(STR|BYTEA)->STR2              #

BYTEA|BSTR || BYTEA|BSTR
bit|octet_length(...)
overlay(...)
position(...)
substring(...)                    #Same as STR, but for BYTEA|BSTR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        INTEGER ARRAYS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INT4_ARR                          #Trusted postgres extension 'intarray' adds more methods for INT4_ARR
                                  #They must not contain any nulls
                                  #1-based indices

INT4_ARR @> INT4_ARR2             #BOOL. Is superset|equal
INT4_ARR <@ INT4_ARR2             #BOOL. Is subset|equal
INT4_ARR && INT4_ARR2             #BOOL. True if any INT = any INT2

query_int                         #TYPE
QUERY_INT                         #'INT' which can use & | ! () e.g. 'INT & !(INT2 | INT3)'
INT4_ARR @@ QUERY_INT
QUERY_INT ~~ INT4_ARR             #BOOL. Whether it matches

icount(INT4_ARR)->NUM             #Number of elements, including duplicates
idx(INT4_ARR, INT2)->UINT         #Index of first element with value INT2
                                  #0 if none

subarray(INT4_ARR, INT2[, INT3])  #Slice from INT2 (included) to INT3 (excluded, def: end)
 ->INT4_ARR                       #INT2|3 can be negative to be index from end

INT4_ARR + INT4[_ARR]2            #Concatenate
INT4_ARR | INT4[_ARR]2            #Concatenate. Remove duplicates and sort.
INT4_ARR & INT4_ARR2              #Intersection. Remove duplicates and sort.
INT4_ARR - INT4[_ARR]2            #Difference. Remove duplicates and sort.

sort(INT4_ARR[, 'desc'])->INT4_ARR#
uniq(INT4_ARR)->INT4_ARR          #Remove duplicates. Must be first sorted


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATE/TIME           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


date                              #TYPE. 4 bytes-long
'YYYY-MM-DD'                      #DATE_UNKNOWN

time                              #TYPE. 8 bytes-long
'HH:MM:SS[.SSSSSS]'               #TIME_UNKNOWN

timestamp                         #TYPE. 8 bytes-long
'DATE TIME'                       #TIMESTAMP_UNKNOWN

TIME[TZ] <= TIMESTAMP[TZ]         #Type cast, removing DATE
DATE <=-> TIMESTAMP[TZ]           #Type cast, removing TIME or filling it with midnight

ENVVAR DateStyle                  #'OUTPUT, INPUT'
                                  #Def: based on lc_time, or 'ISO, MDY'
                                  #INPUT:
                                  #  - how to read ambiguous DATEs (day is from 1 to 12)
                                  #  - can be: "DMY", "MDY" or "YMD"
                                  #OUTPUT:
                                  #  - how to print DATE[TZ]|TIME[STAMP][TZ]
                                  #  - 'ISO': YYYY-MM-DD HH:MM:SS.SSSSSS
                                  #  - 'SQL': DD/MM/YYYY HH:MM:SS.SSSSSS
                                  #  - 'Postgres': Thu May DD HH:MM:SS.SSSSSS YYYY

'[-]infinity'                     #DATE|TIMESTAMP_UNKNOWN
isfinite(DATE|TIMESTAMP|INTERVAL)
 ->BOOL                           #Whether [-]Infinity

'epoch'                           #DATE|TIMESTAMP_UNKNOWN

'yesterday|today|tomorrow'        #DATE|TIMESTAMP_UNKNOWN (midnight)
'allballs'                        #TIME_UNKNOWN (midnight)

'now'                             #DATE|TIME|TIMESTAMP_UNKNOWN
current_time[stamp]|date          #Same but directly DATE|TIMETZ|TIMESTAMPTZ instead of UNKNOWN
local_time[stamp]|date            #Same but using current TZ
statement_timestamp()
 ->TIMESTAMPTZ
transaction_timestamp()
 ->TIMESTAMPTZ                    #Beginning of statement|transaction

DATE + - NUM                      #DATE
DATE - DATE2                      #NUM

DATE + - TIME                     #TIMESTAMP

extract
 (PERIOD from TIME[STAMP]|INTERVL)#PERIOD: millenium, century, decade, [iso]year, quarter, month, week, day, [iso]dow (day of week),
 ->UINT                           #doy, hour, minute, [micro|milli]seconds, timezone[_hour|minute], epoch
date_trunc('PERIOD',
 TIMESTAMP|INTERVAL[, 'TZ'])
 ->TIMESTAMP|INTERVAL             #Truncates until PERIOD

to_char(TIMESTAMP|INTERVAL, STR)  #Use format STR, e.g. 'HH24:MI:SS'
 ->STR2                           #Not documented yet
to_timestamp(STR, STR2)
 ->TIMESTAMPTZ
to_date(STR, STR2)->DATE
to_number(STR, STR2)->FLOAT       #Inverse

generate_series
 (TIMESTAMP, TIMESTAMP2, INTERVAL)
 ->TIMESTAMP_SET                  #From TIMESTAMP to TIMESTAMP2, with step INTERVAL

pg_sleep(FLOAT)                   #Sleeps FLOATS seconds
                                  #Time resolution is OS-specific, often 10ms


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          TIME ZONES           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOCALES ==>                       #Used, including current timezone, unless specified otherwise

timetz                            #TYPE
'TIME[TZ]'                        #TIMETZ_UNKNOWN. TZ is [GMT]-|+NUM

timestamptz                       #TYPE
'TIMESTAMP[TZ]'                   #TIMESTAMPTZ_UNKNOWN

TIME <=-> TIMETZ
TIMESTAMP <=-> TIMESTAMPTZ        #Type cast, using local TZ

TZ                                #Use either of the following ones
ENVVAR TimeZone                   #Def: system one

pg_timezone_names                 #TABLE with TZs by names
pg_timezone_names.name            #STR, e.g. 'America/New_York'
pg_timezone_names.abbrev          #STR, e.g. 'PST'
pg_timezone_names.utc_offset      #INTERVAL
pg_timezone_names.is_dst          #BOOL. Daylight saving time

pg_timezone_abbrevs               #TABLE with TZs by abbreviation
pg_timezone_abbrevs.abbrev
pg_timezone_abbrevs.utc_offset
pg_timezone_abbrevs.is_dst        #

ENVVAR timezone_abbreviations     #Override list of available TZs. Def: 'Default'

TIME[STAMP][TZ] at time zone 'TZ' #TIME[STAMP]TZ


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           INTERVAL            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


interval [PERIOD [to PERIOD2]]    #TYPE. 16-byte long
                                  #PERIOD is year|month|day|hour|minute|second (def: any)

TIME <=-> INTERVAL                #Type cast

'SS[.SSSSSS]'
'[YYYY-[MM-[DD]]]
 [HH:[MM:[SS[.SSSSS]]]]'          #INTERVAL_UNKNOWN

ENVVAR IntervalStyle              #INTERVAL format using in input|output:
                                  #  - 'postgres' (def): 'NUM days HH:MM:SS.SSSSSS'
                                  #  - 'postgres_verbose': '@ NUM days NUM hours NUM mins NUM.SSSSSS secs'
                                  #  - 'sql_standard': 'DDD HH:MM:SS.SSSSSS'
                                  #  - 'iso_8601': 'P...DT...H...M....SSSSSS'

DATE + - INTERVAL                 #TIMESTAMP
TIME[STAMP]|INTERVAL + - INTERVAL #TIME[STAMP]|INTERVAL
TIME[STAMP] - TIME[STAMP]2        #INTERVAL

INTERVAL * / NUM                  #
-INTERVAL                         #

sum|avg(INTERVAL_SET)->INTERVAL   #AFUNC

justify_days(INTERVAL)->INTERVAL  #Modulo on days, e.g. 35 days -> 1 month 5 days
justify_hours(INTERVAL)->INTERVAL #Modulo on hours, e.g. 30 hours -> 1 day 6 hours
justify_interval
 (INTERVAL)->INTERVAL             #justify_days() + justify_hours()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              OID              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


oid                               #TYPE for an ID of a VAR.
                                  #Internally like an int4
                                  #32 bits, i.e. only use for uniqueness if:
                                  #  - TABLE-scoped
                                  #  - either:
                                  #     - random on <2e3 items
                                  #     - serial on <2e9 items
INT8 <=-> OID
INT2 -> OID                       #Type cast

reg*                              #TYPE alias to OID, for specific ENTITYs
                                  #Allows transtyping a OID from|to a 'NAME' instead
                                  #E.g. pg_namespace.oid::regnamespace -> "SCHEMA"
REG* <-> 'NAME'                   #Type cast
                                  #Can quote using '"NAME"'
REG* <-> OID                      #Type cast
to_reg*(TEXT)->REG*               #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            RANDOM             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


random()->FLOAT                   #From 0 to 1
                                  #Cycle of 3e14 numbers
                                  #PRNG not crypto-secure
setseed(FLOAT)                    #FLOAT is from 0 to 1

normal_rand(INT, FLOAT, FLOAT2)   #INT random variables following N(FLOAT, FLOAT2)
 ->INT_SET                        #Trusted postgres extension 'tablefunc'


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             UUID              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


uuid                              #TYPE. 16-byte long
                                  #Trusted postgres extension 'uuid-ossp'
'nnnn-nnnn-nnnn-nnnn-nnnn-
 nnnn-nnnn-nnnn'                  #UUID_UNKNOWN. Dashes all optional

uuid_generate_v1[mc]()->UUID      #UUID v1
                                  #If mc, uses a random multicast MAC address
uuid_generate_v3|5                #UUID v3|5
 (uuid_ns_*(), STR)->UUID         #* is dns|url|oid|x500
uuid_generate_v4()->UUID          #UUID v4


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             JSON              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


json                              #TYPE. Including scalar types.
                                  #Must be well-formed.
                                  #Cannot use = <> < <= > >=, but can cast

'JSON'                            #JSON_UNKNOWN
KEY                               #UINT|STR

JSONB ~> BOOL|NUM|STR             #Type cast. Fail if top-level value is not BOOL|NUM|STR|null

JSON->KEY                         #OBJ[KEY] as JSON2
JSON->>KEY                        #OBJ[KEY] as 'JSON2'
JSON#>KEY_ARR
JSON#>>KEY_ARR                    #Same but with multiple successive KEYs

CONVERSION ==>                    #ARR item <-> SET element
                                  #OBJ <-> ROW
                                  #OBJ key <-> COL

to_json(VAL)->JSON                #
array_to_json(ARR)->ARR_JSON      #
row_to_json(ROW)->OBJ_JSON        #

json[b]_agg(SET)->ARR_JSON[B]     #AFUNC. Converts to ARR_JSON[B]
json[b]_object_agg
 ('KEY'_SET, VAL_SET)->OBJ_JSON[B]#AFUNC. Converts to OBJ_JSON[B]

json_array_elements(ARR_JSON)->SET#Converts ARR_JSON to SET
json_array_length(ARR_JSON)->UINT #ARR.length
json_populate_record              #Converts OBJ_JSON to ROW
 (null::ROW_TYPE, OBJ_JSON)->ROW  #OBJ keys not in ROW_TYPE are omitted
json_populate_recordset
 (null::ROW_TYPE, OBJ_ARR_JSON)
 ->ROW_SET                        #Converts OBJ_ARR_JSON to ROW_SET

json_each(OBJ_JSON)->ROW_SET      #Each OBJ entry -> ROW with COLs: key STR, value JSON
json_each_text(OBJ_JSON)->ROW_SET #Same but value COL is STR
json_object_keys(OBJ_JSON)
 ->STR_SET                        #Each OBJ key -> STR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              XML              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


xml                               #TYPE. Not documented yet
                                  #Cannot use = <> < <= > >=, but can cast

STR ~> XML                        #Type cast

'XML'                             #XML_UNKNOWN

xmlparse(document|content STR)    #Must be well-formed.
 ->XML                            #content: for fragments
xmlserialize(document|content XML
 as [var]char|text)->STR          #
enum_first|last(ENUM_VAL)         #First|last ENUM_VAL of the same type
 ->ENUM_VAL2                      #null::ENUM_TYPE can be used as argument

xml_agg(SET)->XML                 #AFUNC. Converts to XML


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            HSTORE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


hstore                            #TYPE. OBJ where keys are identifiers
                                  #Trusted postgres extension 'hstore'
                                  #Cannot use < <= > >=
VAL                               #'VAL'|null

STR ~> HSTORE                     #Type cast, using 'KEY=>VAL,...'

'KEY=>VAL,...'                    #HSTORE_UNKNOWN
                                  #"" to escape KEY|VAL for whitespace , = >
hstore(['KEY', VAL]_ARR)->HSTORE  #
hstore(['KEY', VAL, ...])->HSTORE #
hstore('KEY'_ARR, VAL_ARR)
 ->HSTORE                         #

HSTORE->'KEY'                     #VAL
HSTORE->'KEY'_ARR                 #VAL_ARR

HSTORE ? 'KEY'                    #BOOL. True if HSTORE->'KEY' exists
defined(HSTORE, 'KEY')->BOOL      #True if HSTORE->'KEY' exists and is not null
HSTORE ?& 'KEY'_ARR               #BOOL. True if all of HSTORE->'KEY' exists
HSTORE ?| 'KEY'_ARR               #BOOL. True if any of HSTORE->'KEY' exists

HSTORE @> HSTORE2                 #BOOL. HSTORE is superset of HSTORE2 (including equal)
HSTORE <@ HSTORE2                 #BOOL. Same with subset (or equal)

akeys(HSTORE)->'KEY'_ARR          #
avals(HSTORE)->VAL_ARR            #
%# HSTORE                         #['KEY', VAL]_ARR
%% HSTORE                         #['KEY', VAL, ...]

HSTORE || HSTORE2                 #Merge. HSTORE2 has priority
HSTORE - HSTORE2                  #Substract. Only for HSTORE2 entries with same key + value
HSTORE - 'KEY'[_ARR]              #Substract.
slice(HSTORE, 'KEY'_ARR)->HSTORE2 #Pick.
                                  #If 'KEY' not found, ignored.

hstore(ROW)->HSTORE               #Using COL names as KEYs
ROW #= HSTORE                     #ROW2. Set ROW values using HSTORE

skeys(HSTORE)->'KEY'_SET
svals(HSTORE)->VAL_SET            #Same as akeys|avals() but as SET
each(HSTORE)->ROW_SET             #With COLs: key STR, value STR

HSTORE ~> JSON[B]                 #Type cast, similar to hstore_to_json()
hstore_to_json(HSTORE)->OBJ_JSON  #OBJ values are all STR|null
hstore_to_json_loose(HSTORE)
 ->OBJ_JSON                       #Same but OBJ values can also be NUM or BOOL (with VAL 't|f')


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             LTREE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ltree                             #TYPE. Array of VAR representing a reference
                                  #0-based indices
                                  #Trusted postgres extension 'ltree'

'VAR.VAR2....'                    #LTREE_UNKNOWN
                                  #VAR is [[:alpha:]_], max 256 characters.
                                  #Max 65e3 VARs
                                  #Can be empty ''

LTREE < <= >= > LTREE2            #Compare first VAR, then second, etc.
                                  #Missing VAR is less

LTREE @> LTREE2                   #BOOL. Is parent of equal.
LTREE <@ LTREE2                   #BOOL. Is child of equal.
LTREE_ARR @> <@ LTREE2
LTREE @> <@ LTREE2_ARR            #BOOL. Same but "any of"
LTREE_ARR ?@> ?<@ LTREE2          #LTREE|null. First LTREE that @> <@ LTREE2

LTREE || LTREE2                   #LTREE3. Concatenate as 'LTREE.LTREE2'

nlevel(LTREE)->UINT               #Number of VARs

index(LTREE, LTREE2[, INT])       #If LTREE2 is inside LTREE, indice of VAR where it starts
 ->INT2                           #Otherwise, returns -1
                                  #Ignore first INT VARs (def: 0).
                                  #INT can be negative to specify from end

subltree(LTREE, UINT, UINT2)
 ->LTREE2                         #Sliced from VAR at index UINT (included) to UINT2 (excluded)
subpath(LTREE, INT[, INT2])       #Same but:
 ->LTREE                          #  - negative INT[2] is indice from the end
                                  #  - otherwise INT2 is number of VARs (def: all)

lca(LTREE_ARR)->LTREE2
lca(LTREE...)->LTREE2             #Common prefix ('' if none)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LQUERY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


lquery                            #TYPE. Globbing-like query against a LTREE (full match)

'VAR.VAR2...'                     #LQUERY_UNKNOWN.
                                  #Same syntax as LTREE with following additional one.
*                                 #Any multiple VARs
*{[NUM],[NUM2]}                   #NUM (def: 0) to NUM2 (def: any) multiple VARs
VAR*                              #Any suffix
VAR%                              #Matches _-separated parts
VAR@                              #Case insensitive
VAR|VAR2                          #Or
!VAR                              #Not

LTREE[_ARR] ~ LQUERY
LQUERY ~ LTREE[_ARR]              #BOOL. Whether [any] LTREE fully matches LQUERY
LTREE[_ARR] ? LQUERY_ARR
LQUERY_ARR ? LTREE[_ARR]          #BOOL. Whether [any] LTREE fully matches any LQUERY

LTREE_ARR ?~ LQUERY               #LTREE|null. First LTREE that ~ LQUERY


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           LTXTQUERY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ltxtquery                         #TYPE. Like lquery but partial match

'...'                             #LTXTQUERY_UNKNOWN
                                  #Single VAR. Also following syntax.
VAR*
VAR%
VAR@
VAR | VAR2
!VAR                              #Like LQUERY
VAR & VAR2                        #And

LTREE[_ARR] @ LTXTQUERY
LTXTQUERY @ LTREE[_ARR]           #BOOL. Whether [any] LTREE partially matches LTXTQUERY

LTREE_ARR ?@ LTXTQUERY            #LTREE|null. First LTREE that @ LTXTQUERY


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INET              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


inet                              #TYPE
'IPv4|6'                          #INET_UNKNOWN

cidr                              #TYPE
'CIDR'                            #CIDR_UNKNOWN

INET|CIDR => STR                  #Type cast
INET => CIDR                      #Type cast

INET >> >>= << <<= INET2          #Containing|contained within [or equal]

INET | & INET2
~INET                             #Bitwise operations

INET + - INET|UINT                #From last to first field.

family(INET)->4|6                 #

network(INET)->CIDR               #
host(INET)->'INET'                #Remove CIDR mask

masklen(INET)->UINT               #NUM of bits of network mask
set_masklen(INET|CIDR, UINT)
 ->INET|CIRD                      #Sets network mask
netmask(INET)->INET2              #Subnet mask
hostmask(INET)->INET2             #Cisco wildcard

broadcast(INET)->INET2            #Broadcast IP

text(INET)->STR                   #Serialize in a long way
abbrev(INET|CIDR)->STR            #Serialize in a short way


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              MAC              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


macaddr                           #TYPE
'NN:NN:NN:NN:NN:NN'               #MACADDR_UNKNOWN

macaddr8                          #TYPE
'NN:NN:NN:NN:NN:NN:NN:NN'         #MACADDR8_UNKNOWN

MACADDR <-> MACADDR8              #Type cast

MACADDR[8] | & MACADDR[8]2
~MACADDR[8]                       #Bitwise operations

trunc(MACADDR[8])->MACADDR[8]     #Put second half as 00


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           GEOMETRY            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


= <> < <= > >=                    #Cannot be used. But can cast to STR

point                             #TYPE. 16-byte long
'(NUM,NUM2)'                      #POINT_UNKNOWN
POINT[0|1]                        #NUM[2]

line                              #TYPE. Infinite LINE. 24-byte long
'((NUM,NUM2),(NUM3,NUM4))'        #LINE_UNKNOWN
'{NUM,NUM2,NUM3}'                 #LINE_UNKNOWN as NUMx + NUM2y + NUM3
LINE[0|1|2]                       #NUM[2|3]

lseg                              #TYPE. Segment LINE. 32-byte long
'((NUM,NUM2),(NUM3,NUM4))'        #LSEG_UNKNOWN
LSEG[0|1]                         #POINT

box                               #TYPE. Rectangle. 32-byte long
'((NUM,NUM2),(NUM3,NUM4))'        #BOX_UNKNOWN (diagonal)
'{(...);...}'                     #BOX_ARR_UNKNOWN must use ;
BOX[0|1]                          #POINT

path                              #TYPE
polygon                           #TYPE. Like closed PATH
'((NUM,NUM2)...)'                 #Closed PATH_UNKNOWN
'[(NUM,NUM2)...]'                 #Open PATH_UNKNOWN

circle                            #TYPE. 32-byte long
'<(NUM,NUM2),NUM3>'               #CIRCLE_UNKNOWN

POINT <~ LSEG <~ BOX
 <~> CIRCLE <~> POLYGON           #Type casts
POINT => BOX
BOX => POLYGON
PATH <=> POLYGON                  #Type casts

cube                              #TYPE. N-dimensional cube
                                  #Trusted postgres extension 'cube'
'(NUM...), (NUM2...)'             #CUBE_UNKNOWN diagonal

GEOMETRY OPERATORS ==>            #Many operators for shift, rotation, scaling, getting points|distance like center, positions, etc. exist
                                  #Including for cube
                                  #Not documented yet


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             RANGE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


RANGE ==>                         #Like an ARR of size 2 indicating start and end.

int4range                         #RANGE TYPE of INT4
int8range                         #RANGE TYPE of INT8
numrange                          #RANGE TYPE of NUMERIC
daterange                         #RANGE TYPE of DATE
ts[tz]range                       #RANGE TYPE of TIMESTAMP[TZ]
anyrange                          #TYPE. Any RANGE TYPE

create type "RANGE_TYPE"
 as range(OPTS)                   #New RANGE TYPE.
OPTS.subtype                      #TYPE of start|end
OPTS.multirange_type_name         #'TYPE' of MULTIRANGE
                                  #Def: replace 'range' by 'multirange', or append '_multirange'
OPTS.subtypediff                  #FUNC(VAL, VAL2)->UINT, computing distance
OPTS.canonical                    #FUNC2(VAL)->VAL (def: noop) transforming each VAL

'[VAL, VAL2]'                     #RANGE_UNKNOWN. Square brackets include, parenthesis exclude.
'[VAL, VAL2)'                     #Def VAL[2]: null
'(VAL, VAL2]'                     #  - noted as empty, not `null`
'(VAL, VAL2)'                     #If TYPE is discrete, VAL2] is normalized to VAL2+1)
RANGE_TYPE(VAL, VAL2[, STR])
 ->RANGE                          #Same. STR is e.g. '[)' (def) or '()'

null                              #As VAL[2]: like [-]Infinity
lower|upper_inf(RANGE)->BOOL      #Whether null

'empty'                           #Empty RANGE. VAL[2] are both null, but not handled like Infinity
isempty(RANGE)->BOOL              #

lower|upper(RANGE)->VAL[2]        #
lower|upper_inc(RANGE)->BOOL      #Whether [] or ()

RANGE @> VAL|RANGE2               #BOOL. Is superset|equal
VAL|RANGE <@ RANGE2               #BOOL. Is subset|equal
RANGE && RANGE2                   #BOOL. Whether overlaps

RANGE << RANGE2                   #BOOL. Whether RANGE end before RANGE2 start
RANGE >> RANGE2                   #Same as RANGE2 << RANGE
RANGE &< RANGE2                   #BOOL. Whether RANGE start before RANGE2 start, and RANGE end before RANGE2 end
RANGE &> RANGE2                   #Same as RANGE2 &< RANGE
RANGE -|- RANGE2                  #BOOL. Whether RANGE end adjacent to RANGE2

RANGE + RANGE2                    #RANGE3. Union. Must overlap or be contiguous.
range_merge(RANGE, RANGE2)->RANGE3#Union. If does not overlap nor contiguous, fill the gap.
RANGE - RANGE2                    #[RANGE.begin, RANGE2.begin]
                                  #If RANGE2.begin < RANGE.begin:
                                  #  - if RANGE2.end > RANGE.end, returns [RANGE.begin, RANGE.end]
                                  #  - otherwise returns empty
RANGE * RANGE2                    #RANGE3. Intersection. Empty if none.

range_agg
 ([MULTI]RANGE_SET)->[MULTI]RANGE #AFUNC. Union
range_intersect_agg
 ([MULTI]RANGE_SET)->[MULTI]RANGE #AFUNC. Intersection

pg_range                          #TABLE with all RANGEs
pg_range.rngtypid                 #REGTYPE of RANGE_TYPE
pg_range.rngsubtype               #REGTYPE of OPTS.subtype
pg_range.rngmultitypid            #REGTYPE of MULTIRANGE TYPE
pg_range.rngcanonical             #REGPROC of OPTS.canonical. 0 if none
pg_range.rngsubdiff               #REGPROC of OPTS.subtypediff. 0 if none
pg_range.rngcollation             #REGCOLLATION of OPTS.collation. 0 if none
pg_range.rngsubopc                #pg_opclass.oid of OPTS.subtype_opclass


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MULTIRANGE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


MULTIRANGE ==>                    #Combination of multiple RANGEs
                                  #Behaves like a RANGE with potential gaps
                                  #If RANGEs overlapped, normalized by being merged

int4multirange
int8multirange
nummultirange
datemultirange
ts[tz]multirange
anymultirange                     #MULTIRANGE TYPEs

RANGE ~> MULTIRANGE               #Type cast

'{RANGE,...}'                     #MULTIRANGE_UNKNOWN
MULTIRANGE_TYPE([RANGE,...])
 ->MULTIRANGE                     #Same

OPERATORS ==>                     #All FUNCs and operators for RANGE work with MULTIRANGE too
                                  #Can mix RANGE and MULTIRANGE arguments, except for + - *

range_merge(MULTIRANGE)->RANGE    #Convert to single RANGE, filling any gap
multirange(RANGE)->MULTIRANGE     #Inverse

unnest(MULTIRANGE)->RANGE_SET     #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ARRAY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TYPE[]                            #TYPE2 with 0-n values of same TYPE
_TYPE                             #Dimensions are defined at write-time.
                                  #Underlying TYPE can be another ARR for multidimensional ARRs.
                                  #1-based indices
                                  #Max length 1.3e8
[any]array                        #TYPE. Any TYPE2[]
anynonarray                       #TYPE.

array[VAL...]                     #ARR literal value.
'{VAL,...}'                       #ARR_UNKNOWN
'[UINT[:UINT2]]...={VAL,...}'     #Same but with specific:
                                  #  - lower bound UINT (included)
                                  #     - def: 0
                                  #  - upper bound UINT2 (included)
                                  #     - def: UINT
                                  #     - must be UINT + ARR.length - 1, i.e. cannot truncate|extend
                                  #One [...] per dimension

array_agg(SET)->ARR               #AFUNC. Converts to ARR

ARR[UINT]                         #VAL. null if out-of-bound.
ARR[UINT:UINT2]                   #ARR2, from UINT (included) to UINT2 (included)
                                  #Uses max(UINT2, ARR.length)
                                  #Empty if UINT2 < UINT, or if UINT > ARR.length
ARR[UINT:]                        #Same as ARR[UINT:ARR.length]
ARR[:UINT2]                       #Same as ARR[0:UINT2]

ARR = <> ARR2                     #BOOL. Done deeply
ARR < <= > >= ARR2                #BOOL, with first item compared first, etc.

VAL OP any|all (ARR)              #BOOL. Whether VAL OP any|all ARR item

ARR @> ARR2                       #BOOL. Is superset|equal
ARR <@ ARR2                       #BOOL. Is subset|equal
ARR && ARR2                       #BOOL. Overlaps, i.e. at least one item equal

array_ndims(ARR)->UINT|null       #NUM of dimensions. null if empty ARR
array_dims(ARR)->'[UINT:UINT2]...'#Each dimensions lower|upper bound
generate_subscripts(ARR, UINT3)
 ->[UINT, UINT2]_SET              #Dimension UINT3 lower|upper bound
array_lower(ARR, UINT3)->UINT     #Dimension UINT3 lower bound
array_upper(ARR, UINT3)->UINT2    #Dimension UINT3 upper bound
array_length(ARR, UINT3)->UINT    #array_upper - array_lower

ARR || VAL
VAL || ARR
array_append(ARR, VAL)->ARR       #Append
ARR || ARR2
array_cat(ARR, ARR2)->ARR         #Concatenates

array(SSUBQUERY)->ARR             #
unnest(ARR)->SET                  #Flattens ARR and returns as SET

array_to_string                   #Cast each ARR item to STR, then join with 'DELIM'.
 (ARR, 'DELIM'[, 'NULL'])->STR    #If 'NULL' specified, nulls are transformed to it. Otherwise, they are ignored.
string_to_array                   #Inverse
 (STR, 'DELIM'|null[, 'NULL'])    #If DELIM is null, split each character.
 ->ARR                            #If DELIM is '', returns STR as ARR with single item

array_fill                        #ARR where all values are VAL
 (VAL, UINT_ARR[, UINT_ARR2])->ARR#Number of dimensions is UINT_ARR[2].length
                                  #UINT_ARR are upper bounds, UINT_ARR2 lower bounds (def: {1,...})

array_remove(ARR, VAL)->ARR       #Filter out any element = VAL
                                  #ARR must be one-dimensional
array_replace(ARR, VAL, VAL2)->ARR#Same but replace with VAL2

YOPTS.subscript                   #FUNC implementing VAL[NUM] for any given TYPE
pg_type.typsubscript              #Common ones:
                                  #  - array_subscript_handler: ARR_TYPEs
                                  #  - raw_array_subscript_handler: NAME, POINT|LINE|LSEG|BOX
                                  #  - jsonb_subscript_handler: JSONB
                                  #  - hstore_subscript_handler: HSTORE

YOPTS.element                     #Underlying TYPE of an ARR_TYPE, if it is an ARR_TYPE
pg_type.typelem                   #Not set if underlying type is not known (e.g. JSONB|HSTORE)

pg_type.typarray                  #PGTYPE of a TYPE, i.e. inverse of pg_type.typelem. 0 if none

YOPTS.delimiter                   #'CHAR' (def: ',') used in '{...}' ARR_UNKNOWN for a given TYPE
pg_type.typdelim                  #Builtin TYPEs: ',' for all except ';' for BOX


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              ROW              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ROW_TYPE                          #Conceptually like an OBJ
                                  #Also called "composed type" or "ctype"
                                  #Values are also called "tuples"
                                  #Max 1600 COLs
record                            #TYPE of any ROW_TYPE

ROW                               #ROW_TYPE value
COL                               #"Column", i.e. ROW property
                                  #Ordered
                                  #Named. Can have duplicate names
                                  #  - e.g. select 1 as a, 2 as a;
                                  #  - but (ROW)."COL" fails

create type "ROW"
 as ("COL" TYPE,...)              #Create a new ROW_TYPE

alter type "ROW"
 add attribute "COL" TYPE
 [restrict|cascade]               #cascade or restrict (def): when "ROW" is a "TABLE", whether to modify the "TABLE" too
alter type "ROW"
 drop attribute
 [if exists] "COL" TYPE
 [restrict|cascade]               #
alter type "ROW"
 alter attribute "COL" type TYPE
 [restrict|cascade]               #
alter type "ROW"
 rename attribute "COL" to "COL2"
 [restrict|cascade]               #

[row](VAL,...)                    #ROW
                                  #COL names: fNUM
                                  #"row" is necessary only if only one VAL
'(VAL,...)'                       #Same as ROW_UNKNOWN
                                  #Can cast to ROW_TYPE but not to record

(ROW)."COL"
"COL"(ROW)                        #VAL

ROW = <> ROW2                     #BOOL. Done deeply
                                  #Must have same amount of COLs, but names do not need to match
ROW < <= > >= ROW2                #BOOL, with first COL compared first, etc.

pg_type.typrelid                  #REGCLASS of a ROW_TYPE. 0 if ROW_TYPE is not related to a RELATION


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              SET              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SET                               #Like ARR except operations|FUNCs automatically iterate over each item
                                  #Always top-level
SCALAR                            #Opposite of SET

SCALAR_FUNC(SET)->SET             #Iterates over SET items
SCALAR_FUNC(SET, SCALAR)->SET     #Same but repeats SCALAR
                                  #If SCALAR is returned by a FUNC(), re-evaluate it each time
SCALAR_FUNC(SET, SET2)->SET       #Same but fills smaller SET with nulls

SCALAR_OP SET
SET SCALAR_OP SCALAR
SET SCALAR_OP SET2
SET_OP SCALAR                     #Same with operators

select SET ...
select SET, SCALAR ...
select SET, SET2 ...              #`select VAL,...` behaves like SCALAR_FUNC

SET_FUNC(SCALAR)                  #Fails


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           SUBQUERY            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SUBQUERY                          #ROW_SET that is the result of select, values, table or execute
                                  #I.e. not returned by a FUNC()
                                  #Can only be used when explicitly documented as such
SSUBQUERY                         #SUBQUERY returning a single COL
ZSUBQUERY                         #SUBQUERY returning a single COL + ROW

REXPR                             #Expression which:
                                  #  - is evaluated once per ROW
                                  #     - except if it does not reference the ROW
                                  #  - can refer to the ROW
                                  #Can also use any (ZSUBQUERY), with same ROW behavior

exists(SUBQUERY)->BOOL            #Whether SUBQUERY has at least one ROW
                                  #Since SUBQUERY values are not used, `select 1 ...` is often used

ROW OP any|all SUBQUERY           #BOOL. Whether ROW OP any|all SUBQUERY_ROW
ROW [not] in SUBQUERY             #Same as ROW =|<> any SUBQUERY


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           RELATION            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


RELATION ==>                      #ENTITYs with COLs
                                  #I.e. TABLE, INDEX, SEQUENCE, [M]VIEW, ROW_TYPE, FTABLE
pg_class                          #TABLE with all RELATIONs
pg_class.oid                      #OID
pg_class.relname                  #"TABLE" name
pg_class.relkind                  #'CHAR' among:
                                  #  - r: TABLE
                                  #  - i: INDEX
                                  #  - p: partitioned TABLE
                                  #  - I: partitioned INDEX
                                  #  - S: SEQUENCE
                                  #  - t: TOAST_TABLE
                                  #  - v: VIEW
                                  #  - m: MVIEW
                                  #  - c: ROW_TYPE created through `create type`
                                  #  - f: FTABLE
pg_class.reltype                  #REGTYPE of ROW_TYPE
                                  #0 for INDEX|SEQUENCE|TOAST_TABLE
pg_class.relrewrite               #REGCLASS. When a RELATION is copied to a new one, old RELATION's REGCLASS.
                                  #Set to 0 after the change. Since those operations are atomic, this is always 0 from user perspective

regclass                          #TYPE alias to OID, to cast pg_class.oid as "NAME"


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             TABLE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TABLE                             #ROW_SET with a known ROW_TYPE, that is persisted
                                  #Unlimited ROWs
                                  #Max 1e9 per DATABASE

"TABLE"                           #Can be used as ROW_TYPE value
                                  #  - CONSTRAINTs are stripped
                                  #But not as a:
                                  #  - "ROW_TYPE" variable
                                  #  - ROW[_SET] value

create
 [temp|unlogged]
 table "TABLE"
 [of "ROW_TYPE"]
 [partition of ...]
 ([COL_ARG,...])
 [for values ...]
 [inherits ...]
 [partition by ...]
 [using ...]
 [with (TOPTS)]
 [on commit ...]
 [tablespace ...]
 [as SUBQUERY ...]                #Creates a TABLE

create table ... of "ROW_TYPE"    #ROW_TYPE of (COL_ARG,...)
                                  #"COL" TYPE ... -> "COL" [with options] ...
                                  #  - with options: noop
                                  #Cannot use: inherits, like "TABLE|ROW_TYPE", compression, collate
alter table "TABLE" of "ROW_TYPE" #
alter table "TABLE" not of        #

create table ...                  #Populates values with SUBQUERY
 as SUBQUERY                      #(COL_ARG,...):
                                  #  - only "COL"
                                  #  - optional
                                  #Cannot use:
                                  #  - of "ROW_TYPE"
                                  #  - inherits
                                  #  - partition by
                                  #  - if not exists
 [with [no] data]                 #Whether to copy values (def) or only ROW_TYPE

select VALS,...                   #Same as `select ...` but creates "TABLE" instead of returning ROW_SET
 into [table] "TABLE" ...         #Prefer `create table as SUBQUERY` as it has more features and is more standard

create table ... with (TOPTS)
alter table ... set (TOPTS)       #"Table storage options"
pg_class.reloptions               #'VAR=VAL'_ARR of TOPTS

pg_tables                         #TABLE with all TABLEs
pg_tables.tablename               #"TABLE" name

pg_class.reloftype                #REGTYPE of ROW_TYPE when `of ROW_TYPE` was used
                                  #0 if none, or if not TABLE

"ROW_ALIAS".tableoid              #REGCLASS of TABLE. System COL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            COLUMNS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SYSTEM COLS ==>                   #Hidden COLs defined for every TABLE ROW: tableoid, ctid, cmin|cmax, xmin|xmax

alter table "TABLE" set
 alter [column] "COL" set(COPTS)  #"Column storage options"

pg_attribute                      #TABLE with all COLs, including system COLs
pg_attribute.attrelid             #REGCLASS of the RELATION
pg_attribute.attname              #"COL" name
pg_attribute.attnum               #INT2. COL's index (1-based)
                                  #Negative for system COLs
pg_attribute.attcacheoff          #INT4. COL's byte offset inside its RELATION
                                  #Only set when ROWs are being copied. Otherwise -1
pg_attribute.atttypid             #REGTYPE of "COL". 0 if COL was deleted
pg_attribute.attoptions           #'VAR=VAL'_ARR of COPTs
pg_attribute.attisdropped         #BOOL. Whether COL was deleted but physically kept.
pg_attribute.atthasmissing        #BOOL. Whether COL is currently being added.
pg_attribute.attmissingval        #ARR (of length 1). Default VAL of the COL being added, when atthasmissing is true. null otherwise

pg_class.relnatts                 #INT2. Number of COLs, excluding system COLs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CONSTRAINT           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CONSTRAINT                        #Condition validating udpates, and making them fail if false
                                  #Auto-dependency child of its COLs
COL_CONSTRAINT                    #Single-COL CONSTRAINT

alter table "TABLE" add COL_ARG   #

"COL" TYPE [COL_CONSTRAINT...]    #COL_ARG. Define a new COL
alter table "TABLE"
 add [column] "COL" TYPE [...]    #
alter table "TABLE"
 rename [column] "COL" to "COL2"  #
alter table "TABLE"
 alter [column] "COL"
 [set data] type TYPE             #
 [using VAL]                      #VAL used when casting (def: COL value)
alter table "TABLE"
 drop [column] [if exists] "COL"
 [restrict|cascade]               #

check(BOOL_REXPR) ...
unique("COL",...) ...
primary key("COL",...) ...
foreign key("COL",...) references
 "TABLE"("COL2",...) ...          #COL_ARG. Multicolumn CONSTRAINTs

constraint "CONSTRAINT" COL_ARG   #Assign "CONSTRAINT" name
                                  #Not with: like "TABLE|ROW_TYPE"
alter table "TABLE"
 rename constraint "CONSTRAINT"
 to "CONSTRAINT2"                 #
alter table "TABLE"
 drop constraint [if exists]
 "CONSTRAINT" [restrict|cascade]  #

pg_constraint                     #TABLE with all CONSTRAINTs, excluding `not null`, including triggers
pg_constraint.oid                 #OID
pg_constraint.conname             #"CONSTRAINT" name
pg_constraint.contype             #'CHAR' among:
                                  #  - 'c': check()
                                  #  - 'x': exclude()
                                  #  - 'u': unique
                                  #  - 'p': primary key
                                  #  - 'f': foreign key
                                  #  - 't': trigger
pg_constraint.conrelid            #REGCLASS of TABLE. 0 if DOMAIN CONSTRAINT.
pg_constraint.conindid            #REGCLASS of INDEX. 0 if check|trigger
pg_constraint.conkey              #ARR of pg_attribute.attnum. COLs using the CONSTRAINT.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        LAZY CONSTRAINT        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


alter table "TABLE" add COL_ARG   #Perform CONSTRAINTs:
 not valid                        #  - not at creation time
alter domain "TYPE"               #  - but when using: alter ... validate constraint ...
 add ... not valid                #Goal: speed up time to perform this statement
                                  #Still perform those CONSTRAINTs on insert|update
                                  #Only for check() and foreign key CONSTRAINTs
alter table "TABLE"
 validate constraint "CONSTRAINT"
alter domain "TYPE"
 validate constraint "CONSTRAINT" #Noop after being used at least once

pg_constraint.convalidated        #BOOL. Whether `not valid` and not validated yet on CONSTRAINT

pg_index.indimmediate             #BOOL. Whether `not valid` was not used on INDEX
pg_index.indisvalid               #BOOL. Whether `not valid` was used and not `validate ...` on INDEX


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             CHECK             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


check(BOOL_REXPR)                 #COL_CONSTRAINT. Fails if BOOL false.
                                  #Def "CONSTRAINT" name: "TABLE_COL_check"
                                  #Must be purely functional

pg_class.relchecks                #INT2. Number of check() CONSTRAINTs in TABLE
pg_constraint.conbin              #PG_NODE_TREE representing check()'s BOOL_REXPR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           NOT NULL            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


not null                          #COL_CONSTRAINT
                                  #Same as check("COL" is not null)
null                              #COL_CONSTRAINT. Not having `not null` constraint (def)
alter table "TABLE"
 alter [column] "COL"
 set|drop not null                #

pg_attribute.attnotnull           #BOOL. Whether COL uses `not null`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            UNIQUE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


exclude                           #COL_ARG
 ("COL"|(REXPR) with OP,...)      #Fails if NEW_ROW."COL" OP ANY_CURRENT_ROW2."COL"
                                  #Can also use REXPR
                                  #OP must be commutative
                                  #Creates a CONSTRAINT "TABLE_COL_excl"
 [where (BOOL_REXPR)]             #Do not fail new ROW if BOOL false

unique                            #COL_CONSTRAINT. Same as exclude("COL" with =) but faster
                                  #I.e. fails if any duplicate value.
                                  #Def "CONSTRAINT" name: "TABLE_COL_key"

primary key                       #COL_CONSTRAINT.
                                  #Same as unique + not null
                                  #Only one per TABLE
                                  #Def "CONSTRAINT" name: "TABLE_pkey"

create unique index ...           #Similar to `unique` or `exclude (... with =)` CONSTRAINTs
                                  #Makes INDEX faster
                                  #Only supported by btree ACCESS_METHOD
create table ...
 exclude|unique|primary key ...   #Those CONSTRAINTs use a btree INDEX, automatically created with the same name
alter table "TABLE" add           #Add the CONSTRAINT using an existing INDEX
 [constraint "CONSTRAINT"]        #Def "CONSTRAINT" name: same as "INDEX"
 unique|primary key using "INDEX" #INDEX must:
                                  #  - not use REXPR
                                  #  - not use `where` (partial INDEX)
                                  #  - not use `asc|desc`
                                  #  - not use `deferrable`
                                  #  - be `unique`
                                  #  - use btree ACCESS_METHOD

pg_constraint.conexclop           #REGOPERATOR_ARR of the CONSTRAINT's `with OP`

pg_index.indisexclusion           #BOOL. Whether INDEX is used as `exclude`
pg_index.indisunique              #BOOL. Whether INDEX is `unique`
pg_index.indnullsnotdistinct      #BOOL. Whether INDEX uses `nulls not distinct`
pg_index.indisprimary             #BOOL. Whether INDEX is used as `primary key`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          FOREIGN KEY          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


references "TABLE2"[("COL2")]     #COL_CONSTRAINT. "Foreign key"
                                  #Ensure every COL value references a COL2 value (but not inverse)
                                  #On added|removed|updated values. On both COL|COL2.
                                  #COL2 must be `unique` or `primary key`
                                  #Def COL2: TABLE2 primary key
                                  #Def "CONSTRAINT" name: "TABLE_COL_fkey"

references ... match full|simple  #See null section

references ... on delete ACTION   #When removing COL2 values, do ACTION:
                                  #  - no action (def): fails, deferrable
                                  #  - restrict: fails, not deferrable
                                  #  - set default ["COL",...]: set COL values (def: all) to default
                                  #  - set null ["COL,"...]: set COL values (def: all) to null
                                  #  - cascade: remove COL values too
references ... on update ACTION   #Same but when updating COL2 values
                                  #Cannot specify "COL",... with set default|null

pg_constraint.confrelid           #REGCLASS of foreign key's TABLE. 0 if none
pg_constraint.confkey             #ARR of pg_attribute.attnum. Foreign key's COLs
pg_constraint.confmatchtype       #'CHAR' of `match` among: 'f' (full) or 'u' (simple)
pg_constraint.confdeltype         #'CHAR' of `on delete ACTION` among:
                                  #  - 'a': no action
                                  #  - 'r': restrict
                                  #  - 'd': set default
                                  #  - 'n': set null
                                  #  - 'c': cascade
pg_constraint.confupdtype         #Same for `on update ACTION`
pg_constraint.conpfeqop           #REGOPERATOR_ARR of the = OPs used to compare current|foreign keys
pg_constraint.conppeqop           #Same with REGOPERATOR_ARR of the = OPs used to compare current keys with itself
pg_constraint.conffeqop           #Same with REGOPERATOR_ARR of the = OPs used to compare foreign keys with itself


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        COLUMN DEFAULT         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


default VAL                       #COL_CONSTRAINT. Value when omitted on insertion.
                                  #VAL is evaluated at insertion time.
alter table "TABLE"
 alter [column] "COL"
 set default VAL                  #
alter table "TABLE"
 alter [column] "COL"
 drop default                     #

pg_attribute.atthasdef            #BOOL. Whether COL has a default value or generated COL

pg_attrdef                        #TABLE with COL default values, including generated COLs
pg_attrdef.oid                    #OID
pg_attrdef.adbin                  #PG_NODE_TREE with default value
pg_attrdef.adnum                  #pg_attribute.attnum of COL
pg_attrdef.adrelid                #REGCLASS of TABLE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       GENERATED COLUMN        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


generated always as (VAL) stored  #COL_CONSTRAINT. "Generated COL". Value on insertion.
                                  #Readonly on write
                                  #  - but can use `default`
                                  #VAL is evaluated at insertion time.
                                  #Can refer to other non-generated "COL"s of same "TABLE"
                                  #VAL must be purely functional
alter table "TABLE"
 alter [column] "COL"
 drop expression [if exists]      #Remove generated COL_CONSTRAINT, not COL itself

pg_attribute.attgenerated         #CHAR. Whether COL is: 's' (generated) or '' (not)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         COLUMNS COPY          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


like "TABLE|ROW_TYPE"             #COL_ARG. Copy (not inherit) the ROW_TYPE as COLs.
                                  #Always copied: COL TYPE, not null
                                  #Never copied: references, where BOOL_REXPR
 [including COL_INFO]...          #Also copy COL_INFO:
                                  #  - defaults: default VAL
                                  #  - indexes: INDEX, primary key, unique, exclude()
                                  #  - constraints: check()
                                  #  - storage: TOPTS
                                  #  - generated: generated COL
                                  #  - identity: identity COL
                                  #  - compression: TOAST compression
                                  #  - statistics: STATISTICS
                                  #  - comments: comment on ... for COL|INDEX|CONSTRAINT
                                  #  - all: of the above
 [excluding COL_INFO]...          #Do not copy COL_INFO. Only useful when using: including all


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          INHERITANCE          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create table ...                  #"CHILD_TABLE" inherits from "PARENT_TABLE"
 inherits ("PARENT_TABLE",...)    #Can be deep inheritance
                                  #Can have multiple parents
                                  #CHILD_TABLE is child dependency of PARENT_TABLE

READING ROWS ==>                  #Reading parent ROWs also reads child ROWs, but not vice-versa
                                  #  - including `select`
                                  #This behaves like `union all` is performed
WRITING ROWS ==>                  #Writing parent|child ROWs only writes parent|child ROWs
                                  #  - including `insert`
                                  #  - can use rule to redirect parent inserts to child, if it has same COLs
UPDATING|DELETING ROWS ==>        #Updating|deleting:
                                  #  - on parent:
                                  #     - can read child ROWs, i.e. can update|delete them
                                  #     - however the write happens on the child only
                                  #  - on child:
                                  #     - the write happens on the child only
                                  #     - but parent can see result since it can read child ROWs

INHERITANCE STATEMENTS ==>        #This only applies to statements related to data
                                  #  - for: select|table, update|delete|truncate|merge and most of alter table
                                  #  - also: `lock` parent applies to child too, but not vice-versa
                                  #  - unless `only "PARENT_TABLE"` is used
                                  #     - can use "PARENT_TABLE*" to do the opposite, but it is default behavior
                                  #  - not for vacuum|analyze: i.e. must be done on each child TABLE

CHILD INHERITS AND CANNOT CHANGE  #  - COL TYPE
 ==>                              #  - generated COL
                                  #  - temp
                                  #  - compression
                                  #  - collate

CHILD INHERITS BUT CAN APPEND ==> #  - COLs
                                  #  - check()
                                  #     - unless check(BOOL_REXPR) no inherit
                                  #  - not null
                                  #  - grant and row security policies
                                  #     - when reading parent ROWs, not writing

CHILD INHERITS BUT CAN OVERRIDE
 ==>                              #Only: default

CHILD DOES NOT INHERIT ==>        #  - unique, primary key, exclude
                                  #     - constraint on parent|child only applies to that TABLE
                                  #  - foreign key, as a:
                                  #     - source
                                  #     - target, i.e. references "CHILD_TABLE" implicitly means "only (CHILD_TABLE)"
                                  #  - INDEX
                                  #  - identity COL
                                  #  - rules, triggers
                                  #  - unlogged
                                  #  - create table ... with (TOPTS)
                                  #  - comments

"ROW_ALIAS".tableoid              #Can be used to distinguish parent|child

alter table "CHILD_TABLE"
 [no] inherit "PARENT_TABLE"      #

pg_inherits                       #TABLE with all `inherits`
pg_inherits.inhrelid              #REGCLASS of "CHILD_TABLE"
pg_inherits.inhparent             #REGCLASS of "PARENT_TABLE"
pg_inherits.inhseqno              #INT4. Order of parent, if multiple parents
pg_inherits.inhdetachpending      #BOOL. True if child is about to be detached

pg_class.relhassubclass           #BOOL. True if is a "PARENT_TABLE" of some "CHILD_TABLE"

pg_attribute.attislocal           #BOOL. If "CHILD_TABLE", whether COL is inherited
pg_attribute.attinhcount          #INT4. If "PARENT_TABLE", number of "CHILD_TABLE"s inheriting this COL

pg_constraint.conislocal          #BOOL. If "CHILD_TABLE", whether CONSTRAINT is inherited
pg_constraint.coninhcount         #INT4. If "PARENT_TABLE", number of "CHILD_TABLE"s inheriting this CONSTRAINT


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PARTITION           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARTITIONING ==>                  #Split a TABLE ROWs into multiple child TABLEs ("partitions")
                                  #Child TABLEs ROWs should:
                                  #  - not overlap other children
                                  #  - while still cover all of it
                                  #Child TABLEs should have same COLs
                                  #Goals:
                                  #  - faster read|write, by only targeting one smaller child TABLE
                                  #  - faster bulk delete, by detaching then deleting child TABLE
                                  #Not goals: splitting data across multiple machines
                                  #Only worth on very large tables, at least larger than RAM

VIEW PARTITIONING ==>             #Partitioning using a VIEW:
                                  #  - parent VIEW uses `union all` to concatenate all child TABLEs
                                  #  - child TABLEs restrict ROWs with a check()
                                  #     - with an INDEX on the underlying CONSTRAINT
                                  #  - rule|trigger that redirects parent ROWs writes to the right child

INHERITANCE PARTITIONING ==>      #Partitioning using `create table ... inherits`:
                                  #Same as view partioning but with `inherits` and an empty parent TABLE

ENVVAR constraint_exclusion       #If 'on', queries skip child TABLEs if `where BOOL_REXPR` does not match its check() CONSTRAINT
                                  #If 'partition' (def, usually best): only when appears to be a child TABLE
                                  #using inheritance partitioning

DECLARATIVE PARTITIONING ==>      #Partitioning using `create table ... partition of`
                                  #Uses inheritance partitioning under the hood
                                  #  - i.e. behaves similarly, except for notes below
                                  #Automatically managed:
                                  #  - check() for the partition key, and underlying INDEX
                                  #  - rule to redirect writes to children
                                  #Child TABLEs clone PARENT_TABLE's:
                                  #  - unique, primary key, foreign key
                                  #     - can only use normal expression, not REXPR
                                  #     - have the same issues as with `inherits` (see above)
                                  #  - INDEXs
                                  #  - triggers
                                  #Parent TABLE cannot use:
                                  #  - exclude()
                                  #  - check(...) no inherit
                                  #  - with (TOPTS)
                                  #Child TABLE cannot override:
                                  #  - not null
PARTITION KEY ==>                 #COL_REXPRs used to decide which child TABLE to use
                                  #Max 32 COL_REXPRs
                                  #Must be included in any unique or primary key CONSTRAINT
                                  #COL_REXPRs should match the `where BOOL_REXPR` expected in queries
PARTITION METHOD ==>              #One of: list, range, hash
                                  #Should use the best one to:
                                  #  - spread ROWs evenly
                                  #  - make queries target as few TABLEs at once as possible
NUMBER OF PARTITIONS ==>          #If too high: slower due to bookeeping and query analyzing time
                                  #If too low: slower due to bigger TABLEs slower

ENVVAR enable_partition_pruning   #"Partition pruning".
                                  #Queries skip child TABLEs if `where BOOL_REXPR` does not match its `for values ...`
                                  #Only with declarative partitioning
                                  #Compared to ENVVAR constraint_exclusion: faster, and works on more complex `where BOOL_REXPR`

ENVVAR enable_partitionwise_join  #"Partitionwise join".
                                  #Allow joins on partitioned TABLEs to directly join their child TABLEs
                                  #Join condition must include all partition keys
                                  #Partition keys must have the same TYPEs
                                  #Slow planning time, i.e. disabled by default
ENVVAR                            #"Partitionwise aggregate".
 enable_partitionwise_aggregate   #Similar for `group by` and AFUNCs

create table "PARENT_TABLE"
 partition by METHOD
 ("COL"|(COL_REXPR),...)          #Declare parent TABLE's partition keys

create table "CHILD_TABLE"
 partition of "PARENT_TABLE"      #Declare ROWs owned by child TABLE
 [(COL_ARG,...)]                  #COL_ARGS,...: same syntax as `of "ROW_TYPE"`
 for values ...                   #Values are evaluated once at TABLE creation time

create table ... partition of ... #Child TABLE will own any ROWs not owned by other child TABLEs
 default                          #Not with `partition by hash`

alter table "PARENT_TABLE"
 attach partition "CHILD_TABLE"
 for values ...
alter table "PARENT_TABLE"
 attach partition "CHILD_TABLE"
 default                          #Mark a TABLE as a new child
alter table "PARENT_TABLE"
 detach partition "CHILD_TABLE"   #Unmark a TABLE as a child
alter index "PARENT_INDEX"
 attach partition "CHILD_INDEX"   #

create table ...
 partition by list ...            #Child TABLE owns ROWs where partition key's value is among VAL,...
create table ... for values       #Partition key must have only 1 COL_REXPR
 in (VAL,...)                     #VAL can be null, but only in one child TABLE

create table ...                  #Child TABLE owns ROWs where partition key's value is between VAL (included) and VAL2 (excluded)
 partition by range ...           #If partition key has multiple COL_REXPRs, there are multiple VAL[2],...
create table ... for values       #VAL[2] can be minvalue|maxvalue
 from (VAL,...) to (VAL2,...)     #  - following COL_REXPRs must then also be minvalue|maxvalue

create table ...                  #Child TABLE owns ROWs where partition key's value % UINT = UINT2
 partition by hash ...            #Usually UINT is number of child TABLEs and UINT2 is index of child TABLE
create table ... for values with  #Can then multiple UINT by 2 to increase number of child TABLEs
 (modulus UINT, remainder UINT2)  #  - can do it one child TABLE at a time
                                  #  - should detach child TABLE, split its data, then re-attach it as two child TABLEs

pg_partitioned_table              #TABLE with declared partioning
pg_partitioned_table.partrelid    #REGCLASS of "CHILD_TABLE"
pg_partitioned_table.partdefid    #REGCLASS of default "CHILD_TABLE". 0 if none
pg_partitioned_table.partstrat    #Strategy among 'l' (list), 'r' (range) or 'h' (hash)
pg_partitioned_table.partnatts    #NUM of COLs in partition key
pg_partitioned_table.partattrs    #INT2_ARR. Partition key "COL" indices. 0 if COL_REXPR instead
pg_partitioned_table.partexprs    #PG_NODE_TREE. Partition key COL_REXPR values. 0 if "COL" instead
pg_partitioned_table.partclass    #ARR of pg_opclass.oid. Partition key COL OPCLASSs
pg_partitioned_table.partcollation#REGCOLLATION_ARR. Partition key COL COLLATION

pg_class.relispartition           #BOOL. Whether RELATION is a partition's child TABLE
pg_class.relpartbound             #PG_NODE_TREE. If is partition's child TABLE, `for values`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             VIEW              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create view "VIEW" as SUBQUERY    #VIEW forward statements to SUBQUERY
                                  #Goal: encapsulation
                                  #Readonly if SUBQUERY uses:
                                  #  - select VAL that is not select "COL"
                                  #  - join
                                  #  - distinct
                                  #  - group by
                                  #  - limit|offset
                                  #  - union|intersect|except
                                  #  - top-level with
                                  #  - with (security_barrier)
                                  #Child dependency of anything mentioned in SUBQUERY (e.g. TABLEs)

create view "VIEW"(COL,...) ...   #Change "COL" names. Omitted COLs keep their names.
alter view "VIEW"
 rename [column] "COL" to "COL2"  #

alter view "VIEW"
 alter [column] "COL"
 drop|set default [VAL]           #Add a `default` CONSTRAINT, forwarded to underlying SUBQUERY

create view "VIEW"
 with (VOPTS) ...                 #
VOPTS.security_barrier            #Ensure ROWs are not securely hidden when using `where BOOL_REXPR`
                                  #Cover channel attacks are still possible
                                  #  - using explain query plans, time of queries, etc.
                                  #  - e.g. to infer size of hidden rows

create recursive view ...         #Allows recursion like in `with recursive ...` CTE

pg_views                          #TABLE with all VIEWs
pg_views.definition               #'SUBQUERY'
pg_views.viewname                 #"VIEW" name


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       MATERIALIZED VIEW       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create materialized view "MVIEW"  #Readonly TABLE that is populated on-demand via a SUBQUERY
                                  #Child dependency of anything mentioned in SUBQUERY (e.g. TABLEs)
 [("COL",...)]                    #Like create view ...
 [with (TOPTS)]                   #Like create table ...
 as SUBQUERY                      #

refresh materialized view "MVIEW" #Re-populates TABLE

create|refresh
 materialized view "MVIEW"
 [with [no] data]                 #with data (def): populate at creation time
                                  #with no data: truncate and cannot query until next refresh materialized view

alter materialized view "MVIEW"
 rename [column] "COL" to "COL2"
alter materialized view "MVIEW"
 alter [column] "COL" set
 statistics|storage|compression
 ...
alter materialized view "MVIEW"
 [alter [column] "COL"]
 [re]set (TOPTS)
alter materialized view "MVIEW"
 cluster on "INDEX"
alter materialized view "MVIEW"
 set without cluster
alter materialized view "MVIEW"
 set access method ...            #Same as alter table ...

pg_matviews                       #TABLE with all MVIEWs
pg_matviews.definition            #'SUBQUERY'
pg_matviews.matviewname           #"MVIEW" name
pg_class.relispopulated
pg_matviews.ispopulated           #BOOL. Whether truncated. True if not MVIEW


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           TEMPORARY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create temp[orary] table ...
select ...                        #Drop TABLE at end of session
 into temp[orary] [table] ...     #No autovacuum, but can manually run analyze
 [on commit ACTION]               #At the end of each successful transaction:
                                  #  - preserve rows (def): nothing
                                  #  - delete rows: truncate TABLE
                                  #  - drop: drop TABLE
ENVVAR temp_buffers               #Max buffers increment size used (def: 8MB)

create temp[orary] view ...       #Drop VIEW at end of session

create temp[orary] sequence ...   #Drop SEQUENCE at end of session
                                  #Can't use any SCHEMA

pg_temp_NUM                       #SCHEMA with user-defined temporary TABLE|VIEW|SEQUENCEs
                                  #Always prepended to ENVVAR search_path
                                  #  - can explicitly add it to override priority order
pg_temp                           #Alias to pg_temp_NUM of current session
pg_my_temp_schema()->REGNAMESPACE #pg_temp_NUM SCHEMA of current session
                                  #0 if none
pg_is_other_temp_schema
 (REGNAMESPACE)->BOOL             #Whether is of pg_temp_NUM of another session

pg_class.relpersistence           #'CHAR'. Whether RELATION is:
                                  #  - 'p': non-temporary|unlogged
                                  #  - 't': temporary
                                  #  - 'u': unlogged


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           SEQUENCE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SEQUENCE                          #TABLE storing a streaming INT algorithm
SEQUENCE.last_value               #INT (def: not set)
SEQUENCE.is_called                #BOOL. Whether setval|nextval() has been called

setval(REGCLASS, INT[, BOOL])     #Sets SEQUENCE.last_value and is_called (def: true)
nextval(REGCLASS)->INT            #Generates and returns new SEQUENCE.last_value
currval(REGCLASS)->INT            #Returns SEQUENCE.last_value. Fails if setval|nextval() not called yet
lastval()->INT                    #Previous SEQUENCE.last_value of last SEQUENCE called. Fails if none.

create sequence "SEQUENCE"
 [as int2|4|8]                    #INT TYPE
 [increment [by] INT]             #Def: 1. Added by each nextval()
 [no min|maxvalue]
 [min|maxvalue INT]               #Def: 1 and max value for INT TYPE
 [start [with] INT]               #Def: 1. First value
 [cache INT]                      #Def: 1. Cache next values in advance.
                                  #Cached values are used even if setval() is called
                                  #If >1, concurrent calls to setval|nextval() might jump some values (but still get unique ones)
 [[no] cycle]                     #When reaching maxvalue:
                                  #  - cycle: set to minvalue
                                  #  - no cycle (def): fail
 [owned by none]
 [owned by "TABLE"."COL"]         #Make dropping COL also drop SEQUENCE

alter sequence "SEQUENCE" ...     #Same syntax as create sequence ...
 [restart [with] INT]             #Like setval()

"COL" [small|big]serial           #COL_ARG with:
 COL_CONSTRAINT...                #  - TYPE int2|4|8
                                  #  - default nextval('SEQUENCE')
                                  #  - not null
                                  #"SEQUENCE" is named 'TABLE_COL_seq'
                                  #Auto-dependency child of its COL

pg_sequence                       #Lower-level TABLE with all SEQUENCEs
pg_sequences                      #Higher-level VIEW with all SEQUENCEs
pg_sequence.seqrelid              #REGCLASS of SEQUENCE
pg_sequences.sequencename         #"SEQUENCE" name
pg_sequence.seqtypid
pg_sequences.data_type            #REGTYPE of INT2|4|8 value
pg_sequences.last_value           #INT8. Last value
pg_sequences.increment_by
pg_sequence.seqincrement          #INT8. Increment
pg_sequences.start_value
pg_sequence.seqstart              #INT8. First value
pg_sequences.min|max_value
pg_sequence.seqmin|max            #INT8. Min|max value
pg_sequences.cache_size
pg_sequence.seqcache              #INT8. Cache size
pg_sequences.cycle
pg_sequence.seqcycle              #BOOL. Whether `cycle`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        IDENTITY COLUMN        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


generated always|by default       #COL_CONSTRAINT. "Identity COL". Same as "COL" *serial, with additional features.
 as identity                      #COL TYPE must be int2|4|8.
                                  #nextval() is used:
                                  #  - `by default`: as default VAL
                                  #  - `always`: as `generated` VAL, i.e. readonly except when setting `default`
 [(...)]                          #Passed to `create sequence "SEQUENCE" ...`
alter table "TABLE"
 alter [column] "COL"
 add generated always|by default
 as identity
alter table "TABLE"
 alter [column] "COL"
 set generated always|by default
alter table "TABLE"
 alter [column] "COL"
 drop identity [if exists]        #Modify|remove identity COL_CONSTRAINT, not COL itself
alter table "TABLE"
 alter [column] "COL" set ...
alter table "TABLE"
 alter [column] "COL" restart ... #Passed to `alter sequence "SEQUENCE" ...`

insert into ...(...)
 overriding system value          #Make `generated always` behave like `generated by default` instead
 overriding user value            #Make `generated by default` behave like `generated always` instead
                                  #Except: inserted VALs are ignored instead of failing

pg_attribute.attidentity          #CHAR. Identity column on COL:
                                  #  - '': none
                                  #  - 'a': generated always
                                  #  - 'd': generated by default


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SELECT             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select
[distinct ...]
[VAL,...]
[from ...]
[where ...]
[group by ...]
[having ...]
[window ...]
[union|intersect|except ...]...
[order by ...]
[limit ...]
[offset ...]                      #Order: from, where, select non-AFUNC, group by, having, window, select AFUNC,
[fetch ...]                       #distinct, union|intersect|except, order, offset, limit|fetch

select VAL[_SET],...              #Evaluates VALs and returns them as a ROW_SET
                                  #Each VAL[_SET] is returned as a different COL
                                  #  - select VAL,...: multiple COLs
                                  #  - select ROW: single COL with a ROW TYPE
select REXPR,...                  #Can be used
select                            #If no VAL[_SET],..., returned ROWs have 0 COLs
                                  #Does not change how many ROWs are returned

select VAL[_SET]                  #"COL_ALIAS" is COL name in:
 [[as] "COL_ALIAS"],...           #  - return value
                                  #  - REXPR
                                  #Def (in priority):
                                  #  - "FCOL_ALIAS"
                                  #  - "COL" name
                                  #  - "FUNC" name
                                  #  - "?column?"
COL_REXPR                         #REXPR which is often just "COL"
                                  #Can use 1-based COL_INDEX UINT too

table "TABLE"                     #Same as select * from "TABLE"

insert|update|delete ...          #Make statement return `select VAL,...` using inserted|updated|deleted ROWs as source
 returning VAL,...                #Can use `[as] COL_ALIAS`
MSUBQUERY                         #Means either a SUBQUERY or a `... returning VAL,...` (which cannot be used as SUBQUERY otherwise)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            VALUES             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


values (VAL,...),...              #Evaluates VALs and returns them as a ROW_SET
                                  #Each (VAL,...) is a ROW
                                  #All ROWs must have same TYPE, including number of COLs
                                  #"COL" name: "columnNUM"
                                  #VAL cannot be a SET
                                  #VAL can use (ZSUBQUERY)s
 [order by ...]
 [asc|desc|using OP]
 [limit ...]
 [offset ...]
 [fetch ...]                      #Same as select ...


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             FROM              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... from ROW_SET           #Evaluates once per ROW:
                                  #  - select VAL,...
                                  #  - REXPR
                                  #If select VAL,... results is a SET itself, do cartesian product
                                  #ROW_SET must be returned by a FUNC()
select ... from NON_SET           #Converts non-SETs to single-ROW SETs
select ... from NON_ROW_SET       #Converts non-ROWs to single-COL ROWs
                                  #By default, COL name is same as "ROW_ALIAS"

select ...                        #Uses SUBQUERY's ROW_SET return value
 from [lateral] (SUBQUERY)        #Only evaluated once
                                  #  - unless SUBQUERY uses a "ROW_ALIAS" from a previous FROM, i.e. with a join
select (SUBQUERY) from ...
select ... from ...
 where|having|order by (SUBQUERY) #SUBQUERY can also use "ROW_ALIAS" from a previous FROM, i.e. with a join
select ... from FROM ...          #"Anti-join". When using `not exists`.
 where not exists (SUBQUERY)      #Instead of adding cartesian product combinations, removes some.

select ... from "TABLE"           #Same as as `from (table "TABLE") as "TABLE"`

select ... from FROM              #Name of each ROW to use in:
 [[as] "ROW_ALIAS",...]           #  - REXPR
                                  #  - any next `from FROM`
                                  #(SUBQUERY) can use "ROW_ALIAS" defined by a previous FROM, but:
                                  #  - not in its own `from ...`
                                  #  - must specify `lateral`
                                  #  - i.e. will behave as a REXPR
                                  #Parent query cannot use "ROW_ALIAS" defined by a (SUBQUERY)
                                  #Def: FUNC(...) name
                                  #Required with `from (SUBQUERY)`
["ROW_ALIAS".]"COL"               #COL
                                  #"ROW_ALIAS" can be omitted if there is a single "COL" named like this among all FROMs
["ROW_ALIAS".]*                   #Same as ["ROW_ALIAS".]"COL", ["ROW_ALIAS".]"COL2", ...
                                  #Only in `select VALs`

select ... [as]                   #Rename "ROW_ALIAS"."COL"
 "ROW_ALIAS"("FCOL_ALIAS",...)    #Omitted ones are not renamed

select ... from ROW_SET [as]
 "ROWALIAS"("FCOL_ALIAS" TYPE,...)
select ... from ROW_SET as
 ("FCOL_ALIAS" TYPE,...)          #Define ROW_TYPE
select ... from                   #Each COL must be defined
 rows from(ROW_SET,... as         #Not with `from (SUBQUERY)` nor `from "TABLE"`
 ("FCOL_ALIAS" TYPE,...))         #If `with ordinality`, `rows from(...)` must be used

select ... from ROW_SET
 with ordinality
select ... from rows from(...)    #Concatenates a COL named "ordinality"
 with ordinality                  #Its values is a bigserial starting with 1


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            FILTER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... where BOOL_REXPR       #Returns no ROWs if BOOL is false
                                  #Conceptually like select ..., BOOL where:
                                  #  - BOOL COL is not returned
                                  #  - ROWs with BOOL false are not returned

select all|distinct ...           #If `distinct`, ignores any ROW if any previous ROW2 is =
 [on (COL_REXPR,...)]             #Computes the values used to compare

select ... from "TABLE"
 tablesample SAMPLE_METHOD(...)   #Removes ROWs randomly
 tablesample bernoulli(0-100)     #Percentage of ROWs to keep
                                  #Percentage is only average over multiple runs
                                  #Probability is applied ROW-wise, i.e. more precise with high number of ROWs
 tablesample system(0-100)        #Same but applied on groups of multiple ROWs at a time
                                  #I.e. faster, but much less precise
 [repeatable FLOAT]               #Random seed (def: different one for each call)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             SORT              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... order by COL_REXPR     #Sort ROWs
                                  #COL_REXPR is the sort value
                                  #  - if union|intersect|except used, can only be "COL"|COL_INDEX
                                  #Unless used, ROWs are unordered
                                  #  - including union|intersect|except result
 [asc|desc]                       #Sorting order (def: asc)

select ... order by ...,...       #Multiple sort values

ENVVAR enable_sort                #"Explicit sort"
                                  #Normal sort when using order COL,...
                                  #As opposed to incremental sort and ordered scans
QUICKSORT ==>                     #Explicit sort method
                                  #Faster in most cases
TOP-N HEAPSORT ==>                #Explicit sort method
                                  #Faster when using `order by` + `limit`
EXTERNAL MERGE SORT ==>           #Explicit sort method
                                  #Slower but preferred when ROW_SET is too large to fit in-memory (according to ENVVAR work_mem)

ENVVAR enable_incremental_sort    #"Incremental sort".
                                  #When using order by COL,..., query planner sometimes sorts COL-by-COL instead of all COLs at once
                                  #This allows next COLs to sort groups of ROWs instead of all ROWs at once.
                                  #Pros:
                                  #  - with limit NUM, allows them to skip some ROWs
                                  #  - lower memory
                                  #  - more paralellism
                                  #Con: cost of splitting + iteration
                                  #  - small groups of ROWs are often grouped together to reduce this

ORDERED SCAN ==>                  #Some ACCESS_METHODs automatically sort
                                  #This allows `order by` or merge joins to use the INDEX
                                  #asc|desc and nulls first|last must match
                                  #Comparison:
                                  #  - sequential scan + sort: CPU cost of additional sort step, but uses few sequential I/O
                                  #  - ordered scan: I/O cost of using multiple random I/O calls
                                  #  - i.e. ordered scan can be slower on large amount of ROWs
                                  #  - i.e. useful for `order by` + `limit`
BACKWARD SCAN ==>                 #Some ACCESS_METHODs can query in backward order
                                  #This allows ordered scan to be either asc|desc except:
                                  #  - on multi-COLs with different asc|desc
                                  #  - when using asc nulls first or desc nulls last
create index ... ("COL"|(REXPR)
 asc|desc [nulls first|last],...)
create table ...
 exclude("COL"|(REXPR) asc|desc
 [nulls first|last],...) ...      #Specifies sorting order for ordered scans.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             SLICE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... offset UINT [row[s]]   #Ignores first UINT rows

select ... limit UINT|all
select ...                        #Only returns first UINT rows, after `offset` applied
 fetch first [UINT] row[s] only   #Def UINT: 1
select ...                        #Returns any following ROW if:
 fetch ... row[s] with ties       #  - its COLs targeted by `order by`
                                  #  - are = than the previous ROW
select ... fetch next ...         #Same as: select ... fetch first ...


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           JOIN ROWS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... union SUBQUERY         #Concatenates ROWs of `select ...` and SUBQUERY
                                  #They must have the same ROW TYPEs
                                  #SUBQUERY "COL" names are not used
                                  #order by, offset, limit:
                                  #  - applied on the merged result
                                  #  - to apply on only SUBQUERY, use parenthesis around it

select ... intersect SUBQUERY     #Same as union, but only keeps ROWs that are = between `select ...` and SUBQUERY

select ... except SUBQUERY        #Same as union, but only removes ROWs that are = between `select ...` and SUBQUERY

select ...                        #Can be done multiple times
 [union|intersect|except ...]...  #Processed left-to-right, except intersect which have priority

select ... union|intersect|except
 distinct|all SUBQUERY            #If "distinct" (def), removes duplicate rows on merged result

ENVVAR enable_parallel_append     #"Parallel append"
                                  #When combining multiple ROW_SETs, distribute producing each in multiple parallel workers


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         JOIN COLUMNS          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ...                        #Concatenates COLs of each ROW_SET
 from rows from(ROW_SET,...)      #Smaller ROW_SETs are filled with nulls

select ... from FROM, FROM2
select ... from FROM              #Concatenates COLs of FROM and FROM2
 cross join FROM2                 #ROWs are cartesian product of FROM and FROM2
select ... from FROM
 [inner] join FROM2
 on BOOL_REXPR                    #Like `cross join` but ignores ROWs where BOOL is false

select ... from FROM              #Like `cross join` but:
 left [outer] join FROM2          #  - if a FROM ROW has BOOL false with every FROM2 ROW
 on BOOL_REXPR                    #  - it is kept as a single ROW, with nulls in FROM2 COLs
                                  #Parenthesis can be used to force order
select ... from FROM
 right [outer] join FROM2
 on BOOL_REXPR                    #Like `left join` but with FROM2 ROWs instead
select ... from FROM
 full [outer] join FROM2
 on BOOL_REXPR                    #Like `left join` + `right join`

select ... from FROM              #Same as: on "ROW_ALIAS"."COL" = "ROW_ALIAS2"."COL" [and ...]
 [...] join FROM2 using("COL",...)#Also merges "ROW_ALIAS"[2]."COL" to a single COL
 [as "ROW_ALIAS3"]                #"ROW_ALIAS3" contains only the "COL",...
select ... from FROM
 natural [...] join FROM2         #Same as `using("COL",...)` with "COL" common between FROM and FROM2

SELF JOIN ==>                     #Using twice the same FROM


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         JOIN METHODS          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENVVAR enable_nestloop            #"Nested loop join".
                                  #Read both FROMs sequentially as outer|inner loops
                                  #One FROM is the outer loop
                                  #  - i.e. only the inner loop can use INDEXs on the join condition
                                  #O(n^2) but efficient if outer loop is small
ENVVAR enable_memoize             #If outer loops has duplicate values, only perform a single inner loop for all of them
                                  #Do it thanks to memoization, i.e. cost memory and CPU if no duplicate values
NESTED LOOP JOIN ON UNIQUE ==>    #If inner loop's join condition is `unique`, can skip each inner loop iterate after finding one match
PARALLEL NESTED LOOP JOIN ==>     #Distribute the outer loop values among parallel workers.
                                  #The inner loop is not parallelized

ENVVAR enable_hashjoin            #"Hash join".
                                  #Build hash tables for each FROM, where the key is a hash of the join condition
                                  #Read sequentially one of the hash table, looking up the other
                                  #Hash table size (number of "buckets")
                                  #  - guessed automatically
                                  #  - if too large, done in several rounds, persisting data on-disk
                                  #O(n) but cost of:
                                  #  - hash computation
                                  #  - on-disk memory + I/O, if hash table too large due to too many ROWs
                                  #Only with OP =
ENVVAR enable_parallel_hash       #"Parallel hash join"
                                  #Create the hash table in parallel workers

ENVVAR enable_mergejoin           #"Merge join".
                                  #Sort each FROM on the join condition, then do a merge join
                                  #O(n log n) but reduced to O(n) when can skip sorting thanks to one of:
                                  #  - an `order by` on the same value as the join condition being used
                                  #  - FROMs using ordered scans
                                  #  - another merge join sorted the other FROMs
                                  #Only with OP =

ENVVAR enable_material            #"Materialization".
                                  #When creating intermediate ROW_SETs (e.g. during joins),
                                  #persist it fully in-memory, as opposed to fetching it on-disk.
                                  #Sometimes faster, but requires memory.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         JOIN STRATEGY         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


JOIN STRATEGY ==>                 #When joining multiple FROMs decides:
                                  #  - in which order to join each pair
                                  #  - when to apply each `where|on` condition
                                  #  - which join method to use

JOIN EXHAUSTIVE SEARCH ==>        #Join strategy comparing the cost of each possible solution
                                  #Best execution time, but high planning time if many FROMs
ENVVAR join_collapse_limit        #NUM (def: same as ENVVAR from_collapse_limit)
                                  #Use left-to-right join order instead of doing a join exhaustive search if:
                                  #  - there are > NUM FROMs
                                  #  - that use `join` keyword (not commas)
                                  #Higher value allow planner to optimize better, but take longer planning time
                                  #Can be 1 (combined with `join` keyword) to force a specific join order

OUTER JOIN STRATEGY ==>           #Outer joins force order:
                                  #  - left|right join:
                                  #     - must first perform any FROM that is a parenthesized sub-join
                                  #     - e.g. FROM left join (FROM2 join FROM3)
                                  #     - e.g. FROM left join (select * from FROM2 join FROM3)
                                  #  - full join:
                                  #     - must join both sides together directly
                                  #     - e.g. FROM full join FROM2 join FROM3 -> (FROM full join FROM2) join FROM3
ENVVAR from_collapse_limit        #NUM (def: 8). Parenthesized sub-join ignore parentheses if:
                                  #  - has <= NUM FROMs
                                  #  - does not use full joins
                                  #E.g. FROM join (FROM2 join FROM3) -> FROM join FROM2 join FROM3
                                  #Higher value allow planner to optimize better, but take longer planning time

ENVVAR geqo                       #BOOL (def: true)
                                  #Enables the "GEnetic Query Optimizer"
                                  #Join strategy using a genetic algorithm
ENVVAR geqo_threshold             #NUM (def: 12). Use GEQO when joining >= NUM FROMs
ENVVAR geqo_effort                #1-10 (def: 5). Higher value spend more time planning, but with better results
                                  #This only sets the values of ENVVAR geqo_pool_size|generations|selection_bias
ENVVAR geqo_pool_size             #NUM of solutions being evaluated in each round. Usually between 100-1000
                                  #Def: 0, i.e. uses geqo_effort
ENVVAR geqo_generations           #NUM of rounds. Usually between 100-1000
                                  #Def: 0, i.e. uses geqo_effort
ENVVAR geqo_selection_bias        #NUM deciding how many solutions to remove after each round
                                  #1-1/NUM are removed. Usually 1.5-2. Def: 2
ENVVAR geqo_seed                  #GEQO randomness seed, from 0 to 1. Def: 0


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            INSERT             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


insert into "TABLE"[("COL",...)]  #Adds ROWs to "TABLE" using SUBQUERY's ROW_SET return value
 SUBQUERY                         #SUBQUERY must match ("COL",...) arity and TYPEs
                                  #Def ("COL",...): all COLs

default                           #Default VAL, or null
                                  #Can be used within `values(...)` as SUBQUERY
                                  #Also used on missing values if:
                                  #  - ("COL",...): does not pick every "COL"
                                  #  - no ("COL",...): SUBQUERY shorter than amount of COLs
insert into "TABLE" default values#Single ROW with (default,...)

insert into "TABLE"               #Name of each ROW to use in other parts of insert ..., except SUBQUERY
 [as "ROW_ALIAS"] ...             #Def: "TABLE"

insert ... on conflict ...        #Configure behavior when inserted VAL fails an unique or exclude() CONSTRAINT
insert ... on conflict do nothing #Ignore ROW, instead of failing
insert ...                        #Updates current ROW instead of inserting a new one, i.e. upserts
 on conflict do update set ARG,...#ARG is same syntax as update ... set ARG,...
                                  #If new VALs still fail, do not repeat
                                  #Can use `excluded` as a "ROW_ALIAS" for the VALs being inserted
                                  #Only with unique CONSTRAINT, not `exclude()`
insert ...
 on conflict do update set ...
 where BOOL_REXPR                 #If false, ignore ROW instead

on conflict                       #Only apply for a specific unique|exclude CONSTRAINT
 on constraint "CONSTRAINT" ...   #Required when using `on conflict do update set`
on conflict ("COL",...)           #Same as `on constraint` but targeting CONSTRAINT through its COLs
on conflict (...)                 #Same as `on constraint` but targeting CONSTRAINT through its underlying INDEX
                                  #(...) is same `create index "INDEX"(...)`
                                  #  - including `(...) where BOOL_REXPR` for partial INDEXs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            UPDATE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


update "TABLE" set                #Set values of current ROWs
 "COL" = VAL,...                  #Other COLs are kept as is
                                  #VAL can be `default`
                                  #VAL is evaludated once per ROW
 ("COL",...) = [row](VAL,...),... #`row` is required if only one COL
 ("COL",...) = (SUBQUERY),...     #SUBQUERY must return either:
                                  #  - a single ROW
                                  #  - no ROWs: converted to a ROW filled with nulls

update "TABLE" [as] "ROW_ALIAS"   #Name of each current ROW
 ...                              #To use in other parts of `update ...`, including in (SUBQUERY)
                                  #Def: "TABLE"
["ROW_ALIAS".]"COL"               #Like `select ...`

update ... from FROM,...          #Allows using FROMs inside other parts of update ...
                                  #Same syntax as `select ... from FROM,...`
                                  #`where BOOL_REXPR` is called with cross-join, i.e. each TABLE_ROW + FROMS_ROW
                                  #For any given TABLE_ROW, if `where BOOL_REXPR` filters in:
                                  #  - 0 ROW: no update is done
                                  #  - 1 ROW: `set ...` can use filtered FROMS_ROW
                                  #  - >1 ROWs: `set ...` uses an unspecified filtered FROMS_ROW (to avoid)

update ... where BOOL_REXPR       #Only sets values on ROWs with BOOL true


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            DELETE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


delete from "TABLE"
[[as] "ROW_ALIAS"]
[using FROM,...]                  #Same as `update ...` but deleting ROWs instead
[where BOOL_REXPR]                #Def BOOL_REXPR: true, i.e. delete all ROWs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           TRUNCATE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


truncate table "TABLE",...        #Delete all ROWs
                                  #Unlike `delete from` on all ROWs, delete any dead ROWs right away
                                  #  - pro: faster, and free memory to OS
                                  #  - con: breaks MVCC, i.e. no concurrent transactions should be using the TABLE
[restart|continue identity]       #If `restart` (def: `continue`): call `alter sequence "SEQUENCE" restart`
[restrict|cascade]                #If `cascade` (def: `restrict`): truncates any "TABLE2" depending on "TABLE" too
                                  #  - e.g. due to foreign keys


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:    COMMON TABLE EXPRESSION    :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


with                              #"CTE". Same as: ... from (MSUBQUERY) as "ROW_ALIAS"[("FCOL_ALIAS",...)]
 "ROW_ALIAS"[("FCOL_ALIAS",...)]  #Except:
 as (MSUBQUERY)                   #  - MSUBQUERY, not just SUBQUERY
 ... from "ROW_ALIAS" ...         #  - evaluates MSUBQUERY once when used multiple times
                                  #  - simpler to read
                                  #The main COMMAND can be select|table|values, insert|update|delete or merge
                                  #MSUBQUERY can use `with ...` itself, but only with `select` sub-MSUBQUERYs

with "ROW_ALIAS" ... as (...),    #Can use multiple ones
  "ROW_ALIAS2" ... as (...), ...  #"ROW_ALIAS" can be used inside the next MSUBQUERYs

with recursive ...                #Allow using "ROW_ALIAS" inside its own MSUBQUERY.
                                  #MSUBQUERY must be:
                                  #  select ... union select ... from "ROW_ALIAS" ... where BOOL_REXPR
                                  #Recursion happens bottom-up:
                                  #  - first recursion skips `union ...`
                                  #  - next recursions are repeated until `union ...` returns no ROWs
                                  #     - usually when BOOL_REXPR false for all ROWs
ENVVAR recursive_worktable_factor #NUM (def: 10). Hint to the query planner of the average depth of a recursive CTE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      PREPARED STATEMENT       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


prepare PREP as COMMAND           #Save COMMAND as PREP
                                  #COMMAND must be select|values or insert|update|delete|merge
                                  #Pre-parse it
                                  #  - i.e. faster to execute
                                  #  - re-parsed on changes, such as:
                                  #     - DDL impacting COMMAND
                                  #     - planner statistics changed
                                  #     - search_path changed
prepare PREP(TYPE, ...)           #TYPEs are parameters
 as COMMAND                       #They can be used in COMMAND as $1, etc.
                                  #Optional:
                                  #  - arguments can be passed even without a corresponding parameter TYPE
                                  #  - they are cast as unknown

execute PREP[(VAL,...)]           #Execute COMMAND from PREP

deallocate PREP|all               #Delete a PREP
                                  #Automatically done at end of each session

ENVVAR plan_cache_mode            #Whether to use:
                                  #  - a generic plan: ignore TYPEs
                                  #     - faster planning time
                                  #  - a custom plan: take TYPEs into account
                                  #     - potentially faster execution time
                                  #Def: 'auto'
                                  #  - first 5 `execute` use a custom plan
                                  #  - a generic plan is used based on them
                                  #  - next `execute` use the generic plan unless it seems like a custom plan is better
                                  #Can also be 'force_generic|custom_plan'

pg_prepared_statements            #TABLE with all PREPs
pg_prepared_statements.name       #'PREP'
pg_prepared_statements
 .parameter_types                 #REGTYPE_ARR of parameters
pg_prepared_statements.statement  #'COMMAND'
pg_prepared_statements.from_sql   #BOOL. Whether COMMAND is SQL
                                  #PREPs can also be created through some low-level protocol
pg_prepared_statements
 .prepare_time                    #TIMESTAMPTZ of creation
pg_prepared_statements
 .generic|custom_plans            #UINT. Number of `execute` which used a generic|custom plan


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CURSORS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


declare "CURSOR" cursor           #Create a CURSOR, i.e. streams a SUBQUERY
 for SUBQUERY                     #Query is executed once during CURSOR creation
                                  #Iterates over that result, i.e. ignores writes
close "CURSOR"|all                #Delete a CURSOR (or all)

declare "CURSOR" ...              #If scroll (def: no scroll), allow backward iteration
 [no] scroll cursor ...           #  - this can sometimes be slower

START POSITION ==>                #Index 0. Before first ROW.
END POSITION ==>                  #Index -1. After last ROW.
START|END POSITION ==>            #Have no ROWs: return empty ROW_SET
                                  #Boundaries: cannot go further
                                  #Does not close, can still move forward|backward
move ... [from|in] "CURSOR"       #Move CURSOR

RELATIVE MOVES ==>                #
move INT ...                      #Move CURSOR by INT
move [next] ...                   #Move CURSOR by 1
move prior ...                    #Move CURSOR by -1
move 0 ...                        #Does not move CURSOR
move all ..                       #Move CURSOR to start|end position
move forward ...                  #Same as move ...
move backward ...                 #Same but backward relative moves

ABSOLUTE MOVES ==>                #Following ones.
                                  #Still traverse each ROW, i.e. O(n), except for absolute 0|-1 which is O(1)
move absolute UINT ...            #Move CURSOR to index UINT
move first|last ...               #Move CURSOR to first|last ROW
move relative ...                 #Same as move ...

fetch ... from "CURSOR"           #Does move ... from "CURSOR"
                                  #Returns ROW_SET with ROWs from last one (excluded) to current one (included)
                                  #If absolute move, only return current ROW

declare "CURSOR" ...              #If `without hold` (def):
 cursor with[out] hold ...        #  - `declare "CURSOR"` must be in a transaction
                                  #  - `close "CURSOR"` automatically done at end of transaction
declare "CURSOR" cursor           #Cannot be used with `scroll` or `with hold`
 for ... select for update|share  #Should use `for update` if `where current of`:
                                  #  - this ensures ROWs remain the same
                                  #  - this is required if SUBQUERY uses group by, order by or joins

declare "CURSOR" binary ...       #Make fetch ... return ROW_SET in Postgres-specific binary format

update|delete ...                 #Perform `update|delete` on current "CURSOR" ROW.
 where current of "CURSOR"        #Does not move CURSOR

pg_cursors                        #TABLE with all CURSOR
pg_cursors.name                   #'CURSOR'
pg_cursors.statement              #'SUBQUERY'
pg_cursors.is_scrollable          #BOOL
pg_cursors.is_holdable            #BOOL
pg_cursors.is_binary              #BOOL
pg_cursors.creation_time          #TIMESTAMPTZ

ENVVAR cursor_tuple_fraction      #0-1 (def: .1). Hint to the query planner at how much percentage of ROWs of TABLEs
                                  #is usually retrieved by a single `fetch ... from "CURSOR"`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             COPY              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


copy (MSUBQUERY) to SOURCE        #Export TABLE to a SOURCE. Append ROWs
copy "TABLE"[("COL",...)]         #SOURCE can be:
 to SOURCE                        #  - 'ABSOLUTE_PATH'
                                  #  - stdout
                                  #  - program 'PROGRAM'
                                  #     - piped to stdin
                                  #     - use current shell
                                  #Only export data, not schema (including TYPEs)
                                  #"TABLE" cannot be "VIEW", but TABLE can be (table "VIEW")
 [where BOOL_REXPR]               #Filter ROWs to export

copy "TABLE"[("COL",...)]         #Import SOURCE to a TABLE. Append ROWs
 from SOURCE                      #SOURCE can be:
                                  #  - 'RELATIVE|ABSOLUTE_PATH'
                                  #     - relative to server's process CWD
                                  #  - stdin
                                  #  - program 'PROGRAM'
                                  #     - using its stdout
                                  #     - use current shell

copy ... with ZOPTS               #
ZOPTS.format                      #Can be:
                                  #  - 'text' (def):
                                  #     - DSV
                                  #     - EOF is line can be '\.'
                                  #     - can use \b \f \n \r \t \v \\ \NNN \xNN \DELIM \NULL
                                  #  - 'csv'
                                  #  - 'binary': Postgres-specific binary format
                                  #TYPEs:
                                  #  - text|csv: serialize to STR
                                  #     - except BOOL, serialized to t|f
                                  #  - binary:
                                  #     - preserve most TYPEs
                                  #     - but cast some TYPEs close to each other, e.g. money|int8 or bpchar|varchar|text

ZOPTS.encoding                    #'ENCODING' (def: client_encoding)
ZOPTS.freeze                      #BOOL (def: false). Do a `vacuum freeze` first

TEXT|CSV ONLY ==>
ZOPTS.delimiter                   #'CHAR'. COL delimiter
                                  #Def: '\t' (text) or ',' (csv)
ZOPTS.null                        #STR used for null values
                                  #Def: '\N' (text) or '' (csv)

CSV ONLY ==>                      #
ZOPTS.header                      #BOOL (def: false) or match.
                                  #Make first CSV row the "COL" names
                                  #If `match` (only with `copy from`), fail if CSV header does not match "COL" names
ZOPTS.quote                       #'CHAR' (def '"')
ZOPTS.escape                      #'CHAR' (def: '\')
ZOPTS.force_quote ("COL",...)|*   #Always quote specific COLs (except null values)
                                  #Only with `copy to`
ZOPTS.force_not_null ("COL",...)  #Cast nulls as STR, in specific COLs
                                  #Only with `copy from`
ZOPTS.force_null ("COL",...)      #Never cast nulls as STR, even if quoted, in specific COLs
                                  #Only with `copy from`

ENVVAR DateStyle                  #Should be set to 'ISO'
ENVVAR IntervalStyle              #Should not be set to 'sql_standard'

pg_stat_progress_copy             #TABLE with ongoing `copy` statements
pg_stat_progress_copy.command     #'COPY FROM|TO'
pg_stat_progress_copy.type        #'PIPE|FILE|PROGRAM' or 'CALLBACK' (used in logical replication)
pg_stat_progress_copy.relid       #REGCLASS of TABLE
pg_stat_progress_copy.datid       #pg_database.oid of DATABASE
pg_stat_progress_copy.datname     #NAME of "DATABASE"
pg_stat_progress_copy.pid         #INT4. PID of server
pg_stat_progress_copy
 .bytes_processed                 #INT8
pg_stat_progress_copy.bytes_total #INT8
pg_stat_progress_copy
 .tuples_processed                #INT8. Rows processed
pg_stat_progress_copy
 .tuples_excluded                 #INT8. Rows excluded by `where BOOL_REXPR` so far


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             GROUP             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... group by COL_REXPR,... #Group ROWs that are equal by all of COL_REXPR,...
select ... group by (COLREXPR,...)#Each ROW_SET group is reduced to a single ROW
                                  #If a COL uses AFUNC(), it:
                                  #  - reduces ROW_SET into a SCALAR
                                  #  - cannot be targeted by a COL_REXPR
                                  #Some COLs cannot be reduced:
                                  #  - if neither:
                                  #     - uses COL_REXPR, or a superset of it
                                  #        - e.g. if COL_REXPR is `a * 2`, using `a * 2 + 1`
                                  #     - uses AFUNC()
                                  #     - "functional dependency", i.e. a COL_REXPR:
                                  #        - is a "COL"
                                  #        - that is a primary key
                                  #        - of the same "TABLE"
                                  #  - then:
                                  #     - cannot use any "ROW_ALIAS"[."COL"]
                                  #     - all those COLs are computed in a single separate ROW_SET, combined with a cartesian product
select ... group by ()            #Single ROW_SET group
                                  #Default behavior

GROUP AGGREGATE ==>               #Sort all COL_REXPR,... to perform the grouping

ENVVAR enable_hashagg             #"Hash aggregate". Alternative to group aggregate
                                  #Build a hash table with hashes of COL_REXPR,... as key to perform the grouping
                                  #Faster unless group aggregate can benefit from an ordered scan
                                  #Also used for:
                                  #  - over (partition by), AFUNC(distinct), distinct, union|intersect|except distinct
                                  #  - union|intersect|except all

select ... having BOOL_REXPR      #Like `where BOOL_REXPR` except:
                                  #  - after `group by`
                                  #  - can use AFUNC()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         GROUPING SETS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select ... group by grouping sets #Same as `select ... group by COL_REXPR` union all `select ... group by COL_REXPR2` ...
 (COL_REXPR,...)                  #I.e. COL_REXPRs:
                                  #  - are independent from each other
                                  #  - produce different ROWs merged with `union all`
                                  #In `select VALs`, each COL_REXPR is null in the other grouping sets
                                  #  - including if COL_REXPR is a sub-expression within `select VAL`
                                  #  - examples:
                                  #     - select a, b ... group by grouping sets(a, b):
                                  #        - first ROWs: `select a, null`
                                  #        - second ROWs: `select null, b`
                                  #     - select coalesce(a, VAL) ... group by grouping sets(a, b):
                                  #        - first ROWs: `select a`
                                  #        - second ROWs: `select VAL`
select ... group by grouping sets #Can use a (...) list.
 ((...),...)                      #I.e. like `select ... group by (...)` union all ...
                                  #It can be an empty (), i.e. like `select ... group by ()`

select ... group by               #Same as single grouping sets (...) with cartesian product
 grouping sets (...),             #Example:
 grouping sets (...)              #  select ... group by grouping sets((a, b), (c, d)), grouping sets((e, f), (g, h))
                                  #  -> grouping sets((a, b, e, f), (a, b, g, h), (c, d, e, f), (c, d, g, h))
select ... group by COL_REXPR,    #COL_REXPR is like grouping sets (COL_REXPR)
 grouping sets (...)              #Example:
                                  #  select ... group by a, grouping sets((b, c), (d, e))
                                  #  -> grouping sets((a, b, c), (a, d, e))
select ... group by distinct ...  #Ignore duplicate grouping sets produced by the cartesian product

select ... group by grouping sets
 (grouping sets(...), ...)        #Same as grouping sets(..., ...)

rollup(COL_REXPR,...)             #Same as: grouping sets ((COL_REXPR1, COL_REXPR2,...), ..., (COL_REXPR1, COL_REXPR2), (COL_REXPR1), ())
                                  #With each grouping set having one less COL_REXPR
                                  #Meant when a series of groups that are subsets of each other
cube(COL_REXPR,...)               #Same as: grouping sets ((COL_REXPR,...), ..., ())
                                  #With all possible combinations of COL_REXPR,... including all|none of them
rollup|cube((...),...)            #If using a (...) list, it is kept as as a list when translating to `grouping sets`,
                                  #then it is flattened.
                                  #E.g. `cube((a, b), (c, d))` -> `grouping sets((a, b, c, d), (a, b), (c, d), ())`

grouping(COL_REXPR,...)->UINT     #AFUNC. UINT indicates the COL_REXPRs used by the current ROW
                                  #UINT is a or'd bitwise flag:
                                  #  - 1 for last COL_REXPR, 2 for previous one, etc.
                                  #  - absence of bit means COL_REXPR is used


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           AGGREGATE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


AFUNC(SET, ...)->SCALAR           #"Aggregate function". Reduce a SET to a SCALAR
                                  #If SCALAR is passed as argument, it is handled like a SET with a single item.
                                  #Must be used inside a REXPR: select, distinct, order by

AFUNC(...)
 filter (where BOOL_REXPR)        #Filter SET values passed to AFUNC()

AFUNC(distinct|all ...)           #If `distinct`, ignore duplicate SET values

AFUNC(... order by ...)           #Sort SET. Same syntax as `select ...`

AFUNC(...)                        #Similar to AFUNC(ARG..., ...) except ARG... are sorted ("ordered-set AFUNC")
 within group (order by ARG,...)  #ARG... are "aggregated arguments" (evaluated once per ROW)
                                  #and ... are "direct arguments" (evaluated once for all ROWs)
                                  #`order by`: same syntax as `select ...`
ZSET                              #Notation that means this AFUNC() argument must be in `within group (order by ZSET)`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            WINDOW             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


AFUNC(SET, ...)->SCALAR over (...)#WFUNC, "Window function"
                                  #Alternative to `group by`. Like `group by`:
                                  #  - specifies the ROW_SET group ("window") passed to AFUNC()
                                  #Unlike `group by`:
                                  #  - window is computed for each ROW and AFUNC()
                                  #  - does not reduce ROW_SET groups to single ROWs
                                  #  - does not restrict which values other COLs can use
                                  #Any AFUNC can be used as WFUNC
                                  #  - but some WFUNC cannot be used as AFUNC
AFUNC (...) over (...) ...        #When combined with `group by`:
 group by ...                     #  - `group by` and `having` are applied first, independently
                                  #     - i.e. WFUNC operates on ROWs after grouping
                                  #  - WFUNC have same "ROW_ALIAS"[."COL"] restrictions as other `select VALs`

AFUNC(...) filter (...) over (...)#Can be used, like a normal AFUNC()

AFUNC(distinct|all ...) over (...)
AFUNC(... order by ...) over (...)#Cannot be used, unlike a normal AFUNC()

AFUNC(ZSET, VAL, ...)             #Must be called with: AFUNC(...) over (... order by COL_REXPR)
                                  #ZSET is the window. VAL is COL_REXPR.

select ...
 AFUNC(...) over "WINDOW"
 ... window "WINDOW" as (WINDOW)  #Same as: `select ... AFUNC(...) over (WINDOW)`
select ...
 AFUNC(...) over ("WINDOW" ...)
 ... window "WINDOW" as (WINDOW)  #Same as: `select ... AFUNC(...) over (WINDOW ...)`

AFUNC(...) over                   #Define window as: the ROWs with same COL_REXPR,... as current ROW
 (partition by COL_REXPR,...)     #  - i.e. similar to `group by`
                                  #If not specified:
                                  #  - if `over (... order by COL_REXPR,...)`:
                                  #     - same as `partition by COL_REXPR,...`
                                  #     - but each window includes any previous ROW
                                  #  - otherwise: single window

AFUNC(...) over (... order by ...)#Sort window ROWs. Same syntax as `select ... order by ...`
                                  #"Peer ROWs" are the preceding|following ones with same sort value

AFUNC(...) over (... rows         #Only include specific range of ROWs in current window
 between FRAME and FRAME2)        #FRAME is start (included) and FRAME2 end (excluded)
                                  #FRAME[2] can be:
                                  #  - unbounded preceding: first ROW
                                  #  - unbounded following: last ROW
                                  #  - current row
                                  #  - VAL preceding: VALnth previous ROW (up to first ROW)
                                  #  - VAL following: VALnth next ROW (up to last ROW)
AFUNC(...) over (... rows FRAME)  #Same as `rows between FRAME and current row`
AFUNC(...) over (... groups ...)  #Same as `rows ...` except FRAME[2]:
                                  #  - current row: include peer ROWs
                                  #  - VAL preceding|following: peer ROWs are counting as 1
AFUNC(...) over (... range ...)   #Same as `rows ...` except FRAME[2]:
                                  #  - current row: include peer ROWs
                                  #  - VAL preceding|following:
                                  #     - VAL is based on the sort value
                                  #     - not based on ROW position within the window
AFUNC(...) over (... range
 between unbounded preceding
 and current row)                 #Default behavior

AFUNC(...) over                   #Exclude specific ROWs in current window, based on EXCLUDE:
 (... rows|groups|range           #  - no others (def): none
 ... exclude EXCLUDE)             #  - current row
                                  #  - ties: peers (except current row)
                                  #  - group: peers + current row


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             RANK              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


count(SET)->UINT                  #AFUNC. Number of values, excluding nulls
count(*)->UINT                    #AFUNC. Number of values, including nulls

rank(ZSET, VAL)->UINT             #AFUNC. 1-based index of VAL in sorted ZSET
                                  #If equal value, same index, but increments index for next value
dense_rank(ZSET, VAL)->UINT       #Same as rank() but if equal value, does not increment index for next value
                                  #I.e. like removing duplicates first
percent_rank(ZSET, VAL)->FLOAT    #Same as rank() but returns as 0-1 percentage
cume_dist(ZSET, VAL)->FLOAT       #AFUNC. Cumulative distribution, i.e. rank() / count()

mode(ZSET)->VAL                   #AFUNC. Mode

percentile_cont(ZSET, FLOAT[_ARR])#AFUNC. Percentile value[s], with interpolation
 ->VAL[_ARR]                      #AFUNC. FLOAT is 0-1 percentage
percentile_disc(...)              #AFUNC. Percentile value[s], without interpolation


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          WINDOW RANK          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


row_number(SET)->UINT             #WFUNC. Current 1-based index within window

nth_value(SET, COL_REXPR, UINT)
 ->VAL|null                       #WFUNC. Returns COL_REXPR value at index UINT within window
first|last_value(SET, COL_REXPR)
 ->VAL                            #WFUNC. Returns COL_REXPR value at first|last index within window

lead(SET, COL_REXPR, UINT[, VAL]) #WFUNC. Returns COL_REXPR value at index row_number() + UINT within window
 ->VAL                            #If none, returns VAL (def: null)
lag(...)->VAL                     #Same as lead but using -UINT

ntile(SET, UINT)->UINT2           #Returns 1 for first index, UINT for last index, and interpolated values in-between.
                                  #If SET length >= UINT, returns row_number() instead


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          STATISTICS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


variance|var_samp(NUM_SET)->NUM   #AFUNC. Variance using n-1
var_pop(NUM_SET)->NUM             #AFUNC. Variance using n
stddev[_samp](NUM_SET)->NUM
stddev_pop(NUM_SET)               #AFUNC. Standard deviation
covar_samp(NUM_SET)->NUM
covar_pop(NUM_SET)                #AFUNC. Covariance

corr(FLOAT_SET, FLOAT_SET2)->FLOAT#AFUNC. r
regr_r2
 (FLOAT_SET, FLOAT_SET2)->FLOAT   #AFUNC. r₂

regr_*(...)                       #AFUNC. Linear regression. Not documented yet


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      TEXT SEARCH PARSER       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARSER                            #Tokenizer. Split a STR into tokens ("lexemes")
                                  #Can produce overlapping tokens
                                  #  - both the compound token and its parts
                                  #  - this allows searching for both
                                  #Max TOKEN size: 2KB

create text search parser "PARSER"#Creates a PARSER.
 (OPTS)                           #Must be superuser ROLE.
                                  #OPTS.* FUNCs not documented yet
OPTS.start                        #FUNC called at start
OPTS.gettoken                     #FUNC that processes next token
OPTS.end                          #FUNC called at end
OPTS.lextypes                     #FUNC that lists token types
OPTS.headline                     #FUNC that summarizes token types

pg_catalog.default                #Default "PARSER", with following tokens
blank                             #Whitespace
asciiword                         #ASCII word
[num]word                         #Non-ASCII word with[out] digits
                                  #Uses locale-specific information, based on lc_ctype
[ascii|num]hword                  #Like *word but hyphenated, as a whole
hword_[ascii|num]part             #Like *word but hyphenated part
int                               #INT
uint                              #UINT
float                             #FLOAT
sfloat                            #FLOAT, as NUMe...
url                               #URL
protocol                          #SCHEME://
host                              #HOST
url_path                          #/PATH of URL
file                              #/PATH of file
email                             #Email address
version                           #X.Y.Z
tag                               #XML tag
entity                            #XML entity

pg_ts_parser                      #TABLE with all PARSERs
pg_ts_parser.oid                  #OID
pg_ts_parser.prsname              #"PARSER" name
pg_ts_parser.prsstart
pg_ts_parser.prstoken
pg_ts_parser.prsend
pg_ts_parser.prslextype
pg_ts_parser.prsheadline          #REGPROC of OPTS.*

ts_token_type('PARSER'|PARSER_OID)
 ->ROW_SET                        #Returns all token types
ROW.tokid                         #TOKEN_TYPE, as INT
ROW.alias                         #"TOKEN_TYPE" name
ROW.description                   #'TOKEN_TYPE_DESCRIPTION'

ts_parse('PARSER'|PARSER_OID, STR)
 ->ROW_SET                        #Tokenize a STR
ROW.token                         #STR value
ROW.tokid                         #TOKEN_TYPE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     TEXT SEARCH TEMPLATE      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create text search                #Creates a TEMPLATE
 template "TEMPLATE"(OPTS)        #Must be superuser ROLE
                                  #Normalizes tokens produced by PARSER: removing|transforming them
                                  #Only defines the backbone of the normalization logic
                                  #  - each DICTIONARY instantiates it with specific arguments (e.g. words from a given language)
                                  #When handling a token, can choose to let next TEMPLATE handle it or not
                                  #  - "filtering": when doing so
                                  #  - final: when not doing it, i.e. must be last TEMPLATE
                                  #OPTS.* FUNCs not documented yet
OPTS.init                         #FUNC
OPTS.lexize                       #FUNC

pg_ts_template                    #Table with all TEMPLATEs
pg_ts_template.oid                #OID
pg_ts_template.tmplname           #'TEMPLATE' name
pg_ts_template.tmplinit
pg_ts_template.tmpllexize         #REGPROC of OPTS.*

SHAREDIR/tsearch_data/FILE.EXT    #File used by TEMPLATEs

simple                            #TEMPLATE removing common words
                                  #Also lowercases words
OPTS.stopwords                    #'FILE' of common words
                                  #Uses SHAREDIR/tsearch_data/FILE.stop
                                  #  - file with one word per line
OPTS.accept                       #BOOL. Whether it is a final TEMPLATE (true, def) or filtering (false)

synonym                           #Filtering TEMPLATE normalizing synonyms to a single form.
                                  #Can also be used to process words, keeping them as is
                                  #  - to avoid next TEMPLATEs from wrongly normalizing them
                                  #  - e.g. Paris -> Paris, to avoid stemmer to misrecognize it as a plural word
OPTS.synonyms                     #'FILE' of synonyms
                                  #Uses SHAREDIR/tsearch_data/FILE.syn
                                  #  - each line has two space-separated fields
                                  #  - second field can end with * to mean it is a prefix
                                  #     - i.e. ...* with TSQUERY
OPTS.casesensitive                #BOOL (def: false). Lowercases words.

thesaurus                         #Filtering TEMPLATE
                                  #Like synonym TEMPLATE but can:
                                  #  - match multiple tokens as input
                                  #     - if multiple choices, longest one is picked
                                  #  - return multiple tokens as output
OPTS.dictfile                     #'FILE' of substitutions
                                  #Uses SHAREDIR/tsearch_data/FILE.ths
                                  #  - each line has two colon-separated fields
                                  #  - first field can contain ? to mean a single token removed by OPTS.dictionary
                                  #  - second field can start with * to prevent next TEMPLATE from processing it
                                  #  - can have #COMMENT
OPTS.dictionary                   #DICTIONARY to apply first ("subdictionary")
                                  #Must end with a final TEMPLATE
                                  #Only used for matching purpose, not kept in final output

ispell                            #Filtering TEMPLATE using Ispell project
                                  #Stemmer like snowball:
                                  #  - more supported languages
                                  #  - based on word list, instead of generic grammar algorithm
                                  #     - i.e. more specific
                                  #     - if word is in list (but might not be), better output
                                  #     - should be followed by snowball
                                  #  - can breakdown compound words even without punctuation|space (e.g. German long words)
OPTS.stopwords                    #Like simple TEMPLATE
OPTS.dictfile                     #'FILE'
                                  #Uses SHAREDIR/tsearch_data/FILE.dict
                                  #Must be manually downloaded and converted to UTF-8
OPTS.afffile                      #'FILE'
                                  #Uses SHAREDIR/tsearch_data/FILE.affix
                                  #Must be manually downloaded and converted to UTF-8

snowball                          #Final TEMPLATE using Snowball project
OPTS.language                     #'LANG' (e.g. 'english').
                                  #Stems words, i.e. remove alternative forms:
                                  #  - lowercase
                                  #  - plural -> singular
                                  #  - conjugation -> infinitive
                                  #  - adjective|adverb suffix (e.g. '*ous', '*ly') -> removed
OPTS.stopwords                    #Like simple TEMPLATE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:    TEXT SEARCH DICTIONARY     :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create text search                #Create a DICTIONARY, i.e. TEMPLATE with arguments
 dictionary "DICTIONARY"(OPTS)    #Parent dependency of TEMPLATE
OPTS.template                     #TEMPLATE
OPTS.*                            #Arguments to TEMPLATE

alter text search
 dictionary "DICTIONARY"(OPTS)    #

pg_ts_dict                        #TABLE with all DICTIONARYs
pg_ts_dict.oid                    #OID
pg_ts_dict.dictname               #'DICTIONARY' name
pg_ts_dict.dicttemplate           #pg_ts_template.oid of TEMPLATE
pg_ts_dict.dictinitoption         #'VAR = VAL,...' of TEMPLATE OPTS

regdictionary                     #TYPE alias to OID, to cast pg_ts_dict.oid as "DICTIONARY" name

ts_lexize(REGDICTIONARY, 'TOKEN') #Normalize 'TOKEN' according to DICTIONARY
 ->STR_ARR|null                   #'TOKEN' is produced by PARSER
                                  #Returns null if token is not transformed and passed to next TEMPLATE
                                  #Returns empty ARR if token is omitted

simple                            #DICTIONARY with simple TEMPLATE but no OPTS.stopwords (i.e. noop)

LANG_stem                         #DICTIONARY with snowball TEMPLATE, OPTS.language 'LANG', OPTS.stopwords 'LANG'

unaccent                          #DICTIONARY with its own TEMPLATE
                                  #Similar to synonym TEMPLATE, but focused on turning Unicode characters into ASCII
                                  #  - removing accent
                                  #  - symbols, e.g. © -> (C) or ʹ -> '
                                  #Trusted postgres extension 'unaccent'
OPTS.rules                        #'FILE' (def: 'unaccent') of fields with|without accent
                                  #Uses SHAREDIR/tsearch_data/FILE.rules
                                  #Default works well for Western language
unaccent(['REGDICTIONARY', ]STR)
 ->STR                            #Applies unaccent to STR

dict_xsyn                         #DICTIONARY with its own TEMPLATE
                                  #Similar to synonym TEMPLATE, but spread word into several synonyms
                                  #Trusted postgres extension 'dict_xsyn'
OPTS.rules                        #'FILE' of synonyms
                                  #Uses SHAREDIR/tsearch_data/FILE.rules
                                  #  - space-separated fields
                                  #  - first field is input
                                  #  - next fields are output
                                  #  - can use #COMMENT
OPTS.keeporig                     #BOOL (def: true). Whether to keep input token in output.
OPTS.keepsynonyms                 #BOOL (def: true). Whether to keep synonyms in output.
OPTS.matchorig                    #BOOL (def: true). When OPTS.keeporig true, whether input token can be processed by next TEMPLATEs
OPTS.matchsynonyms                #BOOL (def: false). Same for OPTS.keepsynonyms

dict_int                          #DICTIONARY with its own TEMPLATE
                                  #Normalize long NUMs
OPTS.maxlen                       #NUM (def: 6). Truncate only first 6 digits of NUMs
OPTS.rejectlong                   #BOOL (def: false). Instead of truncating, omit long NUMs
OPTS.absval                       #BOOL (def: false). Ignore + - signs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      TEXT SEARCH CONFIG       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create text search                #REGCONF, i.e. PARSER + list of DICTIONARYs (for given TOKEN_TYPEs)
 configuration "REGCONF"(OPTS)    #Should put:
                                  #  - most specific DICTIONARYs first
                                  #  - DICTIONARY with final TEMPLATE last
                                  #Parent dependency of PARSER|DICTIONARY
OPTS.parser                       #"PARSER"
OPTS.copy                         #"REGCONF2" to copy PARSER + mappings

alter text search
 configuration "REGCONF"
 add|alter mapping
 for "TOKEN_TYPE",...
 with DICTIONARY,...              #Apply DICTIONARY for given TOKEN_TYPEs
alter text search
 configuration "REGCONF"
 alter mapping
 [for "TOKEN_TYPE",...]
 replace DCTIONARY with DCTIONARY2#
alter text search
 configuration "REGCONF"
 drop mapping [if exists]
 for "TOKEN_TYPE",...             #

initdb -T 'REGCONF'               #Def: 'pg_catalog.LANG' (using lc_ctype)
ENVVAR default_text_search_config #Current REGCONF
get_current_ts_config()->REGCONFIG#

pg_catalog.LANG                   #REGCONF. PARSER pg_catalog.default
                                  #DICTIONARYs:
                                  #  - LANG_stem: [ascii]word, hword_[ascii]part, [ascii]numhword
                                  #  - ignore: blank, tag|entity, protocol
                                  #  - simple: others
pg_catalog.simple                 #REGCONF. Same but without LANG_stem DICTIONARY

pg_ts_config                      #TABLE with all REGCONFs
pg_ts_config.oid                  #OID
pg_ts_config.cfgname              #"REGCONF" name
pg_ts_config.cfgparser            #pg_ts_parser.oid of PARSER

regconfig                         #TYPE alias to OID, to cast pg_ts_config.oid as "REGCONF" name

pg_ts_config_map                  #TABLE with all REGCONFs mappings
pg_ts_config_map.mapcfg           #REGCONFIG of REGCONF
pg_ts_config_map.mapdict          #pg_ts_dict.oid of DICTIONARY
pg_ts_config_map.maptokentype     #TOKEN_TYPE
pg_ts_config_map.mapseqno         #INT with priority order for a given TOKEN_TYPE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      TEXT SEARCH VECTOR       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


tsvector                          #TYPE. STR after:
                                  #  - PARSER tokenization
                                  #  - REGCONFIG normalization
                                  #  - duplicate merging
                                  #The last two are only done with to_tsvector() and @@
                                  #Max 1MB
                                  #  - i.e. might need to split documents into chunks
                                  #Max 2e19 TOKENs
TOKEN:POS,...                     #Specify a TOKEN's position inside STR
                                  #Min 1, max 16383
                                  #Used for relative position comparison
                                  #Must \-escape :
TOKEN:POSWEIGHT,...               #Specify a TOKEN's WEIGHT, i.e. importance (title, subtitle, etc.)
                                  #Each WEIGHT is A|B|C|D
                                  #Def: D
                                  #Duplicates with different POSWEIGHTs are merged
                                  #  - max 256 duplicates per TOKEN
                                  #POSWEIGHTs are parsed as TOKENs themselves

STR <-> TSVECTOR                  #Type cast

'...'                             #TSVECTOR_UNKNOWN

to_tsvector([REGCONFIG, ]STR)
 ->TSVECTOR                       #Type cast

to_tsvector
 ([REGCONFIG, ]OBJ_JSON[B])       #Like to_tsvector() but using all OBJ values (deeply) (STR values only), concatenated
 ->TSVECTOR                       #Positions are ordered with JSON, but unordered with JSONB
json_to_tsvector([REGCONFIG, ]    #Same but filter by STR:
 OBJ_JSON[B], STR[_ARR]_JSON[B])  #  - "string": STR values
 ->TSVECTOR                       #  - "numeric": NUM values
                                  #  - "boolean": BOOL values
                                  #  - "key": OBJ keys (deeply)
                                  #  - "all": keys + values

array_to_tsvector(STR_ARR)
 ->TSVECTOR                       #Join with space
tsvector_to_array(TSVECTOR)
 ->STR_ARR                        #Inverse

TSVECTOR || TSVECTOR2             #Concatenates

length(TSVECTOR)->UINT            #Number of TOKENs

strip(TSVECTOR)->TSVECTOR         #Removes POSWEIGHTs
setweight(TSVECTOR, 'WEIGHT'
 [, 'TOKEN'_ARR])->TSVECTOR       #Sets TSVECTOR WEIGHT on TOKENs (def: all)
to_filter
 (TSVECTOR, 'WEIGHT'[_ARR])
 ->TSVECTOR                       #Only include TOKENs with specific WEIGHTs

to_delete(TSVECTOR, 'TOKEN'[_ARR])
 ->TSVECTOR                       #Exclude specific TOKENs

ts_debug([REGCONFIG, ]STR)
 ->ROW_SET                        #Return each individual TOKEN as a ROW, with before-normalization information
ROW.alias                         #'TOKEN_TYPE'
ROW.description                   #'TOKEN_TYPE_DESCRIPTION'
ROW.token                         #STR. Original input
ROW.dictionaries                  #'DICTIONARY'_ARR chosen
ROW.dictionary                    #Final 'DICTIONARY'|null
ROW.lexemes                       #STR_ARR. Final TOKENs

unnest(TSVECTOR)->ROW_SET         #Return each individual TOKEN as a ROW
ROW.lexeme                        #'TOKEN'
ROW.positions                     #UINT_ARR
ROW.weights                       #'WEIGHT'_ARR

ts_stat('SUBQUERY'[, 'WEIGHT...'])#With SUBQUERY returning TSVECTOR, return each grouped TOKEN as a ROW
 ->ROW_SET                        #If 'WEIGHT...', only include TOKENs with those
ROW.word                          #'TOKEN'
ROW.nentry                        #Number of matches
ROW.ndoc                          #Number of TSVECTORs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       TEXT SEARCH QUERY       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


tsquery                           #TYPE. Query on a TSVECTOR.
                                  #Goes through tokenization + normalization + duplicates like TSVECTOR.
                                  #The last two are only done with to_tsquery() and @@
                                  #Max 4e9 TOKENs

STR <-> TSQUERY                   #Type cast

'...'                             #TSQUERY_UNKNOWN, with following syntax
                                  #Must \-escape: < : & | ( ) !
TOKEN                             #
TOKEN:WEIGHT...                   #Only if this WEIGHT
PREFIX:*                          #Allow matching from start
... & ...                         #And
... | ...                         #Or (not space)
(...)                             #
! ...                             #Not
... <-> ...                       #Must immediately follow ("phrase query"), using :POS
... <POS> ...                     #Must follow after exactly POS distance

tsquery_phrase
 (TSQUERY, TSQUERY2[, NUM])
 ->TSQUERY                        #Returns TSQUERY <-> TSQUERY2 or TSQUERY <NUM> TSQUERY2

to_tsquery([REGCONFIG, ]STR)
 ->TSQUERY                        #Type cast
plainto_tsquery([REGCONFIG, ]STR) #Same as to_tsquery() but:
 ->TSQUERY                        #  - ignores all operators
                                  #  - put & between each TOKEN
phraseto_tsquery([REGCONFIG, ]STR)#Same as to_tsquery() but:
 ->TSQUERY                        #  - ignores all operators
                                  #  - put <-> or <POS> between each TOKEN
websearch_to_tsquery              #Same as to_tsquery() but:
 ([REGCONFIG, ]STR)->TSQUERY      #  - ignores all operators
                                  #  - "..." -> put <-> or <POS> between each TOKEN
                                  #  - or -> |
                                  #  - - -> !

TSQUERY && TSQUERY2               #And
TSQUERY || TSQUERY2               #Or
!! TSQUERY                        #Not

TSQUERY @> TSQUERY2               #BOOL. Is superset|equal
                                  #Only considers TOKENs, not operators
TSQUERY <@ TSQUERY2               #BOOL. Is subset|equal

numnode(TSQUERY)->UINT            #Number of TOKENs and operators

querytree(TSQUERY)->TSQUERY       #Remove any !(...) because cannot be indexed
                                  #Can return empty STR

ts_rewrite
 (TSQUERY, TSQUERY2, TSQUERY3)
 ->TSQUERY                        #Change occurences of TSQUERY2 inside TSQUERY to TSQUERY3
ts_rewrite(TSQUERY, 'SUBQUERY')
 ->TSQUERY                        #Same with TSQUERY2|3 are computed by running SUBQUERY returning 2 COLs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       TEXT SEARCH MATCH       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TSQUERY @@ TSVECTOR
TSVECTOR @@ TSQUERY               #BOOL. true if match

ts_rank_cd([FLOAT_ARR,]           #Counts number of matches with TSQUERY on TSVECTOR
 TSVECTOR, TSQUERY[, UINT])       #Often used with `order_by`
 ->FLOAT                          #FLOAT_ARR is multiplied to matches for WEIGHT D|C|B|A (def: 0.1, 0.2, 0.4, 1)
                                  #Ignores TOKENs without :POS
                                  #UINT computes following operations on return value:
                                  #  - 0 (def): n
                                  #  - 1: n/log(m+1)
                                  #  - 2: n/m
                                  #  - 4: n/x
                                  #  - 8: n/u
                                  #  - 16: n/log(u+1)*log(2)
                                  #  - 32: n/(n+1)
                                  #UINT is or'd flag, i.e. can combine several computations
                                  #With:
                                  #  - n: count of matches
                                  #  - m: count of TOKENs in TSVECTOR (must use :POS)
                                  #  - u: count of unique TOKENs in TSVECTOR
                                  #  - x: mean harmonic distance between matches
                                  #     - sum of each 1/(:POS difference) between each match and the next one
                                  #     - e.g. :1 :11 :111 -> 1/10 + 1/100 -> .11
ts_rank(...)->FLOAT               #Same as ts_rank() but n is first normalized from 0 to 1
                                  #  - using: 1-1/(.9 + 1.644 * n)
                                  #  - i.e. scale follows 1/n
                                  #If no match has :POS, use a single match with :1d
                                  #UINT flags are slightly different:
                                  #  - 1: n/log(m+1)*log(2) instead
                                  #  - 4: not possible

ts_headline([REGCONFIG, ]STR,     #Highlight matches.
 TSQUERY[, 'OPT=VAL,...'])->STR   #For performance, should apply only on ROWs with matches
                                  #  - e.g. select ts_headline(...) from (select ...)
ts_headline([REGCONFIG, ]
 OBJ_JSON[B], TSQUERY
 [, 'OPT=VAL,...'])->OBJ_JSON[B]  #Same but over OBJ values (deeply, STR values only)
OPTS.StartSel                     #STR (def: '<b>'). Prepended to each match
OPTS.StopSel                      #STR (def: '</b>'). Appended to each match
OPTS.FragmentDelimiter            #STR (def: '...'). Ellipsis symbol
OPTS.MinWords                     #UINT (def: 15). Min TOKENs to output
OPTS.MaxWords                     #UINT (def: 35). Max TOKENs to output
OPTS.ShortWord                    #UINT (def: 3). Remove TOKENs with <= UINT characters at start|end of return value,
                                  #unless they are matches
OPTS.HighlighAll                  #BOOL (def: false). Negates OPTS.MinWords|MaxWords|ShortWord
OPTS.MaxFragments                 #UINT (def: 0). Max fragments in output


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       TEXT SEARCH FUZZY       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


FUZZYSTRMATCH ==>                 #Trusted postgres extension

levenshtein(STR, STR2
 [, INS_COST, DEL_COST,SUB_COST]) #Levenshtein distance
 ->INT                            #All COST def: 1
levenshtein_less_equal(..., INT)  #Same but:
 ->INT2                           #  - max return value is INT+1
                                  #  - faster

dmetaphone(STR)->STR2             #Returns main consonant sounds of STR
                                  #Similar sounding consonants use the same characters, i.e. phonetic-style code
                                  #STR must be a single word
                                  #Works with mostly Western languages
                                  #Goal: fuzzy search on homophones, i.e. similar words that only differ in spelling
                                  #Uses Double Metaphone (v2) algorithm
dmetaphone_alt(STR)->STR2         #Uses Double Metaphone alternative algorithm
metaphone(STR, INT)               #Same but using Metaphone v1 (older algorithm)
                                  #Truncates at INT characters (max 255)
                                  #Only for English language
soundex(STR)->STR2                #Same but using Soundex, another algorithm, not as good
                                  #Return value is always 4 characters long
difference(STR, STR2)->INT        #Number of different characters after applying soundex() on each STR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           ENCODING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


'ENCODING'                        #Used by STR
                                  #Among:
                                  #  - SQL_ASCII
                                  #  - UTF8|Unicode
                                  #  - LATIN1-10 or ISO_8859_1-16
                                  #  - WIN1250-58, WIN874, WIN866
                                  #  - ISO_8859_5, WIN1251, KOI8[R], KOI8U: Cyrillic
                                  #  - ISO_8859_2, WIN1250: Czech, Polish
                                  #  - BIG5, GBK, GB18030, SJIS, SHIFT_JIS_2004, UHC, JOHAB,
                                  #    EUC_JP, EUC_CN, EUC_KR, EUC_TW, EUC_JIS_2004: East-asian
                                  #  - MULE_INTERNAL: Emacs
ENCODING                          #ENCODING id INT, from 0 to 41
pg_encoding_to_char
 (ENCODING)->'ENCODING'           #
pg_char_to_encoding
 ('ENCODING')->ENCODING           #

initdb|createdb
 --encoding|-E "ENCODING"
create database "DATABASE"
 encoding [=] "ENCODING"          #Server ENCODING
ENVVAR server_encoding            #Def: locale (UTF8 in Linux) or SQL_ASCII

ENVVAR client_encoding
ENVVAR PGCLIENTENCODING
set names "ENCODING"
\encoding "ENCODING"              #Client ENCODING
pg_client_encoding()->"ENCODING"  #If different from server, applies CONVERSION


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CONVERSION           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create conversion "CONVERSION"    #Creates a CONVERSION, i.e. conversion between 2 ENCODINGs
 for 'ENCODING' to 'ENCODING2'    #Using FUNC(ENCODING, ENCODING2, 'INPUT', 'OUTPUT', INPUT_INT, BOOL)->OUTPUT_INT
 from "FUNC"                      #  - BOOL is false if error should throw
                                  #ENCODING[2] cannot be SQL_ASCII
create default conversion ...     #Preferred CONVERSION for this pair of ENCODINGs

pg_conversion                     #TABLE of all CONVERSIONs
pg_conversion.oid                 #OID
pg_conversion.conname             #"CONVERSION" name, e.g. "windows_866_to_windows_1251"
pg_conversion.conforencoding      #Source ENCODING
pg_conversion.contoencoding       #Destinatio ENCODING
pg_conversion.conproc             #REGPROC performing conversion
pg_conversion.condefault          #BOOL. True if default

BUILT-IN CONVERSIONS ==>          #  - UTF8 <-> all except SQL_ASCII and MULE
                                  #  - ISO_8859_5 <-> KOI8R <-> WIN1251 <-> WIN866: Cyrillic legacy encodings
                                  #  - ISO_8859_2 <-> WIN1250: Czech, Polish
                                  #  - EUC_JP <-> SJIS, EUC_TW <-> BIG5, EUC_JIS_2004 <-> SHIFT_JIS_2004: east-asian
                                  #  - MULE <-> many

convert
 (BYTEA, 'ENCODING', 'ENCODING2')
 ->BYTEA                          #Apply a CONVERSION
convert_to
 (STR, 'ENCODING')->BYTEA         #Like convert(STR, pg_client_encoding(), 'ENCODING')
convert_from
 (BYTEA, 'ENCODING')->STR         #Like convert(STR, 'ENCODING', pg_client_encoding())

convert_to|from(STR, STR2)        #Same, but assumes the dest|original encoding to be the current system's encoding


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LOCALE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENVVAR LANG
initdb|createdb --locale=LANG
create database "DATABASE"
 locale [=] LANG                  #Sets all LC_*, or --icu-locale

ENVVAR LC_ALL                     #Sets all LC_*

initdb|createdb --lc-*=LANG       #Sets single LC_*
ENVVAR LC_*                       #Def is '', i.e.:
                                  #  - local one (e.g. 'en_US.UTF-8')
                                  #  - or 'C' if none
                                  #Server-specific, not client
create database "DATABASE"
 lc_collate|lc_ctype [=] LANG     #

ENVVAR lc_collate                 #Used by:
                                  #  - < > >= <=
                                  #  - order by
                                  #  - how INDEXs are persisted
                                  #     - i.e. cannot be changed without a `reindex`
                                  #  - INDEX on like|~ operators
                                  #Only for collatable TYPEs (such as STR)
                                  #Cannot use a OP|FUNC with argument TYPEs that have a different lc_collate
                                  #Must be compatible with current ENCODING

ENVVAR lc_ctype                   #Used by:
                                  #  - letter|digit character classes in PARSER, REGCONF
                                  #  - [[:...:]] in REGEXPs
                                  #  - case: upper|lower|initcap(), REGEXP|GLOB
                                  #Must be compatible with current ENCODING

ENVVAR lc_messages                #Used in output|error messages translation

ENVVAR lc_monetary                #Used by to_char()
ENVVAR lc_numeric                 #Used by to_char()
ENVVAR lc_time                    #Used by to_char(), ENVVAR DateStyle

C LOCALE ==>                      #Not language-specific, but faster
                                  #Same as POSIX locale


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           COLLATION           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create collation "COLLATION"(OPTS)#Create COLLATION, i.e. combination of ENCODING, lc_collate and lc_ctype

create collation "COLLATION"
 from "COLLATION2"                #Creates an alias

OPTS.locale                       #Value of both lc_collate|ctype, or --icu-locale
OPTS.lc_collate                   #'LANG' value of lc_collate
OPTS.lc_ctype                     #'LANG' value of lc_ctype

initdb|createdb --icu-locale=LANG
create database "DATABASE"
 icu_locale [=] LANG              #Value of --icu-locale

OPTS.deterministic                #BOOL (def: true)
                                  #If false:
                                  #  - allow comparisons to treat equivalent STRs as equal such as:
                                  #     - Unicode normalization
                                  #     - accents
                                  #     - case insensitivity
                                  #  - only with icu provider
                                  #  - slower
                                  #  - cannot use REGEXP|GLOBs

initdb|createdb                   #How to get locale data. Either:
 --locale-provider=PROVIDER       #  - libc (def):
create database ...               #     - ones used by current OS
 locale_provider [=] PROVIDER     #     - uses ENVVAR LC_*
OPTS.provider                     #  - icu:
                                  #     - external ICU, usually better
                                  #     - must have been specified at Postgres build time
                                  #     - does not use ENVVAR LC_*
                                  #     - only a single LANG, specified by --icu-locale

create database "DATABASE"        #Version of OPTS.provider
 collation_version [=] 'X.Y[.Z]'  #Can be empty STR
OPTS.version                      #Meant to detect changes of the actual provider version
                                  #  - compared to the one specified in COLLATION
                                  #  - this can happen if:
                                  #     - libc: OS upgrade of libc
                                  #     - icu: Postgres was re-built with a new ICU version
                                  #  - prints a warning
                                  #  - should rebuild ENTITYs using the COLLATION, e.g. using `reindex`
                                  #     - can be seen by listing pg_depend.refclassid = 'pg_collation'::regclass
                                  #     - then using pg_depend.refobjid to find pg_collation with a wrong collversion
alter collation|database ...
 refresh version                  #Should be done after rebuilding ENTITYs due to OPTS.version change
pg_collation_actual_version       #Actual version of COLLATION
 (REGCOLLATION)->'X.Y[.Z]'        #Differ from pg_collation.collversion if there was a change
pg_database_collation_
 actual_version(pg_database.oid)  #Same with pg_database.datcollversion, for a DATABASE

pg_collation                      #TABLE with All COLLATIONs
pg_collation.oid                  #OID
pg_collation.collname             #"COLLATION"
pg_collation.collencoding         #ENCODING. -1 for current server encoding
pg_collation.collcollate          #OPTS.lc_collate
pg_collation.collctype            #OPTS.lc_ctype
pg_collation.collisdeterministic  #OPTS.deterministic
pg_collation.collprovider         #OPTS.provider as 'd' (--locale-provider), 'c' (libc), 'i' (icu)
pg_collation.collversion          #OPTS.version
pg_collation.colliculocale        #--icu-locale

regcollation                      #TYPE alias to OID, to cast pg_collation.oid as "COLLATION" name

BUILT-IN COLLATIONS ==>           #  - default: any locale-provider, any ENCODING, any locale
                                  #  - C|POSIX: libc, any ENCODING, 'C|POSIX' locale
                                  #  - ucs_basic: libc, UTF8, 'C' locale
                                  #  - C.utf8: libc, UTF8, 'C.utf8' locale
                                  #  - LANG[.utf8] (e.g. en_US.utf8): libc, UTF8, 'LANG[.utf8]' locale
                                  #  - LANG-x-icu (e.g. en-US-x-icu): icu, any ENCODING, current OS locale, --icu-locale=LANG
                                  #  - und-x-icu: icu, any ENCODING, current OS locale, --icu-locale=und (no LANG)
                                  #All: OPTS.deterministic true
DEFAULT COLLATION ==>             #'default'

pg_import_system_collations       #Populates pg_collation based on current OS
 (REGNAMESPACE)->INT              #Automatically done by initdb
                                  #REGNAMESPACE is usually pg_catalog
                                  #Returns number of new COLLATIONs

STR collate "COLLATION"           #Sets COLLATION for a single STR
collation for (STR)               #'COLLATION' of STR

VAL ~<~ ~<=~ ~>=~ ~>~ VAL2        #Like VAL < <= >= > VAL, but with COLLATION always "C"

create type "RANGE_TYPE"
 as range(collation = COLLATION)  #For a RANGE
create domain ...
 collate "COLLATION"              #For a DOMAIN

YOPTS.collatable                  #BOOL (def: false). Whether TYPE is COLLATION-sensitive.
                                  #Built-in types: true for STR ("default"), name ("C"), pg_node_tree ("default")
pg_type.typcollation              #REGCOLLATION of the TYPE. 0 if YOPTS.collatable false. Otherwise set.

create table "TABLE"
 ("COL" TYPE collate COLLATION)
create index "INDEX" on "TABLE"
 ("COL"|(REXPR) collate COLLATION)
create type "ROW"
 as ("COL" TYPE collate COLLATION)
alter table "TABLE"
 add [column] "COL" TYPE
 collate COLLATION
alter type "ROW"
 add attribute "COL" TYPE
 collate COLLATION
alter table "TABLE"
 alter [column] "COL"
 [set data] type TYPE
 collate COLLATION
alter type "ROW"
 alter attribute "COL"
 [set data] type TYPE
 collate COLLATION                #For a COL
create table ... partition by ...
 ("COL"|(COL_REXPR)
 collate COLLATION ...)           #For an underlying INDEX COL

pg_attribute.attcollation         #REGCOLLATION of the COL. 0 if none
pg_index.indcollation             #REGCOLLATION_ARR of indexed COLs.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INDEX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create index "INDEX"              #Create an INDEX
 on "TABLE"("COL"|(REXPR))        #Data structure storing the value "COL"|(REXPR) for each ROW in an optimized data structure
                                  #Used by the planner to automatically speedup read queries
                                  #  - at the expense of write speed, since each write must update INDEX
                                  #Only used by queries:
                                  #  - that use exactly "COL"|(REXPR)
                                  #  - with the clauses supported by the ACCESS_METHOD (e.g. `where`, etc.)
                                  #  - with the OPs supported by the ACCESS_METHOD (e.g. =, etc.)
                                  #     - including equivalent ones, e.g. < > -> between, = -> in, ~~ -> like
                                  #Its contents cannot be directly read|write
                                  #REXPR must be purely functional
                                  #Auto-dependency child of its COL

create index ... with (IOPTS)
alter index ... set (IOPTS)
create table ...
 exclude|unique|primary key ...
 with (IOPTS)                     #"INDEX storage options". Different from TOPTS.

pg_index                          #TABLE with all INDEXs
pg_index.indexrelid               #REGCLASS of INDEX
pg_index.indrelid                 #REGCLASS of TABLE
pg_index.indnatts                 #INT2. Number of COLs
pg_index.indnkeyatts              #INT2. Number of COLs, except non-key COLs
pg_index.indkey                   #ARR of COLs (pg_attribute.attnum) being indexed. 0 if REXPR
pg_index.indexprs                 #PG_NODE_TREE representing REXPR. null if none
pg_index.indcheckxmin             #BOOL. Whether the INDEX cannot be queried due to awaiting concurrent transactions
pg_index.indisready               #BOOL. Whether the INDEX can be written to
pg_index.indislive                #BOOL. Whether the INDEX is being deleted

pg_indexes                        #Higher-level VIEW over pg_index
pg_indexes.tablename              #"TABLE" name
pg_indexes.indexname              #"INDEX" name
pg_indexes.indexdef               #STR from pg_get_indexdef()

pg_get_indexdef(REGCLASS)->STR    #Returns 'CREATE ...' statement that created INDEX

pg_class.relhasindex              #BOOL. Whether RELATION has INDEXs
pg_tables.hasindexes              #BOOL. Whether TABLE has INDEXs
pg_matviews.hasindexes            #BOOL. Whether MVIEW has INDEXs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          INDEX SCANS          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENVVAR enable_seqscan             #"Sequential scan". Query that reads ROWs sequentially, without an INDEX
                                  #Fast if:
                                  #  - small TABLE
                                  #  - query selects most ROWs
ENVVAR                            #STR (def: '8MB'). Minimum amount of TABLE ROWs read to consider using a "parallel sequential scan".
 min_parallel_table_scan_size     #I.e. sequential scan run in parallel: each worker handles different heap pages
TOPTS.parallel_workers            #NUM of workers when doing parallel sequential scan
                                  #Def: guessed based on TABLE size

ENVVAR enable_indexscan           #"[Plain] index scan". Query using an INDEX
                                  #INDEX returns TIDs, which are then used to fetch TABLE ROWs
                                  #Not efficient on query that select only few ROWs out of many
ENVVAR                            #STR (def: '512kB'). Minimum amount of INDEX ROWs read to consider using:
 min_parallel_index_scan_size     #  - a "parallel index scan"
                                  #  - a parallel `vacuum`
                                  #ACCESS_METHOD-specific (only some support it)

ENVVAR enable_bitmapscan          #"Bitmap scan". Alternative to index scan.
                                  #Instead of returning TIDs, the INDEX creates a bitmap ("bitmap index scan")
                                  #  - with 0|1 for each heap page (i.e. sets of ROWs)
                                  #  - initially only 0s, each ROW match sets 1 to its heap page ("bitmap populating")
                                  #The bitmap is then used to read TABLE ROWs ("bitmap heap scan")
                                  #  - using a full sequential scan
                                  #  - unlike index scan which uses random access with TIDs
                                  #  - since a heap page contains 1-n ROWs, must re-apply filters to find the right ROWs ("Recheck Cond")
                                  #Faster than index scan if many ROWs are fetched
                                  #  - but not too many, where a sequential scan is then often faster
PARALLEL BITMAP SCAN ==>          #Bitmap scan where the heap scan (not the index scan) is run as a parallel sequential scan


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       INDEX COMBINATION       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


BITMAP SCAN COMBINATION ==>       #When using '... and|or ...'
                                  #Perform separate bitmap scans, then combine them with bitwise logic
                                  #Sometimes slower than a sequential scan, if too many ...
                                  #Also, individual ordered scans from each ... is lost, i.e. might need to re-sort

create index ...                  #"Multicolumn INDEX"
 on "TABLE"("COL"|(REXPR),...)    #Stores multiple "COL"|(REXPR)
                                  #Can be faster than bitmap scan combination, providing all "COL"|(REXPR) are queried together
                                  #Behavior is ACCESS_METHOD-specific
                                  #Max 32 COLs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        INDEX-ONLY SCAN        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENVVAR enable_indexonlyscan       #"Index-only scan".
                                  #Query that uses only the INDEX without visiting the TABLE, i.e. faster
                                  #Only if query uses only the "COL"|(REXPR) can are indexed
                                  #Not efficient with REXPRs
                                  #Only some ACCESS_METHODs can do it
create index ...
 include ("COL",...)              #"Covering INDEX"
create table ...                  #Stores "COL" in INDEX, but do not use it in query planning, nor uniqueness ("non-key COLs")
 exclude|unique|primary key ...   #I.e. only used to increase probability of index-only scans
 include("COL",...)               #Some ACCESS_METHODs do not support it

VISIBILITY MAP ==>                #Checked before doing index-only scans:
                                  #  - must fetch ROW from TABLE instead of index-only ("heap fetch"):
                                  #     - if heap page lacks all-visible bit
                                  #        - i.e. if was written since last vacuum
                                  #     - reason: there "might" be uncommitted writes
                                  #  - i.e. index-only scans less frequent on write-heavy TABLEs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         PARTIAL INDEX         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create index ... where BOOL_REXPR #"Partial INDEX". Only indexes ROWs with BOOL_REXPR true
                                  #INDEX only used if BOOL_REXPR matches the one used by the query
                                  #BOOL_REXPR can refer to any "COL" from TABLE, not just the indexed ones
                                  #Goal: faster writes when:
                                  #  - only few ROWs are usually read
                                  #  - other ROWs are write-heavy
                                  #Other possible goal: only apply unique constraint on specific ROWs

pg_index.indpred                  #PG_NODE_TREE representing `where BOOL_REXPR`. null if none


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     PARALLEL INDEX BUILD      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARALLEL INDEX BUILD ==>          #Some ACCESS_METHODs can use multiple worker processes with `create index`

ENVVAR                            #Max NUM (def: 2) of parallel worker processes used for parallel
 max_parallel_maintenance_workers #`create index` or `vacuum`
                                  #Must be < ENVVAR max_parallel_workers
                                  #Higher is faster, but consumes more CPU
                                  #NUM of worker processes is also set by TOPTS.parallel_workers

ENVVAR maintenance_work_mem       #STR (def: '64MB'). Max memory used by `create INDEX`, `vacuum` and `create foreign key`
                                  #Good idea to increase it.
                                  #If multiple parallel workers, it is limit for all, not for each


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            REINDEX            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


reindex index "INDEX"             #Re-create INDEX, to make it take less space with current data
                                  #Needed to optimize space after changing IOPTS, or after a very big INDEX size decrease
                                  #Also needed if INDEX is corrupted due to software|hardware issue
reindex table "TABLE"             #Same for the TABLE's INDEXs
reindex schema "SCHEMA"           #Same for the SCHEMA's INDEXs
reindex system "DATABASE"         #Same for the pg_catalog.* INDEXs
reindex database "DATABASE"       #Same for the DATABASE's INDEXs

reindex verbose ...               #Prints a progress bar

pg_stat_progress_create_index     #TABLE with ongoing `create index` and `reindex` statements
pg_stat_progress_create_index
 .command                         #'CREATE INDEX|REINDEX [CONCURRENTLY] ...'
pg_stat_progress_create_index
 .relid                           #REGCLASS of TABLE
pg_stat_progress_create_index.
 index_relid                      #REGCLASS of INDEX. 0 if none
pg_stat_progress_create_index
 .datid                           #pg_database.oid of DATABASE
pg_stat_progress_create_index
 .datname                         #NAME of "DATABASE"
pg_stat_progress_create_index.pid #INT4. PID of server
pg_stat_progress_create_index     #STR among:
 .phase                           #  - 'initializing'
                                  #  - 'waiting for writers before build': await concurrent write transactions
                                  #  - 'building index: SUBPHASE': ACCESS_METHOD build INDEX
                                  #  - 'waiting for writers before validation': await concurrent write transactions
                                  #  - 'index validation: ...': only if `create index`
                                  #     - 'index validation: scanning index': find ROWs to validate
                                  #     - 'index validation: sorting tuples': sort ROWs to validate
                                  #     - 'index validation: scanning table': validate ROWs
                                  #  - 'waiting for old snapshots': await concurrent read transactions
                                  #  - 'waiting for readers ...': only if `reindex`
                                  #     - 'waiting for readers before marking dead': await read locks
                                  #     - 'waiting for readers before dropping': await read locks before `drop`
                                  #Unless concurrently, only 'initializing' and 'building index'
pg_stat_progress_create_index
 .current_locker_pid              #INT8. PID of process with a lock being awaited, during 'waiting for ...'
pg_stat_progress_create_index
 .lockers_done|total              #INT8. NUM of processes locking, during 'waiting for ...'
pg_stat_progress_create_index
 .blocks_done|total               #NUM of page heaps processed, during 'building index' and 'index validation: scanning index|table'
pg_stat_progress_create_index
 .tuples_done|total               #NUM of ROWs processed, during 'building index'
pg_stat_progress_create_index
 .partitions_done|total           #NUM of partitions, if partitioned TABLE. 0 during `reindex`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CLUSTER            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


cluster "TABLE"                   #Optimizes the physical layout of "TABLE" to match its INDEXs ACCESS_METHODs
                                  #Makes ROWs
                                  #  - that are conceptually close to each other (e.g. similar value)
                                  #  - be on the same physical page
                                  #Only on current ROWs: future ROWs are not clustered.
                                  #Should run `analyze` afterwards.
cluster ...                       #Without a "TABLE": all TABLEs previously cluster'd by current ROLE

cluster ... using "INDEX"         #Set the INDEX for next calls of `cluster`
                                  #Must be specified the first time for a given "TABLE"
alter table "TABLE"
 cluster on "INDEX"
alter table "TABLE"
 set without cluster              #Changes the INDEX

cluster verbose ...               #Prints progress
pg_stat_progress_cluster          #TABLE with ongoing `cluster` and `vacuum full` statements
pg_stat_progress_cluster.command  #'CLUSTER ...'
pg_stat_progress_cluster.relid    #REGCLASS of TABLE
pg_stat_progress_cluster.
 cluster_index_relid              #REGCLASS of INDEX. 0 if none
pg_stat_progress_cluster.datid    #pg_database.oid of DATABASE
pg_stat_progress_cluster.datname  #NAME of "DATABASE"
pg_stat_progress_cluster.pid      #INT4. PID of server
pg_stat_progress_cluster.phase    #STR among:
                                  #  - 'initializing'
                                  #  - 'seq scanning heap': sequential scan on TABLE
                                  #  - 'index scanning heap': index scan on TABLE
                                  #  - 'sorting tuples': sort ROWs
                                  #  - 'writing new heap': write sorted ROWs
                                  #  - 'swapping relation files': swap newly written files
                                  #  - 'rebuilding index'
                                  #  - 'performing final cleanup'
pg_stat_progress_cluster
 .heap_tuples_scanned             #NUM of ROWs read during 'seq scanning heap'
pg_stat_progress_cluster
 .heap_tuples_written             #NUM of ROWs written during 'seq scanning heap'
pg_stat_progress_cluster
 .heap_blks_scanned               #NUM of blocks processed during '* heap'
pg_stat_progress_cluster
 .heap_blks_total                 #NUM of blocks in total during '* heap'
pg_stat_progress_cluster
 .index_rebuild_count             #NUM of INDEXs rebuilt during 'rebuilding index'

pg_index.indisclustered           #BOOL. Whether `cluster` was last called on INDEX


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         ACCESS METHOD         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create                            #Create an "index access method"
 access method "ACCESS_METHOD"    #How a TABLE|INDEX is physically read|written, i.e. its data structure and operators.
                                  #Must be superuser
                                  #Parent dependency of any INDEX using it

OPERATIONS ==>                    #Many operations use ACCESS_METHODs
                                  #  - if an INDEX on the COLs|COL_REXPR exist, use its OPCLASS
                                  #     - faster because can re-use INDEX optimized physical structure
                                  #  - otherwise, the default ACCESS_METHOD for that operation + TYPE is used
                                  #Most of those operations are only available with specific ACCESS_METHODs
                                  #Operations:
                                  #  - where|having|on|check VAL OP VAL2
                                  #  - order by "COL"|COL_REXPR (ordered scan, including backward)
                                  #  - order by VAL OP VAL2 (ordering OP)
                                  #  - is [not] null (in BOOL_REXPR)
                                  #  - VAL OP any|all (ARR)
                                  #  - distinct|grouping: distinct, union|intersect|except distinct, AFUNC(distinct), group by, over (partition by)
                                  #  - exclude CONSTRAINT
                                  #  - unique: unique CONSTRAINT, create unique ...
                                  #  - over (range NUM preceding|following)
                                  #  - partition by range|list
                                  #  - partition by hash

SCAN FEATURES ==>                 #Can also support:
                                  #  - multi-COL
                                  #  - plain index scan
                                  #  - bitmap index scan
                                  #  - index-only scan
                                  #  - parallel index scan
                                  #  - parallel index build
                                  #  - predicate lock

INDEX STORAGE ==>                 #ACCESS_METHODs can support:
                                  #  - non-key COLs
                                  #  - storage TYPE
                                  #  - cluster
                                  #  - ENVVAR maintenance_work_mem
                                  #with (TOPTS|IOPTS) of TABLE|INDEX are ACCESS_METHOD-specific

create access method ...          #Whether this is meant for:
 type table|index                 #  - INDEXs ("index method"):
                                  #     - most ACCESS_METHODs
                                  #  - TABLEs ("table method"):
                                  #     - by default, only "heap"
                                  #     - also implements WAL

create access method ...          #Main FUNC of ACCESS_METHOD
 handler "FUNC"                   #Returns:
                                  #  - capabilities
                                  #  - expectations about OPs and helper FUNCs
                                  #  - methods: create|delete TABLE|INDEX, CRUD on ROWs, validation of OP(TYPE, TYPE2), vacuum, etc.

ENVVAR default_table_access_method#Def ACCESS_METHOD for TABLEs
                                  #Def: 'heap'
create table ...
 using "ACCESS_METHOD"
alter table "TABLE"
 set access method "ACCESS_METHOD"#Set TABLE's ACCESS_METHOD

create index ... on "TABLE"
 using ACCESS_METHOD (...)
create table ...
 exclude using ACCESS_METHOD ...  #Set INDEX's ACCESS_METHOD
create table ...
 unique|primary key               #Always use btree ACCESS_METHOD

pg_am                             #TABLE with ACCESS_METHODs
pg_am.oid                         #OID
pg_am.amname                      #"ACCESS_METHOD" name
pg_am.amtype                      #'t' (TABLE) or 'i' (INDEX)
pg_am.amhandler                   #REGPROC of handler

pg_class.relam                    #pg_am.oid of the RELATION's ACCESS_METHOD
                                  #0 if SEQUENCE|VIEW|ROW_TYPE


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        OPERATOR FAMILY        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


OPFAMILY                          #"Operator family", i.e. group of OPCLASSs (or "loose" ones) that can be
                                  #simplified when used together in a single expression
                                  #E.g. >(NUM, NUM2) and <(NUM, NUM2) in `where NUM > NUM2 and NUM < NUM3`
                                  #Usually:
                                  #  - named "TYPE_ops"
                                  #  - an OPCLASS defines all OP(TYPE, TYPE) for a given TYPE + ACCESS_METHOD
                                  #  - if there are easily castable TYPEs:
                                  #     - such as: INT2|4|8, FLOAT4|8, BPCHAR|VARCHAR|TEXT, DATE|TIMESTAMP[TZ], CIDR|INET
                                  #     - then OPFAMILY has one OPCLASS for each
                                  #     - also it defines loose OPs for each OP(TYPE, TYPE2) combination ("cross-type OP")
                                  #  - otherwise, OPFAMILY contains a single OPCLASS, often with same name

create operator family "OPFAMILY" #Creates an OPFAMILY
 using "ACCESS_METHOD"            #Must be a superuser

alter operator family "OPFAMILY"  #Like `create operator class using "ACCESS_METHOD" add operator|function ...` but "loose"
 using "ACCESS_METHOD"            #OPs and support FUNCs can either be in OPCLASSs, or not (loose)
 add operator|function ...        #  - can be deleted without deleting the OPCLASS
                                  #     - i.e. not critical for OPFAMILY to work properly
                                  #  - can use different TYPEs for OP left|right argument
                                  #Otherwise, no differences
alter operator family ...
 drop operator UINT
 "OP"(TYPE, TYPE2)
alter operator family ...
 drop function UINT(TYPE,...)     #

pg_opfamily                       #TABLE with all OPFAMILYs
pg_opfamily.oid                   #OID
pg_opfamily.opfmethod             #pg_am.oid of ACCESS_METHOD
pg_opfamily.opfname               #"OPFAMILY" name


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        OPERATOR CLASS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


OPCLASS                           #"Operator class", i.e. OP and helper FUNCs that an INDEX ACCESS_METHOD can use
                                  #ACCESS_METHOD-specific, including its "OPCLASS" name
                                  #Child auto-dependency of its OPFAMILY
                                  #Parent dependency of INDEX using it
                                  #Usually named "TYPE_ops"

create operator class "OPCLASS"
 for type TYPE
 using "ACCESS_METHOD"
 as ...                           #

create operator class "OPCLASS"   #Make it default for the given TYPE
 default for type ...             #If false, must be explicitly specified when creating INDEX|TABLE

create operator class ...
 family "OPFAMILY" as ...         #Def: creates one with same name as "OPCLASS"

create operator class ...         #Add an OP(TYPE, TYPE2) for the INDEX ACCESS_METHOD to use
 as operator UINT OP[(TYPE,TYPE2)]#INDEX ACCESS_METHODs define each OP they expect:
                                  #  - in an abstract, TYPE-agnostic way
                                  #  - assigning a strategy number UINT for each
                                  #  - OP symbol is unspecified, although usually the same symbol is used by all OPCLASSs
                                  #OPCLASSs implement those OPs for specific TYPEs
                                  #Some ACCESS_METHOD's OPs are optional:
                                  #  - including having no required OPs
                                  #TYPE2 can be none for single-argument OPs
                                  #Def TYPE[2]: same as `for type TYPE`

create operator class ...         #Mark as "search operator" (def)
 as operator ...                  #  - OP returns BOOL
 for search                       #  - used in where|having|on|check (VAL OP VAL2)

create operator class ...         #Mark as "ordering operator":
 as operator ...                  #  - OP returns NUM: distance between arguments
 for order by OPFAMILY2           #  - allow using `order by VAL OP VAL2`
                                  #     - if either VAL|VAL2 indexed, as opposed to whole `VAL OP VAL2`
                                  #     - even for TYPEs that do not have < OP
                                  #OPFAMILY2 is used to sort the return values
                                  #  - must use a b-tree ACCESS_METHOD

create operator class ...         #Add a "support function"
 as function UINT [(TYPE, ...)]   #Similar to OPs, except:
 "FUNC"(TYPE3,...)                #  - not directly used in clauses (like `where`, etc.)
                                  #  - but still used by ACCESS_METHOD internally
                                  #E.g. hash computation with `hash` ACCESS_METHOD
                                  #Def (TYPE,...): same as (TYPE3,...)

create operator class ...         #TYPE used to store the values in the INDEX.
 storage TYPE                     #By default, same as the ones used in query.
                                  #Can differ, e.g. when value can be reduced for the purpose of the INDEX logic
                                  #  - e.g. a polygon reduced to an approximative rectangle

LOSSY OPERATOR ==>                #OP that can return some false positive, with a fast approximate logic
                                  #OP is then called again on the returned values, but with the slower exact logic ("index recheck")
LOSSY ACCESS METHODS ==>          #Some ACCESS_METHODs always need two steps, i.e. lossy as well

pg_opclass                        #TABLE with all OPCLASSs
pg_opclass.oid                    #OID
pg_opclass.opcname                #"OPCLASS" name
pg_opclass.opcmethod              #pg_am.oid of ACCESS_METHOD
pg_opclass.opcfamily              #pg_opfamily.oid of OPFAMILY
pg_opclass.opcintype              #REGTYPE of FUNC arguments
pg_opclass.opcdefault             #BOOL. True if default OPCLASS for the given TYPE
pg_opclass.opckeytype             #REGTYPE of values when stored in INDEX. 0 if same as FUNC arguments TYPE

pg_amop                           #TABLE with all OPFAMILYs OPs
pg_amop.oid                       #OID
pg_amop.amopfamily                #pg_opfamily.oid of OPFAMILY
pg_amop.amopstrategy              #UINT. Strategy number
pg_amop.amopopr                   #REGOPERATOR of OP
pg_amop.amopmethod                #pg_am.oid of ACCESS_METHOD
pg_amop.amoppurpose               #Either 's' (search) or 'o' (ordering)
pg_amop.amopsortfamily            #pg_opfamily.oid of OPFAMILY used for sorting, if ordering
                                  #0 if search

pg_amproc                         #TABLE with all OPFAMILYs support FUNCs
pg_amproc.oid                     #OID
pg_amproc.amprocfamily            #pg_opfamily.oid of OPFAMILY
pg_amproc.amprocnum               #UINT. Function number
pg_amproc.amproc                  #REGPROC of OP
pg_amproc.amprocleft|righttype    #REGTYPE of OP left|right argument

create index "INDEX" on "TABLE"   #OPCLASS used by an INDEX "COL"|REGEXP
 (... OPCLASS[(OOPTS)],...) ...   #OOPTS are OPCLASS-specific

pg_index.indclass                 #ARR of OPCLASSs (pg_opclass.oid) of INDEX
pg_index.indoption                #INT2_ARR of INDEX COLs' OOPTs

create table ... exclude
 ("COL"|(REXPR) OPCLASS,...) ...  #OPCLASS used by `exclude` (def: use underlying INDEX's ACCESS_METHOD)
create table ... partition by ...
 ("COL"|(COL_REXPR) OPCLASS)      #Same for `partition by`
create type "RANGE_TYPE" as
 range(subtype_opclass = OPCLASS) #Same for RANGE OPs

... order by ARG using OP         #Changes OP used by `order by` (def: < or >)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             BTREE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


btree                             #Default ACCESS_METHOD for INDEXs
                                  #Use case: most generic INDEX
                                  #Sometimes called "nbtree"

DATA STRUCTURE ==>                #Uses b-tree
                                  #Self-balancing k-way search tree
                                  #Each node contains a sequence of sub-nodes
                                  #  - if children, each sub-node is in-between 2 children (position-wise and value-wise)
                                  #  - i.e. optimized for sequential read|write of chunks
                                  #Each node is a memory page
                                  #Max index ROW: 2.5KB

TYPES/OPS ==>                     #For any TYPE with < <= = >= > <>
                                  #Except TYPEs without those OPs: HSTORE (except =), POINT|BOX|CIRCLE|POLYGON, CID|XID, ACLITEM

STR like '...%'                   #Can also be used
STR ~ '^...'                      #No glob|REGEXP characters in ...
                                  #Not if case-insensitive
                                  #If COLLATION's lc_collate is not C|POSIX:
                                  #  - must use non-default OPCLASS bpchar|varchar|text_pattern_ops for BPCHAR|VARCHAR|TEXT
                                  #  - this OPCLASS cannot handle < <= >= >

FEATURES ==>                      #  - ordered scan, including backward
                                  #  - is [not] null
                                  #  - VAL OP any|all (ARR)
                                  #  - distinct|grouping
                                  #  - exclude
                                  #  - unique
                                  #  - over (range NUM preceding|following): for NUM, DATE|TIME[STAMP][TZ]|INTERVAL
                                  #  - partition by range|list
                                  #  - plain index scan
                                  #  - bitmap index scan
                                  #  - index-only scan
                                  #  - parallel index scan
                                  #  - parallel index build
                                  #  - predicate lock
                                  #  - non-key COLs
                                  #  - cluster
                                  #  - ENVVAR maintenance_work_mem
                                  #  - IOPTS.fillfactor

MULTI-COL ==>                     #Only efficient when all the non-last COLs use =
                                  #  - the second-to-last COL can also use <>
                                  #  - according to COL order specified by `create index`, not by query

IOPTS.deduplicate_items           #BOOL (def: true). Store duplicate INDEX ROWs as single ROW
                                  #For BOOL, STR, INT|MONEY, BSTR, BYTEA, DATE|TIME[STAMP][TZ]|INTERVAL, ENUM, MACADDR[8]|INET, OID|TID|UUID|XID8|PG_LSN
                                  #Done on create index or reindex
                                  #Skipped if unique COLs
                                  #Usually good, except if:
                                  #  - very write-heavy INDEX
                                  #  - with not many duplicates, but no unique CONSTRAINT
                                  #Cannot be used with:
                                  #  - non-deterministic COLLATION
                                  #  - non-key COLs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             GIST              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


gist                              #ACCESS_METHOD
                                  #Use cases: specific TYPE|OPs

DATA STRUCTURE ==>                #Generalized balanced search tree
                                  #Generalized: any search tree can be implemented by OPCLASSs (B-Tree, etc.)
                                  #Allows using perfect search tree for a given TYPE and OPs

TYPES/OPS ==>                     #For:
                                  #  - TSQUERY|TSVECTOR with @> <@ @@
                                  #  - [MULTI]RANGE with << >> <@ @> &< &> && = -|-
                                  #  - INET with < <= = >= > <> && << <<= >> >>=
                                  #     - requires inet_ops OPCLASS
                                  #  - LTREE with < <= = >= > @> <@
                                  #  - LTREE[_ARR] with <@ @> ~ @ ?
                                  #  - INT4_ARR with = && @> <@ @@
                                  #     - requires gist__int[big]_ops OPCLASS
                                  #        - big_ops: prefer if many ROWs
                                  #  - HSTORE with @> <@ ? ?| ?&
                                  #  - BOX|CIRCLE|POLYGON|POINT with <-> << >> <@ @> &< &> && ~= <<| |>> &<| |&> <^ >^

btree_gist                        #Also for BOOL|NUM|MONEY|STR|CHAR|NAME|BYTEA|BSTR|DATE|TIME[STAMP][TZ]|INTERVAL|OID|UUID|MACADDR[8]|INET|CIDR|ENUM
                                  #With <= < = >= >
                                  #Usually slower than btree
                                  #  - except when creating multi-COL GiST indexes
                                  #Also has OPs
                                  #  - <>
                                  #  - NUM|MONEY|DATE|TIME[STAMP][TZ]|INTERVAL|OID <-> (ordering OP)
                                  #Trusted extension "btree_gist"

FEATURES ==>                      #  - order by BOX|CIRCLE|POLYGON|POINT <-> POINT (ordering operator) ("nearest-neighbor search")
                                  #  - is [not] null
                                  #  - exclude
                                  #  - plain index scan
                                  #  - bitmap index scan
                                  #  - index-only scans for INET|POINT
                                  #  - predicate lock
                                  #  - non-key COLs
                                  #  - cluster
                                  #  - IOPTS.fillfactor

MULTI-COL                         #Only efficient when the first COLs filter out many ROWs
                                  #For LTREE[_ARR], HSTORE, TSVECTOR

IOPTS.buffering                   #BOOL|'auto' (def)
                                  #Cache ROW inserts, for speed
                                  #If auto: when INDEX size > ENVVAR effective_cache_size

OOPTS.siglen                      #NUM (max: 2024)
                                  #Def: 124 (HSTORE), 16 (TSQUERY|INT4_ARR), 8 (LTREE), 28 (LTREE_ARR)
                                  #Some TYPE|OPs use a hash of data for comparison, i.e. lossy
                                  #This its the size (in bytes)
                                  #Higher gives better results but higher INDEX size

OOPTS.numranges                   #NUM (def: 100, max: 253)
                                  #With gist__intbig_ops OPCLASS
                                  #Number of INT_RANGEs to use to summarize each INT4_ARR
                                  #Higher value is faster, but higher INDEX size


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SPGIST             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


spgist                            #ACCESS_METHOD
                                  #Use cases: like GiST but more performant for values that intersect a lot

DATA STRUCTURE ==>                #Like GiST but for unbalanced search trees

TYPES/OPS ==>                     #For:
                                  #  - STR with < <= = >= > ~<~ ~<=~ ~>=~ ~>~ ^@
                                  #  - RANGE with << >> <@ @> = -|-
                                  #  - BOX|POLYGON|POINT with <-> << >> <@ @> &< &> && ~= <<| |>> &<| |&> <^ >^
                                  #     - kd_point_ops: non default OPCLASS for POINT, sometimes faster
                                  #  - INET with < <= = >= > <> && << <<= >> >>=

FEATURES ==>                      #  - order by BOX|POLYGON|POINT <-> POINT (ordering operator)
                                  #  - is [not] null
                                  #  - exclude
                                  #  - plain index scan
                                  #  - bitmap index scan
                                  #  - index-only scans, except for POLYGON
                                  #  - predicate lock
                                  #  - non-key COLs
                                  #  - IOPTS.fillfactor


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              GIN              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


gin                               #ACCESS_METHOD
                                  #Use case: set OPs (subset|superset|intersect), especially with duplicate set elements
                                  #Generally more performant than gist for TSVECTOR|INT4_ARR|HSTORE

DATA STRUCTURE ==>                #Generalized Inverted Index
                                  #B-Tree specialized for storing sets
                                  #Each node is an element of a set, instead of set itself
                                  #I.e. slower inserts due to doing one insert per set element
                                  #Duplicate elements are merged as a single list of TIDs

TYPES/OPS ==>                     #For:
                                  #  - ARR with = && @> <@
                                  #  - INT4_ARR with = && @> <@ @@
                                  #     - gin__int[big]_ops OPCLASS
                                  #        - big_ops (def): prefer if many ROWs but lacks @> <@
                                  #  - TSQUERY @@ TSVECTOR
                                  #  - HSTORE with @> <@ ? ?| ?&
                                  #  - JSONB with @> <@ ? ?| ?& @? @@
                                  #     - jsonb_path_ops OPCLASS
                                  #        - only for OPs @> <@ @? @@
                                  #        - non-default OPCLASS that is more performant

btree_gin                         #Also for BOOL|NUM|MONEY|STR|CHAR|NAME|BYTEA|BSTR|DATE|TIME[STAMP][TZ]|INTERVAL|OID|UUID|MACADDR[8]|INET|CIDR|ENUM
                                  #With <= < = >= >
                                  #Usually slower than btree
                                  #Only useful when creating multi-COL GIN indexes
                                  #Trusted extension "btree_gin"

FEATURES ==>                      #  - bitmap index scan
                                  #  - predicate lock
                                  #  - ENVVAR maintenance_work_mem

MULTI-COL ==>                     #Efficient regardless of which OP is used, and in any order

IOPTS.fastupdate                  #BOOL (def: true)
                                  #Use a "pending list", i.e. buffer elements to insert, so they are inserted in bulk
                                  #Emptied on either:
                                  #  - ENVVAR|IOPTS.gin_pending_list_limit INT (in KB) (def: 4MB)
                                  #     - higher values happens less often, but take longer
                                  #  - vacuum|analyze
                                  #  - gin_clean_pending_list()
                                  #Pro: faster inserts
                                  #Con: slower reads|updates, since pending list must be queried too

ENVVAR gin_fuzzy_search_limit     #NUM (def: 0, i.e. none)
                                  #Max tokens returned by TSQUERY @@ TSVECTOR
                                  #Pro: performance on big TSVECTOR
                                  #Con: less accurate results
                                  #Values of 5e3-20e3 usually good


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             BRIN              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


brin                              #ACCESS_METHOD
                                  #"Block Range INdex"
                                  #Use case:
                                  #  - Very large TABLEs
                                  #  - Data read close to each other, e.g. in chunks
                                  #  - Indexed values increase mostly monotonically

DATA STRUCTURE ==>                #Splits data into block ranges
                                  #Block ranges have a min|max value
                                  #  - better if they do not overlap each other too much
                                  #Block ranges maps to filesystem pages
                                  #Finding right block is fast, but does a sequential read inside the block
                                  #Low memory and CPU overhead of INDEX structure

FEATURES ==>                      #  - is [not] null
                                  #  - bitmap index scan

MULTI-COL ==>                     #Efficient regardless of which OP is used, and in any order

SUMMARIZATION ==>                 #Adding min|max to a block range
                                  #New block ranges are not automatically summarization
                                  #Done on vacuum on the TABLE
IOPTS.autosummarize               #BOOL (def: false). If true, also summarize:
                                  #  - when vacuum on any other TABLE
                                  #  - on any non-last block range
brin_summarize_new_values
 (INDEX_REGCLASS)->UINT           #Summarize all unsummarized block ranges
brin_summarize_range
 (INDEX_REGCLASS, UINT)->UINT2    #Summarize specific block range
brin_desummarize_range            #Remove summary on specific block range
 (INDEX_REGCLASS, UINT)->UINT2    #Goal: when a block range's summary is not accurate anymore due to updates

IOPTS.pages_per_range             #NUM (def: 128) of page heaps per block range
                                  #Should be close to NUM of page heaps usually read by queries
                                  #  - if too high: longer sequential scan
                                  #  - if too low: higher memory|CPU taken by INDEX

CATEGORIES ==>                    #Has multiple categories: minmax, minmax_multi, bloom, inclusion
                                  #Depending on indexed values TYPEs and OPCLASSs

minmax                            #Category for < <= = >= >
                                  #For any TYPE except BOOL, ENUM, MONEY, ARR, RECORD, MULTIRANGE, HSTORE, JSONB, LTREE, CIRCLE|POINT|POLYGON, TSQUERY|TSVECTOR, CID|XID|XID8
                                  #No cross-TYPE OPs for BPCHAR|VARCHAR|TEXT
                                  #Uses normal min|max, i.e. most generic category

minmax_multi                      #Category for same types as `minmax` with < <= = >= >
                                  #  - except B[STR] and STR
                                  #Uses multiple min|max per block range
                                  #  - i.e. good when values have gaps
                                  #Non-default OPCLASSs
OOPTS.values_per_range            #NUM (def: 32, min 8, max 256)
                                  #Max NUM of values used for the min|max ranges
                                  #I.e. max NUM/2 min|max ranges

bloom                             #Category for same types as `minmax` with =
                                  #Uses a Bloom filter
                                  #I.e. good for small INDEX size with OP =, providing false positives are ok
                                  #Non-default OPCLASSs
OOPTS.false_positive_rate         #NUM (def: 0.01, min 0.0001, max 0.25)
                                  #Lower values requires larger INDEX size
OOPTS.n_distinct_per_range        #NUM. Similar to TOPS.n_distinct, but meant as a hint for Bloom filter to reduce its size

inclusion                         #Category for:
                                  #  - RANGE with << >> && &< &> @> <@ -|- < <= = >= >
                                  #  - BOX with << >> && &< &> @> <@ ~= &<| |&> <<| |>>
                                  #  - INET with << >> >>= <<= && =
                                  #Uses multidimensional min|max specific to those TYPEs, e.g. bounding boxes


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             HASH              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


hash                              #ACCESS_METHOD
                                  #Use case: OP = on large TABLE with unique values

DATA STRUCTURE ==>                #Hash table
                                  #Key is hash of data
                                  #  - 4 bytes
                                  #Value are ROW ids with that hash
                                  #  - i.e. slow if many values are duplicates
                                  #Fast with = on many ROWs
                                  #When TABLE size increase, need to split buckets, which is slow and locks the table
                                  #  - i.e. not good if TABLE size increases rapidly
                                  #Always lossy, because requires searching among values with same checksum

TYPES/OPS ==>                     #For any TYPE with =
                                  #  - except BSTR, MONEY, LTREE, TSQUERY|TSVECTOR, POINT|BOX|CIRCLE|POLYGON
                                  #  - no cross-TYPE OPs for DATE|TIMESTAMP[TZ]

FEATURES ==>                      #  - distinct|grouping
                                  #  - exclude
                                  #  - partition by hash
                                  #  - plain index scan
                                  #  - bitmap index scan
                                  #  - predicate lock
                                  #  - IOPTS.fillfactor


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             BLOOM             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


bloom                             #ACCESS_METHOD
                                  #Use case: OP = with many duplicates
                                  #  - sometimes faster than btree when queries use many indexed COLs at once with OP =
                                  #Non-trusted extension "bloom"

DATA STRUCTURE ==>                #Bloom filter
                                  #Lossy due false positives being rechecked
                                  #I.e. true is slower than false

TYPES/OPS ==>                     #For: STR|INT4 with =

FEATURES ==>                      #  - plain index scan
                                  #  - bitmap index scan

MULTI-COL ==>                     #Only efficient when all the non-last COLs use =

OOPTS.length                      #NUM (def: 80, max: 4096)
                                  #Bloom filter size in bytes, for all COLs together
                                  #Larger size are faster

OOPTS.col1-32                     #NUM (def: 2, max: 4095)
                                  #Bits taken by individual COLs inside Bloom filter


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             HEAP              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


heap                              #ACCESS_METHOD
                                  #Unlike other ACCESS_METHODs, meant for TABLEs, not INDEXs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       PARALLEL QUERIES        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARALLEL QUERY ==>                #COMMAND using multiple worker processes in parallel
                                  #Must not:
                                  #  - lock any ROW
                                  #  - be inside another parallel query
PARALLEL COMMAND ==>              #Must be SUBQUERY
                                  #  - or equivalent: create|refresh materialized view, create table as, select into
                                  #Can be:
                                  #  - sequential|index|bitmap scans
                                  #  - joins
                                  #  - partial AFUNCs
                                  #  - union all, partitioning
PARTIAL PLAN ==>                  #Part of the plan executed by a parallel worker

ENVVAR max_parallel_workers       #Max NUM (def: 8) of parallel worker processes
                                  #Must be < ENVVAR max_worker_processes
                                  #Increase memory, CPU and I/O linearly, i.e. resource-intensive
                                  #ENVVAR work_mem is applied for each worker, not for all
ENVVAR                            #Max NUM (def: 2) of parallel worker processes for parallel queries
 max_parallel_workers_per_gather  #Must be < ENVVAR max_parallel_workers

ENVVAR enable_gathermerge         #"Gather merge": parallel query that is sorted, i.e. parent process performs a merge sort

ENVVAR                            #If true (def), parent process can perform some of the workers task instead of waiting for them
 parallel_leader_participation    #However, it increases chances for workers to wait for parent to process their output,
                                  #since parent might be busy.
                                  #  - this is especially likely if many ROWs are returned from workers

create|alter function|aggregate   #Whether [A]FUNC can be:
 ...                              #  - safe: run in parallel workers or their parent
 parallel safe|restricted|unsafe  #  - restricted: not run in parallel workers, but in their parent
                                  #  - unsafe (def): not run in parallel workers nor their parent
                                  #I.e. restricted|unsafe removes the possibility for parallel queries
                                  #Parallel safety requires being purely functional
                                  #Most built-in FUNCs are parallel safe


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            EXPLAIN            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


QUERY PLANNER ==>                 #Internal logic deciding best way to execute COMMANDs

explain [(ZOPTS)] COMMAND         #Show the query plan of COMMAND
                                  #COMMAND can be select|table|values|execute, create table|materialized view as, insert|update|delete|merge, declare
                                  #Should prefer real data close to production, if possible

explain verbose ...               #BOOL (def: false)
ZOPTS.verbose                     #Whether to print more fields

explain analyze ...               #BOOL (def: false)
ZOPTS.analyze                     #Executes the COMMAND, allowing to print fields related to actual execution
                                  #Can use transaction + rollback to avoid side effects

ZOPTS.format                      #text (def) or json|yaml|xml
                                  #text does not show values that are 0

ZOPTS.costs                       #BOOL (def: true)
                                  #Whether to print (costs|rows|width ...)

ZOPTS.timing                      #BOOL (def: true)
                                  #Whether to print (actual time ...)
                                  #Requires ZOPTS.analyze true

ZOPTS.summary                     #BOOL (def: true)
                                  #Whether to print Planning|Execution Time

ZOPTS.buffers                     #BOOL (def: false)
                                  #Whether to print Buffers ...

ZOPTS.wal                         #BOOL (def: false)
                                  #Whether to print WAL ...
                                  #Requires ZOPTS.analyze true

ZOPTS.settings                    #BOOL (def: false)
                                  #Whether to print relevant ENVVARs which value was changed

ENVVAR enable_*                   #Whether specific query optimizations are enabled
                                  #All BOOL with true by default, except enable_partitionwise_*
                                  #Can set to false to set alternative plans considered by `explain`
                                  #Some cannot be completely disabled
                                  #  - i.e. false only discourages it
                                  #  - for: enable_seqscan|sort|nestloop|material


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         EXPLAIN TREE          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TREE ==>                          #Output is a tree with following nodes.
                                  #Each node results include its children.
                                  #Children are executed before parents.

Insert on TABLE                   #`insert`
Update on TABLE                   #`update`
Delete on TABLE                   #`delete`

... on CHILD_TABLE PARENT_TABLE   #Inheritance

Gather [Merge]                    #Parallel query
Workers Planned: NUM              #NUM of parallel worker processes, expected
Workers Launched: NUM             #NUM of parallel worker processes

[Parallel] Seq Scan on TABLE      #

Index [Only] Scan [Backward]
 using INDEX on TABLE             #
Index Cond: EXPR                  #Query "COL"|(REXPR) matching the INDEX's
Rows Removed by Index Recheck: NUM#ROWs excluded by lossy operator recheck
Heap Fetches: UINT                #Index-only heap fetches

Bitmap Heap Scan on TABLE         #
Bitmap Index Scan on TABLE        #
BitmapAnd                         #... and ... (bitmap scan combination)
BitmapOr                          #... or ... (bitmap scan combination)
Recheck Cond: EXPR                #Recheck condition

Values Scan on "..."              #values(...)

TID Scan on TABLE                 #
TID Cond: EXPR                    #`where`

Result                            #COMMAND return value
Output                            #COMMAND return value contents

Filter: EXPR                      #where|having
Rows Removed by Filter: NUM       #ROWs excluded by Filter

Sort                              #Explicit sort
Sort Key: EXPR                    #Explicit sort argument
Sort Method:
 quicksort Memory: NUM
Sort Method:
 top-N heapsort Memory: NUM
Sort Method:
 external merge Disk: NUM         #Explicit sort methods

Incremental Sort                  #
Presorted Key: EXPR               #Incremental sort argument that is sorted

Materialize                       #Materialization of a ROW_SET

InitPlan NUM                      #(SUBQUERY) that does not use upper queries
                                  #NUM is execution order
SubPlan NUM                       #(SUBQUERY) that uses upper queries, i.e. does a join
Subquery Scan on "QUERY"          #(SUBQUERY) that uses upper queries

... [Left|Right|Full] [Anti] Join #

Nested Loop                       #Nested loop join
Join Filter                       #Nested loop join condition (where|on)
Memoize                           #Nested loop join memoization

Hash Join                         #
Hash Cond: EXPR                   #Hash join condition (where|on)
Hash                              #Creation of hash table for a ROW_SET during a hash join
Buckets: NUM                      #Hash table size
Memory Usage: STR                 #Hash table creation peak memory
Batches: NUM                      #Number of rounds to create hash table
Disk Usage: STR                   #To persist data in-between hash table batches

Merge Join                        #
Merge Cond: EXPR                  #Merge join condition (where|on)

Limit                             #offset|fetch

Unique                            #distinct

[Parallel] Append                 #union all, or partitioning
[Parallel] MergeAppend            #union all + order by
Setop Intersect                   #intersect
Setop Except                      #except
HashSetOp ...                     #Same but using hash aggregation

Aggregate                         #AFUNC
Partial Aggregate                 #Parallel AFUNC worker process
Finalize Aggregate                #Parallel AFUNC parent process, merging workers results

Group                             #group by without AFUNCs
GroupAggregate                    #Group aggregation (group by)
HashAggregate                     #Hash aggregation (group by, or distinct)
MixedAggregate                    #grouping sets
Group Key                         #group by's argument

WindowAgg                         #WFUNC

ProjectSet                        #select FUNC()->SET
Function Scan on FUNC             #from FUNC()->SET

CTE ROW_ALIAS                     #with "ROW_ALIAS"
CTE Scan on ROW_ALIAS             #Using CTE "ROW_ALIAS"
RecursiveUnion                    #with recursive ... select ... union ...
WorkTable Scan                    #Recursive loop in `with recursive`

LockRows                          #select ... for ... update|share


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         EXPLAIN COST          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


(costs=NUM..NUM2)                 #Cost. Decides between different strategies
                                  #NUM is "startup cost" (initialization logic), NUM2 is "total cost"
                                  #Can be multiplied by factors set in ENVVARs
ENVVAR parallel_setup_cost        #NUM (def: 1000). Cost of starting a worker process
ENVVAR random_page_cost           #NUM (def: 4). Cost of random I/O read of a heap page
ENVVAR seq_page_cost              #NUM (def: 1). Cost of sequential I/O read of a heap page
ENVVAR parallel_tuple_cost        #NUM (def: 0.1). Cost of sending a ROW from a worker to the parent process
ENVVAR cpu_tuple_cost             #NUM (def: 0.01). Cost of CPU processing of a TABLE ROW
ENVVAR cpu_index_tuple_cost       #NUM (def: 0.005). Cost of CPU processing of an INDEX ROW
ENVVAR cpu_operator_cost          #NUM (def: 0.0025). Cost of CPU execution of a FUNC

ENVVAR effective_cache_size       #STR (def: '4GB'). Hints how much RAM is available.
                                  #RAM is mostly used by the OS to cache I/O by keeping files in-memory.
                                  #I.e. more RAM makes the cost of I/O cheaper.
                                  #Does not change actual cache size.
                                  #Should be 50-75% of the available RAM.

(actual loops=UINT2)              #Number of times this inner node was performed

(actual time=NUM..NUM2)           #NUM[2] (in ms) initialization|total time spent, per inner node loop
Planning Time                     #Time to plan the query
Execution Time                    #Time to execute the query, excluding Planning Time

(rows=UINT)                       #Number of ROWs returned|written expected, per inner node loop
(actual rows=UINT)                #Same but actual number, after query performed
(width=UINT)                      #Mean size (in bytes) of a ROW

Buffers: ...                      #Used to debug both I/O caching, and amount of I/O
Buffers: shared hit|read=NUM      #Heap pages hit|misses
Buffers: shared dirtied=NUM       #Heap pages dirtied
Buffers: shared written=NUM       #Heap pages written
Buffers: local ...                #Same for local heap pages
Buffers: temp ...                 #Same for temporary heap pages

WAL: fpi=UINT                     #Number of full heap pages added to WAL
WAL: records=UINT                 #Number of ROWs added to WAL
WAL: bytes=UINT                   #Number of bytes added to WAL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            ANALYZE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


analyze [verbose]                 #Fill in pg_statistic.
 ["TABLE"[("COL"...)]]            #Parameters are like for vacuum.
                                  #Requires only an access share lock.
                                  #FOREIGNTABLE are analyzed only when explicitly specified, and not always supported.
                                  #Should be run before `explain` if lot of change since last time autovacuum did (because based on pg_statistic)
                                  #Updates pg_class.relpages|reltuples|relallvisible (like vacuum does)

pg_stat_progress_analyze          #TABLE with ongoing `analyze` statements. ???

pg_statistic                      #Stats used by the planner to optimize queries (only indexed COL).
                                  #Not exact stats, because only a random sample of the rows is chosen for efficiency purpose.
                                  #Example of statistics: number of entries, distinct entries, histograms, size (number of disk blocks)
                                  #ENVVAR default_statistics_target (def: 100, from 0 to 10000) decides the sample size.
                                  #Can be set column-wise with:
                                  #  alter table|index ... alter [column] "COL" set statistics INT (-1 means default)
                                  #(same for FTABLE and materialized views).
                                  #  - starelid REGCLASS: of the TABLE
                                  #  - staatnum UINT: COL index. Refers to pg_attribute.attnum.
                                  #  - stainherit BOOL: all COLs have false + (if inherited COL) a row with true, with inherited
                                  #    version of the TABLE
                                  #  - stanullfrac FLOAT: percentage of null values
                                  #  - stawidth INT: average size of null values
                                  #  - stadistinct FLOAT: number of repetitions: -NUM if repetitions (UNIQUE/TOTAL, closer to 0 if
                                  #    many repetitions), +NUM means no repetitions (UNIQUE), 0 means unknown
                                  #  - for NUM statistics:
                                  #     - stakindNUM INT: subtype of the statistic (code number)
                                  #     - staopNUM REGOPER: OP used
                                  #     - stanumbersNUM FLOAT_ARR: stats as FLOAT, or null if COL is not numerical
                                  #     - stavaluesNUM ARR: stats as the same type as the COL

pg_stats                          #User-friendly version of pg_statistic

https://www.postgresql.org/docs/current/planner-stats.html

pg_statistic_ext
pg_statistic_ext_data
pg_stats_ext
pg_stats_ext_exprs                #???

COPTS.n_distinct[_inherited]      #NUM ???

ENVVAR default_statistics_target ???

pg_class.relpages|reltuples       #Set only by analyze, vacuum and some DDL commands like create index
 |relallvisible                   #I.e. estimate only since it might have changed
                                  #Used by the query planner
pg_class.relpages                 #INT4. Size of the file on-disk, in 8KB pages.
pg_class.reltuples                #FLOAT4. Number of non-dead ROWs
                                  #-1 if VIEW|ROW_TYPE, or if not analyze|vacuum'd yet

pg_attribute.attstattarget int4 attstattarget controls the level of detail of statistics accumulated for this column by ANALYZE. A zero value indicates that no statistics should be collected. A negative value says to use the system default statistics target. The exact meaning of positive values is data type-dependent. For scalar data types, attstattarget is both the target number of “most common values” to collect, and the target number of histogram bins to create.

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           HEAP PAGE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


HEAP ==>                          #Memory storing TABLEs (as opposed to INDEXs)

HEAP PAGE ==>                     #8KB memory page containing 1-n TABLE ROWs
                                  #Also called "blocks"

ROW SIZE ==>                      #Min size: 32 bytes (due to SYSTEM COLs)
                                  #  - i.e. max 256 ROWs per heap
                                  #Max size: 8KB (due to heap page max size)
                                  #With TOAST: max 400GB of actual data

HEAP PAGE NUMBER ==>              #Heap page index within a TABLE
                                  #Stored as INT4, i.e. from 0 to 4e9
                                  #Also called "block number" or "blkno"

TABLE SIZE ==>                    #Max size: 32TB (due to heap number max number)

"ROW_ALIAS".ctid                  #TID of the ROW
tid                               #TYPE. Location of a ROW within its TABLE.
                                  #Is (INT4, INT2)
                                  #  - INT4 is heap page number
                                  #  - INT2 is index within heap page
                                  #Changed on write statements
                                  #  - including update, vacuum full
                                  #  - excluding delete
'(UINT, UINT2)'                   #TID_UNKNOWN
ENVVAR enable_tidscan             #"TID scan". When using ctid with `where|on`, can directly fetch the ROWs
                                  #without scanning other ROWs, since the ctid contains the ROW physical location.

HEAP PAGE TYPES ==>               #Heap pages can be:
                                  #  - "shared": non-temp TABLE
                                  #  - "local": temp TABLE
                                  #  - "temporary": short-term ROW_SET, e.g. during sorts, joins, etc.

HEAP PAGE CACHING ==>             #Heap pages can be cached
                                  #  - "hit": when uses cache
                                  #  - "read": when not used
                                  #Heap pages with writes are "dirtied" (using visibility map)
                                  #  - "write": when a dirtied heap page is evicted from cache
                                  #Temporary heap pages are never cached

pg_class.relfilenode              #OID. RELATION's filename on-disk.
                                  #0 if VIEW|ROW_TYPE
                                  #Also 0 on some pg_catalog.* TABLEs: cluster-wide ones, and pg_class|attribute|type|proc


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        FREE SPACE MAP         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


HEAP PAGE FREE SPACE ==>          #Amount of unused bytes
                                  #Only 32 bytes-precise

FREE SPACE MAP ==>                #Data structure storing amount of unused bytes of each heap page
                                  #Binary search tree, with one leaf node per heap page
                                  #Used to know which heap page to use in inserts|updates
                                  #New nodes are automatically created on page heap creation|deletion
                                  #However, free space value is only updated by vacuum
                                  #  - i.e. might not perfectly match current state
                                  #  - new nodes have incorrect value 0 (i.e. no bytes available) until next vacuum
                                  #Stored as a `*_fsm` file sibling to TABLE file
                                  #Also called FSM

pg_freespace(REGCLASS, INT8)->INT2#Returns amount of free space (in bytes) of heap page number INT8 of REGCLASS, according to FSM
                                  #Non-trusted Postgres extension 'pg_freespacemap'
pg_freespace(REGCLASS)->ROW_SET   #Same for all heap pages of REGCLASS, as ROW: blkno INT8, avail INT2


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          LAST WRITE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


"ROW_ALIAS".cmin|cmax             #CID of the last write statement on the ROW
                                  #Used for versioning the ROW, e.g. with MVCC
cid                               #TYPE. Index of a write statement within a given DATABASE + transaction (XID)
                                  #Incrementing, starting at 0
                                  #Cannot use < <= > >=
CID <~=> TEXT                     #Type cast

"ROW_ALIAS".xmin|xmax             #XID of the last write transaction on the ROW
xid[8]                            #TYPE. Index of a write transaction within a given DATABASE
                                  #Incrementing, starting at 2
                                  #1 is a special value ("bootstrap XID") reserved for ROWs created during initdb
                                  #Cannot use < <= > >=
XID[8] <~=> TEXT                  #Type cast

cid|xid                           #4 bytes. Might be re-used in the lifetime of cluster
xid                               #Min|max values are circular
                                  #I.e. previous|next 2e9 values are considered <|>, when taking wraparound into account
xid8                              #8 bytes. Never re-used in the lifetime of cluster
XID8 ~> XID                       #Type cast

"ROW_ALIAS".xmin|ctid             #For state visible in current session:
                                  #  - set if committed by current|other sessions, or if uncommitted by current session
                                  #  - unchanged if uncommitted by other sessions
                                  #On:
                                  #  - commit: other sessions set it, i.e. like current session
                                  #  - rollback: current session reverts it, i.e. like other sessions
                                  #I.e. xmin always <= current transaction's XID (with wraparound considered)
"ROW_ALIAS".xmax                  #For state not visible yet in current session:
                                  #  - 0 if committed by current|other sessions, or if uncommitted by current session
                                  #  - set if uncommitted by other sessions
                                  #On:
                                  #  - commit: other sessions reset it to 0, i.e. like current session
                                  #  - rollback: current session sets it, i.e. like other sessions
"ROW_ALIAS".cmin|cmax             #Concurrency behavior is like xmin for insert|update, and like xmax for delete
                                  #There is no difference between cmin and cmax


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          FROZEN ROWS          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_class.relfrozenxid             #XID. Minimum xmin of the TABLE.
                                  #Set by vacuum. Initially 0.
                                  #Any ROW with xmax <= relfrozenxid is "frozen" by vacuum (using visibility map) ???
                                  #It indicates that the ROW:
                                  #  - is an uncommitted transaction (xmin <> xmax)
                                  #  - is long enough that XID wrapped around
                                  #     - 4-bytes long, i.e. every 2e9 transactions
freezing: always visible, even if xid appears >
vacuum freezes ROWs if old
relfrozenxid: all xid < are frozen

pg_database.datfrozenxid          #Minimum value for relfrozenxid

ENVVAR vacuum_freeze_min_age      #NUM (def: 5e7). Do not freeze ROWs if relfrozenxid younger than NUM transactions
                                  #Must be at least half of autovacuum_freezen_max_age
ENVVAR autovacuum_freeze_max_age  #NUM (def: 2e8). Freeze ROWs if relfrozenxid older than (NUM + vacuum_freeze_min_age) transactions
                                  #Only with autovacuum
ENVVAR vacuum_freeze_table_age    #NUM (def: 1.5e8). Only check for frozen ROWs in heap pages with dead ROWs,
                                  #unless relfrozenxid is older than NUM transactions

vacuum freeze ...                 #Force freezing ROWs, i.e. ignore ENVVAR vacuum_freeze_min|max_age
vacuum (freeze = true)            #Implied by `vacuum full`
                                  #Might break MVCC

pg_class.relminmxid               #XID. Like pg_class.relfrozenxid, but for multixact IDs ???
pg_database.datminmxid            #XID. Same for pg_database.datfrozenxid


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          VISIBILITY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DEAD ROWS ==>                     #Deleted ROWs are kept until vacuum
                                  #This allows ongoing transactions to keep seeing them if either:
                                  #  - they have not committed yet and were started before the deletion
                                  #  - the deleting transaction has not committed yet
UPDATED ROWS ==>                  #`update` does not modify ROWs in-place: it creates a new copy, for the same reason
                                  #I.e. it also creates dead ROWs

VISIBILITY MAP ==>                #Bitmap of all heap pages of a TABLE
                                  #Contains 2 bits per heap page.
                                  #"all-visible bit":
                                  #  - set to 0 by any write
                                  #  - set to 1 by vacuum
                                  #  - i.e. 0 means "might" contain uncommitted writes, or dead ROWs
                                  #  - i.e. 1 means latest data can "certainly" be read by any current|future transaction
                                  #  - also set as a PD_ALL_VISIBLE bit in the data itself, as opposed to in the visibility map
                                  #"all-frozen bit":
                                  #  - 1 if has only frozen ROWs
                                  #  - set to 1 by vacuum
                                  #  - set to 0 by ???
                                  #Fast to access
                                  #  - bitmap, i.e. small
                                  #  - usually kept in-memory
                                  #Saved in a `*_vm` file, next to TABLE physical file

pg_visibility[_map]               #Returns visibility map of heap page number INT8 of REGCLASS
 (REGCLASS, INT8)->ROW            #Non-trusted extension 'pg_visibility'
ROW.all_visible                   #BOOL
ROW.pd_all_visible                #BOOL
                                  #Not returned if `_map` (faster)
ROW.all_frozen                    #BOOL

pg_visibility[_map]
 (REGCLASS)->ROW_SET              #Returns visibility map of all heap pages of REGCLASS
pg_visibility_map_summary
 (REGCLASS)->ROW                  #Counts as ROW: all_visible|all_frozen INT8

pg_check_visible|frozen           #Returns TIDs of ROWs with 0 bits inn all_visible|frozen,
 (REGCLASS)->TID_SET              #which mismatches actual state of the ROW, i.e. visibility map is corrupted
pg_truncate_visibility_map        #Remove visibility map. It will be rebuilt by next vacuum.
 (REGCLASS)                       #Meant if visibility map corrupted

pg_class.relallvisible            #INT4. Number of RELATION's heap pages with all-visible bit in the visibility map.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            VACUUM             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


vacuum (OPTS)                     #Does:
 ["TABLE"[("COL",...)]]           #  - delete dead ROWs on:
                                  #     - TABLEs
                                  #     - INDEXs ("index cleanup")
                                  #        - only after a specific amount of dead ROWs on TABLE have accumulated
                                  #  - freeze ROWs
                                  #  - update visibility map
                                  #  - update FSM values
                                  #  - update pg_class.relhas*|relpages|reltuples|relallvisible
                                  #  - update pg_stat.*
                                  #Def "TABLE": all that current ROLE has permissions
                                  #Def "COL": all
                                  #Cannot be done inside a transaction
                                  #Makes database slower during vacuuming

vacuum analyze ...
OPTS.analyze                      #BOOL (def: false). Run `analyze`

vacuum verbose ...
OPTS.verbose                      #BOOL (def: false). Print details

vacuum full ...                   #BOOL (def: false)
OPTS.full                         #Unless specified:
                                  #  - deleted dead ROWs can be used for new writes
                                  #  - but the memory taken by their heap page, if empty, is not freed to the OS
                                  #  - i.e. the TABLE size does not shrink (from an OS standpoint)
                                  #Do a full TABLE rewrite, i.e. requires `exclusive` lock
                                  #Slow and should be used only when necessary

OPTS.truncate                     #BOOL. When true (def), free the OS space of empty heap pages
TOPTS.vacuum_truncate             #(due to deleted dead ROWs), if at the end of TABLE.
                                  #Requires `access exclusive` lock when this happens
                                  #  - skipped if cannot obtain the lock

OPTS.index_cleanup                #on|off or auto (def). Whether to delete dead ROWs on INDEXs
TOPTS.vacuum_index_cleanup        #If auto, done if there are enough dead ROWs on the TABLE of the INDEXs
                                  #Always `on` if `vacuum full`

OPTS.disable_page_skipping        #BOOL. If false (def), ignore heap pages with a visibility map having either:
                                  #  - all_visible 1
                                  #     - except with `vacuum full`
                                  #  - all_frozen 1
                                  #Also ignores some heap pages if locked by other transactions
                                  #Can be set to true if visibility map is corrupted

OPTS.skip_locked                  #BOOL (def: false). `vacuum` requires `shared update exclusive` lock.
                                  #If true and the lock cannot be obtained immediately, skip vacuuming the specific TABLE.
                                  #Only for normal TABLEs, not for its INDEXs, child TABLEs, or foreign TABLEs.

OPTS.parallel                     #Max NUM of parallel workers when vacuuming INDEXs
                                  #Max 1 per INDEX on a given TABLE
                                  #Def: max, i.e. NUM of INDEXs on the TABLE
                                  #Only on INDEXs matching ENVVAR min_parallel_table_scan_size
                                  #Cannot be used with `vacuum full`

pg_class.relhasindex|relhasrules  #Set to true immediately, but only set to false by `vacuum`.
 |relhastriggers|relhassubclass   #I.e. might be true if ENTITY was dropped recently

pg_stat_progress_vacuum           #TABLE with ongoing `vacuum` statements

pg_stat_progress_vacuum.relid     #REGCLASS of TABLE
pg_stat_progress_vacuum.datid     #pg_database.oid of DATABASE
pg_stat_progress_vacuum.datname   #NAME of "DATABASE"
pg_stat_progress_vacuum.pid       #INT4. PID of server

pg_stat_progress_vacuum.phase     #STR among:
                                  #  - 'initializing'
                                  #  - 'scanning heap': read heap, freeze ROWs
                                  #  - 'vacuuming indexes': remove dead ROWs of INDEX
                                  #  - 'vacuuming heap': remove dead ROWs of TABLE, update visibility map
                                  #  - 'cleaning up indexes'
                                  #  - 'truncating heap'
                                  #  - 'performing final cleanup': update FSM, pg_class.rel*, pg_stat*
pg_stat_progress_vacuum
 .heap_blks_total                 #INT8. Amount of heap pages in total
pg_stat_progress_vacuum
 .heap_blks_scanned               #INT8. Amount of heap pages read, progressing during 'scanning heap'
pg_stat_progress_vacuum
 .heap_blks_vacuumed              #INT8. Number of heap pages vacuumed, progressing during 'vacuuming heap'
pg_stat_progress_vacuum
 .num_dead_tuples                 #INT8. Number of dead ROWs collected on TABLE during 'vacuuming heap'
pg_stat_progress_vacuum
 .max_dead_tuples                 #INT8. After that many number of dead ROWs, INDEX will need to be vacuumed
pg_stat_progress_vacuum
 .index_vacuum_count              #INT8. Number of completed rounds during 'vacuuming indexes'


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          AUTOVACUUM           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENVVAR autovacuum                 #BOOL (def: false). Automatically run `vacuum [analyze]` on TABLEs
                                  #Not run on TEMP TABLEs nor CHILD_TABLEs (including through partition)
                                  #`analyze` not run on FTABLEs
ENVVAR
 autovacuum_vacuum_scale_factor   #NUM (def: 0.2)
ENVVAR autovacuum_vaccum_threshold#NUM (def: 50). Run autovacuum after NUM writes (inserted|updated|deleted ROWs) * autovacuum_vacuum_scale_factor
ENVVAR
 autovacuum_analyze_threshold     #NUM (def: 50). Same for `vacuum analyze`
ENVVAR autovacuum_freeze_*        #Run autovacuum anytime ROWs must be unfrozen, according to ENVVAR vacuum_freeze_*
                                  #which can be overriden by this.
ENVVAR autovacuum_naptime         #STR (def: '1min'). Mininum duration between 2 autovacuums, per DATABASE
ENVVAR autovacuum_max_workers     #NUM (def: 3) of DATABASEs that can be vacuumed at the same time

TOPTS.*autovacuum*                #Set ENVVAR *autovacuum* for this TABLE
                                  #Not for: autovacuum_naptime|max_workers
                                  #Different names:
                                  #  - autovacuum -> autovacuum_enabled
                                  #  - vacuum_freeze_table_age -> autovacuum_freeze_table_age
                                  #  - vacuum_multixact_freeze_min_age -> autovacuum_multixact_freeze_min_age
TOPTS.toast.*autovacuum*          #Same but only for pg_toast_OID TABLE
                                  #Def: same value
                                  #Not for: autovacuum_analyze*


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CONCURRENCY          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SESSION ==>                       #Series of commands in a specific client connection to the server
                                  #E.g. interactive prompt
                                  #Only one DATABASE per SESSION

CONCURRENCY ==>                   #Uses Multiversion Concurrency Control (MVCC) instead of traditional locks, for best performance
                                  #while still reliable:
                                  #  - transactions create a snapshot of the current state ("database version").
                                  #  - read/write locks are "predicate locks": they don't block, they are just informative, and later on
                                  #    might cancel actions if concurrency problem.
                                  #Best to have consistent data is:
                                  #  - if several statements need to be "write all or nothing", use transactions:
                                  #      - if the state being read can be changed by other clients in the middle of the transaction
                                  #        without problems, use read committed statements
                                  #      - otherwise, use serializable statements, but then if abort, must redo the transaction.
                                  #        Client software usually notify of transaction failures: in such case, client needs to send
                                  #        the request again.
                                  #      - in all cases, put as read only if possible, and deferrable when it's a long operation.
                                  #  - otherwise, transactions are not needed: operations are atomic and ask for the relevant blocking
                                  #    locks, so no concurrency problem (are actually atomic transactions)
                                  #When asking for locks:
                                  #  - Transaction asking for locks will wait ENVVAR lock_timeout (def: 0, in ms) before cancelling,
                                  #    and ENVVAR deadlock_timeout (def: 1s) before checking if there is a deadlock (in which case it
                                  #    is cancelled)
                                  #  - all transactions cannot exceed an average of ENVVAR max_[pred_]locks_per_transaction (def: 64)
                                  #    predicate or not-predicate locks.
                                  #FUNC():
                                  #  - always a single read commited transaction, acquiring locks
                                  #     - begin block of PL/PGSQL is logic-wise, not used for concurrency
                                  #  - finer concurrency control is only at SQL level, so need wrap FUNC() call in a SQL transaction
start transaction                 #Start a transaction (ends with rollback|commit), statements are only committed to the database at the
[isolation level WORD,]           #end of the transaction (but "appear" committed locally inside the statement)
[read write|only,]                #A transaction that has any statement with an error will abort (rollback when committed).
[[not] deferrable]                #WORD can be read committed, repeatable read or serializable.
                                  #Effects of WORD, for W2 -> R1 or W1 (same rows, 1 and 2 are transactions)
                                    +-------------------------------------------+-------------------------------------------+
                                    |                  W2 -> R1                 |                  W2 -> W1                 |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| atomic or committed transaction 1 |                                    No problems                                        |
| atomic or committed transaction 2 |                                                                                       |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             |                                           | read committed: ignores W2                |
| ongoing transaction 2             |                                           | non read committed: aborts (will rollback)|
+-----------------------------------+                 ignores W2                +-------------------------------------------+
| atomic or committed transaction 1 |                                           |                  blocks                   |
| ongoing transaction 2             |                                           |                                           |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             | non read committed and R1 -> W2 -> R1:    | non read committed and R1 -> W2 -> W1:    |
| atomic or committed transaction 2 |  ignores W2                               |  aborts (will rollback)                   |
|                                   | else:                                     | else:                                     |
|                                   |  takes W2 into account                    |  takes W2 into account                    |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
                                  #Summary:
                                  #  - read committed (def):
                                  #    - ignores other ongoing transactions, but takes into account changes by committed transactions
                                  #      or atomic statements
                                  #    -> see state for the current statement
                                  #  - repeatable read:
                                  #    - ignores other ongoing transactions, including changes, unless it conflicts, in which case
                                  #      it aborts.
                                  #    -> see state at start of transaction (first statement after "start transaction")
                                  #  - serializable:
                                  #    - like repeatable read, but two concurrent serializable doing a W that depends on a R that is
                                  #      changed by this W: the second will abort.
                                  #      - ex: insert into "TABLE"("COL") select sum("COL") from "TABLE"; by two serializable transactions.
                                  #    -> can virtually consider successful transactions (not aborting) to happen one after the other,
                                  #       while still avoiding blocking locks to achieve it.
                                  #Others:
                                  #  - read write|only (def: read write):
                                  #     - read only allow further concurrency optimization
                                  #     - read only can still write to TEMP
                                  #  - deferrable (def: not deferrable):
                                  #     - if serializable and read only, blocks until sees no chance of being cancelled, then go on.
                                  #     - good if cancellation might take a long time to repeat the transaction (e.g. backups)
                                  #Synchronization of two TRANSACTIONs:
                                  #  - TRANSACTION must:
                                  #     - both be either repeatable read or serializable.
                                  #       If TRANSACTION2 is serializable, so must be TRANSACTION1
                                  #     - if TRANSACTION1 is readonly, so must be TRANSACTION2
                                  #     - Must be just after the "start transaction"
                                  #  - steps:
                                  #     - TRANSACTION1 does select pg_export_snapshot(), which prints a SNAPSHOT_ID
                                  #     - TRANSACTION2 does set transaction snapshot SNAPSHOT_ID, as STR
                                  #Can have extra infos with:
                                  #  - txid_current(): current TRANSACTION_ID
                                  #  - txid_current_snapshot(): current SNAPSHOT_ID
                                  #  - txid_snapshot_xid(SNAPSHOT_ID): returns current transaction ID
                                  #  - txid_snapshot_xmax|xmin(SNAPSHOT_ID)
                                  #  - txid_visible_in_snapshot(TRANSACTION_ID, SNAPSHOT_ID)
set transaction ...               #Changes isolation level, etc. (same as start transaction ...) for current transaction.
                                  #Must be just after the "start transaction"
set session
characteristics                   #Same but for all future transactions.
as transaction ...                #Same as setting ENVVAR default_transaction_isolation|read_only|deferrable
rollback|commit                   #Finishes a transaction.
                                  #For rollback, actions are actually dropped (nothing happens).
savepoint LABEL
rollback to savepoint             #Inside a transaction block, rollback to LABEL go back to the state where savepoint LABEL was (and
LABEL                             #releases all savepoint LABEL that might have been defined after it).
release savepoint LABEL           #Delete a savepoint

prepare transaction STR           #Do a two-phase commit:
                                  #  - transaction is temporary rollbacked (except the ENVVAR changes) and will only be committed once
                                  #    commit prepared STR is done
                                  #  - commit|rollback prepared STR can be done by other clients (not inside a transaction), if same ROLE
                                  #    or superuser ROLE.
                                  #    So goal is to have a single client managing the transaction of other clients
                                  #    ("transaction management system")
                                  #STR must be unique, and less than 200 bytes
                                  #Transaction must not involve notify|[un]listen, TEMP nor CURSOR with hold
                                  #ENVVAR max_prepared_transactions (def: 0, so disabled) is available. Is set, should be
                                  #max_connections * number of prepared_transactions per connection

pg_prepared_xacts                 #All "prepare transaction STR". ???

lock "TABLE" ...                  #Put a blocking lock on "TABLE"... among several mode WORD, and release at end of current transaction:
[in WORD mode] [nowait]           #  - should do it at beginning of transaction if repeatable read or serializable.
                                  #  - locks goal is to conflict with each other
                                  #  - prefer predicate locks
                                  #If can't access lock:
                                  #  - if nowait, only emits error
                                  #  - otherwise, transaction will be rollbacked
                                  #WORD are:
                                  #  - access exclusive (commands that erase data): block everything
                                  #  - exclusive: let up to access share
                                  #  - share row exclusive: let up to row share
                                  #  - share update exclusive (commands that change schemas (clean/analyze/optimize data/alter)): let up
                                  #    to row exclusive
                                  #  - row exclusive (commands that write data)
                                  #  - row share (select for update/share)
                                  #  - access share (commands that read data)
select ... for update             #Gets an access exclusive lock on the rows in ... for "TABLE" (def: all), released at end of transaction
[of "TABLE"...] [nowait]          #(not statement). Cannot use union, intersect or except.
select ... for share
[of "TABLE"...] [nowait]          #Same for share row exclusive

pg_locks                          #All currently held locks. ???

pg_[try_]advisory_                #Gets a lock linked to a ID BIGINT:
[xact_]lock[_shared]              #  - can be "_shared" or not (block lock() attempts, but not lock_shared() attempts)
(BIGINT)                          #  - session-level (stops at unlock() or at end of session)
                                  #    - if "xact_" transaction-level (stops at end of transaction only)
                                  #  - If "try_", only returns false/true but no block if can't get the lock.
                                  #  - can lock several times (needs to unlock several times)
pg_advisory_unlock
[_shared](BIGINT)                 #Returns true if such lock existed.
pg_advisory_unlock_all()          #

set constraints                   #Change the initially deferred|immediate state of CONSTRAINT... (def: all) within the current
all|CONSTRAINT...                 #transaction.
deferred|immediate                #CONSTRAINT can be declared (including during create table ...)
                                  #  - initially immediate (def): constraints are checked at each statement.
                                  #  - initially deferred: at each end of transaction (except not null and check())
                                  #  - not deferrable (def: deferrable): cannot be initially deferred.
                                  #Includes previous statements (retroactively), so can fire at specific point in transaction.
"COL" TYPE ...
 [[not] deferrable]
 [initially deferred|immediate]   #Same for a specific COL_ARG (including multicolumn)
alter table "TABLE"
 alter constraint "CONSTRAINT"
 [[not] deferrable]
 [initially deferred|immediate]   #Same for foreign key CONSTRAINT
alter table "TABLE"
 add unique|primary key ...
 [[not] deferrable]
 [initially deferred|immediate]   #Same for unique|primary key CONSTRAINT
create ... trigger ...
 on "TABLE"|VIEW [from TABLE2]
 [[not] deferrable]
 [initially deferred|immediate]   #Same for a TFUNC

pg_constraint.condeferrable       #BOOL. Whether CONSTRAINT is deferrable
pg_constraint.condeferred         #BOOL. Whether CONSTRAINT is deferred by defaul

alter table ... detach partition
 ... [concurrently|finalize]      #

create index concurrently ...     #Instead of using an exclusive lock, await conflicting transactions for each ROW.
reindex index concurrently ...    #Can fail: must then either:
                                  #  - drop INDEX and recreate it
                                  #  - reindex it
                                  #Slower.
                                  #Cannot be:
                                  #  - in a transaction
                                  #  - on partitioned TABLEs
drop index concurrently ...       #Same for `drop`
                                  #Cannot:
                                  #  - use `cascase`
                                  #  - drop multiple INDEXs

HOT ==>                           #"Heap-Only Tuples"
                                  #Optimization for MVCC that where an `update` does not need to:
                                  #  - INDEX ROWs can be updated directly instead of creating an intermediary ROW
                                  #  - TABLE ROWs can be vacuumed during any statement (including `select`)
                                  #Only happens if:
                                  #  - `update` does not modify COLs that are indexed
                                  #  - there is enough space on the current page


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          TABLESPACE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create tablespace "TABLESPACE"    #Create a TABLESPACE, i.e. a group of DATABASE, TABLE, MATERIALIZED VIEW and INDEX:
 [owner ROLE]                     #  - located on the disk at a specific location STR (must be empty dir)
 location STR                     #  - owned by ROLE (def: current ROLE)
                                  #Must be a superuser to do it.
                                  #Goal:
                                  #  - Unlike SCHEMA, can't be used for permissions.
                                  #    Only permission is create, i.e. possibility to assign a newly created VAR to TABLESPACE
                                  #  - is used to control the physical locations of database objects, in order to:
                                  #     - optimize performance, e.g. putting heavily used INDEX on fast storage
                                  #     - optimize space, e.g. put heavy space DATABASE on high-volume storage, with possibility to move
                                  #       to another is space is not enough anymore
                                  #Is cluster-wide, so can be assigned to objects of different DATABASE.
                                  #Each new VAR has as a TABLESPACE (unless explicity mentionned):
                                  #  - otherwise the default TABLESPACE of the database.
                                  #    It is inherited from its template (pg_default for template0|1), but can be overriden by:
                                  #     - if TEMP or INDEX on TEMP, ENVVAR_ARR temp_tablespace: if used as an ARR, allocate randomly
                                  #       accross TABLESPACE...
                                  #     - otherwise, ENVVAR default_tablespace
                                  #Symlinks to DIR of user-created TABLESPACE can be found in DATADIR/pg_tblspc
drop tablespace "TABLESPACE"      #Can only be done if empty

create table|materialized view ...
 tablespace "TABLESPACE" ...
create index ...
 tablespace "TABLESPACE"
reindex
 tablespace "TABLESPACE" ...
create database
[with] tablespace [=] "TABLESPACE"#Set ENTITY TABLESPACE

create table ...
 exclude|unique|primary key ...
 using index tablespace "TABLSPAC"#Set TABLESPACE of CONSTRAINT's INDEX

alter ENTITY "ENTITY" ...         #Set TABLESPACE of one ENTITY. Moves the data files.
 set tablespace "TABLESPACE"      #For ENTITY: table, materialized view, index, database

alter ENTITY                      #Set TABLESPACE2 of all ENTITYs in a given TABLESPACE
 all in tablespace "TABLESPACE"   #For ENTITY: table, materialized view, index
 [owned by ROLE,...]              #Only ones owned by ROLE,...
 set tablespace "TABLESPACE2"
 [nowait]                         #All ENTITYs are locked.
                                  #If nowait, uses lock ... nowait, i.e. fail if cannot lock right away

pg_global                         #TABLESPACE for cluster-wide ENTITYs

pg_tablespace                     #All TABLESPACE
                                  #???

pg_class.reltablespace            #pg_tablespace.oid of RELATION's TABLESPACE
                                  #0 for default TABLESPACE
pg_tables.tablespace              #pg_tablespace.spcname of TABLE
pg_indexes.tablespace             #pg_tablespace.spcname of INDEX
pg_matviews.tablespace            #pg_tablespace.spcname of MVIEW


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EFFICIENCY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TOPTS|IOPTS.fillfactor            #10-100. Keep 1-INT% additional free space on the page storing TABLE|INDEX
                                  #Def: 100 (TOPTS) or 90 (IOPTS), i.e. none
                                  #Memory allocation happens on `insert`
                                  #Goal:
                                  #  - faster memory allocation if lots of `update`
                                  #  - `update`s can keep their `cluster` optimization, since they do not need new memory allocations
                                  #  - increase changes of Heap-Only Tuples

WRITING ==>                       #Best way to write a big amount of data fast:
                                  #  - put in only one transaction/statement
                                  #  - use copy or (if copy not possible) prepare, if possible on an empty TABLE
                                  #  - create the INDEX and foreign keys constraints after the data has been put into
                                  #  - less checkpoints (increasing ENVVAR checkpoint_segments and checkpoint_timeout)
                                  #  - temporarily disabling WAL or replication
                                  #Should run analyze afterwards.

PERFORMANCE ==>                   #Can:
                                  #  - disable durability, by:
                                  #      - putting fsync and full_page_writes off (risky)
                                  #      - putting synchronous_commit off (more durable and almost same performance gain)
                                  #  - less checkpoints (see below)

PERFORMANCE TUNING ==>            #ENVVAR:
                                  #  - work_mem (def: 1MB): max memory used by a single command for each of its sort operations and hash
                                  #    tables, before writing temp files to disk.
                                  #    Average total memory taken will be average_number_of_hash/sort_operations_by_command *
                                  #    number_of_connections * work_mem. Should not be more than RAM taken by:
                                  #      kernel + other applications + shared_buffers + let memory for kernel buffer
                                  #  - wal_buffers (def: -1, which auto-select a value): memory used for caching WAL, i.e. number of
                                  #    WAL segments (16MB) that can be cached for all sessions before flushing them.
                                  #    If high number of transactions, might consider increasing for better performance.
                                  #    Best is 16MB.
                                  #  - max_stack_depth (def: 2MB):
                                  #     - higher can provoke stack overflow (crashing the server) if higher than the kernel limit, with
                                  #       a safety margin of 2MB
                                  #       current OS limit can be seen with ulimit -s (8MB)
                                  #     - lower can cancel complex queries requiring more stack.
                                  #  - temp_file_limit (def: -1, unlim): max memory for temp files, in KB
                                  #  - max_files_per_processes (def: 1000): max opened files per session (see limit with ulimit -S|H -n)
                                  #    Def on Linux: 1024, so that's good.
                                  #  - effective_io_concurrency: when using several disks at same time (e.g. RAID), number of disks that
                                  #    can write at same time
                                  #  - shared_buffers (def: 128MB): memory for shared buffers (def: 128MB).
                                  #    Good value is 25% of RAM (if > 1GB total RAM)
                                  #Look at amount of memory taken with pgcluu or pgbadger

HARDWARE ==>                      #  - more RAM -> more cache
                                  #  - good hard drives. RAID0 or RAID1 is good idea
                                  #  - CPU less important, but still important for complex functions


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           FUNCTIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create [or replace]               #Creates a user-defined function, that can used anywhere.
function FUNC([[VAR]              #  - or replace: if already existing, will be overwritten (fails if different types)
TYPE [default VAL]...])           #  - arguments:
returns TYPE2                     #    - VAL is the default value
as 'STR'                          #    - TYPE can be any but also:
language LANG                     #      - ROW|TABLE:
[window] [immutable|              #        - TABLE are converted to ROW
stable|volatile]                  #        - calling FUNC(TABLE) needs a reference to "TABLE". Examples:
[leakproof] [strict]              #           - select FUNC(TABLE) from "TABLE"
[security                         #           - select * from "TABLE", FUNC(TABLE)
invoker|definer]                  #        - FUNC("TABLE"|"ROW") can also be called "TABLE"|"ROW".FUNC, so it is unwise to name FUNC same
[cost FLOAT]                      #          as a member of "TABLE"|"ROW"
[rows UINT]                       #      - any*:
                                  #        - Must be all of the same type (args and return value), for a specific call.
                                  #        - anyarray|range must have same type as other any*
                                  #        - if return value is polymorphic, at least one arg must be too.
                                  #        - $0 is a VAR with the polymorphic type, initialized to null.
                                  #          Can be used to store return value or do computations.
                                  #      - cstring (null-terminated, for C only)
                                  #    - variadic ARG (must be ARR):
                                  #      - elements of ARR must then be of same TYPE
                                  #      - can fire it with FUNC(..., variadic ARR) or FUNC(..., ...)
                                  #      - is not optional by default, needs to add e.g. variadic VAR TYPE default array[]::TYPE[]
                                  #    - overloading is possible. If multiple choice of casting, the pg_type.typeispreferred is used.
                                  #    - can call by name: FUNC(VAR := VAL)
                                  #  - return value:
                                  #    - always returns a TABLE (TYPE actually means TABLE_TYPE_VAL)
                                  #    - returns first row only, unless setof TYPE is used
                                  #    - can also be in the function args instead of returns ...:
                                  #      - as out ARG...:
                                  #        - ARG... can be assigned inside FUNC instead of the usual way to return a value
                                  #          (in language that permits assigning to VAR, e.g. not SQL but PL/SQL)
                                  #        - if several ARG...:
                                  #          - returns as a multicolumn TABLE in a from clause
                                  #          - otherwise, returns as one ROW
                                  #        - to return more than first row, put also returns setof TYPE (record if several ARG...)
                                  #      - inout ARG is same as ARG out ARG (both ARG and return value)
                                  #    - TYPE2 is like TYPE, but can also be:
                                  #      - void: if no return value
                                  #      - "ROW"|"TABLE" or table(COL TYPE ...): like specifying several out ARG...
                                  #      - record: FUNC must be called with FUNC(...) as [ALIAS]("COL" TYPE ...) to precise types of
                                  #        RECORD
                                  #  - VAR%type: TYPE of a VAR, including "TABLE"."COL". Can be a VAR from the FUNC too
                                  #    (but not for an ARG from another ARG)
                                  #  - body:
                                  #     - STR. Better to use $$...$$ notation
                                  #  - can be written in different LANG (see below):
                                  #     - SQL
                                  #     - pl*: procedural languages, faster than SQL functions, among:
                                  #       - already in PostgreSQL (just need create extension pl*)
                                  #         - plpgsql
                                  #         - plpython
                                  #         - plperl
                                  #         - pltcl
                                  #     - C, faster than pl*
                                  #     - internal:
                                  #       - builtins functions. Can only refer to them by putting the C function name in the body STR
                                  #       - possible (only) use: renaming
                                  #       - actually C functions, but compiled in (not loaded with a shared library)
                                  #  - window:
                                  #     - means it is a WFUNC (only in C and some PL/*)
                                  #       As such, it passes some extra arguments about the current window.
                                  #  - immutable|stable|volatile: used for performance optimization. Can be:
                                  #     - immutable: doesn't read|write global state (including the database or the pg_catalog.*)
                                  #     - stable: doesn't write global state
                                  #     - volatile (def): write|read global state
                                  #  - leakproof: used for performance and security optimization. Means it is immutable, and
                                  #    doesn't give informations about arguments VAL aside from return value (e.g. does not throw errors
                                  #    for some return values but not others, prints arguments VAL, etc.)
                                  #  - strict:
                                  #     - returns automatically null if an arg is null.
                                  #     - for VARIADIC arg, only works if whole VARIADIC is null, not only part of it.
                                  #  - privilege are the ones of:
                                  #      - security invoker (def): the caller
                                  #      - security definer: the user creating the FUNC. Forbids using set role.
                                  #        Should also define function-specific ENVVAR search_path by adding a temporary SCHEMA at the
                                  #        end (to avoid malicious VAR shadowing)
                                  #  - cost FLOAT: CPU cost for the planner (cpu_operation_cost) (def: 1 for C FUNC, 100 for others)
                                  #  - rows UINT: average number of rows returned, for the planner (def: 1000). Only if return value
                                  #    returns several rows.
                                  #Is parent dependency of anything that uses the FUNC (e.g. VIEW, etc.)
alter function FUNC(...)
...                               #... is any of the options after language LANG. Can use not leakproof.

alter routine VAR(...)            #Like alter function ... except works also on AFUNC and PROCEDURE
                                  #Cannot use:
                                  #  - called on null input, returns null on null input, strict
                                  #  - support FUNC
drop routine VAR(...),...         #Like drop function ... except works also on AFUNC and PROCEDURE

do [language LANGUAGE]
STR                               #Execute an anonymous function STR (body of function), from LANGUAGE (def: plpgsql)

pg_proc                           #All FUNC. ???

regproc                           #TYPE alias to OID, to cast pg_proc.oid as "FUNC" name
regprocedure                      #TYPE alias to OID, to cast pg_proc.oid as "FUNC(TYPE,...)" name


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            TRIGGER            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create [constraint]               #Execute a FUNC for a specific EVENT on "TABLE" or VIEW.
trigger TFUNC                     #Multiple TFUNC are fired alphabetically.
before|after|instead              #  - EVENT can be insert|delete|truncate, or update [of "COL"...]
of EVENT [or EVENT2...]           #    "COL"... means "COL" or "COL2", etc.
on "TABLE"|VIEW [from TABLE2]     #    Truncate only on before|after and on "TABLE".
[for each row]                    #  - before|after|instead of is for the constraint checking + the operation itself (row-wise).
[when (BOOL)]                     #    So before fired even if constraint fails, but not after.
execute procedure FUNC(STR...)    #    If COMMAND is the one triggering EVENT "for each row":
                                  #       - before|instead of are executed row-wise according to COMMAND (so next rows in TFUNC see
                                  #         changes of previous rows by COMMAND)
                                  #       - after is executed table-wise according to COMMAND (so rows in TFUNC see changes of all rows
                                  #         by COMMAND)
                                  #    before|instead of are executed on all rows
                                  #    instead of can only be on VIEW and cannot use when BOOL. Usually used to modify the underlying
                                  #    "TABLE" so users can modify VIEW.
                                  #    Can also use or EVENT2...
                                  #  - FUNC: any language (e.g. PL/* or C) but not SQL. See doc about TFUNC for those languages.
                                  #    Must take no arguments (arguments are passed with special variables) and with a trigger return
                                  #    type, but actually returning:
                                  #     - "before" + "for each row":
                                  #        - null: skip further TFUNC and cancel statement (for that row)
                                  #        - "ROW"|"TABLE" with same structure as new|old (including them):
                                  #          - modified row: modify new and return it.
                                  #          - unmodified row: return old|new according to the operation
                                  #     - otherwise: return null
                                  #  - for each row: fired for each manipulated row (not fired if no manipulated row), and not once for
                                  #    all rows.
                                  #    Necessary for constraint trigger and instead of.
                                  #    Impossible on non-truncate VIEW.
                                  #  - constraint: makes it possible to:
                                  #     - change not deferrable, etc. with set constraints TFUNC deferred|immediate (def: immediate)
                                  #     - use not deferrable, etc. (same as for CONSTRAINT).
                                  #    Must be "after" and "for each row".
                                  #    Can be used to simulate a constraint, in which case an exception should be raised.
                                  #    Can also use set constraints to fire trigger at specific point during current transaction.
                                  #  - when (BOOL):
                                  #     - can use old|new like trigger functions
                                  #     - BOOL cannot be a subquery.
                                  #A TFUNC can fire another one ("cascading triggers"), including recursively.
                                  #Can see current TFUNC depth with pg_trigger_depth()
                                  #To create a TFUNC notifying of each modification:
                                  #  - triggered_change_notification([STR]) is a function to use to do notify STR, for each row,
                                  #    after insert and/or update and/or delete, with payload explaining the modification.
                                  #  - Trusted postgres extension 'tcn'
                                  #Auto-dependency child of its TABLE|COL (even with `drop ... restrict`)
alter table "TABLE"               #all|user:
 enable|disable                   #  - not for RULE
 [replica|always]                 #  - same but all include system TRIGGER (enforcing CONSTRAINT), not user
 rule|trigger                     #replica|always:
 RULE|TRIGGER|all|user            #  - def: affects only non-replication sessions (ENVVAR session_replication_role "origin" (def) or
                                  #    "local")
                                  #  - replica: affects only replication sessions (session_replication_role "replica")
                                  #  - always: affects all sessions

pg_trigger                        #All TFUNC. ???

pg_class.relhastriggers           #BOOL. Whether RELATION has TFUNCs
pg_tables.hastriggers             #BOOL. Whether TABLE has TFUNCs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         EVENT TRIGGER         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/



create event trigger              #Like TFUNC but EVENT can be DDL not DML:
EFUNC on EVENT                    #  - ddl_command_start|end: before|after a create, alter or drop (except for cluster-wide objects
[when tag in (STR...)]            #    and TFUNC|EFUNC)
execute procedure FUNC()          #  - sql_drop: same but only for drop
                                  #     - FUNC() can use pg_event_trigger_dropped_objects() which returns a TABLE with one row for
                                  #       each dropped objects and COL...:
                                  #        - classid: DATABASE OID
                                  #        - objid: object OID
                                  #        - objsubid: e.g. for columns the attnum
                                  #        - object_type STR
                                  #        - schema_name STR
                                  #        - object_name STR
                                  #        - object_identity STR: SCHEMA.OBJECT name
                                  #STR... are commands (e.g. 'drop table') to filter EFUNC.
                                  #FUNC must return have an event_trigger return type, and takes no arguments.
                                  #Must be superuser.
alter event trigger
 disable|enable
 [replica|always]                 #

pg_event_trigger                  #All EFUNC. ???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             RULE              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create [or replace]               #Creates a macro that modifies a command matching EVENT into COMMANDS (add or replace according to
rule RULE as on EVENT             #"instead"), for "TABLE", when BOOL:
to "TABLE" [where BOOL]           #  - EVENT:
do [instead] COMMANDS             #     - select|insert|update|delete
                                  #     - select must be "instead", non-"where BOOL" and with select COMMAND
                                  #       (similar to create a VIEW)
                                  #  - BOOL:
                                  #     - can refer to old|new."COL" (like triggers)
                                  #COMMANDS:
                                  #  - nothing: with "instead", original EVENT is dropped
                                  #  - COMMAND or (COMMAND;...):
                                  #     - Must be select|insert|update|delete|notify.
                                  #     - For insert, the original EVENT is performed before COMMAND, for others after.
                                  #     - "instead" non-"where BOOL" COMMAND insert|update|delete must be MSUBQUERY
                                  #       so that calling command can use returning itself.
                                  #When to use rules:
                                  #  - VIEW are better than RULE with select EVENT.
                                  #  - rules are similar to TFUNC, often faster but harder and less flexible.
                                  #    Prefer triggers unless performance is critical, in which case check if actually faster.
                                  #Child auto-dependency of its TABLE|COL

pg_rewrite                        #All RULE. ???
pg_rules                          #Same, but more for user-defined RULE. ???

pg_class.relhasrules              #BOOL. Whether RELATION has RULEs
pg_tables.hasrules                #BOOL. Whether TABLE has RULEs


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           OPERATOR            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create operator OP                #Creates an OP, i.e. a FUNC using a sign. Can have one or two arguments.
(left|rightarg = TYPE,            #Can be an already existing OP, with different TYPE (overloading)
procedure = FUNC                  #OP is [+-*/<>=~! #%^&|`?]+
[, commutator = OP2]              #Other args are for optimization (hints the planner in rewriting queries so they match an INDEX):
[, negator = OP3]                 #  - OP2 if VAL OP VAL2 is equivalent to VAL2 OP2 VAL
[, restrict = FUNC2]              #  - OP3 if VAL OP VAL2 is equivalent to not VAL2 OP3 VAL, or OP VAL to not OP3 VAL
[, join = FUNC3]                  #  - FUNC2 tells when using VAL OP VAL2 (returning BOOL), how much portion of the COL is likely to be chosen.
[hashes]                          #    Can choose among default ones:
                                  #      - eqsel: chooses a small portion, like =
                                  #      - neqsel: large portion, like <>
                                  #      - scalar[l|g]tsel: semi-large portion before or after, like < <= or > >=
                                  #  - FUNC3 is like FUNC2 but for VAL OP VAL2. Default ones:
                                  #      - eqjoinsel: like =
                                  #      - neqjoinsel: like <>
                                  #      - scalar[l|g]tjoinsel: like < <= or > >=
                                  #      - areajoinsel: like BOX operators
                                  #      - positionjoinsel: like POINT operators
                                  #      - contjoinsel: like <@ @>
                                  #  - hashes: VAL OP VAL2 is true if hash(VAL) = hash(VAL2)

pg_operator                       #All OPERATOR. ???

regoper                           #TYPE alias to OID, to cast pg_operator.oid as "OP" name
regoperator                       #TYPE alias to OID, to cast pg_operator.oid as "OP(TYPE,...)" name


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      FUNCTIONS LANGUAGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_language                       #All activated LANGUAGE, including:
                                  #  - lanpltrusted BOOL: can not access filesystem (so non-superuser can create function)
                                  #    Is false for C, R and SH but true to SQL and PL/PGSQL
                                  #???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            PL/SQL             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SQL FUNCTIONS ==>                 #  - execute SQL statements, returns the last one:
                                  #    - a MSUBQUERY
                                  #    - or returns void
                                  #  - either VAR or $NUM is used to refer to ARG...
                                  #    - if VAR is a COL in the current statement, use "TABLE".VAR (for the COL) or FUNC.VAR (for the VAR)
                                  #    - are readonly
                                  #  - some forbidden COMMAND: transactions blocks, vacuum, etc.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PL/PGSQL            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PL/PGSQL ==>                      #PL/PGSQL ends with ; but structure have only one ; at end of the structure.
                                  #Arguments:
                                  #  - if VAR is a COL in the current statement, use "TABLE".VAR (for the COL), LABEL.VAR (for a VAR
                                  #    declared in a block LABEL) or FUNC.VAR (for an argument)
                                  #"TABLE" (including COLs) must have same types accross executions because of caching. Solutions:
                                  #  - using RECORD instead of "ROW"|"TABLE"
                                  #  - using execute STR (doesn't cache)
MSUBQUERY ==>                     #In PL/PGSQL, also implies execute MSUBQUERY

<<LABEL>>                         #Scope block. Main function must be in a scope block. Variables are local to the block.
[declare                          #VAR must all be declared, as VAR [constant] TYPE [not null] [default VAL];...
  DECLARATIONS...]                #exception ... catches exceptions:
begin                             #  - EXCEPTION is either:
  STATEMENTS...                   #     - exception word (see appendix online or error messages) or "others" meaning any other exception.
[exception                        #     - SQLSTATE, i.e. an error code NUM typecasted to sqlstate: sqlstate '...'
  when EXCEPTION then ...         #  - can use the local variables when the exception happened.
  ...]                            #  - can use special variables sqlstate (current SQLSTATE) and sqlerrm (error message STR)
end [LABEL]                       #  - get stacked diagnostics VAR = WORD2 ... assigns to VAR2 according to WORD2 among:
                                  #     - returned_sqlstate: like sqlstate
                                  #     - message_text: like sqlerrm
                                  #     - column|constaint|pg_datatype|table|schema_name: column|...|schema raising the exception
                                  #     - pg_exception_detail|hint: extra error messages
                                  #     - pg_exception_context: call stack
                                  #Is also a transaction block, with exceptions rollbacking the transaction.
raise [LEVEL]                     #Raises an exception.
[using VAR = STR ...]             #LEVEL can be exception (def), warning, notice, info, log or debug. Exception throw an exception, but
                                  #others don't, they just print a message.
                                  #VAR = STR provide informations among:
                                  #  - errcode: like EXCEPTION in exception block, but SQLSTATE is as STR
                                  #  - message: like sqlerrm
                                  #  - detail, hint, column|...|schema: like in get stacked diagnostics
                                  #If no using..., rethrow a currently catched exception.
raise [LEVEL] EXCEPTION
 ...                              #Like raise [LEVEL] using errcode = EXCEPTION ...
raise [LEVEL] 'STR',
VAL...  ...                       #Like raise [LEVEL] using message = STR ..., where STR can substitute % symbols with VAL...

return [VAL]                      #Return statement of the function.
                                  #VAL can be omitted:
                                  #  - if out ARG... are used
                                  #  - if void if the return type
return next [VAL]                 #With setof ... return type (multiple rows), add VAL (ROW for multiple columns) or MSUBQUERY to
return query MSUBQUERY            #the returned TABLE. Needs to be called several time, then a return; will return the whole set.
                                  #If VAL is ommitted and out ARG... are used, use the current value of those ARG... instead.

VAR := VAL                        #Assignment
SQL_COMMAND                       #Any SQL command can be performed but:
                                  #  - select ... (except select ... into) (including select VAL or select FUNC()) must be written
                                  #    perform ..., and has no return value
                                  #    - written perform (...) for a with ... query
null;                             #To do nothing (empty line), write null;
MSUBQUERY                         #Put the first row returned into VAR, which can be:
into [strict] VAR                 #  - VAR... for each column
                                  #  - "ROW"|"TABLE" or record for all columns
                                  #If more than one row is returned (and, if select, strict is used), an exception TOO_MANY_ROWS is
                                  #fired. If no row returned and strict, an exception NO_DATA_FOUND if fired.
                                  #select ... into is PL/PGSQL, different than the SQL select ... into

execute STR                       #Execute SQL (not PL/PGSQL) command STR.
[using VAL...]                    #STR can contain $NUM that will substituted by each VAL...:
                                  #  - must be used for all VAR coming from the FUNC ARG...
                                  #  - can only be used in select|insert|update|delete
                                  #  - cannot be used by "TABLE" and "COL": they must be supplied as a STR concatenation
                                  #STR should be escaped:
                                  #  - $$...$$ for the command
                                  #  - use quote_* for dynamic variables

found                             #Variable containing true if last SQL command returned|manipulated at least one row.
get diagnostics                   #WORD can be either:
VAR = WORD                        #  - row_count (assign to VAR the number of rows returned|manipulated by last SQL command)
                                  #  - result_oid: OID of last row manipulated (if table has OIDs)

if BOOL then ...
[elseif BOOL then ...]
[else ...] end if;

case ... end case;                #Like SQL case ... end

[<<LABEL>>]
[while BOOL] loop ...
end loop [LABEL]                  #BOOL is true by def.

[<<LABEL>>]
for "INT" in [reverse]
NUM..NUM2 [by NUM3] loop
... end loop [LABEL]              #

[<<LABEL>>]                       #Iterates over rows returned by MSUBQUERY
for VAR in MSUBQUERY              #VAR can be:
loop ...                          #  - VAR... for each column
end loop [LABEL]                  #  - "ROW"|"TABLE" or record for all columns

[<<LABEL>>]                       #Looping through a VAR2:
foreach VAR [slice NUM]           #  - ARR
in array VAR2 loop ...            #  - ROW: VAR is then VAR... for each value
end loop [LABEL]                  #If NUM > 0, VAR receives an ARR2 of dimension NUM each time (for multidimensional VAR2)

exit [LABEL] [when BOOL]          #Exit a loop or block with LABEL (def: innermost one), if BOOL (def: true)
continue ...                      #Same as exit ... but skip next statements to start a new cycle.

refcursor                         #Special PL/PGSQL type to store a CURSOR.
                                  #Similar as SQL CURSOR (move and close identical). Differences are below.
                                  #Can be declared as:
                                  #  - CURSOR [scroll] cursor [(ARGS...)] for select ...:
                                  #    - like a SQL CURSOR, but:
                                  #      - not opened, needs to do open CURSOR;
                                  #      - if ARGS, can pass arguments to open CURSOR(...);
                                  #  - CURSOR refcursor:
                                  #    - same but needs to specify the query with open, either with open CURSOR [scroll] for select ...
                                  #      or open CURSOR [scroll] for execute ...
                                  #Can be returned or passed as argument:
                                  #  - pass the CURSOR as STR, casted as refcursor, e.g. FUNC(refcursor 'CURSOR')
                                  #  - to use a CURSOR in the calling FUNC, either:
                                  #     - use the names of the CURSOR.
                                  #       No need to return them, they close at end of transaction of calling function.
                                  #       If CURSOR not provided as argument, will generate a random name
                                  #     - return them, and use the return value
fetch ... into VAR                #Just like select ... into VAR (including: cannot fetch several rows)
[<<LABEL>>]
for RECORD
in REFCURSOR[(...)]
loop ... end loop [LABL]          #Loops in a REFCURSOR. Implicitely open and close the REFCURSOR

TFUNC FOR PL/PGSQL ==>            #Return type is trigger, but must return either:
                                  #Arguments are in tg_argv STR_ARR (index starts at 0 and is of size tg_nargs INT).
                                  #Can also use:
                                  #  - new|old RECORD: current row for "for each row" TFUNC (only if "for each row")
                                  #    new is absent if delete, and old is absent if insert.
                                  #  - tg_name STR: TFUNC name
                                  #  - tg_when STR: before|after|instead of
                                  #  - tg_level STR: statement|row ("for each row")
                                  #  - tg_op STR: insert|update|delete|truncate
                                  #  - tg_relid OID: of the TABLE
                                  #  - ts_table_name STR: name of the TABLE
EFUNC FOR PL/PGSQL ==>            #Return type is event_trigger, but doesn't use return.
                                  #Can also use:
                                  #  - tg_event STR: the EVENT
                                  #  - tg_tag STR: the COMMAND


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             PL/SH             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PL/SH ==>                         #plsh, for any installed shell, including Bash:
                                  #  - Impossible: ROW, any*, out ARG, setof
                                  #  - Body: starts with correct shabang.
                                  #  - Arguments: same as the shell (for Bash: $1, "$@", $#, etc.)
                                  #  - Return value:
                                  #    - stdout, with a newline stripped
                                  #    - null if exit (exit code 0) with nothing on stdout
                                  #  - Exception:
                                  #     - printing to stderr
                                  #     - non-0 exit code
                                  #  - SQL commands: fired through psql STR command line.
                                  #  - Can use any executable, as postgres user.
                                  #  - TFUNC and EFUNC:
                                  #    - defines PLSH_TG_* like tg_* in PL/PGSQL (old|new unavailable)
                                  #    - TFUNC() est executee, mais ne modifie jamais le row courant


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             PL/R              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PL/R ==>                          #plr: for R:
                                  #  - Arguments: named arguments.
                                  #  - Return value: return(VAL)
                                  #  - Types (other than obvious):
                                  #    - dimensions (max 3):
                                  #      - 0 dimension  (TYPE)                    <-> VAL
                                  #      - 1 dimension  (setof TYPE, ARR)         <-> VALv
                                  #      - 2 dimensions (setof ROW, ARR(2 dim)) <-> DATA.FRAME|ARR(2dim)
                                  #        - "TABLE"|"ROW" as argument -> DATA.FRAME, but must use as.data.frame(DATA.FRAME) if want to be returned
                                  #      - 3 dimensions (ARR (3 dim))             <-> ARR(3dim)
                                  #    - null <-> NA|NULL(pref)
                                  #    - BYTEA <-> OBJECT ([un]serialize on RAW)
                                  #    - everything else <-> STR
                                  #  - TFUNC: defines pg.tg.* like tg_* in PL/PGSQL
                                  #  - WFUNC: will pass:
                                  #     - fargNUM...: other values in window (NUM starts at 1)
                                  #     - fnumrows: window size
                                  #     - prownum: offset of window
                                  #  - Global data are possible across calls, including global functions.

PL/R FUNC() ==>                   #
install_rcmd(STR)                 #Execute R code STR (e.g. a function definition).
plr_environ()                     #Returns all environment variables as TABLE with STR columns name and value.
plr_set_display(STR)              #Change the DISPLAY env variable (useful for plots)

PL/R R FUNCTIONS ==>              #
pg.spi.exec(STR)                  #Execute SQL command STR.
                                  #For select ..., returns query as DATA.FRAME (null -> NA).
                                  #For others, returns number of manipulated rows.
pg.spi.prepare                    #Like prepare in SQL
(STR, INT_ARR)                    #INT_ARR are the types oid:
                                  #  - same as in pg_type.oid
                                  #  - can use SQL FUNC() load_r_typenames() to create R variables holding types oid.
                                  #    To see those variables, use SQL FUNC() r_typenames()
                                  #  - must be NA if no arguments
                                  #Returns a PLAN.
pg.spi.execp(PLAN, LIST)          #Like pg.spi.exect(), but executing a PLAN.
                                  #LIST is unnammed:
                                  #  - must contain only NA if no argument.
                                  #  - must be NA for a NULL in SQL
pg.spi.factor(DATA.FRAM)          #Convert non-NUM columns of DATA.FRAME into FACTOR.

pg.spi.cursor(STR, PLAN[, LIST])  #Creates and returns a readonly CURSOR names STR for the command defined by PLAN and LIST.
pg.spi.fetch(CURSOR, BOOL, INT)   #Same as fetch forward|backward (true|false) INT in SQL
pg.spi.close(CURSOR)              #

pg.spi.lastoid()                  #If last action was an insert of a single row, returns OID of that row.
pg.thrownotice|error(STR)         #Like raise notice|exception in PL/PGSQL
pg.quoteliteral|ident(STR)        #Like quote_literal|ident in SQL

PL/R OTHERS ==>                   #
plr_modules                       #TABLE executing plr_modules.modsrc (R code as STR) at start of each session or if reload_plr_modules()
                                  #is called.
                                  #plr_modules.modseq INT is the priority/order of execution.
                                  #Needs to be created as plr_modules(modseq int4, modsrc text). Should be readable by all, writable
                                  #only by trusted users.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       CUSTOM AGGREGATE        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARTIAL AGGREGATION ==>           #AFUNC which can compute its result in parallel divide-and-conquer
                                  #I.e. has a combine function to merge intermediary results
                                  #  - using the same initcond
                                  #Cannot rely on SET items order
                                  #Cannot use ZSET, grouping sets, AFUNC(distinct ...) nor AFUNC(... order by ...)
                                  #Must be marked `parallel safe`
                                  #Most builtin AFUNCs allow partial aggregation, except for *agg, mode|percentile*, *rank, cume_dist

create aggregate AFUNC            #Creates an AFUNC(COL,...):
(TYPE...) (sfunc = FUNC,          #  - TYPE are type of each COL.
stype = TYPE2                     #  - each VAL... of each COL is passed inside successive FUNC(VAL2, VAL...)
[, finalfunc = FUNC2]             #     - VAL2 is the return value of the last FUNC(), and its type is TYPE2
[, initcond = STR]                #     - its value in the first FUNC() is TYPE2 STR (typecasting from STR) (def: null)
)                                 #  - last FUNC can be different if FUNC2(VAL2) is defined: takes only one ARG, and can return any type.
                                  #  - last VAL2 is returned
                                  #  - by convention, null should be ignored in FUNC, unless there are only null, where null should be
                                  #    returned

pg_aggregate                      #All AFUNC. ???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     USERS AND PRIVILEGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ROLES ==>                         #ROLE are users or group of users for a given cluster.
                                  #Difference between OS_USER (OS-specific) and ROLE (cluster-specific).
                                  #Default installation creates a OS_USER "postgres", with group "postgres", often used as owner of
                                  #clusters.
                                  #Each cluster has a superuser ROLE:
                                  #  - defined as the owner OS_USER (user that created the cluster using initdb)
                                  #  - should have OS permissions over the DATADIR of the cluster
                                  #By default, client connects as ROLE using OS_USER, but can use flags to do otherwise.
                                  #  - so anyone that can login as the owner OS_USER of a cluster on a local computer can be superuser
                                  #    ROLE too.
                                  #ROLE own VAR they create.

create role ROLE                  #Create a ROLE:
[superuser]                       #  - superuser: creates another superuser ROLE (must be superuser ROLE)
[createdb|role]                   #    Cannot drop the initial superuser ROLE.
[[encrypted] password STR]        #    readonly ENVVAR is_superuser ('on|off') is available.
[login] [replication]             #  - createdb|role|login|replication: gives createdb|role|login|replication privilege (see PRIVILEGE)
[connection limit INT]            #  - password STR:
[noinherits]                      #     - either plaintext or "md5STR" where STR is a md5 hash.
[valid until TIMTMPTZ]            #     - If encrypted is specified, STR must be plaintext -> it is converted to a "md5STR".
[in role ROLE2...]                #     - can be null for no password.
[role passwordcheck ROLE2...]     #     - only useful with authentication methods with passwords (i.e. "password" and "md5")
[admin ROLE2]                     #     - will cancel if password is weak.
                                  #       Must also put '$libdir/passwordcheck' in shared_preload_libraries.
                                  #       Will not work on md5 hashes, only plaintext.
                                  #       It is recommended to rebuild the module by modifying the Makefile to enable CrackLib
                                  #       (better weak password recognition). Must be enabled at buildtime anyway.
                                  #     - report password failure after ENVVAR auth_delay.milliseconds (def: 0), to avoid bruteforce
                                  #       (but makes DDoD easier).
                                  #       Must put auth_delay in shared_preload_libraries
                                  #        - Trusted postgres extension 'auth_delay'
                                  #  - valid until TIMESTAMPTZ: validity of the password
                                  #  - in role ROLE2...: grants ROLE as a member of ROLE2... (prefer using grant|revoke)
                                  #  - role ROLE2...: inverse
                                  #  - admin ROLE2...: same as role ROLE2, but ROLE2 are added with admin_option
                                  #  - connection limit: how many connections at same time (def: -1, unlim)
                                  #  - noinherits: see grant|revoke
                                  #Parent dependency of every ENTITY it owns
alter role ROLE ...               #Can be all options of create role but:
                                  #  - can use no..., e.g. nosuperuser
                                  #  - no in role, role or admin
alter ENTITY "ENTITY"             #Set owner
 owner to ROLE                    #ROLE can be current_role, current_user or session_user
                                  #Not for ENTITY: extension, group, index, policy, role, rule,
                                  #text search parser|template, trigger, user, user mapping
                                  #Also for pseudo-ENTITY: large object OID
drop role ROLE...                 #Drop ROLE and cluster-specific objects owned by ROLE.
                                  #Doesn't work if ROLE own database-specific objects (use reassign|drop owned by ROLE first) or
                                  #privileges.
                                  #If client connection, doesn't stop it.
drop owned by ROLE...             #Drop all objects (except cluster-specific objects) owned by ROLE, and
[restrict|cascade]                #revoke privileges given to ROLE.
reassign owned
by ROL... to ROLE2                #Change ownership.

drop role ...                     #Can only be done if not used by any DATABASE

create schema ["SCHEMA"]          #Set SCHEMA's owner. Def: current_user
 authorization ROLE               #Same ROLE as `alter ... owner to ROLE`
                                  #Def "SCHEMA": ROLE

set [local] role ROLE             #Same syntax as set, but here change current ROLE
                                  #If not superuser, must be a ROLE that the session_user is member of (but not inverse, not other
                                  #members of same ROLE), directly or indirectly.
                                  #There are two types of users:
                                  #  - current user: current ROLE. Used for permission checking.
                                  #  - session user: ROLE that (usually) started the session.
                                  #    Used to switch roles with set role and set session authorization.
                                  #    So for a session, can switch back and forth between same possible ROLE (as long as set
                                  #    session authorization is not called)
                                  #VAR session_user (one that started the session) and current_user are available.
                                  #If ROLE is none, reset to current session_user.
                                  #ROLE can be written as STR
                                  #  - local: same as set local ...
set [local] session               #Same but:
authorization ROLE                #  - for both session_user and current_user
                                  #  - must be superuser
                                  #  - use default to reset to initial session_user

pg_authid|roles                   #All ROLE, including:
                                  #  - rolcatupdate BOOL (def: false): if false, all ROLE, including superuser ROLE, cannot write on
                                  #    system catalogs.
                                  #pg_roles blanks out the password
                                  #???

regrole                           #TYPE alias to OID, to cast pg_authid.oid as "ROLE" name

pg_auth_members                   #ROLE memberships, including:
                                  #  - roleid|member|grantor REGROLE: means ROLE2 is a member of ROLE, which has been granted by ROLE3
                                  #???
pg_user                           #TABLE with users. ???

grant PRIVILEGE...                #Gives permissions PRIVILEGE... on VAR... to ROLE...:
on [TYPE] VAR...                  #  - TYPE:
to ROLE...                        #     - is table (def)|sequence|database|domain|function|language|schema|tablespace|type|
[with grant option]               #       large object|foreign data wrapper|foreign server
                                  #         - table include VIEW and FOREIGNTABLE
                                  #     - [TYPE] VAR can be all TYPEs in schema SCHEMA for table|sequence|function
                                  #  - PRIVILEGE:
                                  #     - can be:
                                  #        - on table:
                                  #           - insert (a): insert or copy from
                                  #           - delete (d): delete
                                  #           - truncate (D): truncate
                                  #           - references (x): needed on both TABLE to use foreign key
                                  #           - trigger (t): create trigger
                                  #        - on table|sequence|large object:
                                  #           - select (r):
                                  #              - select, copy or using VAR in update|delete
                                  #              - sequence: same + currval()
                                  #              - large object: being read
                                  #           - update (w)
                                  #              - update, select for share|update
                                  #              - sequence: same + nextval|setval()
                                  #              - large object: being written
                                  #        - on sequence|domain|foreign data wrapper|foreign server|language|schema|type:
                                  #           - usage (U):
                                  #              - language:
                                  #                - create function for this PL/* language
                                  #                - careful if functions can access OS (e.g. Bash)
                                  #              - schema: reading VAR in SCHEMA (without permission can still look up VAR names in
                                  #                SCHEMA)
                                  #              - sequence: currval() + nextval()
                                  #              - type|domain: using it in creation of any VAR (including FUNC() and TABLE)
                                  #              - foreign data wrapper: create server using FDW
                                  #              - foreign server: create foreign table using FSERVER
                                  #        - on database:
                                  #           - connect (c): can start client session
                                  #           - temp (T): create temp
                                  #        - on database|schema|tablespace:
                                  #           - create (C):
                                  #             - database: create schema
                                  #             - schema: create any VAR inside SCHEMA, and rename (must be owner too)
                                  #             - tablespace: create table|index|temp in it, and use TABLESPACE in create database
                                  #        - on function:
                                  #           - execute (X): executing *FUNC() (including TFUNC())
                                  #        - on all:
                                  #           - all privileges
                                  #     - Other PRIVILEGE which can be obtained differently:
                                  #        - drop|alter:
                                  #           - no way to grant them to others:
                                  #              - drop or alter definition of VAR:
                                  #        - grant|revoke:
                                  #           - grant PRIVILEGE ... with grant|admin option:
                                  #              - grant for PRIVILEGE for the specific PRIVILEGE and TYPE
                                  #              - admin for ROLE membership
                                  #        - createrole:
                                  #           - create|alter role ... createrole:
                                  #              - create role (or drop|alter)
                                  #              - To do so on superuser ROLE, must be superuser.
                                  #              - implies admin option
                                  #              - can create roles with higher permissions or memberships, so can be dangerous
                                  #        - createdb:
                                  #           - create|alter role ... createdb:
                                  #              - create database (or drop|alter)
                                  #        - login:
                                  #           - create|alter role ... login:
                                  #              - ROLE can initiate a client session (e.g. with psql)
                                  #                Without it (def) ROLE can only be assigned with set role ROLE
                                  #        - replication:
                                  #           - create|alter role ... login:
                                  #             - ROLE can use pg_basebackup
                                  #     - PRIVILEGE can be PRIVILEGE("COL"...) for select|insert|update|references
                                  #     - Def. PRIVILEGE is all for TABLE owner and superuser.
                                  #       They can revoke their own privileges though.
                                  #     - Permissions are optimistic: has privileges in following cases:
                                  #        - granted at table-level but revoked at column-level
                                  #        - granted to ROLE2 to which is member, but revoked to the specific ROLE
                                  #     - SQL only has VAR, not VAR...
                                  #  - ROLE:
                                  #     - if ROLE2 is a member of ROLE (directly or indirectly), it is targeted too:
                                  #         - generally ROLE is just a group name, and ROLE2 real users, or subgroups
                                  #         - but could also copy and extend|restrict permissions of another user
                                  #         - inherit:
                                  #           - by def., ROLE2 inherit privileges of ROLE (except "Other PRIVILEGE")
                                  #           - with noinherits, can still use set role ROLE (but then loses its inital PRIVILEGE)
                                  #     - can be public:
                                  #        - meaning all current and future ROLE.
                                  #        - cannot be member of a ROLE.
                                  #        - has default privilege: connect, temp, usage and create on SCHEMA public, execute, usage on
                                  #          LANGUAGES. Can be revoked.
                                  #     - If current user is superuser or can grant thanks to membership or "with grant option", looks
                                  #       like PRIVILEGE has been granted by VAR owner
revoke [grant|admin               #Inverse.
option for] PRIVILEGE...          #"with grant|admin option" is revoked too. If "grant|admin option for", only "with grant|admin option"
on [TYPE] VAR...                  #is removed.
from ROLE...                      #Revoke privileges granted to others by ROLE2 too: if restrict, command will fail if there are some
[restrict|cascade]                #ROLE can only revoke ROLE2 for PRIVILEGE it previously personnally granted the other or a ROLE the
                                  #other is member of.

alter default privileges          #Change default PRIVILEGE for ROLE2 (in ...) of all VAR (write TYPEs, not TYPE VAR) that will be
[for ROLE...]                     #created in the future by ROLE (must be current user (def) or a ROLE3 that ROLE is member of), in
[in schema SCHEMA]                #SCHEMA (if specified)
grant|revoke...                   #Only for TABLE, SEQUENCE, FUNC or TYPE.

grant ROLE... to ROL2...          #
[with admin option]               #
revoke [admin option
for] ROL... from ROL2...          #Grant|revoke ROLE2... as members of ROLE...
[restrict|cascade]                #Gives|revokes associated PRIVILEGE too.

security label [for PROVIDER]
 on VAR is STR                    #Used to implement SE-Linux (see online doc)

has_TYPE_privilege                #TYPE can also be:
 ([ROLE, ]VAR, PRIVILEGE)         #  - any_column
                                  #  - column: adds a "COL" arg after VAR
pg_has_role
 ([ROLE, ]ROLE2, PRIVILEGE)       #With membership

pg_init_privs                     #TABLE with initial privileges. ???

pg_namespace.pg_nspowner          #REGROLE of SCHEMA's owner
pg_type.typowner                  #REGROLE of TYPE's owner
pg_largeobject_metadata.lomowner  #REGROLE of LARGEOBJ's owner
pg_class.relowner                 #REGROLE of RELATION's owner
pg_tables.tableowner              #REGROLE of TABLE's owner
pg_views.viewowner                #REGROLE of VIEW's owner
pg_matviews.matviewowner          #REGROLE of MVIEW's owner
pg_sequences.sequenceowner        #REGROLE of SEQUENCE's owner
pg_ts_dict.dictowner              #REGROLE of DICTIONARY's owner
pg_ts_config.cfgowner             #REGROLE of REGCONF's owner
pg_conversion.conowner            #REGROLE of CONVERSION's owner
pg_collation.collowner            #REGROLE of COLLATION's owner
pg_opfamily.opfowner              #REGROLE of OPFAMILY's owner
pg_opclass.opcowner               #REGROLE of OPCLASS's owner


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              ACL              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


aclitem                           #ACLITEM
                                  #Record ENTITY's PRIVILEGEs, on its pg_catalog.* TABLE
                                  #Cannot use < <= > >=

'[ROLE]=CHAR.../ROLE2'            #ACLITEM_UNKNOWN
                                  #ROLE: granted (def: public)
                                  #ROLE2: granter
                                  #CHAR: PRIVILEGE (shown above between parenthesis)
                                  #CHAR* if `with grant option`
                                  #Default PRIVILEGEs are not shown

pg_namespace.aclitem              #ACLITEM_ARR of SCHEMA
pg_type.typacl                    #ACLITEM_ARR of TYPE
pg_class.relacl                   #ACLITEM_ARR of RELATION
pg_attribute.attacl               #ACLITEM_ARR of COL
pg_largeobject_metadata.lomacl    #ACLITEM_ARR of LARGEOBJ

pg_default_acl                    #All default privileges. ???
pg_parameter_acl                  #TABLE with privileges. ???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        AUTHENTICATION         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_hba.conf                       #File under a cluster DATADIR controlling:
                                  #  - who can connect (host address, OS_USER)
                                  #  - to which database
                                  #  - with which connection method
                                  #  - under which ROLE
                                  #Have five whitespaces-separated fields (can contain whitespace if double-quoted) (with #Comment):
                                  #  - type (protocol):
                                  #    - local: local connection (Unix sockets)
                                  #    - host: TCP/IP
                                  #    - host[no]ssl: TCP/IP with|without SSL. For SSL:
                                  #       - must set ENVVAR ssl to "on" (def: "off") at server start
                                  #       - must have been enable when installing|building PostgreSQL
                                  #  - database:
                                  #    - connect to which DATABASE... (comma-separated)
                                  #    - sameuser means DATABASE with same name as ROLE
                                  #    - samerole means DATABASE with same name as ROLE or a member of ROLE
                                  #    - replication: special DATABASE used by pg_basebackup
                                  #    - @FILE... means list of DATABASE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                                  #    - can be all
                                  #  - user:
                                  #    - under which ROLE... (comma-separated) can connect
                                  #    - +ROLE means any member of ROLE
                                  #    - @FILE... means list of ROLE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                                  #    - can be all
                                  #  - address:
                                  #    - which IPv4|6 addresses (along the netmask range) can connect
                                  #    - can also be a hostname (can be slow)
                                  #      - including a .DOMAINNAME for hosts under DOMAINNAME
                                  #    - can be all
                                  #    - samehost: any of the machine own IPs
                                  #    - samenet: any of the machine own subnet
                                  #  - netmask (optional):
                                  #    - IPv4|v6 netmask
                                  #  - auth. method:
                                  #    - trust|reject: always accept|refuse
                                  #    - ident [map=MAP]:
                                  #      - use MAP in pg_ident.conf (ENVVAR ident_file) to determine which ROLE correspond to OS_USER
                                  #      - def: ROLE = OS_USER
                                  #    - password|md5: ask for password (md5 hashes it but not crypto-secure)
                                  #      - libpq variable password:
                                  #         - To use if is demanded. Def: PGPASSWORD
                                  #         - Can also use a FILE PAGPASSFILE (def: ~/.pgpass), which should contain lines with format:
                                  #             host:port:database:user:password (first four field can be *)
                                  #           Permission must be 0600
                                  #    - SSL:
                                  #      - libpq variables:
                                  #        - sslmode: priority of SSL over non-SSL (def: PGSSLMODE):
                                  #          - disable: non-SSL
                                  #          - prefer (def): first SSL, then non-SSL
                                  #          - require: SSL. If root CA, verify certificate
                                  #          - verify-ca: SSL. Always verify certificate
                                  #          - verify-full: SSL. Verify certificate, and that hostname match in certificate
                                  #        - sslcert: certificate FILE (def: ~/.postgresql/postgresql.crt or PGSSLCERT)
                                  #        - sslkey:
                                  #          - Secret key (def: ~/.postgresql/postgresql.key or PGSSLKEY).
                                  #          - Can also be OpenSSL engines, as ENGINE:KEY
                                  #        - sslrootcert: CA certificate FILE (def: ~/.postgresql/root.crt or PGSSLROOTCERT)
                                  #        - sslcrl: CA revocation list FILE (def: ~/.postgresql/root.crl or PGSSLCRL)
                                  #    - krb5: Kerberos5
                                  #      - libpq variable krbsrvname: Kerberos server name (def: PGREALM or PGKRBSRVNAME)
                                  #    - GSSAPI:
                                  #      - libpq variable gsslib: on Windows, set to "gssapi" to use GSSAPI instead of SSPI
                                  #        (def: PGGSSLIB)
                                  #  - auth. method options (optional):
                                  #    - as VAR=VAL ...
                                  #    - see above
                                  #Default settings:
                                  #  - host and local connections: initdb --auth-host|local=STR (or -A STR for both)
                                  #  - default password: initdb -W|--pwfile=FILE, from stdin or from first line of FILE (if using
                                  #    password authentication)
                                  #Only the first matching line is chosen. Put most likely first for efficiency.
                                  #Location can be changed by ENVVAR hba_file
                                  #Read on server startup, or when receiving a SIGHUP.

requirekeeper                     #LIBPQ variable: OS_USER behind the server process must match STR.
                                  #Avoid another OS_USER starting the server while legit one is rebooting it.

pg_hba_file_rules                 #TABLE with pg_gba.conf contents. ???
pg_ident_file_mappings            #TABLE with pg_ident.conf contents. ???
pg_stat_ssl                       #???
pg_stat_gssapi                    #???

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      ROW-LEVEL SECURITY       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DOCUMENT, including: ???
  - alter table ... row level security
  - pg_policies, pg_policy

pg_class.relrowsecurity           #BOOL. Whether RELATION has row-level security
pg_tables.rowsecurity             #BOOL. Whether TABLE has row-level security
pg_class.relforcerowsecurity      #BOOL. Whether RELATION's row-level security applies to owner


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        SECURITY LABELS        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DOCUMENT, including: ???
   - pg_[sh]seclabel, pg_seclabels


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EXTENSIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create extension                  #Activate an EXTENSION, i.e. a collection of objects (library), except DATABASE, ROLE, INDEX and
EXTENSION                         #TABLESPACE, for a specific DATABASE.
[schema SCHEMA]                   #STR is the version (def: the one in the default_version)
[version STR]                     #SCHEMA is for objects defined in EXTENSION (default: current SCHEMA)
                                  #Some EXTENSIONs are already present in the default compilation of PostgreSQL, or personalized
                                  #compilations (but usually not activated).
                                  #Permissions are the ones required in executing the underlying SQL file.
                                  #Dependency parent to all its ENTITYs (even with `drop ... restrict`)
                                  #Files must already be present in SHAREDIR/extensions/:
                                  #  - a SQL file "EXTENSION--VERSION.sql" defining new ROW, FUNC, etc.:
                                  #     - implicitly inside a transaction, so cannot use transaction statements
                                  #     - do select pg_extension_config_dump(STR, STR2); for a TABLE named STR used for users to
                                  #       personalize the extension.
                                  #       It will make the TABLE backupable, as opposed to rest of EXTENSION.
                                  #       STR is a filter, e.g. 'where BOOL' or ''.
                                  #     - should include protective lines:
                                  #       -- complain if script is sourced in psql, rather than via CREATE EXTENSION
                                  #       \echo Use "CREATE EXTENSION pgrowlocks" to load this file. \quit
                                  #  - an ASCII text file "EXTENSION.control" defining metadata as VAR = VAL ..., with #comment possible:
                                  #     - default_version STR: current version
                                  #     - relocatable BOOL (def: false): if SCHEMA can be changed after create extension
                                  #     - comment STR: description
                                  #     - directory STR: DIR of the SQL file (def: same as *.control file)
                                  #     - encoding STR (def: client's encoding): to be defined in no ASCII characters
                                  #     - module_pathname STR:
                                  #       - can be used as MODULE_PATHNAME in the *.sql file
                                  #       - can use $libdir, e.g. '$libdir/myextension'
                                  #     - requires STR...: other extensions this one depends on
                                  #     - superuser BOOL (def: true): if only superuser can create extension
                                  #     - schema STR: def. SCHEMA (only if non-relocatable)
                                  #  - additional EXTENSION--VERSION.control with same format but for a specific VERSION can be defined
                                  #  - additional EXTENSION--VERSION--VERSION2.sql are executed to go through VERSION to VERSION2 with
                                  #    alter extension update
alter extension EXTNSIN
add|drop TYPE VAR                 #

alter ENTITY "ENTITY" [no]        #Mark as dependency, for `drop ... cascade`
 depends on extension "EXTENSION" #For ENTITY: function, routine, procedure, trigger, materialized view, index

load STR                          #Loads a shared library located at STR, in order:
                                  #  - an absolute path
                                  #  - or look into ENVVAR dynamic_library_path, which a colon-separated list (def: $libdir),
                                  #    where items can start with $libdir (def: /usr/lib/postgresql/9.3/lib/)
                                  #    (can be seen with pg_config --pkglibdir)
                                  #If not superuser, must be inside $libdir/plugins/
                                  #STR path convention is OS-specific.
                                  #Not useful if only functions definitions because they are loaded automatically with create function...
                                  #Can also use ENVVAR shared_preload_libraries (comma-separated list), without the extension .so

pg_extension                      #Activated EXTENSION
                                  #???
pg_available_extensions           #Possible EXTENSION for their current version
                                  #???
pg_available_extension_versions   #Possible EXTENSION for all versions
                                  #???

create foreign data               #Creates a FDW, i.e. functions that permits using another DBMS inside PostgreSQL.
wrapper FDW                       #Can be slow and not optimized.
[handler FUNC]                    #Based on a standard implemented by other DBMS, including noSQL.
[validator FUNC2]                 #FUNC and FUNC2: see doc. on how to create them.
[options (VAR STR ...)]           #Must be superuser.
                                  #Available ones (as EXTENSION):
                                  #  - postgres_fdw: for PostgreSQL to other DBMS
                                  #     - FSERVER options (can also use libpq variables)
                                  #        - host STR
                                  #        - dbname STR
                                  #        - port STR
                                  #     - user mapping options:
                                  #        - user STR
                                  #        - password STR
                                  #        - client_encoding STR (def: ENVVAR client_encoding)
                                  #     - FTABLE options:
                                  #        - schema|table|column_name STR: if name is different
                                  #     - FSERVER or FTABLE options!
                                  #        - updatable BOOL (def: true): read-write
                                  #     - details:
                                  #        - transaction read committed -> repeatable read
                                  #  - file_fdw: for CSV file (or other formats of copy COMMAND), read-only:
                                  #     - FTABLE options:
                                  #        - filename STR: absolute path
                                  #        - ...: same options as copy COMMAND, except force* and oids
                                  #Parent dependency of foreign table and server
alter foreign data wrapper FDW
[no handler|handler FUNC]         #
[no validator|validator FUNC]     #
options
 ([add|set|drop] VAR VAL ...)     #

pg_foreign_data_wrapper           #All FDW. ???
pg_attribute.attfdwoptions text[] Attribute-level foreign data wrapper options, as “keyword=value” strings ???

create server FSERVER [type STR]
[version STR2]                    #Creates a FSERVER, i.e. a connection to a specific DATABASE using a FDW.
foreign data wrapper FDW          #STR are all FDW-dependent options. They usually specify the connection details
[options (VAR STR3...)]           #(host and port, database name, etc.)
alter server FSERVER
[version STR]
[options
 ([add|set|drop] VAR STR2...)]    #

pg_foreign_server                 #All FSERVER. ???

create user mapping
for ROLE|current_user
server FSERVER                    #Specify authentication details on FSERVER for ROLE.
                                  #Child dependency of server
[options (VAR STR ...)]           #STR are all FDW-dependent options.
alter user mapping ...
[options ([add|set|drop] ... )]   #

pg_user_mapping[s]                #All USERMAPPING. With s, leaves out the option field when user should not see it
                                  #???

create foreign table "FTABLE"
 ("COL" TYPE
 [options (VAR STR ...)]
 CONSTRAINT|not null|
 default VAL ...)                 #Creates a FTABLE (can be used as a TABLE) while connected to a FSERVER.
server FSERVER                    #TABLE, COL, TYPE, CONSTRAINT, etc. should be same as on FSERVER, unless specified in the options.
[options (VAR STR2 ...)]          #But can only select some of the foreign COL.

alter foreign table "FTABLE"
 rename "COL" to "COL2"           #

alter foreign table "FTABLE"
 add COL_ARG                      #

alter foreign table "FTABLE"
 drop [if exists] "COL"
 [restrict|cascade]               #

alter foreign table "FTABLE"
 alter "COL" type TYPE            #

alter foreign table "FTABLE"
 alter "COL" set|drop
 not null|default [VAL]           #Same as alter table ...

alter foreign table "FTABLE"
 [alter "COL"] options
 ([add|set|drop] VAR VAL ...)     #

pg_foreign_table                  #All FTABLE. ???

ENVVAR enable_async_append, and asynchronous appends ???

"Foreign Scan" when using explain ???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CLUSTER            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CLUSTER ==>                       #Set of DATABASEs, managed by a single instance of a server daemon (postgres).
                                  #Linked to a DATADIR

CLUSTER-WIDE ENTITYS ==>          #The following ENTITYs are cluster-wide, not DATABASE-wide:
                                  #  - DATABASE (including their ENVVAR, templates)
                                  #  - TABLESPACE
                                  #  - ROLE (including membership, ACL, security label)
                                  #  - LANGUAGE
                                  #  - logical replication (including SUBSCRIPTION)

                                  #Also dependencies and COMMENTs on cluster-wise ENTITYs
pg_class.relisshared              #BOOL. Whether RELATION is cluster-wide
                                  #Only true for pg_catalog.* TABLEs (and their INDEX and TOAST_TABLEs) about cluster-wide ENTITYs

CLUSTER ENVVARS ==>               #ENVVARs can be cluster-wide


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATABASE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create database DATABAS           #Create a DATABASE, i.e. set of SCHEMAs
                                  #Max 4e9 DATABASEs per CLUSTER
[owner ROLE]                      #  - ROLE is current one by def
[template DATABASE2]              #  - DATABASE2 is the initial state of DATABASE:
[connection limit INT]            #     - It is a default DATABASE2 called template1 by def. (created by initdb)
                                  #     - defines locales (see initdb)
                                  #     - template0 is the same as template1, but template1 can be modified to have specific initial
                                  #       state of DATABASE, while keeping template0 as the initial template if needed.
                                  #     - DATABASE2 cannot be accessed just before and during create database
                                  #  - connection limit: number of concurrent non-superuser client sessions max (def: -1, i.e. unlim)
                                  #Cannot be inside a transaction.
alter database DATABASE
connection limit INT              #

drop database ...                 #Cannot be done if currently connected to it
 [with force]                     #Terminate ongoing connections. Fail if could not

pg_database                       #All DATABASE, including:
                                  #  - datistemplate BOOL: can be used in create database template DATABASE
                                  #  - datallowconn BOOL: if false, no one can connect to it
                                  #???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SERVER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INSTALLATION ==>                  #  - Server: postgresql
                                  #  - Client: postgresql-client, pgadmin3
                                  #  - Extensions: postgresql-contrib, postgresql-EXTENSION
                                  #  - Extension building: postgresql-server-dev
--bin|doc|html|include|
pkginclude|lib|pkglib|            #Print DIR. By default, useful DIR (so without headers, docs, etc.) are:
locale|man|share|                 #  - /usr/lib/postgresql/VERSION/: bin and lib
sysconfdir                        #  - /usr/share/postgresql{,-common}/VERSION/
--includedir-server|pgxs          #  - /etc/postgresql-common/: few conf files
--configure                       #Print all configuration flags (usually to mimic another installation)
--cc|cppflags|
cflags[_sl]|
ldflags[_ex|sl]|libs              #Print makefile variables

pg_config                         #TABLE with compile-time config. ???

REGRESSION ==>                    #Running the regression tests (to unsure installation is fine):
                                  #  - download source, configure and build it
                                  #     - after building, before installation: make check (must not be root)
                                  #     - after installation: make installcheck[-parallel] (localhost, unless PGPORT or PGHOST is used)
                                  #  - when failed:
                                  #     - check src/test/regress/expected/*.out to see the test and its expected output, and
                                  #       compare with diff on src/test/regress/results/*.out
                                  #     - or directly look at src/test/regress/regression.diffs
UPGRADING ==>                     #Only needs to care about upgrade incompatibilities for major upgrades (e.g. 9.3 to 9.4), every year.
                                  #How:
                                  #  - use pg_dumpall backup
                                  #  - use pg_upgrade -b OLDBINDIR -B NEWBINDIR -d OLDDATADIR -D NEWDATADIR. Flags are:
                                  #     -u USER
                                  #     -c dry-run
                                  #     -j NUM: multiprocess run
                                  #     -k: use hard links instead of copies. Faster.
                                  #     -p|P NUM: old|new port number (can also use PGPORTOLD|NEW)
                                  #    NEWBINDIR and NEWDATADIR must be a new install, with similar settings.
                                  #    OLD*DIR must be erased afterwards.
                                  #    Connects to OLDDATADIR -> might want to put authentication as trust temporarily
                                  #    Both servers must be down.
                                  #    Must use NEWBINDIR/pg_upgrade, not OLDBINDIR.
                                  #    Cannot upgrade a log shipping standby.

STARTING ==>                      #If DATADIR (e.g. /usr/local/pgsql/data), as postgres user:
                                  #  - initdb -D DATADIR: creates cluster
                                  #  - postgres -D DATADIR 2> LOG &: starts the server
                                  #  - createdb DATABASE (default "postgres" DATABASE could be used though)
                                  #  - psql [DATABASE]
                                  #Autostart at boot time are system-specific.
                                  #Have a lot of memory PC to avoid memory crash if big data.
STOPPING ==>                      #Terminate server:
                                  #  - Can send signals:
                                  #    - SIGTERM (15): doesn't accept new connections, but let current ones finish.
                                  #       - server process stops, but will redo uncaught statements when restarting
                                  #    - SIGINT (2, CTRL-C): close all connections with a SIGTERM: abort current statements but close
                                  #      server itself propertly
                                  #    - SIGQUIT (3, CTRL-\): close all connections with a SIGQUIT: abort statements and server.
                                  #  - to send a signal on Windows, use pg_ctl kill ...
                                  #  - PID of a cluster can be found in DATADIR/postmaster.pid (ENVVAR external_pid_file) (first line)
                                  #    or with pg_backend_pid().
                                  #Terminate a client session (superuser or same member, but not same user):
                                  #  - pg_terminate_backend(PID)
                                  #  - send SIGTERM (15)
                                  #  - can see client PID with pg_stat_activity (pid)
                                  #Terminate a client current statement (superuser or same member, but not same user):
                                  #  - pg_cancel_backend(PID)
                                  #  - send INT (2, CTRL-C)
LOCATIONS ==>                     #  - Each cluster has a DATADIR (must have permissions 0700)
                                  #  - SHAREDIR, e.g. /usr/share/postgresql/VERSION/

initdb [-D DATADIR]               #Creates a cluster, i.e.:
                                  #  - populates or create DATADIR. Def: PGDATA
                                  #     - directory under which all data of the cluster are stored.
                                  #     - /usr/local/pgsql/data/ or /var/lib/pgsql/data/ are often used.
                                  #  - creates template0|1 and an empty database "postgres"
                                  #  - create cluster-specific system catalogs
                                  #  - initialize cluster-wide ENVVARs
                                  #User:
                                  #  - should have right to write in DATADIR
                                  #  - will use current OS_USER as the superuser ROLE (will create the ROLE in the cluster)
                                  #     - good idea to use a OS_USER which doesn't have any permissions on the filesystem apart from
                                  #       the directories used by the server.
                                  #        - A OS_USER "postgres" is created by def. installation for this purpose.
                                  #  - can't be root
-U ROLE                           #Name of the superuser ROLE (by def. the current OS_USER).
                                  #Change the ROLE name but not:
                                  #  - the fact that OS_USER will be superuser
                                  #  - nor the name of the default database "postgres" (but could rename it)
-E STR                            #See encoding in this doc.
--auth-host|local=STR             #
-A STR                            #
-W
--pwfile=FILE                     #Default authentification, see pg_hba.conf
-X DIR                            #Log DIR

LIBPQ VARIABLES ==>               #Used in several places in this doc.
host[addr]                        #Def is localhost or PGHOST[ADDR]. Can be an absolute path to a Unix socket.
                                  #With addr:
                                  #  - specifies IP address
                                  #  - avoid host name lookup, faster.
                                  #  - Can't be used with Kerberos, GSSAPI, SSPI or verify-full SSL
port                              #Def is PGPORT (5432)
dbname                            #Def: OS_USER or PGDATABASE
user                              #ROLE to use. Def: OS_USER or PGUSER
connect_timeout                   #In seconds. Def: 0 (unlim). Should not be <2. Def: PGCONNECT_TIMEOUT
client_encoding                   #Def: "auto" (defaults OS locales). Def: PGCLIENT_ENCODING
options                           #Set postgres flags at runtime, client-side. Def: PGOPTIONS
                                  #Can use -c ENVVAR=VAL to set ENVVAR.
[fallback_]application_name       #Name of the application (psql, pg_dump, etc.)
                                  #Used in logs (title of the client connection). Can be env. variable PGAPPNAME too.
                                  #If application_name or PGAPPNAME is blank, defaults to fallback_* (let users override it)
keepalives                        #1 to enable it (def). If connection seems lost, sends packet to check it actually is.
keepalives_                       #Send max *count packets, every *interval seconds. Start sending after non-activity from server for
[idle|iterval|count]              #*idle packets.
                                  #Can also use ENVVAR tcp_keepalives_*
OTHERS ==>                        #See Authentication section
service                           #Additional parameters (def: PGSERVICE[FILE])

postgres [-D DATADIR]             #Starts the server daemon for a specific cluster.
                                  #To start postgres on several clusters on same machine, use different ports.
                                  #Only one server can be launched for a given cluster. Each client session spawns a new process.
                                  #Should be logged as the same OS_USER who used initdb. It should have permissions to access files that
                                  #will be used:
                                  #   - DIR of TABLESPACE
                                  #   - FILE referenced by copy from|to, load STR or create function ... as STR
                                  #DATADIR is PGDATA by def (unset by def). Can also use ENVVAR data_directory, and PGDATA or -D DATADIR
                                  #will point to the directory containing the postgresql.conf.
                                  #Print log messages on stderr. Should be launched in the background.
                                  #Can call version(), ENVVAR server_version[_num] and also a file DATADIR/PG_VERSION.
                                  #Commands that started the server can be found under DATADIR/postmaster.opts
                                  #Connections are done using either:
                                  #  - Unix sockets ("local"), on a socket at DIR/.s.PSQL.PORT (created at server start), where
                                  #    DIR is designated by ENVVAR unix_socket_directories (def: /tmp or /var/lib/postgresql/),
                                  #    comma-separated list by order of preference. Can be "" to disallow "local" connections.
                                  #    OS_USER must (to avoid spoofing) have:
                                  #      - permissions on DIR, preferably only that user
                                  #      - socket itself will be owned by OS_USER and OS_GROUP_USER (can be changed with ENVVAR
                                  #        unix_socket_group)
                                  #      - socket itself will have permission ENVVAR unix_socket_permissions (def: 0777). Could set to
                                  #        0770 or 0700 (only OS_GROUP_USER+OS_USER or only OS_USER can connect)
                                  #    Client must connect by using DIR as host
                                  #  - TCP/IP ("host"), which uses socket designated by IP addresses specified in ENVVAR
                                  #    listen_addresses (comma-separated) with IPv4|6 addresses (def: "localhost"). Should be set at
                                  #    server start.
                                  #    "*" means all IPv4|6, "0.0.0.0" all IPv4, "::" all IPv6
                                  #If crash, sessions will restart automatically if ENVVAR restart_after_crash (def: on)
-p NUM                            #Port number (def: 5432).
                                  #Can also use ENVVAR port at server start.
-k STR                            #Sets ENVVAR unix_socket_directories.
-h STR                            #Sets ENVVAR listen_addresses
-i                                #Same as -h "*"
-N                                #Sets ENVVAR max_connections (def: 100), which is the number of superuser + normal users max
                                  #connections. The last ENVVAR superuser_reserved_connections (def: 3) are kept for superuser only.
                                  #Look at work_mem to see estimate of memory used by this configuration.
                                  #Lower max_connections means more connections denied, higher means more chance to crash server.
                                  #Look at real life usage to set this setting.
                                  #Can also sets database-wise with connection limit (see create database)
--single                          #Single-user mode: start both the daemon and a superuser client session
                                  #Must be put before -D DATADIR
                                  #Quits with EOF (C-D). No readline.
                                  #By default use newlines instead of semicolons. Use -j to use EOF instead (then there will be only one
                                  #command).
-l                                #Enable SSL connections

pg_isready                        #Returns (exit code) according to server up or down status:
                                  #  - 0: OK
                                  #  - 1: refusing connections
                                  #  - 2: no server response
                                  #  - 3: could not send request
-d -h -p -U -w|W                  #Connection options (see psql)
-t NUM                            #Timeout (in sec, def 3, 0 to disable)
-q                                #quiet

ENVVAR ==>                        #Environment variables. Can be specified with:
                                  #  - cluster-specific:
                                  #     - postgres -c VAR=VAL or postgres --VAR=VAL
                                  #     - editing DATADIR/postgresql.conf (ENVVAR config_file)
                                  #        - has lines VAR = VAL, and #comment
                                  #        - can have include[_if_exists] 'FILE' or include_dir 'DIR' (includes DIR/*.conf, by
                                  #        - alphabetical order)
                                  #        - can use pg_reload_conf() or send SIGHUP to server daemon postgres
                                  #        - can see load time with pg_conf_load_time()
                                  #     - PGOPTIONS
                                  #  - role-specific:
                                  #     - alter role ROLE ... set ENVVAR to VAL | from current
                                  #                           reset ENVVAR|all
                                  #  - database-specific:
                                  #     - alter role all in database DATABASE ... set ENVVAR from current
                                  #                                               reset ENVVAR|all
                                  #  - role-database-specific:
                                  #     - alter role ROLE in database DATABASE ... set ENVVAR to VAL | from current
                                  #                                                reset ENVVAR|all
                                  #  - session-specific:
                                  #     - set ...
                                  #     - setting PGOPTIONS with '-c VAR=VAL ...' before launching the client command
                                  #  - function-specific:
                                  #     - do create function ... set ENVVAR to VAL | from current:
                                  #        - from current: use current VAL as VAL
                                  #     - inside a function:
                                  #        - set local (if FUNC() used create function ... set ...)
                                  #  - transaction-specific:
                                  #     - set local ...
                                  #Some require superuser ROLE to write.
                                  #Some cannot be session-specific.
                                  #Are always STR. BOOL actually use 'on' and 'off'.
set [local] ENVVAR to VAL         #VAL can be default
                                  #For an ARR, write VAL...
                                  #Can also use set_config('ENVVAR', 'VAL', BOOL) (BOOL is is_local)
reset ENVVAR|all                  #Same as set ... to default
show ENVVAR|all                   #Print their values.
                                  #Can also use current_setting('ENVVAR')
                                  #Can also use postgres ... -C ENVVAR, which is done against a running server
alter TYPE VAR ... set ENVVAR
 to VAL|from current              #
alter TYPE VAR ...
 reset ENVVAR|all                 #For database, function or role

pg_settings                       #All ENVVAR, with also when is readonly, type, min|max value, unit
                                  #???
pg_db_role_setting                #All role and/or database-specific ENVVAR
                                  #???
pg_file_settings                  #TABLE with config ENVVARs. ???

discard plans|temp|all            #Remove session-specific information:
                                  #  - plans: cached explain plans
                                  #  - temp: TEMP
                                  #  - all: cached explain plans, TEMP, session-specific ENVVAR, PREP, CURSOR, unlisten *,
                                  #    pg_advisory_unlock_all(), put session_user and current_user to default

pg_postmaster_start_time()        #
current_database()                #Can also use information_schema.information_schema_catalog_name.catalog_name
current_query()                   #Current executing statement
inet_client|server_addr|port()    #

statement_timeout                 #ENVVAR (in ms) after which client requests fail (def: 0).

FILESYSTEM ACCESS ==>             #
pg_ls_dir(STR)                    #Returns 'COL'. Must be relative path (.. not allowed)
pg_read[_binary]_file(STR)        #Returns as STR|BYTEA
pg_stat_file(STR)->TABLE          #Single row: size, access (atime), modification (mtime), change (ctime), creation (creation time, only Windows), isdir.
pg_file_write(STR, STR2, BOOL)    #BOOL is append
                                  #Trusted postgres extension 'adminpack'
pg_file_read(STR, STR2, BOOL)     #BOOL is append
pg_file_rename(STR, STR2)         #
pg_file_unlink(STR)               #
pg_file_length(STR)               #

DATADIR ==>                       #  - base/: databases main data.
                                  #    Querying database|tables oids:
                                  #      - utility oid2name:
                                  #         - without option: database oid -> name
                                  #         - -H -p -U -P: usual connection flags
                                  #         - -d DATABASE: show table oid -> name
                                  #           - -S: include system catalogs, views and TOAST
                                  #         - -f|o NUM: for tables with filenode|oid NUM
                                  #         - -t STR: same with name
                                  #         - -i: include SEQUENCE and INDEX
                                  #         - -s: info about TABLESPACE
                                  #      - pg_relation_filenode|filepath(OID|STR)
                                  #    6MB for an empty database. Each file is a table with optionally:
                                  #      - a FILE_vm: Visibility Map
                                  #      - a FILE_fsm: Free Space Map
                                  #    TOAST are special subfiles when a file is too big.
                                  #  - global/: cluster-wide data.
                                  #  - pg_xlog/: WAL
                                  #  - pg_log/: logs (see log_directory)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CLIENT             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


psql                              #Starts interactive commands while connecting to a DATABASE
                                  #Read from standard input, so <<<"STR" or <FILE are possible.
                                  #Use readline so can use a .inputrc
                                  #The five first options are connection-related, and shared by other commands. When available, can
                                  #also always use a STR argument instead with either:
                                  #  - VAR=VAL... as space-separated libpq variables
                                  #  - postgres://[USER:PASSWORD@][HOST][:PORT]/[DATABASE][?VAR=VAL] (VAR are libpq variables)
-d DATABASE                       #Def: "postgres"
-h HOST                           #Def: PGHOST or (if absent) local host.
-p PORT                           #Def: PGPORT or (if absent) default port.
-U USER                           #Def: PGUSER or (if absent) current OS user.
-w|W                              #Force to not use or use a password

-c STR
-f FILE                           #Redirect input from STR or FILE (non-interactive shell)
-o FILE                           #Redirect output to FILE
-L FILE                           #Prints queries to FILE (logging)

-X                                #Do not read init files (files read at session start containing any command that PSQL understands)
                                  #Init file can be:
                                  #  - systemwide (/etc/postgresql-common/psqlrc)
                                  #  - user-specific (shell variable PSQLRC, by def HOME/.psqlrc)
                                  #Can append -NUM[.NUM2[.NUM3]] to target only specific PostgreSQL versions.
-n                                #Don't use readline (useful when pasting)
-s                                #Ask for confirmation before each command
-1                                #Wrap commands in a transaction block. Commands should not contain transaction blocks themselves.
                                  #Disable all EFUNCs
-l                                #Does \list then exits

\COMMAND                          #psql can use special commands. Finished by newline not semicolon.
                                  #Can use \n \t \r \000 \x00 in 'STR'. Can do shell substitution with `COMMAND`
                                  #Can appear anywhere a SQL command can.
                                  #Some commands use SREGEXP:
                                  #  - . only means SCHEMA separation (otherwise current schema).
                                  #    For all objects outside current SCHEMA, do *.*
                                  #  - $ not present
                                  #  - ? and * are globbing
                                  #  - case-insensitive unless ""

\q                                #Exits
\h [COMMAND]                      #Help on SQL commands
\?                                #Help on PSQL commands
\! [COMMAND]                      #Execute a shell command according to the shell variable SHELL
\timing [on|off]                  #Show time taken by commands

\pset VAR [VAL]                   #If VAL, sets printing options, if not either toggle or print current value.
                                  #VAR [VAL] can be:
format STR                        #How things are printed. Can be unaligned, aligned (def), wrapped, html, latex[-long table], troff-ms.
                                  #  - Wrapped: like aligned but wrap long lines according to columns INT (if 0, only on terminal
                                  #    output, and uses shell env variable COLUMNS or detected screen size)
                                  #  - unaligned: can control separators with field|recordsep STR or field|recordsep_zero (use \0)
x [on|off|auto]                   #Put in expanded mode (switch cols and rows, good for table with long width). If auto, do it only for
                                  #wide tables.
t                                 #No footer nor headers
footer [on|off]                   #footer is command tag, etc.
title STR                         #Print STR in front of all tables
border INT                        #Column width
linestyle ascii|unicode           #
null STR                          #What to print for null strings (def: "")
numericlocale on|off              #Locale-specific NUM
pager on|off|always               #Using env variable PAGER (def: less)
tableattr STR                     #In HTML output format, HTML attributes of <table>

END OF LINE ==>                   #The seven following commands should be put at the end of a command (instead of ;):
\w FILE| |COMMAND                 #Print current input to FILE or pipe to COMMAND (don't execute it)
\p                                #Same as \w |cat
\g [FILE| |COMMAND]               #Redirect output like \o, but for current input only.
\r                                #Clears current input.
\e [FILE]                         #Edit current command (or if FILE, FILE) with editor (shell variable [PSQL_]EDITOR, i.e. vim), which
                                  #becomes the new command.
                                  #New command is only executed if terminated by ;
\ef [FUNC[(...)]]                 #Same but the command is a "create or replace function FUNC". If no FUNC, creates a empty definition.
\watch INT                        #Execute current input every INT seconds, until interrupted (or error).

\o [FILE| |COMMAND]               #Redirect stdout (not stderr) (def: to stdout)
\i[r] FILE                        #Execute command in FILE.
                                  #If r and non-interactive, relative path to script DIR, otherwise relative to PWD)
\copy ...                         #Like SQL copy but performed locally, not on the server.
                                  #Can use pstdin|out to avoid any stdin|stdout redirection (i.e. use terminal input|output)
\lo_export OID STR
\lo_import STR                    #Like lo_ex|import(...), but performed locally, not on the server.

\[q]echo [-n] VAL                 #Prints VAL (use q if \o has been used), without trailing newline if -n
                                  #Can appear anywhere in a SQL command.
\setenv VAR [VAL]                 #Sets shell environment variables

\d...[S][+] [SREGEXP]             #Show info about VAR specified in ... (S for also system ones, + for more info) among:
                                  #  - nothing (all RELATIONs), t (table), v (view), i (index), m (materialized view), s (sequence)
                                  #  - u (ROLE), dp (default user privilege), p (all RELATIONs with privileges)
                                  #  - d (constraint, operator*, rule, trigger)
                                  #  - n (SCHEMA)
                                  #  - b (tablespace)
                                  #  - L (LANGUAGE), x (EXTENSION)
                                  #  - T (TYPE), D (domain), C (cast)
                                  #  - O (COLLATION), c (conversion)
                                  #  - E|et (ftable), es (foreign server), eu (user mapping), ew (foreign data wrapper)
                                  #  - f[n|a|w|t] (func, afunc, wfunc, tfunc), y (efunc), o (OPERATOR)
                                  #  - F (REGCONF), Fd (DICTIONARY), Fp (PARSER), Ft (TEMPLATE)
                                  #  - l (large objects)
                                  #  - rds: ENVVAR, ROLE-specific (SREGEXP) and optionally database-specific too (use a second SREGEXP)
                                  #  - \l[+] (not \d): DATABASE
                                  #TODO: move those to their respective chapters in this doc???
\sf [FUNC[(...)]]                 #Prints definition of FUNC

\c [DATABASE [USER]
[HOST] [PORT]]                    #Connect to different database. Def is current
\conninfo                         #Print current connection info
\cd [DIR]                         #Change PWD (def: HOME)

\[un]set INTVAR [VAL]             #Internal VAR. INTVAR is case-sensitive. Are not ENVVAR.
                                  #Can also use psql -v INTVAR[=[VAL]]
                                  #Can be created INTVAR. Substitution is done with:
                                  #  - :INTVAR: macro expansion of INTVAR
                                  #  - :'INTVAR': same but surround with '', unless already present (do with STR)
                                  #  - :"INTVAR": same with "" (do with VAR)
\gset [WORD]                      #Put at end of command (like \p, \r, etc.)
                                  #Command output is redirected to new INTVAR (one by column) called [WORD_]"COL".
                                  #Columns must be named. Null give unset variables, failing commands don't change variables
\prompt [-f] [STR]
INTVAR                            #Prompt for INTVAR, with message STR. Use terminal (no -f) or stdin/stdout (-f)

DBNAME                            #
HOST                              #
PORT                              #
USER                              #Connections info

ON_ERROR_STOP                     #When set, errors terminate script (non-interactive) or line (interactive)
                                  #ENVVAR exit_on_error is also available, where errors terminate whole session.
ON_ERROR_ROLLBACK                 #If on, errors in transactions are just ignored. If off (def), they abort the whole transaction.
on|interactive|off                #Interactive means on for interactive and off for non-interactive.
IGNOREEOF INT                     #Number of EOF (C-D) to send to terminate a session (def: 1)

PROMPT1|2|3                       #Prompt. Literal STR with possible sequences escaped by %: M (full host), m (short host), > (port),
                                  #n (user), / (database), ~ (database, but ~ if default one), # (# if superuser, > otherwise),
                                  #R (= if normal, ! if disconnected), x (transaction block), NNN (octal), `command`, :VAR:,
                                  #[ and ] (ansi escaping sequences). Def is %/%#
                                  #PROMPT1 is normal, PROMPT2 when continuing on a new line, PROMPT3 when reading from stdin
COMP_KEYWORD_CASE
[preserve-]lower|upper            #Completion case. If preserve, tries to keep current word case.
QUIET                             #Don't print welcome message
ECHO                              #When set to '' (def), do nothing.
                                  #When 'queries', print all input to output.
                                  #When 'all', same but only for non-interactive input.
ECHO_HIDDEN                       #When set, prints commands behind \ commands

HISTFILE                          #Def: ~/.psql_history. Written at exit.
                                  #Can for example use user, database-specific, etc. hist files
                                  #Can also use shell variable PSQL_HISTORY
                                  #Can also use \s [FILE] (always relative to PWD) (def: print to stdout)
HISTSIZE                          #Number max of commands (def: 500)
HISTCONTROL
ignoredups|space|both             #Like in Bash

AUTOCOMMIT on|off                 #By def (on), each individual command is wrapped in a single transaction. When off, a start
                                  #transaction is implicitly fired but needs to manually commit it
LASTOID                           #OID of last written object
FETCH_COUNT                       #Number of max rows in memory at once

PGADMIN3 ==>                      #GUI client. Can do almost anything that can be done with psql.
                                  #Installing/launching:
                                  #  - MaintenanceDB:
                                  #     - DATABASE where pgAdmin connects first (should use postgres)
                                  #     - should install adminpack extension
                                  #  - Should use the same OS_USER we would use for a normal psql session
                                  #Usage:
                                  #  - Is not refreshed automatically: needs to refresh it manually.
                                  #  - Some types of objects are hidden by default (e.g. AFUNC or TYPE): can change in options
                                  #  - Servers have names, and can be grouped.
                                  #Useful tools:
                                  #  - Easy access to objects, with properties and statistics, and conf files
                                  #  - Can open a psql session in a console
                                  #  - Edit/View data: simple spreadsheet.
                                  #    Read-only if no primary key.
                                  #    blank is null (needs to write '' for '')
                                  #  - Server status: current clients, locks, log (needs to edit path in options), transactions
                                  #Less useful tools:
                                  #  - Grant wizard (available when on a SCHEMA): generate SQL grant statements with GUI
                                  #  - Reports: generating HTML files for objects properties, statistics, dependencies.
                                  #  - Query tool: useless, use the console (unless needs a SQL debugger, which needs to be installed)

PGAGENT ==>                       #  - SQL "cron", inside pgadmin3
                                  #  - to install on a cluster:
                                  #     - connect to postgres
                                  #     - install plpgsql
                                  #     - as superuser ROLE, execute pgagent.sql (i.e. in pgadmin3 sharedir).
                                  #       It will create TABLE and FUNC used for storing/manipulating the jobs and schedules, in SCHEMA
                                  #       pagent
                                  #     - launch pgagent 'LIBPQ'
                                  #        - this daemon will look at those tables and determine if need to launch job
                                  #        - run as root
                                  #        - options:
                                  #           -s LOG_FILE
                                  #           -l1 (best verbosity)
                                  #           -t (poll time, def: 10 sec)
                                  #        - do no put password in 'LIBPQ' (use passfile instead)
                                  #        - automatically in background and detached from current terminal tab
                                  #        - should automatically launch it at startup
                                  #  - configure jobs using pgadmin, under "Jobs":
                                  #     - steps are:
                                  #         - SQL commands (will be executed as superuser ROLE)
                                  #         - or "batch" (shell commands, run as root, must start with #!/bin/bash)
                                  #     - schedules are when it is done
                                  #  - monitor with check_postgres

TEAMPOSTGRESQL ==>                #Alternative to pgAdmin3. Fewer functions, but is a web interface instead of a dekstop app.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              IPC              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


notify WORD[, 'STR']              #Sends a message 'STR' (def: "") on channel WORD. Also communicates the server PID.
                                  #Any client connected to the same server and listening to WORD will get a notification.
                                  #How this notification is handled depends on the client:
                                  #  - psql: print to stderr
                                  #If sends twice the same WORD + 'STR', might notify only once.
                                  #'STR' is max 8KB
                                  #pg_notify(STR, STR2) is also available: same but don't need to use constant 'STR'.
[un]listen WORD                   #WORD can be * for unlisten
                                  #listening is session-wise: unlisten * is executed at end of each session
pg_listening_channels()           #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LOGGING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOGGING ==>                       #Controlled by several ENVVARs
log_destination                   #Where to put log messages (comma-separated list):
                                  #  - stderr (def)
                                  #  - csvlog:
                                  #     - with logging_collector, will output as CSV files
                                  #     - goal is to import them in tables with copy from
                                  #  - syslog:
                                  #     - prefer using the logging_collector
                                  #     - Needs to put local0.* /var/log/postgresql/ in syslog conf file
                                  #     - Can use ENVVAR syslog_facility, syslog_ident and event_source
logging_collector                 #When on, use the logging facility:
                                  #  - redirect stderr to file in ENVVAR log_directory too (can be relative to DATADIR) (def: pg_log).
                                  #      - files are named according to ENVVAR log_filename, which can use %... (data escape)
                                  #        (def: "postgresql-%Y-%m-%d_%H%M%S.log") for file creation time
                                  #      - a new file is created every ENVVAR log_rotation_age (def: 1d)
                                  #        or every time the file is more than ENVVAR log_rotation_size (def: 10MB)
                                  #        or when pg_rotate_file() is called
                                  #    If csvlog, will be in CSV format and use name ENVVAR application_name.
                                  #  - Files have permission ENVVAR log_file_mode (def: 0600, only server owner can read/write)

log|client_min_messages           #Between debug5-1, log, notice, warning, error, fatal, panic (def: notice for client, warning for log) for
                                  #either client messages or logging.
                                  #postgres -d NUM can set log_min_messages (from 0 to 5, def: 0), for DEBUG5-1
log_error_verbosity               #Verbosity of messages: default, verbose (include SQLSTATE error code) or terse (no defail, hint,
                                  #query nor context)
log_min_error_statement           #Same as log_min_message, but for logging the statements themselves (def: error).
log_statement                     #Which statements to log among none (def), all, ddl or mod (include ddl)
log_min_duration_                 #Logs time of statement execution, when it is higher than this limit (in ms, 0 to log all, -1 not to
statement                         #log it (def))
log_statement_stats               #If on, server prints to stderr the statements executed and the time it took
log_line_prefix                   #What to put in beginning of each log line. Can include %-escapes:
                                  #  - a: application_name
                                  #  - u: user
                                  #  - d: database
                                  #  - r: host+port
                                  #  - h: host
                                  #  - p: PID
                                  #  - t|m: timestamp (to seconds|ms). Timezone is controlled by ENVVAR log_timezone (def: 'localtime')
                                  #  - s: process start time
                                  #  - c: session id (process start time + PID)
                                  #  - i: command
                                  #  - e: error code
                                  #  - l: log number, session-wise
                                  #  - x|v: [virtual] transaction ID
log_checkpoints                   #Logs checkpoints (def: off)
log_[dis]connections              #Logs [dis]connections attempts (def: off)
log_lock_waits                    #Logs deadlocks (see deadlock_timeout)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MONITORING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


check_postgres                    #Performs several sanity monitoring tests, and outputs warnings.
-db -H -u -p                      #Connection options. Some actions requires several clusters/databases: then use STR...
--dbpass=STR...                   #Can also use STR... for any action to perform the check separately on several clusters/databases
                                  #(will return problem if any of them is wrong), or on master/slave
--output=STR                      #Output format (this doc only talks about nagios):
                                  #  - nagios (def):
                                  #     - compatible with NAGIOS (server network monitoring application)
                                  #     - one line with test name, then OK|WARNING|CRITICAL|UNKNOWN (and exit code 0 to 3 accordingly),
                                  #       followed by colon and description.
                                  #       UNKNOWN is when test cannot be performed, and WARNING|CRITICAL are set up according to
                                  #       -w|c VAL ("thresholds", depends on action). If warning = critical, turn off warnings.
                                  #     - can use option:
                                  #       --showperf=1|0: show performance at end of line (def: 1)
                                  #           --perflimit=NUM: limit --showperf to NUM items (def: 0, i.e. unlim)
                                  #           --showtime=1|0: show queries execution time (def: 1)
                                  #  - mrtg
                                  #     - compatible with MRTG (traffic load monitoring application)
                                  #     - four lines: NUM (usually main info), description (usually 0), blank and DATABASE
                                  #       (only when relevant)
                                  #     - can use option --mrtg=VAL to pass arguments to MRTG
                                  #     - usually don't issue warnings|critical
                                  #  - simple
                                  #     - like mrtg, but only first line (NUM)
                                  #     - can be followed by unit, e.g. --output=simple,MB
--action=STR...                   #Checks to perform, among:
                                  #Connections:
                                  #  - connection:
                                  #    Checks if server is up.
                                  #    CRITICAL + psql error description if no, OK + server version if yes.
                                  #  - backends:
                                  #    Checks if number of connections is more than threshold (NUM or % of max_connections) or if more
                                  #    than threshold connections are left (-NUM).
                                  #    Can only select --noidle connections.
                                  #    Look at --include below for how to specify per DATABASE.
                                  #  - pgbouncer_backends:
                                  #    Same but with pgBouncer (max_client_conn)
                                  #  - pgb_pool_maxwait|cl_active|waiting|sv_active|idle|used|tested|login:
                                  #    Checks if any pgBouncer pool has more than thresholds (see show pools in pgBouncer)
                                  #Space usage (look at --include below):
                                  #  - disk_space:
                                  #    Checks if any partition used by any data in the cluster (DATADIR, tablespaces, WAL dir, log dir)
                                  #    is using more than thresholds of memory (can use percentage, "MB", etc. units, and "and|or")
                                  #  - database|relation|index|table_size:
                                  #    Checks if any DATABASE|TABLE|INDEX is more than thresholds (can include "MB", etc.).
                                  #    Prints size in bytes (first line), and name (last line) of biggest one.
                                  #  - bloat:
                                  #    Checks if there are more than threshold (NUM (unit: 'KB', etc.) or % of TABLE size) dead rows
                                  #    in any TABLE|INDEX (only consider ones with > 10|15 pages)
                                  #  - wal_files:
                                  #    Checks if there are more than thresholds WAL files.
                                  #    Number of WAL files is usually comprised in a given range, unless there is a malfunction
                                  #    (long transaction, wrong archive_command, etc.), creating disk space usage risk.
                                  #Unusual state:
                                  #  - pgagent_jobs:
                                  #    Checks if all pgagent jobs since threshold (unit 's|m|h|d') have an exit code of 0
                                  #  - logfile:
                                  #    Checks if redirection to log file is happening correctly.
                                  #    Must provide log full path with --logfile=STR (can use "%Y|%m|%d|%H").
                                  #    Does not work if redirection to stderr without logging collector on.
                                  #  - commitratio:
                                  #    Checks if commit ratio (non-rollbacked transactions/transactions) is lower than thresholds.
                                  #  - disabled_triggers:
                                  #    Checks if number of disabled triggers is >= thresholds.
                                  #  - locks:
                                  #    Checks if number of locks held >= threshold.
                                  #  - txn_idle:
                                  #    Checks if there are more than thresholds idle current transactions (waiting for locks), and if
                                  #    any has lasted more than threshold (unit 's|m|h|d')
                                  #  - prepared_txns:
                                  #    Checks max. age of prepared transactions (not prepared statements).
                                  #Corruption:
                                  #  - sequence:
                                  #    Checks if sequence has been used more than threshold (%)
                                  #  - txn_wraparound:
                                  #    Checks if more than thresholds transactions have not been vacuumed, creating risk for
                                  #    xid wraparound. Wraparound happends every 2e9, so value should be e.g. 1.5e9
                                  #  - autovac_freeze:
                                  #    Checks if number of old transactions is more than threshold (%) of autovacuum_freeze_max_age
                                  #Performance (look at --include below):
                                  #  - query_runtime:
                                  #    Checks if queries specified by --queryname=STR (VIEW or FUNC) runs in more than time specified
                                  #  - txn|query_time:
                                  #    Same for running transactions|queries
                                  #  - hitratio:
                                  #    Checks if cache hit ratio is lower than thresholds.
                                  #  - last_[auto]analyze|vacuum:
                                  #    Checks if has been run (auto only checks autovacuum|analyze, other checks all) since threshold
                                  #    (in s|m|h|d, def: 1d for vacuum, 2d for analyze).
                                  #    Should exclude tables with no dead rows.
                                  #  - dbstats:
                                  #    For each DATABASE, print one line with backends (number of processes), commits|rollbacks
                                  #    (number since beginning), read|hit (number of blocks since beginning), ret|fetch|ins|upd|del
                                  #    (number of rows), dbname, idx..., seq...
                                  #    Cannot use alternate outputs.
                                  #Comparison:
                                  #  - same_schema:
                                  #    Compares two or more databases, schema-wise (not data-wise).
                                  #    If only one host: make a time-based comparaison: next time it will be executed, will compare
                                  #    with previous version.
                                  #    To do so, create a file at ./check_postgres.audit.port.PORT.db.DATABASE:
                                  #      - Use --replace to overwrite it.
                                  #      - can add .STR to the filename with suffix=STR
                                  #    Can exclude objects with:
                                  #      --filter=nouser|schema|table|view|index|sequence|constraint|trigger|perm|funcbody|function[='REGEXP']
                                  #      --filter=noposition: don't compare columns positions
                                  #  - settings_checksum:
                                  #    Compares two settings (ENVVAR...) for a given user.
                                  #    First use -c 0 to get checksum, then do -w|c=CHECKSUM
                                  #  - pgbouncer_checksum:
                                  #    Same but with pgBouncer
                                  #  - timesync:
                                  #    Checks if local time diff >= threshold (in sec., should not be <5)
                                  #Standbies (can all test standby mode with --assume-standby|prod-mode):
                                  #  - hot_standby_delay:
                                  #    Checks if delay between current database (master) and slave >= threshold (number of WAL lines)
                                  #  - replicate_row:
                                  #    Checks that updates of a single row takes no more than threshold to replicate using replication.
                                  #    Should choose column to change (should pick one not likely to be changed by another process),
                                  #    with --repinfo=TABLE,PKEY,PKEY_VAL(to select row),"COL",OLD_VAL,NEW_VAL
                                  #  - checkpoint:
                                  #    Checks if last checkpoint was run more than threshold ago (unit: 's|m|h|d').
                                  #    Meant to be run on a slave. Must supply --datadir DATADIR
                                  #Upgrades:
                                  #  - new_version_bc|cp|pg:
                                  #    Checks if new version of Bucardo|check_postgres|PostgreSQL is available.
                                  #    Only nagios. UNKNOWN if binary not here, CRITICAL is revision upgrade, WARNING is major upgrade.
                                  #  - version:
                                  #    Checks that server version is at least threshold
                                  #Custom:
                                  #  - custom_query:
                                  #    Checks a custom --query=STR, which returns a single column called "result", if any row value,
                                  #    depending on type of -w|c VAL:
                                  #      - NUM: >= NUM
                                  #      - NUM[KB, etc.]: >= NUM
                                  #      - STR's|m|h|d': older or same as STR
                                  #      - STR: same as STR
--in|exclude=STR...               #Limit the objects checked:
                                  #  - DATABASE: for backends, database_size, locks, query_time, txn_idle, txn_time
                                  #  - TABLE|INDEX: for bloat, index|table|relation_size, last_[auto]vacuum|analyze
                                  #  - FILESYSTEM: disk_space
                                  #include alone means "include only", but not alone means "include also" (to reinstate objects that
                                  #have been excluded with --exclude).
                                  #STR:
                                  #  - ending with . matches a schema
                                  #  - starting with ~ is a REGEXP (otherwise full VAR name)
--in|excludeuser=STR...           #Same for objects owned by 'ROLE'...
                                  #Works for relation|database_size, query|txn_time, last_[auto]vacuum|analyze.
-t NUM                            #Timeout (in secs, def: 10) after which returns UNKNOWN status, per cluster.
-v ...                            #Verbosity. Do several times to increase verbosity.
--debugoutput=LETTER...           #Prints also the psql output for a (all), c (critical), w (warning), o (ok), u (unknown)
--PGBINDIR=STR                    #psql directory (see man page on precautions to use)

pgbadger FILE[...]                #FILE... are the log files (stderr, csvlog (need Text::csv_xs module) or syslog format).
                                  #FILE can be - for stdin (not for csvlog).
                                  #Recognize compressed files from extensions .gz, .bz2 or .zip
                                  #Should put:
                                  #  - log_statement to 'none' (do not enable it)
                                  #  - log_min_duration_statement to 0
                                  #  - log_checkpoints|[dis]connections|lock_waits to 'on'
                                  #  - log_temp_files to 0
                                  #  - lc_messages to 'C'
                                  #If stderr:
                                  #  - log_line_prefix to '%t [%p]: [%l-1] user=%u,db=%d,host=%h,application=%a'
                                  #    Use pgbadger -p '%t ...' (same as above) -f stderr
                                  #Use latest release (3.3 is not)
                                  #Needs to put as much as possible in logs to get all graphs.
                                  #Can zoom it with shift button.
-f stderr|csvlog|syslog           #Def: stderr
                                  #For syslog:
                                  #  -i STR: Program name used as ident for syslog
-o FILE                           #Output file and format (among .html, .txt and .tsung). Def is output.html
                                  #Can also use -x text|html|tsung. Tsung is <sessions> tag for XML config file with most usual session.
-q                                #Quiet

-c HOST
-d DATABASE
-u USER
-N APPLICATION_NAME               #Filter for only this parameter (can be used several times)
-U USER                           #Filter for excluding USER (can be used several times)
-b|e DATE                         #Start|end time to be processed.
-l FILE                           #Only use logs starting from this log file.

-a NUM                            #Step (in min, def: 5) for the average number of query per second.
-s NUM                            #Number of sample queries (def: 3)
-t NUM                            #Number of top queries (def: 20)
--pie-limit NUM                   #Minimum percentage for pie chart slices

-S                                #Only analyze select queries
--exclude-query STR               #Exclude queries matching regexp STR
--exclude-file FILE               #Same but regexps are in FILE
--include-...                     #Inverse: include only.
-T                                #HTML <title> (def: "pgBadger")
-C                                #Remove /*comment*/ from queries
--disable-error|hourly|
type|session|temporary|
connection|query|lock|
autovacuum|checkpoint             #Remove a specific part in the report

-j|J NUM                          #Multiprocessing. Cannot be used with compressed files, csvlog or on Windows.
                                  #j is number of jobs/log file, J is number of log files in same time.

pgcluu_collectd DIR               #GUI that gives info on resource and space usage (similar to pgbadger, but gives some different stats).
                                  #pgcluu_collectd is the daemon collecting stats, pgcluu the tool creating reports
                                  #Should be run as the OS_USER owning the cluster, on a DIR owned by this OS_USER.
                                  #Good idea is to put inside DATADIR, with same permissions as other folders.
                                  #psql, sar (from package sysstat) should be installed. Their path should be given with -P|s STR if not
                                  #in /usr/bin/
                                  #Can find a sar file and several CSV files in DIR/
-d -h -p -U                       #Connection options
-D                                #Run as daemon. Can be killed with pgcluu_collectd -k
-i NUM                            #Frequency in seconds (def: 60)
-f FILE                           #PID FILE (def: /tmp/pgcluu_collectd.pid)
--stat-type all                   #Includes also system catalogs stats.
-m STR                            #Restrict data collection with a comma-separated list of metrics to perform (list can be found with
                                  #pgcluu_collectd --list-metric)
--pgbouncer-args=STR              #If pgbouncer (connection pooling utility) is used, arguments to pass to it (e.g. connection options)

pgcluu DIR                        #Creates report. DIR is the pgcluu_collectd DIR
                                  #Should be run as same OS_USER as pgcluu_collectd
                                  #sadf (from package sysstat) should be installed. Its path should be given with -s STR if not
                                  #in /usr/bin/
                                  #Can zoom in graphs
-b|e DATETIME                     #Begin|end time when to report.
-d DATABASE                       #Filter for only DATABASE (can be used several times)
-T TABLE                          #Same for TABLE (don't seem to work)
-t                                #Per table stats (don't seem to work)
-p DEVICE                         #Filter I/O info for only DEVICE (can be used several times)
-o DIR                            #DIR to create the HTML files (def: $PWD)

pg_top [NUM]                      #Show info about running PostgreSQL clients and servers in realtime, tables|indexes read|write.
                                  #psql is shown as "postgresql" command.
                                  #Must be run as the OS_USER owning the server.
                                  #If NUM, only show NUM first processes.
                                  #Accepts the following keystrokes:
                                  #  C-L: refresh
                                  #  R|X: switch with table|index stats
                                  #    t: show cumulative, not instant stats
                                  #  i: toggle display of idle processes
                                  #  k: kill
                                  #  o: change sorting
                                  #  Q: show current query
                                  #  u: show only specific user
                                  #Also available for smartphones/tablets.
-I                                #Do not display idle processes.
-o FIELD                          #Sorts according to FIELD
-z USER                           #Filter for only USER
-x [NUM]                          #Prints NUM first processes (def: "all"), then exits.
-c                                #Show command name instead of full command line
-s NUM                            #Delay in seconds (def: 5)
-r                                #Connects to a remote database. Needs to use -h -p -U -W connection options.

DETAILED MONITORING ==>           #Usually not needed, because there are higher-level monitoring tools:
                                  #  - ps auxww | grep ^postgres: see individual processes and description:
                                  #     - postgres master process
                                  #     - several master background processes: checkpoints, WAL, autovacuum, statistics collector
                                  #     - each client connection has one process with description showing CLIENT DATABASE HOST ACTIVITY
                                  #       (autoupdate can be turned on|off by ENVVAR update_process_title)
                                  #  - statistic collector:
                                  #     - daemon that fill in pg_stat* system views
                                  #     - used to collect statistics on activity
                                  #     - controlled by several ENVVAR BOOL:
                                  #        - track_activities (def: 'on') (COMMAND executed and time of execution)
                                  #           - track_activity_query_size (size of tracks in track_activities, def: 1024). Can only be
                                  #             set at server start.
                                  #        - track_counts (def: 'on') (general activity). Also allows explain.
                                  #        - track_io_timing (def: 'off') (I/O timing). Can be slow.
                                  #        - track_functions (def: 'none') (FUNC calls). Can also be 'pl' (PL/*) or 'all' (PL/*, SQL
                                  #          and C functions)
                                  #     - temp stats are stored in ENVVAR DATADIR/stats_temp_directory (def: 'pg_stat_tmp').
                                  #       Putting it in a RAM disk can improve performance.

pg_stat_activity                  #Server processes, with names, usernames, start|last time, addresses and command activity.
pg_stat_bgwriter                  #Background writer process's activity, e.g. for checkpoints.
pg_stat_database                  #DATABASE, with number of servers/clients, transactions, temp files, tuples manipulated
                                  #(fetch|select|insert|update|delete), blocks read|hits, time spent on I/O read|write, deadlocks.
pg_stat[_xact|io]                 #TABLE, with:
 all|sys|user_tables              #  - not io: number of sequential|indexed scans (and tuples they fetched), tuples manipulated
                                  #    (insert|update|delete), number of rows (live|dead), [auto]vacuum|analyze activity
                                  #  - io: disk read|hits for all, index-only, TOAST and TOAST index
                                  #Can be for only system catalogs (sys) or not (user).
                                  #If xact_, take the current transaction into account.
pg_stat[io]_all|sys|user_indexes  #INDEX, with:
                                  #  - not io: number of scan (with tuples fetched: bitmap + simple index scan, or simple index scan
                                  #    only)
                                  #  - io: index blocks read|hits (efficient if low read/hits %)
pg_statio_all|sys|user_sequences  #SEQUENCE, with number of blocks read|hits
pg_stat[_xact]_user_functions     #FUNC, with number of calls and total time (only FUNC, or FUNC called by it too).
                                  #ENVVAR track_functions must be on.

pg_backend_memory_contexts
pg_shmem_allocations
pg_stat_slru                      #???

pg_stat_statements                #VIEW for all queries (query, time, number of rows, I/O).
                                  #Can be reset with pg_stat_statements_reset().
                                  #Can use ENVVAR pg_stat_statementsmax (def: 1000)
                                  #Must put pg_stat_statements in ENVVAR shared_preload_libraries and use EXTENSION pg_stat_statements.

OBJECT SIZES ==>                  #Can also look at pg_class.relpages (a page is 8KB)
pg_column_size(VAL)               #
pg_database_size(OID|STR)         #
pg_tablespace_size(OID|STR)
pg_indexes_size(OID|STR)
pg_relation_size(OID|STR[, STR2]) #Size in bytes. Can be a TABLE, INDEX or TOAST.
                                  #STR2 can be 'main' (def), 'vm' of 'fsm'
pg_total_relation_size(OID|STR)   #Same but with INDEX included
pg_table_size(OID|STR)            #Same with INDEX excluded and only for TABLE
pg_size_pretty(UINT)              #Convert a bytes size into human readable STR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      RECOVERY & BACKUPS       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DURABILITY ==>                    #PostgreSQL has durability: write operations will succeed even after a crash.
                                  #A crash will lead to:
                                  #  1) an inconsistent state if transaction was half written because of:
                                  #     - cache:
                                  #        - ENVVAR fsync (def: 'on'): don't use cache, i.e. flush WAL records as they are written.
                                  #          Can be disabled at server startup with postgres -F
                                  #        - ENVVAR wal_sync_method tells which OS command to use to flush cache (when using fsync):
                                  #          open_datasync, fdatasync, fsync, fsync_writethrough or open_sync (def: fdatasync). Best one
                                  #          can be determined with:
                                  #             pg_test_fsync -f FILE (FILE must be on same filesystem than DATADIR)
                                  #        - HDD cache on Linux:
                                  #           - queried with: hdparm -I /dev/FILE | grep "Write cache" (* at beginning if cache enabled)
                                  #           - disabled with: hdparm -W 0 /dev/FILE
                                  #        - Filesystem caching through journaling: disable it with mount options (e.g. data=writeback
                                  #          on ext3).
                                  #     - partial writes, controlled by ENVVAR full_page_writes (def: 'on').
                                  #       Putting to 'off' can improve performance.
                                  #  2) lost transactions (but no inconsistency) because of:
                                  #     - no synchronous commit: doesn't wait for the WAL to be written to report success of operation.
                                  #       ENVVAR synchronous_commit can be activated (local|remote_write|on (def)) or not ('off')
                                  #     - WAL is written after ENVVAR commit_delay (def: 0ms) in hope several operations will happen in
                                  #       the delay, which will then use a single flush. Best value is half the time of a single
                                  #       8kB write, as reported by last line of pg_test_fsync
                                  #       Only happens when min. ENVVAR commit_siblings (def: 5) transactions are currently opened.
                                  #     - ENVVAR wal_writer_delay (INT in ms, def: 200) too high: delay between each WAL writes
                                  #1) is dangerous: database could not be restared without a restore.
                                  #2) gives similar performance gain without that problem.
                                  #Putting DATADIR in a RAM disk is hardcore non-durable, and limits space to RAM space, but highly
                                  #efficient.

BACKUP VS HIGH                    #Backup strategy (saving data) is different from, but should be combined with high availability
AVAILABILITY ==>                  #strategy (quick restore of system if a node falls down)

BACKUPS ==>                       #Either:
                                  #  - pg_dump: more stable from one PostgreSQL version to another, or from one architecture to another.
                                  #  - WAL archiving: provides continuous archiving and PITR
                                  #Best: do both too (do a pg_dumpall after each base backup).
                                  #Good idea to compress backups
                                  #Backups methods are all hot backups.

pg_dump [DATABASE]                #Def DATABASE: PGDATABASE or OS_USER
                                  #Must be connect as superuser ROLE (for both backups and restores).
                                  #Unless "all", doesn't backup cluster-specific information. Remember then to restore them before
                                  #psql <FILE.
                                  #template1-specific information are backuped too: remember to restore from template0.
                                  #"INT" ON_ERROR_STOP should be set.
                                  #Recommendations for restore:
                                  #  - psql --single can be used to make everything rollback if error
                                  #  - Make sure tablespace DIR are good
                                  #  - should analyze restored databases.
-F p|c|d|t                        #Format of the output:
                                  #  - p (def): sql command in text format
                                  #  - c: custom compressed format (must be restored with pg_restore)
                                  #  - d -f DIR: put in a directory DIR (must be restored with pg_restore) with one file by TABLE, and a
                                  #    table of content file
                                  #  - t -f DIR: same but use tar (not compressed). Limit of GB per table.
-a                                #Don't save SCHEMAs (save only data)
-s                                #Save only SCHEMAs (not data)
-n SREGEXP                        #Only VAR... in SCHEMA matching SREGEXP (see psql). Can be specified several times.
                                  #Caution: doesn't dump VAR of other SCHEMA2... that SCHEMA might depend on.
-N                                #Inverse: SCHEMA not matching SREGEXP
-t SREGEXP                        #Same for TABLE matching SREGEXP. Incompatible with -n or -N
-T SREGEXP                        #Inverse: TABLE not matching SREGEXP.
--exclude-table-data=
SREGEXP                           #Same but only exclude TABLE data, not definition
-b                                #Include large objects, which is the default unless -n, -s or -t is used
-o                                #Includes OID
-O                                #Don't save ROLE ownership (with -F p). Will be able to restore backups without being superuser, but
                                  #restorer will get ownership of all objects.
-x                                #Don't save privileges (grant/revoke)
--no-tablespaces                  #Save everything in same, default tablespace.
--no-unlogged-table-data          #Don't save content of unlogged TABLEs
--no-security-labels              #Don't save seLINUX labels (when using it)
-c                                #Put cleaning commands first (drop VAR before trying to create it)
-C                                #Create the database in the beginning (otherwise need to create it).
--[column-]inserts                #Use insert instead of copy (slower). With column, put "COL" names instead of using positions. Is
                                  #much slower.
--serializable-deferrable         #Execute command in a serializable deferrable transaction (useful only when dumping to clone to
                                  #another machine)
--disable-dollar-quoting          #Use standard SQL quoting ' ' instead of $$ $$
--disable-triggers                #Create commands (with -F p) which disable triggers on tables before dump is restored.
-E STR                            #Encoding (def: PGCLIENTENCODING)
-j NUM                            #Use several threads in same time (faster but uses more resources). Make sure ENVVAR max_connections
                                  #is high enough. Doesn't work if any exclusive lock is being requested meanwhile.
--lock-wait-timeout=NUM           #Wait for NUM seconds when asking for locks (def: unlim)
-d -h -p -U -w|W                  #Connection options (see psql)
--role=ROLE                       #ROLE when getting the dump data

pg_dumpall                        #Same as pg_dump, but for the whole cluster (except template0)
                                  #Use same options as pg_dump, except ones that are irrelevant, and selection options (like -T).
                                  #All databases must already exist.
-g                                #Only saves cluster-wide objects
-t                                #Only saves tablespaces
-r                                #Only saves ROLE

pg_restore [FILE]                 #Restore a backup produced by pg_dump -F (except for normal format, which should be restored with psql).
                                  #Def FILE is stdin. If no -d DATABASE is specified, print a text version of the restoration instead.
-a
-c
-C
-F c|d|t
-j NUM
-n SCHEMA
-O
-s
-S ROLE
-t SREGEXP
-x
--disable-triggers
--no-tablespaces
--no-security-labels
-d -h -p -U -w|W                  #Like pg_dump
-e                                #Sets ENVVAR exit_on_error
-1                                #Put in only one transaction
-L FILE                           #Restore only objects present in FILE (can be produced with pg_restore -l, then manipulated)

WAL ==>                           #Write-ahead logs. Logs that store every operation on the cluster before they are performed.
                                  #Goal:
                                  #  - when starting the server, if the last operations of the WAL have not been applied to the data
                                  #    (i.e. if the DATADIR data don't match the WAL), last operations are performed.
                                  #    Goal is to recover from crash.
                                  #  - can also be used for backups (see below)
                                  #Only WAL log are garanteed to be flushed (faster), not real operations, to ensure durability.
                                  #Structure:
                                  #  - Use 16MB segments. A log "line" is a "record".
                                  #    Every write on the cluster adds a new record on the last segment.
                                  #    New segments are automatically added and rotated.
                                  #    Are in DATADIR/pg_xlog/ but could be moved to a faster storage using symlinks.
                                  #Checkpoints are when operations recorded by WAL are flushed to the disk (as opposed to flushing
                                  #the WAL itself, which is controlled by fsync, etc.):
                                  #  - last one is where to restart in crash recovery
                                  #  - are performed at min. time between ENVVAR checkpoint_segments (number of segments, def 3) and
                                  #    checkpoint_timeout (time between checkpoints, def '5min').
                                  #    Increasing it will improve performance but increase crash recovery time (values between 32 to 256
                                  #    are often used for checkpoint_segments, and checkpoint_timeout can be one day)
                                  #    If ENVVAR checkpoint_warning (def: '30s') is less than the time between checkpoints, but more
                                  #    than checkpoint_timeout, a warning will be issued to the server log.
                                  #  - can also issue SQL command checkpoint to do it
                                  #  - flushes performed by a checkpoints are spread to the next checkpoint. The spread is
                                  #    ENVVAR checkpoint_completion_target, i.e. percentage of size spread for the free time allowed
                                  #    between checkpoints (def: 0.5, best is 0.9). Can go up to 0.9 will improve performance, but
                                  #    increase recovery time. Can only be set at server start.
pg_xlogdump [FILE]                #Show a WAL file in human readable format
                                  #When in DATADIR, can also use FILE FILE2 to go from FILE to FILE2
pg_resetxlog                      #To use when WAL is corrupted. Look at online doc
pg_controldata DATADIR            #Show debug info for WAL

create unlogged table|sequence ...#TABLE|SEQUENCE is not written to WAL
alter table "TABLE" set [un]logged#Faster, but truncated on unclean shutdown
select ... into unlogged table ...#Also, cannot use replication to standby servers

SEQUENCE.log_cnt                  #0-32 (def: 0). Decrements at each setval|nextval() then cycles to 32.
                                  #When at 0, writes SEQUENCE to WAL.
                                  #I.e. only every 32 new value of SEQUENCEs are written to WAL, for performance

WAL ARCHIVING /                   #  - goals:
ONLINE BACKUP ==>                 #     - "continuous archiving". Just need to archive new WAL segments.
                                  #     - point in time recovery (PITR): instead of single snapshots, can recover to specific time in
                                  #       past
                                  #  - enabled by ENVVAR wal_level to 'archive|hot_standby' (def: 'minimal') and archive_mode to 'on'
                                  #  - backing up WAL segments continuously, and DATADIR at regular times:
                                  #     - backup in different folders, let's call them DIR1 and DIR2
                                  #     - events since the last DATADIR since the crash are then restored thanks to the archived WAL
                                  #       segments
                                  #  - backup of WAL segments:
                                  #     - each time a new WAL segment is about to be erased (because of rotation), ENVVAR
                                  #       archive_command STR is fired to back it up:
                                  #        - can include %p for its path and %f for its filename, e.g.:
                                  #            '[ ! -f "DIR1/%f" ] && cp -a "%p" "DIR1/%f"'
                                  #        - should give exit code != 0 if error, so that it retries it
                                  #        - should not allow overwritting files
                                  #        - on Linux, use sh, not Bash
                                  #        - should be faster than the speed at which WAL segments appear
                                  #        - check permissions of server daemon to execute command
                                  #     - new segments are automatically made. But can be created manually by:
                                  #        - running pg_switch_xlog()
                                  #        - can be made every max. every ENVVAR archive_timeout (def: 0, in seconds). Should not be
                                  #          under 60s.
                                  #          Goal is for databases with low traffic: new segments are rarely created but still want to
                                  #          archive the little traffic.
                                  #     - archived WAL segments before the last "base backup" can be erased to save space, up until when
                                  #       we want to do a PITR
                                  #     - Can also use command pg_receivexlog -D DIR, which archive WAL segments to DIR, according to
                                  #       connection options (see psql) -d -h -p -U -w|W
                                  #  - backup of DATADIR ("base backups"):
                                  #     - manually, steps are:
                                  #        - connect to any DATABASE of the cluster and fire pg_start_backup(STR) as superuser.
                                  #          STR should be the number of this unique backup
                                  #           - creates a text file DATADIR/pg_xlog/FILE.*.backup, where FILE is the last WAL segment
                                  #             archived, with information used by the recovery process (e.g. last WAL segment of
                                  #             current DATADIR)
                                  #           - creates DATADIR/backup_label, which is a very similar file
                                  #        - backup DATADIR with any command (such as cp -a) to DIR2:
                                  #           - don't include postmaster.* nor pg_xlog/*
                                  #           - don't forget directories that might be elsewhere, e.g. tablespaces or directories using
                                  #             symlinks postgresql.conf, pg_hba.conf, pg_ident.conf could also be put somewhere else
                                  #             with ENVVAR config|hba|ident_file
                                  #           - copy might issue warnings because DATADIR files change on the fly (since cluster is
                                  #             running): it's fine
                                  #        - fire pg_stop_backup() as superuser.
                                  #           - removes backup_label file
                                  #        - utilities (not necessarily needed):
                                  #           - pg_is_in_backup(), pg_backup_start_time()
                                  #           - pg_start|stop_backup() returns the WAL segment as STR: to translate into filenames:
                                  #              - pg_xlogfile_name[_offset](STR)
                                  #              - pg_xlog_location_diff(STR, STR2)
                                  #     - pg_basebackup:
                                  #        - automate all this. Options are:
                                  #            -h -p -U -w      Connection options
                                  #            -D DIR           DIR to copy to. Can be - (stdout) for tar mode
                                  #            -F p|t           If p, do a simple copy. Files pointed by symlinks (such as tablespaces),
                                  #                             will be copied to the destination using the same absolute path
                                  #                             If t, will tar it under the filename base.tar (symlinks files are tar'd
                                  #                             too, under their abs. path)
                                  #                             Can also use -z to gzip it and -Z 1-9 for the compression level (def: 6)
                                  #            -R               Put a recovery.conf sample if the backup
                                  #            -X s             Includes first WAL segment in the backup (def: doesn't include any WAL
                                  #                             segment).
                                  #                             Will use two clients in max_wal_senders
                                  #            -l STR           Label used in backup_label (def: 'pg_basebackup base backup')
                                  #            -c fast|spread   Change the checkpoint_completion_target (def: spread)
                                  #            -P               Progress bar
                                  #        - use same privileges as streaming replication (max_wal_senders, replication privilege, etc.)
                                  #     - in all cases, need to be done regularly, e.g. with a cron script
                                  #        - more regular base backups require more storage, but make faster recoveries
                                  #  - recovery:
                                  #     - steps:
                                  #        - stop server
                                  #        - replace DATADIR by DIR2, but keeping the WAL segments:
                                  #           - move DATADIR/* to temporary DIR3 (including tablespaces, etc., see above)
                                  #           - copy DIR2/* to DATADIR (including tablespaces, etc., see above), with right ownership
                                  #             and permissions
                                  #           - replace DATADIR/pg_xlog/* by DIR3/pg_xlog/*, with right ownership|permissions
                                  #             (in case some WAL segments were not archived but still present in DATADIR)
                                  #        - copy archived WAL segments from DIR1 to DATADIR:
                                  #           - create DATADIR/recovery.conf (its presence instructs server start to be in recovery mode)
                                  #              - can copy template SHAREDIR/recovery.conf.sample
                                  #              - must set variables:
                                  #                 - restore_command STR: just like archive_command, but to copy the WAL segments from
                                  #                   DIR1 to DATADIR/pg_xlog/
                                  #                   Should overwrite existing ones.
                                  #                   Will emit warnings because try to copy files that might not exist.
                                  #                   Ex: 'cp -a "DIR1/%f" "%p"'
                                  #              - can recover to a specific time (PITR):
                                  #                 - by setting (in recovery.conf) any of:
                                  #                    - recovery_target_time TIMESTAMP
                                  #                    - recovery_target_xid STR: the transaction ID
                                  #                    - recovery_target_name STR: STR is a restore point, which must have been
                                  #                      previously created by pg_create_restore_point(STR)
                                  #                 - time must be after the creation time of DIR2/*
                                  #                 - recover just before|after according to variable (in recovery.conf)
                                  #                   recovery_target_inclusive (def: true, i.e. after)
                                  #                 - will stop (unless variable pause_at_recovery_target is set to false or if
                                  #                   hot_standby mode), so we can check if the state is fine. Can resume by firing
                                  #                   pg_xlog_replay_resume()
                                  #                 - must remove WAL segments that have been archived after that time, to restart
                                  #                   archiving them normally
                                  #           - start the server in single user mode
                                  #           - recovery will happen: when done, recovery.conf will be recovery.done
                                  #        - make sure everything is ok, then restart the server normally
                                  #     - timelines:
                                  #        - each time a recovery suceeds, it increments the first number of the WAL segment files, e.g
                                  #          00...00100..0034 to 00...00200..0034
                                  #        - the first number is the timeline ID. Goal it that following WAL archives doesn't overwrite
                                  #          previous WAL archives created between the recovery and the crash, in case we want to come
                                  #          back to that point.
                                  #        - by default, recover to the timeline that was used during the base backup, but can specify
                                  #          recovery_target_timeline STR with "latest" in recovery.conf, or with the specified
                                  #          timeline ID

pg_stat_archiver                  #???
pg_stat_recovery_prefetch         #???
pg_stat_progress_basebackup       #TABLE with ongoing base backups. ???
pg_stat_wal                       #???
pg_stat_wal_receiver              #???

pg_lsn TYPE???

HIGH AVAILABILITY ==>             #Can use:
                                  #  - log shipping: master ships WAL to a DIR, then standby gets it from DIR
                                  #  - streaming replication: ships directly WAL from master to slave. Probably better.
                                  #     - async. (better performance) or sync. (better availability)
                                  #Any standby can also be a hot standby (makes more sense for a streaming replication one) to improve
                                  #load balancing (watch out precautions)

LOG SHIPPING /                    #  - Goal: not backup (but can be combined with backup) but to maintain a copy of the master server, so
WARM STANDBY ==>                  #    a switchover to the standby can happen quickly if there is a problem with the master
                                  #  - Idea: the standby machine keeps on reading the WAL archive (master must do WAL archiving) and
                                  #    applies them right away.
                                  #  - How:
                                  #     - start a cluster with a base backup, with a recovery.conf file in it.
                                  #       recovery.conf variable standby_mode should be on.
                                  #     - will continuously call recovery.conf variable restore_command (same format as archive_command)
                                  #       to copy WAL archive DIR1 to its own pg_xlog/, e.g. 'cp -a "DIR1/%f" "%p"'
                                  #        - will show error messages for next WAL segment, and .history file -> it's normal
                                  #     - Put recovery_target_timeline to "latest" (to stay sync. with the timeline chosen by the master)
                                  #     - If don't want to use DIR1 for backup purpose, clean every WAL archive that has been copied by
                                  #       setting variable archive_cleanup_command:
                                  #        - %r is the filename (not path) of the first WAL file to keep
                                  #        - pg_archivecleanup is a command line often used:
                                  #            - pg_archivecleanup "DIR" "%r"
                                  #            - flags are -d (verbose), -x STR (use it if WAL segments have this extension,
                                  #              e.g. -x .gz) and -n (dry-run)
                                  #     - Can stop standby mode and become a master:
                                  #        - by creating file specified by recovery.conf variable trigger_file, or firing
                                  #          pg_ctl promote.
                                  #           - change recovery.conf to recovery.done
                                  #        - never two masters at same time:
                                  #           - should turn off former master shortly before
                                  #           - before restarting, former master should become the new slave
                                  #        - good idea to prepare already the slave to become a master by setting up WAL archiving, etc.
                                  #        - recovery_end_command STR will be fired (%r is the same as archive_cleanup_command)
                                  #        - automatic failover is only possible using external packages.
                                  #  - Precautions:
                                  #     - DIR1 should not be on the master machine.
                                  #     - WAL segments are sent async (don't wait for shipping to execute), so there's a window for data
                                  #       loss, that can be reduce by lowering archive_timeout
                                  #     - standby and master should have similar config:
                                  #        - logically, e.g. symlinks (including table spaces)
                                  #        - software-wise
                                  #        - hardware wise. CPU architecture must be same.
                                  #     - switchover is manual: should have own mechanism to notify when the primary server is down, and
                                  #       to automatically failover

ASYNC. STREAMING                  #  - Goal: like log shipping, but smaller delay between master and slave state (still small one)
REPLICATION ==>                   #  - Idea: like log shipping, but doesn't use WAL archive DIR1 (nor restore_command,
                                  #    recovery_target_timeline, archive_cleanup_command), but directly get WAL from the server (over
                                  #    TCP connection).
                                  #  - How:
                                  #     - Set recovery.conf variable primary_conninfo (as "VAR=VAL ...", using libpq variables) for how
                                  #       to connect to the master.
                                  #     - Same as above for recovery_target_timeline
                                  #     - Must have privileges:
                                  #        - to connect to "replication" virtual DATABASE (in pg_hba.conf)
                                  #        - replication and login privileges (better to create a ROLE than to set up as superuser).
                                  #        - max number of connections is ENVVAR max_wal_senders (def: 0).
                                  #     - slave must keep up with the pace:
                                  #        - can increase ENVVAR wal_keep_segments on the master (number of segments that should be
                                  #          recycled but are kept, def: 0)
                                  #        - can use log shipping in parallel.
                                  #        - If fall behind, can redo a base backup.
                                  #        - Can tell by:
                                  #           - comparing pg_current_xlog_[insert_]location() on the master (current WAL),
                                  #             pg_last_xlog_receive|replay_location|timestamp() on the slave
                                  #           - use pg_stat_replication system view
                                  #        - Connection waits only for ENVVAR wal_receiver_timeout (def:60s) from slave to master, and
                                  #          wal_sender_timeout (def: 0, turned off) from master to slave.
                                  #     - Cascading replication:
                                  #        - Just use replication from downstream to upstream servers.
                                  #        - Goal: to reduce cost for master, but introduces more delay for other standbies.
                                  #        - sync. replication doesn't work for downstream servers.

SYNC. STREAMING                   #  - Goal: like async. streaming replication, but reduces data loss window to nothing (at expense of
REPLICATION ==>                   #    performance): every write transaction returns only after WAL is sent to standby ("2-safe
                                  #    replication").
                                  #  - How:
                                  #     - Master must set ENVVAR synchronous_standby_names with standbies:
                                  #        - comma-separated-list, only picks the first connected in the list
                                  #        - names must match application_name in primary_conninfo
                                  #           - can be * for any application_name
                                  #        - def is walreceiver
                                  #     - Actually waits according to ENVVAR synchronous_commit:
                                  #        - on (received and flushed to disk on slave), remote_write (only received) or local|off
                                  #          (nothing).
                                  #        - Makes it possible to set synchronous_commit specific values for databases, users or
                                  #          transactions, for different durability/performance tradeoff.
                                  #  - Precautions:
                                  #     - If last standby loses connection, will wait forever
                                  #        - if last standby needs to be down, must first put synchronous_commit to off in a
                                  #          pg_start|stop_backup() block

HOT STANDBY ==>                   #  - Goal: use a standby server (streaming replication or log shipping) for readonly queries (load
                                  #    balancing).
                                  #  - How:
                                  #     - Must set ENVVAR hot_standby to on on standby and ENVVAR wal_level to hot_standby for master
                                  #     - Must start with a new base backup (if switching from non hot standby to hot standby)
                                  #  - Precautions:
                                  #     - Watch out for the delay between master write and ability to read it in standby. If an arriving
                                  #       WAL archive is conflicting with a current query (e.g. if master dropped a table while standby
                                  #       is querying it), it will wait ENVVAR max_standby_archive|streaming_delay (for streaming
                                  #       replication mode or not) (def: 30000 ms, -1 for unlim) then cancel
                                  #        - low value provokes more cancels, but standby and master are more in sync: good if goal is
                                  #          more High availability, bad if goal is more load balancing
                                  #        - could be set at approx max time of queries.
                                  #        - Can also increase ENVVAR vacuum_defer_cleanup_age if lot of vacuum-related conflicts.
                                  #          Cancels can be seen on system view pg_stat_database_conflicts.
                                  #     - Hot standby stops at startup when standby tries to catch up servers WAL segments. During that
                                  #       period, there might be seemingly weird behavior.
                                  #       pg_is_in_recovery() will return true.
                                  #     - those ENVVAR must be superior or equal on the standby than the master: max_connections,
                                  #       max_prepared_transactions, max_locks_per_transaction
                                  #     - advisory locks can't be shared between master and slave
                                  #     - isolation level serializable not available


repmgr                            #Must have rsync, pg_ctl and pg_config in $PATH. Must be installed from source (see online doc).
                                  #Actions can be:
                                  #  - standby clone "NODE": make it possible to put as standby (do a base backup).
                                  #  - master|standby register: put as master|standby (master should be done first)
                                  #  - standby promote|follow: in case of a failover, automatical new master to promoted, and followers
                                  #    will replicate from it. Automatical or manual???
-d -h -p -U                       #Connection options
-D DATADIR                        #Cluster to target
-f DIR                            #repmgr.conf DIR (def: same as executable).
                                  #repmgr has three lines: cluster STR, node number INT, libpq_conninfo STR
--force                           #Do with standby clone when a master is up again after having being down, to get back the changes
                                  #since then from the new master.

repmgrd                           #Daemon doing automatic failover.
                                  #Needs to do all the standby register first.
-f DIR                            #

TOPTS.user_catalog_table          #BOOL???

alter table "TABLE"
 replica identity ...             #???

pg_replication_origin             #???
pg_replication_origin_status      #???
pg_replication_slots              #???
pg_stat_replication               #WAL sender processes.
pg_stat_replication_slots         #???
pg_stat_database_conflicts        #DATABASE, with query cancelled due to recovery on standby servers.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      LOGICAL REPLICATION      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DOCUMENT ??? including:
  - pg_publication, pg_publication_namespace, pg_publication_rel, pg_publication_tables
  - pg_subscription, pg_subscription_rel, pg_stat_subscription, pg_stat_subscription_stats

pg_class.relreplident             #'CHAR' with RELATION's COLs used to form `replicate identity`:
                                  #  - 'd': default (primary key, if any)
                                  #  - 'n': none
                                  #  - 'f': all COLs
                                  #  - 'i': index with indisreplident set
                                  #By default: 'n' for all, except sql_* which use 'd'

pg_index.indisreplident           #BOOL. Whether INDEX is a replica identity


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     CLUSTERING AND POOLING    :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pgbouncer PGBFILE                 #Does connection pooling: maintains a single connection to be reused for each DATABASE+USER pair.
                                  #Goal is to lower connection time.
                                  #Can be much faster when connection time is important (small sessions).
                                  #Disadvantages:
                                  #  - requires more fds, so might needs lower max_client_conn than PostgreSQL's server
                                  #  - hides original host+port information in logs (all traffic goes though pgbouncer)
                                  #  - cannot implement all authentication method
                                  #Act as layer of abstraction: connection to pgbouncer DATABASE can be redirected to DATABASE of
                                  #other names, or of different clusters/machines.
                                  #Should be run as same OS_USER as the server.
                                  #Should be installed:
                                  #  - on database server, if lot of web servers connect to it (because it is the center of all
                                  #    connections that should be pooled)
                                  #  - on web server, if connects to lot of database servers
                                  #Each pool (DATABASE+USER) has:
                                  #  - clients (to pgBouncer) and servers (connection of pgBouncer to PostgreSQL).
                                  #    1 client = 0|1 server: 0 server when client just connected (session pool_mode) or is not issuing
                                  #    a request (transaction pool_mode)
                                  #  - cl_active (from show pool, see below) is number of clients on a pool.
                                  #    If more than pool size, clients will be cl_waiting instead until cl_active is lower.
                                  #    Pool size is determined by:
                                  #      - [default_]pool_size: cluster-wide and database-specific pool size.
                                  #      - min_pool_size: at first client, opens at least NUM idle servers, to make it more responsive
                                  #        in the first requests.
                                  #      - reserver_pool_size: extra pool size used for clients waiting (cl_waiting) for more than
                                  #        reserve_pool_timeout
                                  #  - max_client_conn is max number of cl_active for all pools together.
                                  #    When reached, clients don't wait, they crash.
                                  #  - Optimize limits:
                                  #     - number of file descriptors used = 2 + 1 per client (max_client_conn) + 1 per server
                                  #       (pool_size * number of users * number of databases)
                                  #         - pool_size should be at max without creating more servers than PostgreSQL's max_connections
                                  #         - max_client_conn should be max number of servers + expected number of idle clients
                                  #         - total should not exceed max number of file descriptors
                                  #Pool mode:
                                  #  - when server is not used anymore, returns back to pool (sv_active -> sv_idle|used)
                                  #  - it is done according to pool_mode, either after each session (def), transaction or query (avoid).
                                  #    transaction doesn't support session states, i.e. [re]set ENVVAR, listen|notify, with hold CURSOR,
                                  #    PREP, load, user-defined volatile FUNC
                                  #    Use transaction if lot of idle times in sessions, or if long queries.
                                  #Look at check_postgres for monitoring.
-d                                #Run in background.
                                  #Needs to give pidfile = FILE in [pgbouncer] in PGBFILE (FILE is created with the PID)
-R                                #Online restart: closes current running pgbouncer and inherits its connections without interrupting
                                  #anything (current running pgbouncer will be closed). Useful to upgrade without interrupting anything.
-q                                #Quiet mode
-v                                #Verbosity (can do several times)

PGBFILE ==>                       #Usually called pgbouncer.ini
                                  #Has two parts, each started with [databases], then [pgbouncer] on a single line, and separated by
                                  #blank line.
                                  #Each part has VAR = VAL ... (STR don't have any quoting)

                                  #[databases]:
DATABASE                          #STR, libpq string, but with only:
                                  #  - dbname: def. is same as DATABASE
                                  #  - host: def. is using Unix socket.
                                  #  - port: def. 5432
                                  #  - user: def. is same user
                                  #  - password
                                  #When client asks PgBouncer to connect on DATABASE, will use STR to connect to server.
                                  #Can also specify:
                                  #  - pool_size: Per-database pool size
                                  #  - connect_query: query done at connection start.
                                  #    For connection end (not DATABASE-specific), use server_reset_query (def: "discard all")
                                  #Can use * DATABASE to mean "any other database"

                                  #[pgbouncer]:
listen_port                       #Proxy port (which client should connect to in order to reach server)
listen_addr                       #Same for proxy address. Can be *
unix_socket_dir|mode|             #Like PostgreSQL ENVVAR unix_socket_directories|permissions|group
group                             #Def. are /tmp, 0777 and ""
auth_type                         #Similar as in pg_hba.conf. Can be md5 (def), plain (like password in pg_hba.conf), trust or any.
                                  #any is like trust, except that users are not even remembered which means:
                                  #  - all DATABASE must specify user=VAL in their libpq STR
                                  #  - control with admin_users is not effective
auth_file                         #"USER" "PASSWORD" ...
                                  #Necessary (only USER with a line in it will be able to connect)
admin_users                       #USER... (pgBouncer USER) allowed to connect to pgBouncer and issue statements on it.
stats_users                       #Same but can only use show ENVVAR (except show fds)

logfile                           #Redirect stderr to FILE, without stopping stderr
log_[dis]connections              #Logs them (def: 1)
log_pooler_errors                 #Logs errors sent to client (def: 1)
stats_period                      #Logs stats every NUM seconds (def: 60)
syslog[_ident|facility]           #

pool_mode                         #See above (def: session)
                                  #If transaction, server_reset_query should be ""
max_client_conn                   #(def: 100)
default_pool_size                 #(def: 20)
min_pool_size                     #(def: 0)
reserve_pool_size|
timeout                           #(def: 0 and 5 seconds)

server_check_delay                #After NUM seconds (def: 30), goes from sv_idle to sv_used, i.e. run sanity check query on server
                                  #connections when going from idle to active.
server_lifetime                   #Closes server connections opened for more than NUM seconds (def: 3600)
server_idle_timeout               #Same but for idle server connections (def: 600)
server_connect_timeout            #Same but for connecting time (def: 15)
client_login_timeout              #If client connects but does not login before NUM seconds (def: 60), drops it.
autodb_idle_timeout               #Closes pools (using * in [databases]) that have been unused for more than NUM seconds (def: 3600)

server_login_retry                #Waits NUM seconds (def: 15) after each failed authentification.
dns_max_ttl                       #DNS (host resolution) cache time in seconds (def: 15)
max_packet_size                   #Max packet size between PostgreSQL and pgBouncer, in bytes (def: 2GB)

server_round_robin                #If 0 (def), reuse connections in LIFO manner. If 1, in a random manner (better if TCP round-robin
                                  #distributing load between servers)

pgbouncer DATABASE ==>            #Virtual DATABASE, where only show ENVVAR is allowed, with some commands:
reload                            #Reload PGBFILE (can also use SIGHUP)
pause [DATABASE]                  #Safest way to stop pgBouncer: wait for clients to complete. Can also use SIGINT (CTRL-C)
shutdown                          #Like pause, but exit pgBouncer completely. Can also use SIGTERM
suspend                           #Drop clients, but flush buffers
kill DATABASE                     #Least saft way to stop DATABASE
resume [DATABASE]                 #Resume from pause|resume

                                  #Can also show the following ENVVAR:
lists                             #Snapshot of all other info
databases                         #DATABASE: connection+pool_size
stats                             #DATABASE:
servers                           #
clients                           #
pools                             #
fds                               #File descriptors
users                             #All users in auth_file
config                            #PGBFILE info
dns_hosts
dns_zones                         #Host resolution


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            TESTING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PGTAP UNITS ==>                   #Needs to be installed.
                                  #Tested database must enable PL/PGSQL.
                                  #Idea is to create assertions (see below for list) in a separate test unit SQL file. Usually put in a
                                  #tests/ folder
                                  #Usually do test manipulation on the database between assertions, so in the end should rollback:
                                  #  - put in a transaction block that rollbacks.
                                  #  - put "INT" ON_ERROR_ROLLBACK and ON_ERROR_STOP
                                  #For best formatting:
                                  #  - put "INT" ECHO to nothing, QUIET to 1
                                  #  - \pset format unaligned, pager on, t true
                                  #  - if using psql, use -X to bypass init files
                                  #Each assertion produce a TAP format line (common text format for test results, that is runned and
                                  #parsed by "test harnesses")
                                  #Trusted postgres extension 'pgtap'
select plan(NUM)                  #Starts a unit with exactly NUM assertions. If not sure use select * from no_plan() instead (avoid).
select * from finish()            #Completes a test unit

PGTAP XUNIT TESTS ==>             #Same as normal pgTap units (same output and assertions) but:
                                  #  - use functions, not files. Functions just execute the assertions functions, and returns them as a
                                  #    STR_ARR:
                                  #      create or replace function SCHEMA.FUNC()
                                  #      returns setof text as
                                  #      $$begin
                                  #          return next ASSERTION_FUNC(ARGS)...
                                  #        end$$
                                  #      language pgsql
                                  #  - don't need anything to start|finish the unit but needs to run the unit with:
                                  #      select * from runtests([STR][, STR2])
                                  #    where STR is SCHEMA, and STR2 is a regular expression to match the FUNC to choose.
                                  #    Since STR and STR2 are both optional (def: 'public' and '^test') cast to name if using only STR
                                  #     - Use transaction on all tests as a whole, then on each individual test (including fixture
                                  #       functions)
                                  #  - can use fixture functions (same definition as above), special FUNC used at specific moments.
                                  #    Each set run in alphabetical order. FUNC name must start with the name of the phase:
                                  #     - startup, once before all tests
                                  #     - setup, once before each test
                                  #     - teardown|shutdown: same as setup|startup, but after tests
                                  #    Watch out to exclude the fixture functions in runtests(). Good practice is to use a SCHEMA, and
                                  #    '^test' for tests.

pg_prove [DIR|FILE...]            #Test harness for pgTap (executes tests).
                                  #Like doing psql, but better output, and don't need to set \pset and "INT"
                                  #(but should still put in a transaction block)
                                  #For DIR, recognize test files according to extension ".pg"
                                  #Same for xUnit-style tests, but:
                                  #  - don't use FILE... but -R, which fires runtests()
                                  #  - use -s SCHEMA and -x REGEXP to specify arguments to runtests(STR, STR2)
-d -U -h -p                       #Connection options
-P|S VAR=VAL                      #Does \[p]set VAR VAL
--ext STR                         #With DIR, use extension STR, not ".pg"
-r                                #With DIR, recursive

--shuffle|reverse                 #Modify test execution order
--state STR,...                   #Which tests to run:
                                  #  - last: same as last time
                                  #  - all: in normal order
                                  #  - failed|passed: only ones that failed|passed
                                  #  - hot: most recent failure first
                                  #  - todo: only test with todos
                                  #  - slow|fast: in speed order
                                  #  - new|old: in mtime order
                                  #  - fresh: only the ones that have been modified
                                  #  - save: save state in a file ./.pg_prove (must be done first to be able to do --state next time)

-f                                #Print failed tests
-q|Q                              #Quiet, or even quieter
--verbose                         #Outputs full TAP format
--no-comments                     #Don't show diag() messages
--directives                      #Only show skip() messages and todo() tests
-D                                #Dry-run
-t                                #Show time of execution of each test

-j NUM                            #Number of jobs in parallel
-b FILE                           #PSQL location

PGTAP ASSERTIONS ==>              #They are FUNC that all come with an optional (but recommended) last arg STR for error message
                                  #(def: '').
                                  #They print the result of the assertion as a STR

GENERAL ASSERTS ==>               #
ok(BOOL)                          #Asserts that BOOL is true.
                                  #Prefer other function when possible, e.g. is(VAL, VAL2) over ok(VAL = VAL2), because more descriptive
                                  #output.
pass|fail()                       #Like ok(true|false) (avoid them)
is[nt](VAL, VAL2)                 # is [not] distinct from
[i]matches(VAL, VAL2)             # ~[*]
doesnt_[i]match(...)              # !~[*]
[un][i]alike(VAL, VAL2)           # [not] [a]like
cmp_ok(VAL, 'OP', VAL2)           # VAL OP VAL2
isa_ok(VAL, STR)                  # pg_type(VAL) = STR

SQL QUERIES RESULTS==>            #Asserts results of sql select ...:
'SQL'                             #Means SQL statement STR (either as is, or name of a PREP (recommended)).
                                  #A PREP with arguments needs to be written as is, i.e. not 'PREP' but 'execute PREP(ARGS)'
throws_ok('SQL'[, STR2 [, STR3]]) #Asserts that 'SQL' throws an exception, with errcode STR2 and errmessage STR3 (each can be null
                                  #(def) for all errcode|errmessage)
lives_ok('SQL')                   #Inverse of throws_ok('SQL')
throws_[i]like|matching
 ('SQL'[, STR2])                  #Same as throws_ok('SQL', null, STR2), but STR2 needs to match with [i]like or ~[*], not = <>
performs_ok('SQL', INT)           #Asserts that 'SQL' performs in less than INT ms.
results_eq|ne('SQL'|CURSOR,       #Asserts that both queries compare with = <>
 'SQL2'|ARR|CURSOR2)              #Is row-wise, so make sure they are ordered the same.
                                  #CURSOR iterates over all rows (must be STR casted as refcursor)
                                  #ARR represents a single-column (values ... could also be used for several columns)
bag|set_eq|ne('SQL',              #Same but compares not row-wise, but the whole set of values together (so order doesn't matter).
'SQL2'|ARR)                       #set removes duplicates, bag doesn't.
bag|set_has[nt]('SQL', 'SQL2')    #Same as bag|set_eq|ne, but only for subset, i.e. asserts that 'SQL' includes 'SQL2'
is[nt]_empty('SQL')               #Asserts number of rows = <> 0
row_eq('SQL', "ROW")              #Same as results_eq, but for a single row

SCHEMA CONFORMANCE ==>            #Asserts that current variables are exactly this.
schemas|tablespaces|roles
 |languages|casts_are(STR_ARR)    #
tables|views|sequences|functions
 |opclasses|types|domains|enums
 |operators_are                   #Can restrict to a SCHEMA, otherwise use search_path.
 (['SCHEMA', ]STR_ARR)            #Functions are only the name, without arguments.
columns|indexes|triggers
 |rules_are
 (['SCHEMA', ]'TABLE', STR_ARR)   #

SCHEMA EXISTENCE ==>              #Asserts that variable exist.
has[nt]_schema|role|language(STR) #
has[nt]_table|view|sequence
 |foreign_table|type|composite
 |domain|enum|opclass|relation
 (['SCHEMA', ]STR)                #relation is table|view|sequence|ctype
has[nt]_index(['SCHEMA',]
 'TABLE', 'INDEX'[, 'COL'[_ARR]]) #'COL'[_ARR] not with hasnt.
has[nt]_trigger|rule
 (['SCHEMA', ]'TABLE', STR)       #
has[nt]_function(['SCHEMA',]
 'FUNC'[, 'ARGSTYPE'_ARR])        #
has[nt]_cast('TYPE', 'TYPE2'
 [, 'SCHEMA'][, 'FUNC'])          #
has_operator('TYPE'[, 'SCHEMA'],
 'OP', 'TYPE2'[, 'RETURNTYPE'])   #
has_left|rightop(['SCHEMA'],
 'OP', 'TYPE'[, 'RETURNTYPE'])    #
has[nt]_tablespace(STR[, STR2])   #Can use a STR2 as tablespace location (not with hasnt).

COL ATTRIBUTES ==>                #
has[nt]_column
 (['SCHEMA', ]'TABLE', 'COL')     #
col_not|is_null|
has[nt]_default|pk|fk|
unique|check(['SCHEMA',]
 'TABLE', 'COL'_[ARR])            #pk is primary key, fk foreign key constraint.
is_clustered|
index_is_unique|primary
 (['SCHEMA', ]['TABLE', ]'INDEX') #Asserts properties for an index COL
has[nt]_unique|check|pk|fk
 (['SCHEMA', ]'TABLE')            #TABLE has at least those constraints.
col_default_is(['SCHEMA',]
 'TABLE', 'COL', VAL)             #
fk_ok(['SCHEMA', ]'TABLE',
 'COL'[_ARR], 'TABLE2',
 'COL2'[_ARR])                    #Asserts that COL references COL2

TYPES ==>                         #
col_type_is(['SCHEMA', ]'TABLE',
 'COL', ['SCHEMA2', ]'TYPE')      #
index_is_type(['SCHEMA',]
 ['TABLE', ]'INDEX', 'TYPE')      #'TYPE' is 'btree', 'hash', etc.
domain_type_is[nt](['SCHEMA',]
 'DOMAIN', ['SCHEMA2', ]'TYPE')   #'TYPE' is 'btree', 'hash', etc.
enum_has_labels(['SCHEMA', ]
 'ENUM_TYPE', 'VAL'_ARR)          #

FUNCTIONS ==>                     #
can(['SCHEMA', ]'FUNC'_ARR)       #Same as has_function, but without 'ARGSTYPE'_ARR, and with FUNC_ARR
function_lang_is
 (['SCHEMA', ]'FUNC'
 [, 'ARGSTYPE'_ARR], 'LANGUAGE')  #
function_returns
 (['SCHEMA', ]'FUNC'
 [, 'ARGSTYPE'_ARR], 'TYPE')      #
volatility_is
 (['SCHEMA', ]'FUNC'
 [, 'ARGSTYPE'_ARR], STR)         #
function_is_definer|
strict|aggregate(['SCHEMA',]
 'FUNC', ['ARGSTYPE'_ARR])        #
cast_context_is
 ('TYPE', 'TYPE2', STR)           #STR can be 'implicit', 'assignment', 'explicit'
trigger_is
 (['SCHEMA', ]'TABLE', 'TFUNC',
 ['SCHEMA2', ] 'FUNC')            #Asserts that TFUNC executes FUNC
rule_is_instead
 (['SCHEMA', ]'TABLE', 'RULE')    #
rule_is_on(['SCHEMA',]
 'TABLE', 'RULE', 'EVENT')        #

ROLES AND SECURITY ==>
db|schema|tablespace|
 language_owner_is(STR, 'ROLE')   #
table|view|sequence|composite
 |foreing_table|relation|opclass
 |type_owner_is
 (['SCHEMA', ]STR, 'ROLE')        #
index_owner_is(['SCHEMA',]
 'TABLE', 'INDEX', 'ROLE')        #
function_owner_is(['SCHEMA', ]
 'FUNC', 'ARGSTYPE'_ARR, 'ROLE')  #
*_privs_are                       #Same as *_owner_is(...), but asserts PRIVILEGE[_ARR] for ROLE. Differences:
 (..., 'PRIVILEGE'[_ARR])         #  - db -> database
                                  #  - no relation, view, composite, foreign_table, index, opclass, type
                                  #  - there is also:
                                  #     - column*(..., 'TABLE', 'COL', ...) and any_column*(..., 'TABLE')
                                  #     - fdw|server(FDW|'FSERVER', ...)

is[nt]_superuser(ROLE)            #
is_member_of
 ('ROLE', 'ROLE2'[_ARR])          #
language_is_trusted('LANGUAGE')   #

UTILITIES ==>                     #
diag(STR...)                      #Returns STR (separated by newline), in front of a #, to add comments to the output.
skip(STR[, INT])                  #Skip the next INT (def: 1) PGTAP functions, with explanation STR.
                                  #To put in a conditional branch (e.g. case when ...) when a test might provoke the whole unit test
                                  #to throw an exception (language or function not available).
collect_tap(ASSERT_FUNC(...)...)  #Do several PGTAP assertions functions at once.
                                  #Useful when can't be put several COMMAND; but only one, for example in a SQL case when
todo(STR[, INT])                  #Same, but instead of skipping, just declares that tests are expected to fail, because still on the
                                  #todo list.
todo_start|end(STR)               #Do todo() for all tests between start and end.
in_todo()                         #Returns true if in a todo_start|end block.
os_name()                         #e.g. 'linux'

OWN ASSERTION_FUNC ==>            #Just create a plpgsql function that returns text, with a last optional text argument, and which
                                  #returns ok() if test passes, or returns error message if not.

datafiller.py [FILE]              #Script printing commands filling randomly some TABLE...
                                  #FILE (def: stdin) is a list of DDL commands creating the TABLE.
                                  #Hints on how to fill are provided with --comments:
                                  #  - syntax:
                                  #     -- df [MACRO]: VAR[=VAL]
                                  #       - with MACRO, can do elsewhere use=DIRECTIVE to repeat all the VAR[=VAL]...
                                  #          - some predefined MACRO:
                                  #             words: word=/etc/dictionaies-common/words
                                  #       - VAL can use '' for STR and TIMESTAMP
                                  #     - can also use -- df T=TABLE A="COL": ... to target a TABLE or "COL" on a separate line
                                  #       after it. TABLE cannot use skip=FLOAT.
                                  #     - to specify a VAR, I write $VAR, but it should be written VAR
                                  #  - supported VAR:
                                  #     - all TABLE (put comment on a line by itself)
                                  #        - size, offset, mangle, null, seed: see below
                                  #     - TABLE (put comment after the opening parenthesis of creation):
                                  #        - size=INT: number of tuples to fill.
                                  #          Can only be on TABLE, not COL (except for gen=serand)
                                  #        - mult=INT: multiply $size for this TABLE
                                  #          mult (def: 1) should be done on each TABLE (relative size with each other)
                                  #          size (def: 100) only once for all TABLE (to scale it)
                                  #        - skip=FLOAT: divide $size for this TABLE.
                                  #          As opposed to mult, actually produce the rows, but randomly don't output them
                                  #        - nogen, null=FLOAT: same as below
                                  #     - COL (put comment after it):
                                  #        - all:
                                  #           - type=TYPE: generate another TYPE, then casted to the actual type
                                  #           - nogen: no random data (use only default values)
                                  #           - null=FLOAT: percentage of nulls
                                  #           - seed=INT: set random seed (def: use OS (usually depends on current time))
                                  #        - BOOL:
                                  #           - rate=FLOAT: percentage of true (def: 0.5)
                                  #        - INT (integer, not int)|DATE|TIMESTAMP|INTERVAL|INET|CIDR|MAC:
                                  #           - gen=STR: distribution, among:
                                  #              - serial: counter, increments $step (def: 1, must not be divider of $size) from
                                  #                $shift (def:0), then modulo $size, then adds $offset (def: 1)
                                  #                If $mangle, choose random $shift and $step
                                  #              - uniform: uniform distribution, from $offset to $offset + $size - 1
                                  #              - serand: serial up to $size1, then uniform to $size2 - $size1 ($size1 and $size2
                                  #                are the COL-level, and TABLE-level $size)
                                  #              - for other distributions: just use type=float, then use float distributions
                                  #           - offset, shift, step, size, mangle: see above
                                  #        - FLOAT:
                                  #           - gen=WORD: distribution, among:
                                  #              - uniform, gauss|norm, log (lognormal), beta, gamma, weibull, vonmises: use $alpha and
                                  #                $beta
                                  #              - exp, pareto: use $alpha
                                  #           - alpha|beta: see above
                                  #        - STR, followed by:
                                  #           - nothing: prefix followed by repetition of number, separated by _
                                  #              - prefix=STR (def: "COL")
                                  #              - length|lenvar=NUM: average length and diff from average of STR (def: 12 and 3)
                                  #           - chars=STR: choose random characters among a dictionary built with random words using
                                  #             characters in STR
                                  #              - cgen=MACRO: specifies INT parameters (to be used like use=) to specify how selection
                                  #                is done
                                  #              - length and lenvar: see above
                                  #           - text: can use INT parameters, choose from list of words
                                  #              - word=FILE|:STR,...: list of words, of 'size' words
                                  #              - length and lenvar: see above
                                  #           - word=FILE|:STR,...: same as above, but length to 1 and lenvar to 0. Can support unique.
                                  #        - DATE|TIMESTAMP|INTERVAL:
                                  #           - start|end=...
                                  #           - prec=NUM: in days for DATE, in seconds for TIMESTAMP
                                  #        - TIMESTAMP:
                                  #           - tz=STR
                                  #        - INTERVAL:
                                  #           - unit=s|m|h|d|mon|y (def: s)
                                  #        - BYTEA:
                                  #           - length and lenvar: see above
                                  #        - INET|CIDR:
                                  #           - network=STR
                                  #        - MAC
                                  #COL can't be unique for FLOAT, STR chars|text and BYTEA.
                                  #Uniqueness is tried 10 times (can be changed with datafiller.py --tries=NUM)
--[no-]filter                     #Output also FILE (commands creating the TABLE...), before commands filling the TABLE...
--drop                            #Output also commands dropping the TABLE...
--truncate                        #Sale for truncating
--size|offset|seed INT
--null FLOAT
--mangle                          #Sets VAR
-T                                #Put in a single transaction (normal isolation level)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            COMMENT            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


comment on ... is STR             #Add a comment STR, that can be read with:
                                  #  - col_description(TABLEOID, NUM)
                                  #  - [sh]obj_description(OID, DATABASE): shobj is for cluster-wide objects
                                  #  - a psql command \d* under "description"
                                  #... is TYPE followed by the definition, e.g. table "TABLE", trigger TFUNC on "TABLE", column "TABLE"."COL"
                                  #cast (TYPE as TYPE2), aggregate AFUNC(...), etc.
                                  #TYPE can be any object that can be used with create TYPE ...
                                  #If STR is null, remove the comment.
                                  #To write must be owner or superuser

pg_[sh]description                #All COMMENT [on cluster-wide objects]. ???


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           TRANSFORM           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DOCUMENT ??? including:
  - pg_transform


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              JIT              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DOCUMENT ??? including:
  - https://www.postgresql.org/docs/15/jit.html
  - in `explain`


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            OTHERS             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TO DO ==>                         #In online doc:
                                  #  - Clients all rely on a backend protocol.
                                  #  - libpq is a C library that implement it and often used by clients.
                                  #    Large objects are a specific type handled by libpq.
                                  #  - Embedded SQL is a preprocessor that allows to write SQL in C code (then processed to libpq)
                                  #  - C can be used to write functions: see online doc, including background worker processes, SPI,
                                  #    creating new types, writing a PL/* handler, writing a FDW


      
   PG  
      


VERSION ==>                       #3.6.2

PG                                #Installation requires libpq

new PG.Client([STR|OBJ],          #Connect to the database specified by STR|OBJ, then fire FUNC.
 FUNC(ERROR, CLIENT,FUNC2(ERROR)))#FUNC2 must be fired when all operations are done to close connection.
                                  #STR is "[connectionname://][user[:password]@][host[:port]][/database]"
                                  #(all defaults if no first arg) or a IPC socket folder path.
                                  #Connectionname can be anything, it just differentiate sessions. OBJ has members :
                                  #  - user (def: process.env.USER)
                                  #  - database (def: process.env.USER)
                                  #  - password (def: null)
                                  #  - port (def: 5432)
                                  #  - host (def: null): if not URL, use DIR/.s.PGSQL.PORT
                                  #  - ssl (def: false)
                                  #Defaults are in PG.defaults.VAR
                                  #Other defaults:
                                  #  - PG.defaults.parseInt8:
                                  #     - PSQL bigint (such as result of count()) is too big for JavaScript INT.
                                  #     - If false (def), bigint -> STR. If true, bigint -> INT
                                  #Returns CLIENT, but should only be used for events. Use CLIENT in callback for connect|end()
CLIENT.connect
([FUNC(ERROR, CLIENT)])           #
CLIENT.end()                      #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            QUERIES            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CLIENT.query(STR[, VAL_ARR]       #Returns a QUERY from PostgreSQL command STR.
 [, FUNC(ERROR, OBJ)])            #If FUNC, also execute it and event handlers of QUERY cannot be used (so QUERY is useless then).
                                  #OBJ is same as in QUERY end event handler.
                                  #If VAL_ARR, each "$1", "$2", etc. in STR will be replaced by those VAL, providing it does not
                                  #point to a TABLE, a COL or a SCHEMA. It is slower but it prevents SQL injections (VAL_ARR are
                                  #properly escaped instead of using risky STR concatenation).
CLIENT.query(OBJ[, FUNC])         #Same but OBJ can have members :
                                  #  - text: same as STR
                                  #  - values: same as VAL_ARR
                                  #  - name STR3:
                                  #     - make it PREP: but using PSQL Extended Protocol, so same effect (skip parsing phase when
                                  #       calling came query with same name (will use same text|values)), but not actual PREP
                                  #     - parsing is only done when values VAL_ARR is used, so only useful then

QUERY.on("row", FUNC(OBJ))        #Execute QUERY and fire event handler for each row OBJ: {VAR: VAL}...
QUERY.on("end", FUNC(OBJ))        #Execute QUERY and fire event handler for all rows. OBJ has members :
                                  #  - command STR : SQL command
                                  #  - rowCount UINT
                                  #  - oid DOUBLE
                                  #  - rows OBJ_ARR: {VAR: VAL}...
                                  #  - fields OBJ_ARR:
                                  #     - name STR
                                  #     - format 'TYPE'
                                  #     - tableID DOUBLE
                                  #     - columnID DOUBLE
                                  #     - dataTypeID DOUBLE
                                  #     - dataTypeSize UINT
                                  #     - dataTypeModifier UINT
QUERY.on("error", FUNC(ERROR))    #

CLIENT.query
 (new PG-QUERY-STREAM(STR))       #Like CLIENT.query(STR) but returns as ISTREAM.
CLIENT.query
 (new PG-CURSOR(STR)[, VAL_ARR])  #Like CLIENT.query(STR[, VAL_ARR]) but returns a CURSOR (version 0.2.0).
CURSOR.read
 (UINT, FUNC(ERROR, OBJ_ARR))     #OBJ_ARR is {VAR: VAL}... or [] if no more rows
CLIENT.copyFrom|To(STR)           #COPY...FROM|TO statement must use this instead of CLIENT.query().
                                  #Returns a I|OSTREAM (must execute I|OSTREAM.end()).
                                  #Can use stdin (not stdout) in STR if writing|reading from I|OSTREAM
CLIENT.pause|resumeDrain()        #Stops|resumes emission of drain events (useful when async operations need to complete first)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       OTHER OPERATIONS        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/



CLIENT.on("drain", FUNC())        #Fired each time all queries have been executed
CLIENT.on("error", FUNC(ERROR))   #
CLIENT.on                         #Fired with listen/notify SQL statements
 ("notification", FUNC(OBJ))      #OBJ:
                                  #  - name "notification"
                                  #  - channel STR
                                  #  - payload STR
                                  #  - length NUM
                                  #  - processId NUM
CLIENT.on("notice", FUNC(STR))    #Fired with warning messages (otherwise printer in stdout)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            POOLING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PG.pools.getOrCreate([OBJ])       #Returns POOL (from GENERIC-POOL) of CLIENT that has extra method:
                                  #  - connect(FUNC(ERROR, CLIENT, FUNC2(ERROR2))): acquire a CLIENT and fires FUNC()
                                  #Created with params (either OBJ or PG.defaults):
                                  #  - name: OBJ stringified
                                  #  - max <- poolSize
                                  #  - idleTimeoutMillis <- poolIdleTimeout
                                  #  - reapIntervalMillis <- reapIntervalMillis
                                  #  - log <- poolLog
                                  #Other OBJ passed to new PG.CLIENT(OBJ)
                                  #If no POOL used, would use one new connection for each query.
PG.pools.all                      #POOL_OBJ

PG.connect(OBJ, FUNC)             #Like new PG.CLIENT(OBJ, FUNC).connect() but uses PG.pools.getOrCreate(OBJ)
PG.on
 ("error", FUNC(ERROR, CLIENT))   #
PG.end()                          #Close all CLIENT, even if currently querying.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PGMODELER           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PGMODELER ==>                     #GUI modelling tool:
                                  #  - goal is to create/modify the DDL of a database, using a GUI.
                                  #     - can import DDL from existing database (of objects the user has permissions to query).
                                  #  - outputs SQL commands (or send to a database) or PNG image.
                                  #  - can validate DDL and issue warnings (requires connection to a database)
                                  #  - most DDL is available except foreign wrapper, etc., materialized views, event triggers,
                                  #    dictionaries, unlogged|temp tables, reference to VIEW COL
                                  #  - relationships: generalization is inherits, copy is create table like, others are foreign keys (with proper uniqueness)
                                  #  - there is a tree to go through object on the right panel


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            PGBENCH            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pgbench                           #Does a benchmark, to compare machines or server conf speed.
                                  #Must first do a pgbench -i to initialize it (creates four pgbench_* tables), with following options
                                  #while initializing:
-F NUM                            #Percentage of not-null in pgbench_* tables (def: 100)
-s NUM                            #Multiply default number or rows in pgbench_* tables. Should be at least >= -c NUM
--[index-]tablespace=TABLESPACE   #Use a custom TABLESPACE for tables or indexes (to do if used in production)
--unlogged-tables                 #Create pgbench_* as unlogged tables
                                  #While not initializing:
                                  #  - tps is transactions per seconds.
                                  #  - has following options:
-h -p -U                          #Connection options (see psql)
-c NUM                            #Number of concurrent connections. Should be close to average in real production.
-t NUM                            #Number of transactions per client. Higher gives more precision.
                                  #Should be high enough to run few minutes
-j NUM                            #Number of threads
-n -f FILE                        #Execute SQL FILE, instead of default one (simple update, select and insert statements)
                                  #Can include commands:
                                  #  - \setrandom "INT" MIN MAX
                                  #  - \setshell "INT" COMMAND ARGS
-S                                #Perform only select statements
-r                                #Show execution time for clients, per statements.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            PGTUNE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pgtune -i FILE                    #Checks postgresql.conf FILE, and prints an optimized version (mostly for performance ENVVAR)
                                  #Can do -o FILE2, but should pipe it to diff - FILE, to see differences.
-M NUM                            #Total memory in bytes (def: guess it)
-c NUM                            #Number of connections expected (change max_connections and work_mem)
-T WORD                           #Type of application, among DW (OLAP), OLTP, Web or Desktop. Def is Mixed (-> unspecified)
                                  #Desktop assumes lower cache, mem and connections, DW moderate, and Web and OLTP very high.
