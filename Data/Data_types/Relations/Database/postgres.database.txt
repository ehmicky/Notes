
            
   POSTGRES  
            



Relationships:
  - can be 0, 0|1, many or 0|many on each side
  - 1-to-1: foreign key
  - all others: third table with foreign keys for both sides
Enum:
  - prefer small tables when possible
How to check that foreign key cover all the primary key values?
Primary key:
  - should always create surrogate key as primary key, even when natural keys are candidate keys
  - should then put non-primary candidate keys with unique constraint
  - check article stackoverflow answered in StackOverflow


TO FINISH ==>                     #  - SSH, GPG
                                  #  - repmgr
                                  #  - Chapter 17, 19, and 18.3.2, and postgres -l
                                  #  - pgcrypto
                                  #  - 39.1. Installing procedural languages
                                  #  - Postgres-XC

VERSION ==>                       #9.3.5


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MAIN TASKS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DATABASE DESIGN ==>               #Modelling (pgModeler)
                                  #  - create tables with right columns, constraints (pkey, fkey, default, check, not null, unique, exclude) and properties (inherits)
                                  #  - using:
                                  #     - normal types (NUM, BOOL, STR, BSTR, BYTEA, DATE|TIME, ENUM) and arrays, ranges
                                  #     - special types (net, dictionary, xml, json, hstore, ltree, geometry) and created types (ctype, domain)
                                  #     - id vs uuid
                                  #     - views for encapsulation, or security restriction per column
                                  #     - [event] triggers
                                  #     - sequences
                                  #     - schemas
                                  #     - foreign tables (including file_fdw for CSV files)
                                  #     - listen|notify for clients communication
                                  #Security:
                                  #  - roles and privileges
                                  #  - authentication
                                  #  - unix_socket_directories
                                  #  - FUNC definition (leakproof, security definer)
                                  #Multithread-safety (transactions, locks)
                                  #Watch out for:
                                  #  - null possibility in queries
                                  #  - SQL injection when concatenating SQL_STR (use quote_*() or format())

PERFORMANCE ==>                   #  - on design, check using:
                                  #     - materialized views instead of views
                                  #     - rules instead of triggers
                                  #     - index
                                  #     - cursors
                                  #     - partitions
                                  #     - prepared statements
                                  #     - large objects
                                  #     - tablespaces
                                  #  - ENVVAR tunning:
                                  #     - disabling durability, decreasing checkpoints frequency, using RAM disks
                                  #     - setting right resources needed for [maintenance_]work_mem, effective_cache_size, wal_buffers, max_stack_depth, temp_file_limit,
                                  #       max_files_per_processes, effective_io_concurrency, shared_buffers, max_connections, statement_timeout
                                  #     - using pgtune
                                  #  - optimizing queries with explain
                                  #  - using pgbench
                                  #  - for big data write, see below best practices
                                  #  - use connection pooling (pgBouncer)
                                  #  - upgrading hardware
                                  #  - FUNC definition (volatility, cost, rows)
                                  #  - TABLE fillfactor, fastupdate
                                  #  - autovacuum tunning

SETUP FOR END USERS               #  - create [A|W]FUNC (possibily from PL/* languages), prepared statements, comments
 AND FUTURE MAINTENANCE ==>       #  - logging
                                  #  - [hot] standby with [a]sync. streaming replication
                                  #  - use pgagent for regular tasks:
                                  #     - pgbadger and pgcluu reports creation
                                  #     - check_postgres
                                  #        - good idea to merge pgbadger, pgcluu and check_postgres into one HTML file with a script
                                  #     - pg_dumpall
                                  #  - if durability, check proper cache usage (wal_sync_method, HDD|filesystem cache)

TESTING ==>                       #  - random filling (datafiller.py)
                                  #  - unit testing (pgTap)
                                  #  - load testing (Tsung)

MAINTENANCE ==>                   #  - monitoring:
                                  #     - use pgadmin, with Server status window, and opening a psql within pgadmin (create proper .psqlrc), or use teamPostgreSQL
                                  #     - resource (should not exceed max_connections and work_mem), space usage or other problems:
                                  #        - pgbadger and pgcluu
                                  #        - pg_top
                                  #        - check_postgres
                                  #  - data update (should create functions):
                                  #     - partitionning
                                  #     - copy TABLE from
                                  #  - cluster/reindex (ask for exclusive lock)
                                  #  - check PostgreSQL upgrades, and use pg_upgrade
                                  #  - create restore points with pg_create_restore_point(STR) after critical operations


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          POSTGRESQL           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ARCHITECTURE ==>                  #RDBMS with focus on standard compliance and extensibility.
                                  #Conform to SQL:2016 for most of it
                                  #Client (psql, pgadmin, etc.) / server (postgres) architecture

LIMITS ==>                        #  - database size: unlim
                                  #  - table size: 32TB
                                  #  - cols count: 250-1600 depending on TYPE
                                  #  - rows count: unlim
                                  #  - cell size: 1GB
                                  #  - file descriptors: OS-specific (see its doc)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SYNTAX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COMMAND;                          #"Statement"
CLAUSE ==>                        #Part of a COMMAND attached to keywords
                                  #E.g. WHERE BOOL

WHITESPACES ==>                   #Ignored

CASE-SENSITIVITY ==>              #Case-insensitive for keywords and VARs
"VAR"
U&"VAR2"                          #Make VAR case-sensitive

--COMMENT ...
/* COMMENT */                     #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             TYPES             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INT                               #smallint|integer|bigint|numeric|decimal|money
FLOAT                             #real|double precision
NUM                               #INT|FLOAT
STR                               #[var]char|text|unknown

smallint                          #TYPE. Signed 2 bytes integer
integer                           #TYPE. Signed 4 bytes integer
bigint                            #TYPE. Signed 8 bytes integer

real                              #TYPE. 4 bytes float
                                  #Can be 'NaN' or '[-]Infinity'
double precision                  #TYPE. 8 bytes float

numeric|decimal(NUM, NUM2)        #TYPE. Fixed precision integer, where NUM2 is number of decimals and NUM total number of digits
real(NUM)                         #TYPE. Fixed precision float, with NUM mantissa precision

money                             #TYPE. Like bigint divided by 100, so with two decimals. To use with money (as opposed to real)

boolean                           #TYPE
true|false|null                   #BOOL
BOOL or BOOL2                     #
BOOL and BOOL2                    #
not BOOL                          #
BOOL is [not] BOOL2               #Like = <>, except if BOOL|BOOL2 null (see below)

[var]char(NUM)                    #TYPE. String with NUM max chars
                                  #If no var, pads rest with spaces (slower and takes more space)
                                  #UTF-8
text                              #TYPE. Like varchar, but unlimited length
'...'                             #[VAR]CHAR
                                  #'' to escape a '
                                  #Newlines are ignored.
                                  #Can include Unicode characters as is
U&'...'                           #Like '...' but can also include \NNNN codepoint
E'...'                            #Like '...' but can use backslash escape sequences
                                  #\b, \f, \n, \r, \t, \NNN, \xNN, \uNNNN, \UNNNNNNNN
$[TAG]$...$[TAG]$                 #Like '...' but escapes newline, ' and \

unknown                           #Unknown TYPE. Coerced to a STR when needed

bit [varying]([NUM])              #TYPE. BSTR, i.e. like STR, but bit-wise
                                  #Def NUM: unlim
B'...'                            #BSTR with 0|1s
X'...'                            #BSTR with hex chars

bytea                             #TYPE. BSTR that is byte-wise
E'\\xNNNN...'                     #

date                              #TYPE
'YYYY-MM-DD'                      #DATE

time                              #TYPE
'HH:MM:SS[.SSSSSS]'               #TIME

timetz                            #TYPE
'TIME[TZ]'                        #TZ is -NUM, GMT+NUM, etc.

timestamp[tz]                     #TYPE
'DATE TIME[TZ]'                   #TIMESTAMP[TZ]

interval ...                      #TYPE
'[YYYY-][MM-][DD] [HH:[MM:]...]'  #INTERVAL
                                  #... can restrict which fields to use: year, month, ... [to year, month, ..., second]
                                  #Two identical INTERVALs might be printed differently (e.g. 24 hours and 1 day)
ENVVAR intervalstyle              #INTERVAL format among: 'postgres' (def), 'sql_standard' or 'ISO_8601'

epoch
[-]infinity                       #DATE|TIMESTAMP
today
tomorrow
yesterday                         #DATE|TIMESTAMP (midnight)
allballs                          #TIME (midnight)
now                               #DATE|TIME|TIMESTAMP

TZ                                #Use either of the following ones
pg_timezone_names                 #TABLE with zone names (e.g. 'America/New_York')
pg_timezone_abbrevs               #TABLE with zone abbreviations (e.g. 'PST')
ENVVAR timezone                   #

ENUM                              #TYPE. Values are STR.
create type ENUM as enum(STR...)  #Ordering in STR... is used for < <= >= >
alter type ENUM add value
 [if not exists] STR
 [before|after STR2]              #

point                             #TYPE
'(NUM,NUM2)'                      #POINT

line                              #TYPE. Infinite LINE
lseg                              #TYPE. Segment LINE
'((NUM,NUM2),(NUM3,NUM4))'        #LINE

box                               #TYPE. Rectangle. Same notation as LINE (diagonal)
(BOX;...)                         #BOX_ARR

path                              #TYPE
polygon                           #TYPE. Like closed PATH
'((NUM,NUM2)...)'                 #Closed PATH
'[(NUM,NUM2)...]'                 #Open PATH

circle                            #TYPE
'<(NUM,NUM2),NUM3>'               #CIRCLE

cube                             ##TYPE. N-dimensional cube
                                 ##Postgres extension 'cube'
'(NUM...), (NUM2...)'            ##CUBE diagonal

GEOMETRY OPERATORS ==>            #Many operators for shift, rotation, scaling, getting points|distance like center, positions, etc. exist
                                  #Including for cube
                                  #Not documented yet

macaddr                           #TYPE
'NN:NN:NN:NN:NN:NN'               #MACADDR

inet                              #TYPE
'IPv4|6'                          #INET

cidr                              #TYPE
'CIDR'                            #

xml                               #TYPE. Not documented yet
xmlparse(document|content STR)    #Must be well-formed.
 ->XML                            #content: for fragments
xmlserialize(document|content XML
 as [var]char|text)->STR          #

json                              #TYPE
'JSON'                            #JSON
                                  #Must be well-formed.

hstore                           ##TYPE. Key-value store
'VAR=>VAL,...'                   ##"VAR|VAL" to escape whitespace , = >
                                 ##VAL can be null

ltree                            ##TYPE
                                 ##Array of VAR representing a reference
'VAR.VAR2....'                   ##LTREE
                                 ##VAR is [[:alpha:]_]. Max 256 characters.

lquery                           ##TYPE
                                 ##Globbing-like query against a LTREE (in whole)
                                 ##Looks like a LTREE but can contain the following (can be combined)
*[{[NUM],[NUM2]}]                ##Instead of a VAR, e.g. *.VAR.*
                                 ##NUM[2]specifies how many VAR can match (def: {1,})
VAR*                             ##Any suffix
VAR%[*]                          ##Any suffix starting with _
                                 ##With *, also any suffix at end of _-separated words in VAR
VAR@                             ##Case insensitive
VAR|VAR2                         ##Or
!VAR                             ##Not

ltxtquery                        ##TYPE
                                 ##Like lquery, but combination of VAR with tsquery-like & | ! matching any VAR inside LTREE
                                 ##E.g. VAR* & VAR2@

uuid                             ##TYPE
                                 ##Postgres extension 'uuid-ossp'
'nnnnnnnnnnnnnnnnnnnn
 nnnnnnnnnnnn'
'nnnnnnnn-nnnn-nnnn-nnnn-
 nnnnnnnnnnnn'
'nnnn-nnnn-nnnn-nnnn-nnnn-
 nnnn-nnnn-nnnn'                 ##UUID
uuid_generate_v1[mc]()->UUID     ##UUID v1
                                 ##If mc, uses a random multicast MAC address
uuid_generate_v3|5               ##UUID v3|5
 (uuid_ns_*(), STR)->UUID        ##* is dns|url|oid|x500
uuid_generate_v4()->UUID         ##UUID v4

oid                               #TYPE
                                  #ID of a VAR.
                                  #32 bits, so can't be use reliably for uniqueness with more than 1000 (TABLE.oid are TABLE-specific), so deprecated.
                                  #Can be cast to UINT.
                                  #Other system types include tid, xid and cid (tuple, transaction and command).
regproc (FUNC)
regprocedure (FUNC(...))
regoper (+)
regoperator (+(...))
regclass (TABLE)
regtype (TYPE)
regconfig (REGCONF)
regdictionary                     #OID aliases but for semantic purposes include (can all be casted to OID)

pg_typeof(VAL)->'TYPE'            #Returns TYPE

IMPLICIT CASTING ==>              #How:
                                  #  - for a FUNC arg or the TYPE of a COL
                                  #  - it looks at pg_cast to see if a casting function is available. Examples:
                                  #    - between NUM
                                  #    - between date/time and STR
                                  #    - no casting between STR and NUM
cast(VAL as TYPE)                 #Explicit casting
TYPE STR                          #Same but shorter and must provide VAL as STR

create cast (TYPE as TYPE2)
with function FUNC(...)           #Creates a type casting FUNC with a 'f' castmethod (see pg_cast.castfunc for FUNC(...))
[as assignment|implicit]          #assignment|implicit designates the 'a' and 'i' castcontext (def: 'e')
create cast ... with inout ...    #Same but with a 'i' castmethod (see pg_cast)
create cast ...
 without function ...             #Same but with a 'b' castmethod (see pg_cast)

null                              #Represents a missing data. Error-prone, to avoid.
                                  #Is of type unknown until it resolves to an actual type (any TYPE can have null values)
                                  #All OP and FUNC with null returns null, including:
                                  #  - NUM OP null, STR OP null
                                  #  - FUNC(null)
                                  #  - null = <> < null, case VAL when null
                                  #  - exists(), CTYPE_VAL in TABLE_VAL, etc.
                                  #Exceptions:
                                  #  - null = null for anything about duplicates, except unique
                                  #    (group by, partition by, union|intersect|except [all], distinct)
                                  #  - BOOL:
                                  #     - null or true -> true, null and false -> false
                                  #     - null -> false in where BOOL, but null -> true in check (BOOL)
                                  #Working around nulls:
                                  #  - VAL is [not] distinct from VAL2: like = <>, but treats null as any other value.
                                  #     - null is [not] null -> same as null is [not] distinct from null
                                  #  - concat(STR...): same STR || ..., but treats null as ""
                                  #  - nullif(VAL, VAL2): same as case VAL when VAL2 then null else VAL end
                                  #  - coalesce(VAL...): returns the first VAL that is not null
                                  #AFUNC() removes nulls first, except count(*)
                                  #Foreign keys (COL2 is the primary key, COL the foreign key):
                                  #  - null COL value match any COL2 (but not inverse)
                                  #  - for multicolumn COL..., match if at least one (match simple (def)) or all (match full) null COL value
                                  #Best to do:
                                  #  - use not null constraints when possible
                                  #  - if not:
                                  #    - append "or|and VAL is [not] null" to BOOL
                                  #    - always think about possibility of null is expressions

TABLE                             #Name of a TABLE
TABLE_VAL                         #TABLE or table expression (expression that returns a TABLE_VAL), including subqueries
COL_VAR                           #Name of a COL. Must be TABLE.COL or COL(TABLE) if several TABLE.
VAR                               #Can be max 63 characters, [[:alnum:]_]+
"VAR"                             #Like VAR, but cannot be mistaken as keyword (e.g. "select" as a VAR), and is case-sensitive
VAL                               #Either a COL or scalar VAL, treated as a COL of size 1
                                  #When VAL OP VAL2 with different sizes, the smaller one is being repeated.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           OPERATORS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


operator([SCHEMA.]OP)             #Another way to write OP (default SCHEMA for OP is pg_catalog)

OPERATORS ==>                     #Operators shared by all type, except geometry, json, xml (must cast to STR)
VAL = <> > >= < <= VAL2           #true > false.
VAL between [symmetric]
VAL2 and VAL3                     #Same as VAL >= VAL2 and VAL <= VAL3. If symmetric and VAL3 < VAL2, swap them.
greatest|least(VAL...)            #Use > >= < <=
VAL OP any|all (ARR)              #True if VAL OP any|all ARR_VAL
VAL [not] in (VAL2...)            #Same as VAL =|<> any ARR, but with VAL2...

exists(TABLE_VAL)                 #Returns true if TABLE_VAL has at least one row
                                  #Often:
                                  #  - TABLE_VAL will not care about the VAL in select VAL ..., so write select 1 ...
                                  #  - used row-wise in a where clause
                                  #     - e.g. select COL from TABLE where exists(select 1 from TABLE2 where TABLE2.COL2 = TABLE.COL)
CTYPE_VAL OP any|all              #Returns true if any|all of the rows of TABLE_VAL OP CTYPE_VAL
TABLE_VAL                         #Often used row-wise in a where clause.
CTYP_VL [not] in TBL_VAL          #Same as CTYPE_VAL =|<> any TABLE_VAL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            NUMBERS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


NUM - + - / * NUM2                #

NUM & | # << >> NUM2              #Bitwise operations
~ NUM                             # # is xor

power(NUM, NUM2)
NUM ^ NUM2                        #

mod(NUM, NUM2)
NUM % NUM2                        #

div(NUM, NUM2)                    #trunc(NUM/NUM2)

|/ NUM                            #Square root
cbrt(NUM)
||/ NUM                           #Cube root

INT!
!! INT                            #Factorial

exp(NUM)                          #
ln(NUM)                           #
log(NUM[, NUM2])                  #Def: 10

abs(NUM)
@ NUM                             #
sign(NUM)                         #-1, 0 or 1

ceil(NUM)                         #
floor(NUM)                        #
trunc(NUM[, NUM2])                #Def: 0
round(NUM[, NUM2])                #Def: 0

[a]cos|sin|tan(NUM)               #
cot(NUM)                          #
atan2(NUM, NUM2)                  #
degrees|radians(NUM)              #Conversion
pi()                              #

width_bucket(NUM, NUM2, NUM3,NUM4)#In a histogram from NUM2 to NUM3 with NUM4 buckets, returns in which bucket number would fall NUM1.
generate_series(INT, INT2[, INT3])
 ->TABLE_VAL                      #From INT to INT2, with step INT3 (def: 1)

random()->NUM                     #From 0 to 1
                                  #Cycle of 3e14 numbers
                                  #PRNG not crypto-secure
setseed(NUM)                      #NUM is from 0 to 1
normal_rand(INT, REAL, REAL2)    ##INT random variables following N(REAL, REAL2)
 ->TABLE_VAL                     ##Postgres extension 'tablefunc'


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            STRINGS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


STR || STR2                       #Concatenation
concat(STR...)                    #Same but null are ignored, instead of producing null as a result
concat_ws(STR, STR2...)           #Same but with separator STR
[bit|char|octet_]length(STR)      #Def is char
repeat(STR, UINT)                 #
reverse(STR)                      #
lower|upper(STR)                  #
initcap(STR)                      #upper() to first letters, lower() to rest. Use locales.

position(STR in STR2)->INT        #Index of STR inside STR2. 0 if not found.
substring
 (STR from UINT [to UINT2])->STR2 #From character UINT to UINT2 (def: end)
left|right(STR, UINT)->STR2       #Same from left|right
overlay(STR placing STR2
 from UINT[to UINT2])->STR3       #Replace character UINT to UINT2 (def: end) of STR by STR2
trim([trailing|leading|both]
 [STR from] STR2)->STR3           #Remove (def: both) characters among STR (def: ' ') from STR2
l|rpad(STR, UINT[, STR2])->STR3   #Pads STR2 repeatingly from left|right of STR, until it has length UINT (if inferior, truncate STR).
translate(STR, STR2, STR3)->STR4  #Like Unix command tr


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PATTERNS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SQL GLOBBING ==>                  #Use % and _, same as normal globbing * and .
REGEXP ==>                        #Also with some different syntaxes:
                                  #  - QUANTIFIER* -> QUANTIFIER?
                                  #  - \< \> -> \m \M
                                  #  - \b \B -> \y \Y
                                  #  - no \u \l
                                  #Use locales for case sensitivity and [[:...:]]
STR [not] [i]like STR2            #Same as STR = STR2, but can use SQL globbing (i is case-insensitive)
 [escape STR3]                    #STR3 is escape character (def: '\').
STR [!]~[*] STR2                  #Same but with regexps. STR2 needs to match only part of STR. * means case-insensitive.
substring(STR, STR2)              #Returns the part of STR that matches the regexp STR2, or null if no match.
                                  #If regexp contains parenthesis, only returns the part in the first outer set of parenthesis.
regexp_matches(STR, STR2[, STR3]) #Same except:
                                  #  - returns as a TABLE_VAL, containing ARR for each match for a set of parenthesis
                                  #    (if no parenthesis, the whole match)
                                  #  - if several matches and flag 'g', returns several rows.
                                  #  - If no match, returns no row. So output rows don't always match input rows, unless it is a
                                  #    subquery crossjoined and no flag 'g', e.g. select COL, (select regexp_matches(COL2, STR2))
regexp_replace                    #Same as sed 's/STR2/STR3/[STR4]' <<<STR
 (STR, STR2, STR3[, STR4])        #STR3 can contain \1, \&, etc.
replace(STR, STR2, STR3)          #Same but without regexps (faster)
regexp_split_to_array             #Returns the split of STR, using delim STR2, with flags STR3, as an ARR.
 (STR, STR2[, STR3])              #If delim not to be found, returns only STR in ARR.
regexp_split_to_table()           #Same but returns as a TABLE_VAL, with one row for each element.
                                  #Same thing as regexp_matches() for the crossjoin.
split_part(STR,STR2,INT)          #Same but without regexps, and returns the field number INT.

quote_ident(VAL)                  #Returns as STR with extra "" to escape when needed (for COL or VAL), when constructing SQL commands as STR.
quote_literal(VAL)                #Same as quote_ident(VAL) but using ''
quote_nullable(VAL)               #Same as quote_literal(VAL), but if VAL is null, returns 'NULL', not null
format(STR, ...)                  #Similar to sprintf() in C
                                  #Types can be %I (quote_ident), %L (quote_nullable) or %s (quote_literal without the extra '')

ascii(STR)
chr(UINT)                         #Conversion from|to a single character and Unicode codepoint.
to_hex(NUM)                       #Returns as STR hexadecimal representation

md5(STR)                          #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            BINARY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


encode(BYTEA, STR)                #Returns BYTEA as a textual form, according to STR among "base64" or "hex"
decode(STR, STR2)                 #Inverse
trim(...)
md5(...)                          #Same as STR, but for BYTEA

BYTEA|BSTR || BYTEA|BSTR
bit|octet_length(...)
overlay(...)
position(...)
substring(...)                    #Same as STR, but for BYTEA and BSTR
get_byte(BYTEA, UINT)             #
set_byte(BYTEA, UINT, UINT2)      #
get_bit(BYTEA|BSTR, UINT)         #
set_bit(BYTEA|BSTR, UINT, UINT2)  #

BSTR & | # << >> BSTR2            #
~ BSTR                            #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATE/TIME           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


to_char(TIMESTAMP|INTERVAL, TEXT) #Prints TIMESTAMP as TEXT according to format (cf online doc), e.g. "HH24:MI:SS"
                                  #Use locales.
to_date(TEXT, TEXT2)              #Inverse, but with DATE
ENVVAR DateStyle                  #When reading ambiguous DATE (day is from 1 to 12), how to read it: "DMY" (def), "MDY" or "YMD"
to_timestamp(DOUBLE)              #DOUBLE is seconds since epoch.
to_char(NUM, TEXT)
to_number(TEXT, TEXT)             #Same but for NUM

DATETIME + - DATETIME2            #
-INTERVAL                         #
INTERVAL / * NUM                  #
current_time[stamp]|date
local_time[stamp]                 #From beginning of transaction
transaction_timestamp()           #Same as current_timestamp, but from beginning of this statement
TIME[STAMP][TZ] at time zone STR  #
extract
 (WORD from INTERVAL|TIME[STAMP]) #WORD can be millenium, century, decade, [iso]year, quarter, month, week, day, [iso]dow (day of week),
 ->UINT                           #doy, epoch, hour, minute, [micro|milli]seconds, timezone[_hour|minute]
date_trunc(STR, TIMESTAMP|INTERVL)#STR is same as WORD in extract() (for most of it). Truncates the date until STR.
isfinite(DATE|TIMESTAMP|INTERVAL) #
justify_days|hours|interval
 (INTERVAL)                       #Adjust an INTERVAL so that extra hours become days, extra days become months, or both
generate_series
 (TIMESTAMP, TIMESTAMP2, INTERVAL)
 ->TABLE_VAL                      #From TIMESTAMP to TIMESTAMP2, with step INTERVAL
pg_sleep(DOUBLE)                  #Sleeps DOUBLE seconds (10ms resolution)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ENUM              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


enum_first|last(ENUM)             #Returns first|last ENUM elem
enum_range(ENUM[, ENUM2])         #Returns ENUM_ARR ranging from ENUM to ENUM2, included (def: last one)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INET              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INET >> >>= << <<= INET2          #Containing|contained within [or equal]
INET|MAC | & INET2|MAC            #
~INET|MAC                         #Bitwise operations
INET + - INET|UINT                #From last to first field.
abbrev(INET|CIDR)                 #Show as STR
text(INET)                        #Same but longer version
broadcast(INET)                   #
host(INET)                        #IP address without mask
hostmask(INET)                    #Only part of IP addres with mask
netmask(INET)                     #Mask as INET
masklen(INET)                     #Mask as NUM
set_masklen(INET|CIDR, UINT)      #Modifies mask
network(INET)                     #Part of IP without last number
family(INET)                      #4|6 for ipv4|6
trunc(MAC)                        #Put second half as 00


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             JSON              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


JSON->UINT|STR                    #Same as OBJ[UINT|STR]
JSON->>UINT|STR                   #Same but returns as STR
JSON#>ARR
JSON#>>ARR                        #Same but with several successive indexes

to_json(VAL)                      #If a cast function exists, return cast(VAL as json), otherwise, converts VAL to STR first.
array_to_json(ARR)                #Returns ARR as JSON { ARR }
json_array_length(JSON)           #Length of ARR in JSON { ARR }
row_to_json(CTYPE)                #Returns CTYPE as JSON { "f1": ..., "f2": ... ... }
json_each[_text](JSON)            #Returns JSON as a TABLE_VAL (each first dimension VAR is a row, and its VAL the second column)
                                  #text convert to STR after.
json_object_keys(JSON)            #Same but with only the first column (the VARs)
json_populate_record              #Same as json_each(), but horizontal, and only keeps VAR that have a matching COL in TABLE.
(null::TABLE, JSON[, BOOL])       #If true (def: false), will convert to false too.
json_populate_recordset
(null::TABLE, JSON[, BOOL])       #Same but from a JSON array, and do one row for each OBJ of the array.
json_array_elements(JSON)         #Returns a JSON array as a TABLE_VAL with one column, and a row for each VAL.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            HSTORE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


HSTORE                           ##Postgres extension 'hstore'
HSTORE->STR
HSTORE->STR_ARR                  ##Gets VAL[_ARR] whose key is STR[_ARR], as STR
HSTORE || HSTORE2                ##Concatenation
HSTORE - STR[_ARR]               ##Returns HSTORE without keys specified in STR[_ARR] or keys+values specified in HSTORE2
HSTORE - HSTORE2                 ##STR must be of type text, not unknown.
slice(HSTORE, STR_ARR)           ##Returns HSTORE with keys specified in STR_ARR (if keys are absent, not an error)
TABLE_VAL #= HSTORE              ##Returns TABLE_VAL with changes specified by HSTORE (key is column name, value is new value)
                                 ##To use TABLE_VAL, needs to do:
                                 ##  select (ALIAS).* from (select TABLE_VAL #= HSTORE as ALIAS from TABLE) ALIAS2;

HSTORE ? STR
HSTORE ?& ?| STR_ARR             ##True if HSTORE contains key STR, or all|one of key in STR_ARR
defined(HSTORE, STR)             ##True if HSTORE contains key STR, and value is not null
HSTORE @> <@ HSTORE2             ##Contains/is contained

%% %# HSTORE                     ##Converts to STR_ARR[_ARR].
hstore(STR_ARR[_ARR])            ##Inverse
hstore(STR_ARR, STR2_ARR)        ##Converts to HSTORE, where STR_ARR are the keys and STR2_ARR the values.
hstore(TABLE_VAL)                ##Converts to HSTORE (with name of columns, or fNUM if anonymous CTYPE_VAL)
                                 ##Ex: select hstore(TABLE) from TABLE;
a|skeys|vals(HSTORE)             ##Gets keys or values of HSTORE as a STR_ARR (a) or TABLE_VAL (s)
hstore_to_json[_loose](HSTORE)   ##If loose, NUM and BOOL will have this type in JSON (otherwise, will stay as STR)
each(HSTORE)                     ##Converts to TABLE_VAL, with two columns key and value


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             LTREE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LTREE                            ##Postgres extension 'ltree'
LTREE < <= >= > <> = LTREE2      ##
LTREE || LTREE2                  ##Concatenation
LTREE[_ARR] ~ LQUERY
LTREE[_ARR] ? LQUERY_ARR
LTREE[_ARR] @ LTXTQUERY          ##Does it match LQUERY[_ARR]|LTXTQUERY [any of]
LTREE[_ARR] <@ @> LTREE2[_ARR]   ##Contained|contains [any of]
LTREE_ARR ?@> ?<@ ?~ ?@ ...      ##Returns the first LTREE in LTREE_ARR that returns true to @> <@ ~ @ ... (null if none)

nlevel(LTREE)                    ##Number of VAR
index(LTREE, LTREE2[, INT])      ##Position of first VAR starting LTREE2 inside LTREE, from INT
 ->UINT                          ##Def INT: 0. INT can be negative to specify from end
                                 ##Returns -1 if not found
subltree(LTREE, INT, INT2)       ##Slice it from VAR numero INT to numero INT2-1
subpath(LTREE, INT[, INT2])      ##Same but from numero INT (can be negative to be from end) with length INT2 (can be negative to
                                 ##specify how many left on the right) (def: all)
lca(LTREE_ARR|LTREE...)          ##Number of VAR matching at beginning of each LTREE ('' if none)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ARRAY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TYPE array                        #Array for any TYPE (can be used as any TYPE).
TYPE[]                            #Dimensions are defined at write-time.
array[VAL...]                     #ARR_LIT. VAL can be a subarray, i.e. [VAL...] or {VAL...} for multidimensional arrays.
'{VAL...}'                        #STR must be double-quoted inside '{...}'.
'[INT1:INT2]...={VAL..}'          #Same but with specified lower bounds.
array(TABLE_VAL)                  #TABLE_VAL must have only one column, and not be the result of a FUNC() (unless written as a subquery (select FUNC()))

ARR[UINT]...                      #Accessing ARR. Index starts at 1. Returns null if any index is out-of-bound.
ARR[UINT1:UINT2]...               #Accessing slices of ARR.
                                  #Subsequent dimensions must use slices too (but can take a single element of the slice).
                                  #Returns null if one index is out-of-bound, and empty ARR if both are out-of-bound.
array_ndims(ARR)                  #Returns number of dimensions as UINT
array_dims(ARR)                   #Returns number of dimensions as STR, e.g. '[1:3][1:4]'
array_lower|upper(ARR, UINT)      #Returns the lower and upper bound of dimension UINT, as UINT2.
array_length(ARR, UINT)           #Same as array_upper - array_lower
generate_subscripts(ARR, UINT)    #Returns the indexes of dimension UINT, as a TABLE_VAL

ARR = ARR_LIT                     #Any expression can be used to write an ARR or part of it. If index is out-of-bound and only one
ARR[...]=VAL|ARR_LIT              #dimension, size of ARR is extended (if less than 0, modifies lower bound).
VAL|ARR || ARR2
ARR2 || VAL|ARR                   #If VAL|ARR has dimension n-1 and ARR2 dimension n, appends|prepends. If same dimension, concatenate
array_append|cat(ARR, VAL|ARR2)   #Same as ||, but throw error if not VAL|ARR2
ARR @> ARR2                       #True if ARR2 is contained within ARR (all elements of ARR are in ARR2)
ARR <@ ARR2                       #Inverse
ARR && ARR2                       #True if ARR overlaps ARR2 (any element of ARR is in ARR2)
any(ARR) OP VAL                   #Same as ARR[1] OP VAL or ARR[2] OP VAL or ... Must be on the right operand.
and(ARR) OP VAL                   #Same with and

array_fill(VAL, ARR[, ARR2])->ARR3#ARR3 filled with VAL, of dimensions ARR and lower bound ARR2 (def: {1...})
array_remove(ARR, VAL)->ARR2      #One-dimensional ARR2 without any element = VAL
array_replace(ARR, VAL, VAL2)     #Same but replace with VAL2
array_to_string(ARR, STR2[, STR3])#Returns concatenation of ARR as STR, with STR2 as delim, and STR3 replacing null values (def: removes them)
string_to_array(STR, STR2[, STR3])#Inverse. If delim is null, each character is separated with next one. If "", returns whole STR as one element ARR
unnest(ARR)                       #Flattens ARR, and returns it as a TABLE_VAL with one column and one row for each VAL.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        INTEGER ARRAYS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INT_ARR                          ##All following will not work if contains null. ARR must be one-dimensional.
                                 ##Postgres extension 'intarray'
INT_ARR | INT2[_ARR]             ##Union
INT_ARR & INT2_ARR               ##Intersection
INT_ARR && INT2_ARR              ##Overlaps.
INT_ARR <@ | @>
INT2_ARR                         ##Is contained | contains
INT_ARR @@ TSQUERY               ##Matches TSQUERY, containing NUM

INT_ARR +|- INT_ARR              ##
INT_ARR +|- INT                  ##Append or remove entries matching INT

icount(INT_ARR)                  ##Number of elements
sort(INT_ARR[, 'desc'])          ##
uniq(INT_ARR)
idx(INT_ARR, INT2)               ##Index of first element matching INT2
subarray(INT_ARR, INT2, INT3)    ##


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         COMPOSED TYPE         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create type CTYPE as (VAR TYPE...)#Composed type (array with different types). Can be used as any TYPE.
                                  #create table TABLE creates a CTYPE with same name, which means TABLE === CTYPE:
                                  #  - as TYPE (strip their constraint though)
                                  #  - but not as a VAR nor TABLE_VAL
alter type CTYPE add attribute
 VAR TYPE cascade|restrict        #  - cascade|restrict: when CTYPE is of a TABLE, modify the TABLE too
alter type CTYPE drop attribute
 [if exists]
 VAR TYPE cascade|restrict        #
alter type CTYPE alter attribute
 VAR type TYPE cascade|restrict   #
alter type CTYPE rename attribute
 VAR to VAR2 cascade|restrict     #

[row](VAL...)                     #CTYPE_LIT. null is specified by writting nothing, e.g. (1,,3). row is necessary only if only one VAL.
(CTYPE_VAL).VAR                   #Accessing single types of composite types.
                                  #Sometimes an extra set of parenthesis is needed around CTYPE_VAL (e.g. if produced by a FUNC())
                                  #CTYPE < <= > >= CTYPE2 compares the first element first, etc.
VAR(CTYPE_VAL)                    #Other notation


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       USER-DEFINED TYPE       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create domain TYPE as TYPE2       #Creates a user-defined TYPE, interchangeable with TYPE2, but with following
 [default VAL]                    #Default VAL (def: null, can be default)
 [constraint CONSTRAINT]          #Constraint: BOOL can use "value", which refers to the values used.
                                  #CONSTRAINT just specify the name.
                                  #Ex: email adresses
 [not null|check( BOOL )]         #

alter domain TYPE set default VAL #
alter domain TYPE
 set|drop not null                #
alter domain TYPE add
 [constraint CONSTRAINT]
 [not null|check( BOOL)]
 [not valid]                      #not valid: like alter table ... not valid
alter domain TYPE drop constraint
 [if exists]
 CONSTRAINT restrict|cascade      #
alter domain TYPE
 rename constraint
 CONSTRAINT to CONSTRAINT2        #

create type TYPE
 (input = FUNC,
  output = FUNC2, ...)            #Creates user-defined TYPE, based on C functions (see online doc)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             RANGE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


RANGE ==>                         #Like an ARR of size 2, but with semantic indicating start and end.
                                  #TYPE can be :
                                  #  - int4range (integer)
                                  #  - int8range (bigint)
                                  #  - numrange (numeric)
                                  #  - tsrange (timestamp)
                                  #  - tstzrange
                                  #  - daterange
create type TYPE as range
 (subtype = TYPE2,
  subtypediff = FUNC
  [, canonical = FUNC2]
  [, subtype_opclass = OPCLASS]   #Personalized RANGE. FUNC(x,y) -> x-y (as TYPE2)
  [, collation=COLLATION])        #FUNC2(x) -> x is done first. To use TYPE in FUNC2, must use a dummy create type TYPE; first before this call.

'[VAL, VAL2]'
'[VAL, VAL2)'                     #RANGE_LIT. Square brackets include, parenthesis exclude.
'(VAL, VAL2]'                     #Def of VAL|VAL 2 is [-]Infinity. null is equivalent to [-]Infinity
'(VAL, VAL2)'                     #For discrete types, ] is automatically converted to ) and 1 added to VAL2.
RANGE_TYPE(VAL, VAL2[, STR])      #Same. STR is e.g. '[)' (def) or '()'

RANGE @> VAL|RANGE2               #True if VAL|RANGE2 is contained within RANGE
VAL|RANGE <@ RANGE2               #Inverse
RANGE && RANGE2                   #True if RANGE overlap RANGE2.
                                  #Can be used with exclude ( RANGE with && ) to make RANGE not overlap in one COL.
RANGE << >> RANGE2                #True if end|begin of RANGE is before begin|end of RANGE2
RANGE &< &> RANGE2                #True if begin and end of RANGE is before begin and end of RANGE2 (or inverse for &>)
RANGE -|- RANGE2                  #True if end of RANGE is adjacent to RANGE2
RANGE + RANGE2                    #Returns union (error if not contiguous)
RANGE - RANGE2                    #Returns [RANGE.begin, RANGE2.begin]. If RANGE2.begin < RANGE.begin: if RANGE2.end > RANGE.end,
                                  #returns RANGE, otherwise returns empty
RANGE * RANGE2                    #Returns intersection RANGE3 (empty if no intersection)

lower|upper(RANGE)                #Returns VAL or VAL2 of the RANGE
lower|upper[_inc](RANG)           #True whether bounds are inclusive or not (brackets or parenthesis)
lower|upper[_inf](RANG)           #True whether bounds are [-]Infinity or null
'empty'                           #Special RANGE where [VAL,VAL2), and VAL = VAL2 = null
isempty(RANGE)                    #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           STRUCTURE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


case [VAL]
  when VAL2a then
    VAL2b
  [when VAL3a then
    VAL4b]...
  [else                           #Switch statement. Substitutes to VAL|VAL2|VAL3
    VAL4]                         #Def VAL is true, i.e. if VAL*a are BOOL, is like an if statement
end                               #Use =, e.g. VAL = VAL2a, so can't compare to null (should use "case when VAL is null" if can be null)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              DDL              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PGMODELER ==>                     #GUI modelling tool:
                                  #  - goal is to create/modify the DDL of a database, using a GUI.
                                  #     - can import DDL from existing database (of objects the user has permissions to query).
                                  #  - outputs SQL commands (or send to a database) or PNG image.
                                  #  - can validate DDL and issue warnings (requires connection to a database)
                                  #  - most DDL is available except foreign wrapper, etc., materialized views, event triggers,
                                  #    dictionaries, unlogged|temp tables, reference to VIEW COL
                                  #  - relationships: generalization is inherits, copy is create table like, others are foreign keys (with proper uniqueness)
                                  #  - there is a tree to go through object on the right panel

drop WORD [concurrently]          #All create WORD have a related drop WORD.
 [if exists] ...                  #  - if exists (all): no error if not existing
 [on TABLE]                       #  - cascade|restrict:
 cascade|restrict                 #     - drop also objects that depend on it (cascade) or not restrict (def, should always be mentioned)
                                  #     - doesn't work for database and tablespace: need to remove all objects manually first
                                  #  - concurrently (index):
                                  #     - like create index concurrently ...
                                  #     - needs to be outside a transaction, restrict and only one INDEX
                                  #... is VAR except:
                                  #  - VAR... (domain|table|sequence|[materialized ]view|schema|extension|index|foreign table|type)
                                  #  - VAR(...) (aggregate, function)
                                  #  - VAR(none|TYPE, none|TYPE2) (operator)
                                  #  - VAR on TABLE (trigger, rule)
                                  #  - ( TYPE as TYPE2 ) (cast)
                                  #  - VAR using INDEXMETHOD (operator class|family)
                                  #  - for ROLE|current_user server SERVER (user mapping)
                                  #drop table ... cascade|restrict also remove related INDEX, RULE, TFUNC and CONSTRAINT

alter TYPE VAR                    #Every create TYPE VAR (except cast) has a corresponding alter TYPE VAR ... to change options after creation
 [if exists] ...                  #if exists is available if create ... if not exists is available.
                                  #Other alter ... are documented in this doc
alter TYPE VAR ...
rename to VAR2                    #For all TYPE but extension, operator

create [temp|unlogged]            #Creates a TABLE containing ARG columns, which can be:
table [if not exists]             # - COL_VAR TYPE [constraint CONSTRAINT] ... [initially deferred|immediate] [[not] deferrable]:
TABLE [of CTYPE](ARG...)          #    - ... force the COL to respond true to a condition
[with ( VAR = VAL ... )]          #    - CONSTRAINT is the variable name
[on commit                        #    - [initially deferred|immediate] [[not] deferrable]: see concurrency chapter
delete rows|drop]                 #    - ... can be:
[inherits ( TABLE2... )]          #       - check(BOOL) [no inherit]:
[tablespace TABLESPACE]           #          - def for CONSTRAINT is TABLE_COL_check
[as TABLE_VAL                     #       - not null:
with [no] data]                   #          - same as check(COL_VAR [is] not null)
                                  #          - cannot have a name CONSTRAINT
                                  #       - unique [with (VAR = VAL...)] [using index tablespace TABLESPACE]:
                                  #          - same as check("no duplicate in COL_VAR")
                                  #          - creates a btree INDEX called CONSTRAINT_VAR
                                  #          - def for CONSTRAINT_VAR is TABLE_COL_key
                                  #          - VAR = VAL...: storage options, cf below
                                  #       - primary key [with (VAR = VAL...)] [using index tablespace TABLESPACE]:
                                  #          - same as not null unique ..., but :
                                  #             - there must be only one primary key
                                  #             - def for CONSTRAINT_VAR is TABLE_pkey
                                  #       - references TABLE2(COL2_VAR) [match full|simple] [on delete WORD] [on update WORD]:
                                  #          - creates a foreign key:
                                  #             - 1) inserted COL values must be one of the COL2 values
                                  #             - 2) dropped COL2 values must not be referenced by COL values
                                  #             - 3) 4) same but with update ... set
                                  #          - COL2 must be unique
                                  #          - on update|delete:
                                  #             - doesn't change 1) or 3) behavior
                                  #             - but WORD is the action when 2) (delete) or 4) (update) happens:
                                  #                - no action (def): emits an error
                                  #                - restrict: same but cannot be deferrable
                                  #                - cascade: drop|change COL values too
                                  #                - set default|null: set COL values to null|default
                                  #          - match simple|full: when inserting nulls in COL: see null
                                  #          - def for CONSTRAINT_VAR is TABLE_COL_fkey
                                  #       - default VAL:
                                  #          - default value when insert into omet COL_VAR
                                  #          - VAL is evaluated at insertion, not table creation time
                                  #          - cannot have a named CONSTRAINT
                                  #  - like TABLE|CTYPE [including defaults|constraints|indexes|storage|comments|all]
                                  #     - like putting the definitions of all the COL of TABLE|CTYPE. Can be used among other ARG.
                                  #     - by def. only include COL_VAR TYPE + not null. Other things can be included with including ...
                                  #        - indexes is for INDEX, primary key and unique
                                  #        - foreign key cannot be copied
                                  #  - [small|big]serial (with no TYPE):
                                  #     - same as integer not null default nextval('TABLE_COL_SEQ'), after creating a TABLE_COL_SEQ, owned by COL
                                  #     - can be followed by other constraints, i.e. unique, primary key, references or check
                                  #  - multicolumn constraint:
                                  #    - same syntax as usual ARG, but without COL_VAR TYPE, and constraint can use several COL_VAR...
                                  #      (including for primary|foreign key) of the TABLE:
                                  #       - e.g. unique ... -> unique(COL_VAR...) ...
                                  #    - exceptions:
                                  #       - references TABLE(COL2_VAR...) -> foreign key(COL_VAR...) references TABLE(COL2_VAR...)
                                  #       - not for not null nor default
                                  #    - can also use this constraint:
                                  #       - exclude [using INDEXMETHOD] (COL_VAR [OPCLASS] [asc|desc] [nulls first|last] with OP ...)
                                  #         [where BOOL] [using index tablespace TABLESPACE]:
                                  #          - do not insert new ROW that returns true with ROW OP (any already existing) ROW2
                                  #          - only for ROWs that match `where BOOL`
                                  #          - OP must be commutative
                                  #          - has similar goal as unique, but more advanced. Could also use create domain TYPE.
                                  #          - cree CONSTRAINT TABLE_COL_excl
                                  #          - based on an INDEX:
                                  #             - INDEXMETHOD cannot be gin
                                  #             - can specify a TABLESPACE for it
                                  #             - [OPCLASS] [asc|desc] [nulls first|last]: passed to the create index ...
                                  #Others:
                                  #  - inherits:
                                  #     - TABLE becomes child of TABLE2...:
                                  #        - i.e. TABLE inherits COL... of TABLE2 (but keeps its own ARG...)
                                  #        - manipulation on TABLE2 include COL... of TABLE unless "only (TABLE2)" is used:
                                  #           - the tableoid of the row can show which TABLE actually owns each row
                                  #           - commands like reindex, vacuum, etc. don't support inheritance
                                  #     - TABLE:
                                  #        - doesn't inherit unique, primary key, foreign key constraints (not inherited, and
                                  #          references TABLE implicitly means "only (TABLE)"), indexes, rules and triggers
                                  #            -> makes inherits mostly useless and dangerous
                                  #        - but inherits check() (unless no inherit is used) and not null
                                  #  - temp:
                                  #     - TABLE will be dropped at end of session
                                  #        - should manually run analyze on it (no autovacuum)
                                  #     - with "on commit", at the end of a successful transaction:
                                  #        - delete rows: TABLE will be truncated
                                  #        - drop: TABLE will be dropped
                                  #     - max buffers increment size used is ENVVAR temp_buffers (def: 8MB)
                                  #  - unlogged:
                                  #     - WAL doesn't take TABLE into account, which makes it faster, but also not crash-safe, and cannot use replication to standby servers
                                  #  - of CTYPE:
                                  #     - declare the TYPE of the COL_VAR according to CTYPE
                                  #        - should use COL_VAR with options ... instead of COL_VAR TYPE ...
                                  #        - CTYPE must not be a TABLE, but a "real" CTYPE created with create type CTYPE
                                  #     - TABLE will be dropped by drop CTYPE cascade
                                  #     - cannot use inherits
                                  #  - with (VAR = VAL...):
                                  #     - fillfactor INT (10 to 100 (def)):
                                  #        - keep additional free space on the page in which the TABLE is stored, so that future updates do not
                                  #          need to allocate it
                                  #        - 1-INT is amount of free space
                                  #        - if lot of updates, more efficient to have lower than 100.
                                  #     - autovacuum_*: set ENVVAR autovacuum_* at the table-level
                                  #     - buffering on|off|auto
                                  #        - use a cache to speed up row inserts
                                  #        - def: auto, i.e. on when INDEX size > ENVVAR effective_cache_size
                                  #        - for gist INDEX only
                                  #     - fastupdate:
                                  #        - def: on
                                  #        - speed up writes, but can slow queries
                                  #        - for gin INDEX only
                                  #     - alter TYPE VAR ... set (VAR = VAL...) and alter TYPE VAR ... reset (VAR...) are also available for all TYPE that support with (VAR = VAL...)
                                  #        - can also do it at the COL-level: alter table TABLE alter COL_VAR [re]set ... (same for materialized views)
                                  #  - as TABLE_VAL:
                                  #     - creates TABLE as a copy of the subquery TABLE_VAL
                                  #     - ARG... can only be COL_VAR..., and is optional
                                  #     - of CTYPE, if not exists, inherits are not allowed
                                  #     - with no data (def: with data): only copies the TYPE of TABLE_VAL, not the content

alter table TABLE
 rename CONSTRAINT to CONSTRAINT2 #

alter table TABLE
 rename COL_VAR to COL_VAR2       #

alter table TABLE add COL_ARG     #Same as the ARG in create table TABLE( ARG )

alter table TABLE drop [if exists]
 COL_VAR restrict|cascade         #

alter table TABLE alter COL_VAR
 type TYPE                        #
 [using VAL]                      #If VAL, convert VAL instead of the COL_VAL... (VAL usually perform an operation on the COL_VAL...)

alter table TABLE alter COL_VAR
 set|drop not null|default [VAL]  #

alter table TABLE
 [no] inherit TABLE2              #

alter table TABLE [not] of CTYPE  #

alter table TABLE
 add constraint CONSTRNT          #CONSTRAINT can be deffered with "not valid". Will be checked when doing:
 ... [not valid]                  #  alter table TABLE validate constraint CONSTRAINT

alter table TABLE drop constraint
 [if exists]
 CONSTRAINT cascade|restrict      #

create [or replace]               #Store the TABLE_VAL result of COMMAND (select ...) in a VIEW.
[temp] [recursive]                #  - COL_VAR...: rename (and not select) COL...
view VIEW[(COL_VAR...)]           #  - temp: like for TABLE (def if TABLE is temp)
[with(security_barrier)]          #  - recursive: like with (...)
as COMMAND                        #Is readonly in following cases of COMMAND:
                                  #  - join TABLE_VAL...
                                  #  - top-level with, distinct, group by, limit|offset
                                  #  - union|intersect|except
                                  #  - COL_VAL not being simple COL_VAR
                                  #  - with (security_barrier)
                                  #To make it read|write in such cases, use TFUNC or RULE.
                                  #Goal is encapsulation, with triggers to:
                                  #  - manipulate underlying more complicated tables while maintaining simple unique interface
                                  #  - restrict user permission (privileges on VIEW are not also on underlying TABLE):
                                  #    - per column
                                  #    - per row with where BOOL. Caution: hidden rows from TABLE in VIEW are securely hidden only if
                                  #      using create view VIEW with (security_barrier)
                                  #      Cover channel attacks are still possible to infer size of hidden rows for example
                                  #      (from explain query plans, time of queries, etc.).
                                  #  - include metadata (timestamp, etc.)
alter view VIEW
alter COL_VAR
drop|set default [VAL]            #Can have different default than the underlying TABLE, when writing default values (not reading)

create materialized               #Just like create table as ..., but must be manually updated with:
view TABLE[(COLVAR...)]           #  refresh materialized view TABLE [with [no] data]
[with (VAR = VAL...)]             #Does denormalization: manipulated like a VIEW but faster (doesn't query COMMAND, but write it).
[tablespace TABLESPACE]           #Others:
as COMMAND                        #  - COL_VAR: like create view
[with [no] data]                  #  - with [no] data: controls whether the VIEW should forbid querying (def: with data (no forbid))
                                  #  - with (VAR = VAL...): like create table
alter materialized view
TABLE rename
COL_VAR to COL_VAR2               #

with [recursive] TABLE            #Makes it possible to refer to the TABLE returned by GETCOMMAND inside COMMAND2 or inside further TABLE as (GETCOMMAND).
[(COL_VAR...)]                    #Is a prettier shortcut that avoids nesting subqueries (CTE, "Common Table Expressions"), similar to assigning subqueries to variables
as (GETCOMMAND),...               #GETCOMMAND is done before COMMAND2, and is executed only once, even if COMMAND2 references it several times.
COMMAND2                          #If GETCOMMAND is not a select, COMMAND2 needs to be top-level (not a subquery), but subqueries can see TABLE from superqueries anyway.
                                  #COL_VAR... define new column name for the return TABLE.
                                  #If recursive, TABLE can be used inside GETCOMMAND:
                                  #  - usually this is a:
                                  #      select VAL union select ... from TABLE ... where BOOL (recursion stops when BOOL is false)
                                  #  - it usually also use (COL_VAR...) so the calling recursion can select the columns of the last recursion (which is just select VAL).
                                  #  - only select ... will be possible (with possible union, etc.) in GETCOMMAND

create [temp] sequence            #Creates a SEQUENCE, which is a special TABLE-like (can be used as TABLE_VAL) storing a streaming BIGINT algorithm
SEQUENCE                          #Columns are:
[increment by BIGINT]             #  - sequence_name STR
[min|maxvalue BGNT]               #  - is_called BOOL (def: false): whether setval() or nextval() have been called
[start BIGINT]                    #  - last_value BIGINT (def: 1): current value
[cache BIGINT] [cycle]            #  - start_value BIGINT (def: 1): first value (if not is_called)
[owned by TABL.COL]               #  - increment_by BIGINT (def: 1): how much to increment after each nextval()
                                  #  - min|max_value BIGINT (def: 1 to bigintmax)
                                  #  - cache_value (def: 1):
                                  #     - how many numbers to cache in advance
                                  #     - if more than 1 and concurrency, still works, but cache reserve the numbers, which produces "holes" (but numbers are still unique)
                                  #  - is_cycled BOOL (def: false): when reaching end, either go to start, or produce error
                                  #If temp, session-only, and can't use any SCHEMA.
                                  #If owned by TABLE.COL, if COL is dropped, SEQUENCE is dropped. Must be in same SCHEMA and same owner.
alter sequence SEQUENCE           #Same ... as create sequence ... but:
...                               #  - can use no min|maxvalue, owned by none, no cycle
                                  #  - restart BIGINT makes the cycle start again too

nextval(SEQUENCE_STR)             #Go to next value, then returns current value. To keep case-sensitivity, write '"SEQUENCE"'
currval(SEQUENCE_STR)             #Returns current value
lastval()                         #Returns current value of the last SEQUENCE called.
setval(SEQUENCE_STR,
BIGINT[, BOOL])                   #Change last_value and is_called (def: true)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              DML              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select [distinct                  #Returns COL_VAL... of TABLE_VAL, with records filtered by BOOL
[on (COL_VAL...)]]                #VAL can be [TABLE.]* to mean all columns (bad practice in production)
VAL [[as] COL_ALIAS]...           #ALIAS:
[from TABLE_VAL]                  #  - goal:
[[ as] TABLE_ALIAS]...            #     - shorter version
[where BOOL]                      #     - can give name to temporary VAL (e.g. AFUNC(COL) or COL + VAL)
[order by COL_VAL...              #     - necessary for subqueries
[asc|desc|using OP]               #  - can be:
[nulls first|last]]               #     - TABLE_ALIAS: TABLE_ALIAS[( COL_ALIAS... )]
[group by COL_VAL                 #     - COL_ALIAS
[having BOOL]]                    #  - ALIAS is the new name (former VAR will not be usable anymore in current query)
[offset UINT]                     #  - "as" is recommended
[fetch first UINT2 rows           #VAL... can be:
only]                             #  - COL_VAL
                                  #  - any VAL (e.g. select NUM;)
                                  #  - FUNC()
                                  #from TABLE_VAL:
                                  #  - can use ... join (see below)
                                  #  - can use a FUNC() returning a TABLE_VAL.
                                  #    In standard SQL, from FUNC(...) -> from lateral (select FUNC(...)) ALIAS
                                  #  - can be ommitted, e.g. select VAL;
                                  #distinct:
                                  #  - remove rows will all entries duplicate.
                                  #  - on (COL_VAL...): only targets COL_VAL..., which must be among the leftmost COL_VAL... in order by ...
                                  #where BOOL:
                                  #  - filter rows according to BOOL
                                  #group by COL_VAL:
                                  #  - all VAL... in select ... (except COL_VAR if group by COL_VAR) must be scalar values, typically through a AFUNC()
                                  #  - can also be NUM like order by
                                  #  - only non-duplicate entries of COL_VAL are selected, and COL_VAL2 are aggregated according to groups of duplicate COL_VAL, using AFUNC() if provided.
                                  #having BOOL:
                                  #  - like where BOOL, but on the groups created by group by
                                  #  - must use scalar values, typically though a AFUNC()
                                  #order by:
                                  #  - sort according to COL_VAL... (def: asc, and nulls first for desc, nulls last for asc)
                                  #  - using OP: using < is like asc, etc.
                                  #  - can also be NUM, to be the VAL... number NUM
                                  #offset and fetch:
                                  #  - select only the first UINT2 rows, or according to an offset UINT
                                  #  - order by should also be used to be consistent
                                  #Order of execution:
                                  #  - select 6 from 2, (1) where 3 group by 4 having 5 order by 7 offset|fetch 8 (1 is a subquery)

TABLE_VAL [QUAL] join             #Produces a TABLE_VAL from two TABLE_VAL, for a select ... from TABLE2_VAL:
TABLE2_VAL                        #  - can use all TABLE_VAL in the other clauses. Should qualify all COL by using TABLE_VAL.COL
[on BOOL|                         #Conditions are impossible for cross join, necessary for others:
using(COL_VAR...)]                #  - on BOOL: filter according to BOOL
                                  #  - using(COL...) and natural: avoid them (can do same with on BOOL)
                                  #QUAL can be :
                                  #  - cross:
                                  #     - uses the cartesian product of the TABLE_VAL...:
                                  #        - all combinations of values, starting from first TABLE
                                  #        - i.e. a TABLE_VAL with length = product of lengths of all TABLE_VAL
                                  #     - can also be written TABLE_VAL, TABLE2_VAL
                                  #  - inner (def): same (but can use on BOOL)
                                  #  - left:
                                  #     - same but instead of filtering, fill TABLE2_VAL with null, and remove duplicate rows that have been filled this way.
                                  #     - i.e. every TABLE_VAL value will be present, with unfiltered TABLE2_VAL combinations, null if none
                                  #  - right: same but fill with null in TABLE_VAL
                                  #  - full: left outer + right outer
SELF JOIN ==>                     #Are used to output twice the same column.
                                  #Must use ALIAS to produce virtual copies of the same TABLE.
                                  #E.g.
                                  #  - replacing TABLE.COL and TABLE.COL2 (foreign keys refering to TABLE2.COL), with TABLE2.COL2
                                  #      select ALIAS.COL2, ALIAS2.COL2 from TABLE, TABLE2 ALIAS, TABLE2 ALIAS2 where TABLE.COL = ALIAS.COL and TABLE.COL2 = ALIAS2.COL

values (VAL...)...
order by ...                      #Produce a TABLE_VAL with constant VAL...
offset ... fetch ...              #Arguments: see select

(select ...)                      #Subquery, returning a TABLE_VAL. If in select VAL, must have a TABLE_ALIAS[( COL_ALIAS... )]
(values ...)                      #() are needed only when there might be confusion. Otherwise useless.
                                  #TABLE (and TABLE_ALIAS) are visible from superqueries to subqueries (but not inverse):
                                  #  - in a from ... clause:
                                  #     - only if preceded by lateral
                                  #        - ex: select * from TABLE, lateral (select TABLE.COL, TABLE2.COL from TABLE2)
                                  #     - for a FUNC(TABLE), must be after TABLE in the from ... clause
                                  #        - ex: select * from TABLE, FUNC(TABLE)
                                  #  - otherwise: makes a crossjoin ("correlated subquery")

select ...
union|intersect|except            #Both select ... must have same COL... with same types.
[all] select ...                  #If not "all", removes duplicates afterwards.
[order by ...]                    #order by must be a NUM or a COL_VAR, not a COL_VAL

insert into TABLE                 #Adds TABLE_VAL records to COL... of TABLE
(COL_VAR...)                      #Omitted COL get default value
TABLE_VAL|default values          #If TABLE_VAL is values ... (often the case), can use default as VAL.

update TABLE                      #Assign VAL... to COL... records (filtered by BOOL)
[[as] TABLE_ALIAS]                #To reference other TABLE, either:
set ... [from ...]                #  - use with ...
[where BOOL]                      #  - use from ... (same as select ... from ..., makes a cross join)  (try to avoid it)
                                  #  - use a subquery in BOOL, e.g. where COL in (select ... from TABLE2 where BOOL2)
                                  #set ... can be either:
                                  #  - set COL_VAR = VAL ...
                                  #  - set ( COL_VAR... ) = ( VAL... ) ...
                                  #VAL can be default
                                  #TABLE is implicit in the query. TABLE.COL should not be used, but COL.

delete from TABLE
[[as] TABLE_ALIAS]
[using ...]                       #Erases records of TABLE according to BOOL (by default all records)
[where BOOL]                      #using ...: same as from ... in update.

GETCOMMAND ==>                    #insert|update|delete can be finished by: returning ...
                                  #  - ... is same as select ..., but points to the rows inserted|updated|deleted
                                  #  - make those commands return like a select
                                  #  - cannot be used in a subquery
                                  #GETCOMMAND refers to any of them, or to select

truncate table TABLE...           #Like delete from TABLE with no BOOL (but faster and more vacuum-friendly)
[restart identity]                #  - restart identity: sequences are set to start_value
cascade|restrict                  #  - cascade: truncates TABLE2... depending on TABLE too
                                  #Is not concurrency-safe.

copy TABLE[(COLVAR...)]           #Append TABLE to a FILE_STR or from a FILE_STR, for COL...
from|to ...                       #Copy TABLE_VAL to is also possible
[with OPTIONS...]                 #VIEW is only possible through copy (select * from VIEW)
                                  #... can be:
                                  #  - FILE_STR (must use absolute path)
                                  #  - stdin|stdout
                                  #  - program STR, which use input|output piping to shell command STR
                                  #OPTIONS are space-separated and can be:
                                  #  - text(def)|csv|binary
                                  #  - oids: copies OIDS too
                                  #  - delimiter STR (def in text '\t', in csv ',')
                                  #  - null STR (def in text '\N', in csv ''): STR used to represent null
                                  #  - header (only in csv): first CSV row === field names
                                  #  - quote STR (def '"')
                                  #  - escape STR (only in csv, def: '\')
                                  #  - force quote( COL_VAR... )|* (only in csv, and copy to): quote all values, except null
                                  #  - force not null( COL_VAR... ) (only in csv, and copy from)
                                  #  - encoding STR (def: client_encoding)
                                  #  - freeze: force a vacuum freeze first
                                  #For portability, ENVVAR DateStyle should be 'ISO' for copy to, and IntervalStyle should not be 'sql_standard'


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           FUNCTIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


AGGREGATE FUNCTION ==>            #Transform a COL_VAL to a scalar VAL.
                                  #If AFUNC(VAL), VAL is converted to COL_VAL.
                                  #WFUNC are performed after AFUNC, and AFUNC after all the later clauses (e.g. BOOL).
                                  #To include a former in a later, use a command substitution, e.g. instead of AFUNC(COL_VAL):
                                  #  (select AFUNC(COL_VAL) [from COL_VAL])
AFUNC([distinct] ...              #For all AFUNC. Same semantic as select ...
[order by ...])                   #Nulls are ignored
sum(COL_VAL)
count(COL_VAL)                    #Can do count(*)
avg(COL_VAL)
min|max(COL_VAL)
bit_and|or(COL_VAL)               #Bitwise and|or on all values
bool_and|or(COL_VAL)              #Boolean and|or on all values
exists(COL_VAL)                   #Returns false if COL has length 0
array|json_agg(COL_VAL)           #Returns as an ARR or JSON array.
string_agg(COL_VAL, STR)          #Returns as a STR delimited by STR

corr(COL_VAL, COL_VAL2)           #r
regr_r2(COL_VAL,
COL_VAL2)                         #r₂
stddev|var|covar_
pop_samp(COL_VAL,
COL_VAL2)                         #Standard deviation, variance, covariance, / (n-1) or /n
regr_count(COL_VAL,
COL_VAL2)                         #Exclude row with one null
regr_avgx|y(COL_VAL,
COL_VAL2)                         #sum(VAL[2]/regr_count(VAL, VAL2))
regr_intercept|slope
([TABL]_VAL,[TABL_]VAL2)          #
regr_sxx|syy|sxy                  #In order:
([TABL]_VAL,[TABL_]VAL2)          #  - sxx: sum(VAL^2) - sum(VAL)^2/regr_count(VAL,VAL2)
                                  #  - syy: sum(VAL2^2) - sum(VAL2)^2/regr_count(VAL,VAL2)
                                  #  - sxy: sum(VAL*VAL2) - sum(VAL)*sum(VAL2)/regr_count(VAL,VAL2)

AFUNC (COL_VAL) over              #Window function WFUNC:
([partition by COL_VAR]           #  - the whole expression returns a COL of length n (not 1)
 [order by ...]                   #  - for each element, AFUNC() is fired on a moving WINDOW, for each COL group. Groups are defined as:
 [rows|range between              #     - if "partition by": according to identical VAL in COL_VAR
 WORD and WORD2])                 #     - if no "partition by":
                                  #        - if "order by": same, but each group also include previous groups
                                  #        - if no "order by": only one group
                                  #  - WINDOW is:
                                  #     - inside each group, between WORD and WORD2, which can be:
                                  #        - unbounded|VAL preceding|following
                                  #        - current row: according to current element
                                  #     - def is between unbounded preceding and unbounded following, which means AFUNC() is applied on
                                  #       the whole group each time
                                  #     - if there is a group of identical VAL..., current row means:
                                  #        - if range: for each VAL, the first one for WORD, and the last one for WORD2
                                  #        - if rows: for each VAL, that VAL
                                  #  - the output is sorted by COL2, inside each group
AFUNC([COL_VAL]) over
WINDOW,
AFUNC2([TABLE2_VL]) over          #If multiple AFUNCs in a statement use the same WINDOW, it can use an alias WINDOW, then put a later
WINDOW ...                        #clause, as: window WINDOW as ( ... )

WFUNC-SPECIFIC AFUNCs
 ==>                              #
row_number()                      #Index inside WINDOW
cume_dist()                       #Row number inside WINDOW, divided by number of rows for the group
first|last_value(COL_VR)          #Returns first|last element of the WINDOW
nth_value(COL_VAR,UINT)           #Returns element number UINT of the WINDOW (null if not existing)
lag|lead(COL_VAR,UINT
[, VAL])                          #Returns the element UINT times before|after in the WINDOW (VAL (def: null) if not existing)
rank()                            #Numeral rank, according to "order by", inside WINDOW
dense_rank()                      #Same but if equal value, equal rank too
percent_rank()                    #Same but from 0 to 1
ntile(UINT)                       #Returns 1..UINT, starting from lower to upper value, with equal (if possible) number of values in
                                  #each 1..UINT (buckets)

create aggregate AFUNC            #Creates an AFUNC(COL_VAL...):
(TYPE...) (sfunc =FUNC,           #  - TYPE are type of each COL.
stype = TYPE2                     #  - each VAL... of each COL is passed inside successive FUNC(VAL2, VAL...)
[, finalfunc = FUNC2]             #     - VAL2 is the return value of the last FUNC(), and its type is TYPE2
[, initcond = STR]                #     - its value in the first FUNC() is TYPE2 STR (typecasting from STR) (def: null)
)                                 #  - last FUNC can be different if FUNC2(VAL2) is defined: takes only one ARG, and can return any type.
                                  #  - last VAL2 is returned
                                  #  - by convention, null should be ignored in FUNC, unless there are only null, where null should be
                                  #    returned

create [or replace]               #Creates a user-defined function, that can used anywhere.
function FUNC( [[VAR]             #  - or replace: if already existing, will be overwritten (fails if different types)
TYPE[ default VAL]...])           #  - arguments:
returns TYPE2                     #    - VAL is the default value
as STR_LIT                        #    - TYPE can be any but also:
language LANG                     #      - CTYPE|TABLE:
[window] [immutable|              #        - TABLE_VAL are converted to CTYPE
stable|volatile]                  #        - calling FUNC(TABLE_VAL) needs a reference to TABLE. Examples:
[leakproof] [strict]              #           - select FUNC(TABLE_VAL) from TABLE
[security                         #           - select * from TABLE, FUNC(TABLE_VAL)
invoker|definer]                  #        - FUNC(TABLE|CTYPE) can also be called TABLE|CTYPE.FUNC, so it is unwise to name FUNC same
[cost FLOAT]                      #          as a member of TABLE|CTYPE
[rows UINT]                       #      - record:
                                  #        - like CTYPE of any*
                                  #      - pseudo-type:
                                  #        - any[element|[non]array|enum|range]:
                                  #          - polymorphic types.
                                  #          - Must be all of the same type (args and return value), for a specific call.
                                  #          - anyarray|range must have same type as other any*
                                  #          - if return value is polymorphic, at least one arg must be too.
                                  #          - $0 is a VAR with the polymorphic type, initialized to null.
                                  #            Can be used to store return value or do computations.
                                  #        - cstring (null-terminated, for C only)
                                  #    - variadic ARG (must be ARR):
                                  #      - elements of ARR must then be of same TYPE
                                  #      - can fire it with FUNC(..., variadic ARR) or FUNC(..., ...)
                                  #      - is not optional by default, needs to add e.g. variadic VAR TYPE default array[]::TYPE[]
                                  #    - overloading is possible. If multiple choice of casting, the pg_type.typeispreferred is used.
                                  #    - can call by name: FUNC( VAR := VAL )
                                  #  - return value:
                                  #    - always returns a TABLE_VAL (TYPE actually means TABLE_TYPE_VAL)
                                  #    - returns first row only, unless setof TYPE is used
                                  #    - can also be in the function args instead of returns ...:
                                  #      - as out ARG...:
                                  #        - ARG... can be assigned inside FUNC instead of the usual way to return a value
                                  #          (in language that permits assigning to VAR, e.g. not SQL but PL/SQL)
                                  #        - if several ARG...:
                                  #          - returns as a multicolumn TABLE_VAL in a from clause
                                  #          - otherwise, returns as one CTYPE_COL_VAL
                                  #        - to return more than first row, put also returns setof TYPE (record if several ARG...)
                                  #      - inout ARG is same as ARG out ARG (both ARG and return value)
                                  #    - TYPE2 is like TYPE, but can also be:
                                  #      - void: if no return value
                                  #      - CTYPE|TABLE or table( COL TYPE ... ): like specifying several out ARG...
                                  #      - record: FUNC must be called with FUNC(...) as [ALIAS](COL_VAR TYPE ...) to precise types of
                                  #        RECORD
                                  #  - VAR%type: TYPE of a VAR, including TABLE.COL. Can be a VAR from the FUNC too
                                  #    (but not for an ARG from another ARG)
                                  #  - body:
                                  #     - STR. Better to use $$...$$ notation
                                  #  - can be written in different LANG (see below):
                                  #     - SQL
                                  #     - pl*: procedural languages, faster than SQL functions, among:
                                  #       - already in PostgreSQL (just need create extension pl*)
                                  #         - plpgsql
                                  #         - plpython
                                  #         - plperl
                                  #         - pltcl
                                  #     - C, faster than pl*
                                  #     - internal:
                                  #       - builtins functions. Can only refer to them by putting the C function name in the body STR
                                  #       - possible (only) use: renaming
                                  #       - actually C functions, but compiled in (not loaded with a shared library)
                                  #  - window:
                                  #     - means it is a WFUNC (only in C and some PL/*)
                                  #       As such, it passes some extra arguments about the current window.
                                  #  - immutable|stable|volatile: used for performance optimization. Can be:
                                  #     - immutable: doesn't read|write global state (including the database or the pg_catalog.*)
                                  #     - stable: doesn't write global state
                                  #     - volatile (def): write|read global state
                                  #  - leakproof: used for performance and security optimization. Means it is immutable, and
                                  #    doesn't give informations about arguments VAL aside from return value (e.g. does not throw errors
                                  #    for some return values but not others, prints arguments VAL, etc.)
                                  #  - strict:
                                  #     - returns automatically null if an arg is null.
                                  #     - for VARIADIC arg, only works if whole VARIADIC is null, not only part of it.
                                  #  - privilege are the ones of:
                                  #      - security invoker (def): the caller
                                  #      - security definer: the user creating the FUNC. Forbids using set role.
                                  #        Should also define function-specific ENVVAR search_path by adding a temporary SCHEMA at the
                                  #        end (to avoid malicious VAR shadowing)
                                  #  - cost FLOAT: CPU cost for the planner (cpu_operation_cost) (def: 1 for C FUNC, 100 for others)
                                  #  - rows UINT: average number of rows returned, for the planner (def: 1000). Only if return value
                                  #    returns several rows.
alter function FUNC(...)
...                               #... is any of the options after language LANG. Can use not leakproof.


do [language LANGUAGE]
STR                               #Execute an anonymous function STR (body of function), from LANGUAGE (def: plpgsql)

create [constraint]               #Execute a FUNC for a specific EVENT on TABLE or VIEW.
trigger TFUNC                     #Multiple TFUNC are fired alphabetically.
before|after|instead              #  - EVENT can be insert|delete|truncate, or update [of COL_VAR...]
of EVENT [or EVENT2...]           #    COL_VAR... means COL_VAR or COL_VAR2, etc.
on TBL|VIEW [from TABL2]          #    Truncate only on before|after and on TABLE.
[not deferrable]                  #  - before|after|instead of is for the constraint checking + the operation itself (row-wise).
[initially immediate|             #    So before fired even if constraint fails, but not after.
deferred] [for each row]          #    If COMMAND is the one triggering EVENT "for each row":
[when ( BOOL )]                   #       - before|instead of are executed row-wise according to COMMAND (so next rows in TFUNC see
execute procedure                 #         changes of previous rows by COMMAND)
FUNC(STR...)                      #       - after is executed table-wise according to COMMAND (so rows in TFUNC see changes of all rows
                                  #         by COMMAND)
                                  #    before|instead of are executed on all rows
                                  #    instead of can only be on VIEW and cannot use when BOOL. Usually used to modify the underlying
                                  #    TABLE so users can modify VIEW.
                                  #    Can also use or EVENT2...
                                  #  - FUNC: any language (e.g. PL/* or C) but not SQL. See doc about TFUNC for those languages.
                                  #    Must take no arguments (arguments are passed with special variables) and with a trigger return
                                  #    type, but actually returning:
                                  #     - "before" + "for each row":
                                  #        - null: skip further TFUNC and cancel statement (for that row)
                                  #        - CTYPE|TABLE with same structure as new|old (including them):
                                  #          - modified row: modify new and return it.
                                  #          - unmodified row: return old|new according to the operation
                                  #     - otherwise: return null
                                  #  - for each row: fired for each manipulated row (not fired if no manipulated row), and not once for
                                  #    all rows.
                                  #    Necessary for constraint trigger and instead of.
                                  #    Impossible on non-truncate VIEW.
                                  #  - constraint: makes it possible to:
                                  #     - change not deferrable, etc. with set constraints TFUNC deferred|immediate (def: immediate)
                                  #     - use not deferrable, etc. (same as for CONSTRAINT).
                                  #    Must be "after" and "for each row".
                                  #    Can be used to simulate a constraint, in which case an exception should be raised.
                                  #    Can also use set constraints to fire trigger at specific point during current transaction.
                                  #  - when ( BOOL ):
                                  #     - can use old|new like trigger functions
                                  #     - BOOL cannot be a subquery.
                                  #A TFUNC can fire another one ("cascading triggers"), including recursively.
                                  #Can see current TFUNC depth with pg_trigger_depth()
                                 ##To create a TFUNC notifying of each modification:
                                 ##  - triggered_change_notification([STR]) is a function to use to do notify STR, for each row,
                                 ##    after insert and/or update and/or delete, with payload explaining the modification.
                                 ##  - Postgres extension 'tcn'
create event trigger              #Like TFUNC but EVENT can be DDL not DML:
EFUNC on EVENT                    #  - ddl_command_start|end: before|after a create, alter or drop (except for cluster-wide objects
[when tag in (STR...)]            #    and TFUNC|EFUNC)
execute procedure FUNC()          #  - sql_drop: same but only for drop
                                  #     - FUNC() can use pg_event_trigger_dropped_objects() which returns a TABLE_VAL with one row for
                                  #       each dropped objects and COL...:
                                  #        - classid: DATABASE OID
                                  #        - objid: object OID
                                  #        - objsubid: e.g. for columns the attnum
                                  #        - object_type STR
                                  #        - schema_name STR
                                  #        - object_name STR
                                  #        - object_identity STR: SCHEMA.OBJECT name
                                  #STR... are commands (e.g. 'drop table') to filter EFUNC.
                                  #FUNC must return have an event_trigger return type, and takes no arguments.
                                  #Must be superuser.
alter event trigger
disable|enable
[replica|always]                  #
alter table TABLE                 #all|user:
enable|disable                    #  - not for RULE
[replica|always]                  #  - same but all include system TRIGGER (enforcing CONSTRAINT), not user
rule|trigger                      #replica|always:
RULE|TRIGGER|all|user             #  - def: affects only non-replication sessions (ENVVAR session_replication_role "origin" (def) or
                                  #    "local")
                                  #  - replica: affects only replication sessions (session_replication_role "replica")
                                  #  - always: affects all sessions

create [or replace]               #Creates a macro that modifies a command matching EVENT into COMMANDS (add or replace according to
rule RULE as on EVENT             #"instead"), for TABLE, when BOOL:
to TABLE [where BOOL]             #  - EVENT:
do [instead] COMMANDS             #     - select|insert|update|delete
                                  #     - select must be "instead", non-"where BOOL" and with select COMMAND
                                  #       (similar to create a VIEW)
                                  #  - BOOL:
                                  #     - can refer to old|new.COL (like triggers)
                                  #COMMANDS:
                                  #  - nothing: with "instead", original EVENT is dropped
                                  #  - COMMAND or ( COMMAND;...):
                                  #     - Must be select|insert|update|delete|notify.
                                  #     - For insert, the original EVENT is performed before COMMAND, for others after.
                                  #     - "instead" non-"where BOOL" COMMAND insert|update|delete must be GETCOMMAND
                                  #       so that calling command can use returning itself.
                                  #When to use rules:
                                  #  - VIEW are better than RULE with select EVENT.
                                  #  - rules are similar to TFUNC, often faster but harder and less flexible.
                                  #    Prefer triggers unless performance is critical, in which case check if actually faster.

create operator OP                #Creates an OP, i.e. a FUNC using a sign. Can have one or two arguments.
( left|rightarg = TYPE,           #Can be an already existing OP, with different TYPE (overloading)
procedure = FUNC                  #OP is [+-*/<>=~!##%^&|`?]+
[, commutator = OP2]              #Other args are for optimization (hints the planner in rewriting queries so they match an INDEX):
[, negator = OP3]                 #  - OP2 if VAL OP VAL2 is equivalent to VAL2 OP2 VAL
[, restrict = FUNC2]              #  - OP3 if VAL OP VAL2 is equivalent to not VAL2 OP3 VAL, or OP VAL to not OP3 VAL
[, join = FUNC3]                  #  - FUNC2 tells when using VAL OP VAL2_LIT (returning BOOL), how much portion of the COL is likely to
[hashes]                          #    be chosen.
                                  #    Can choose among default ones:
                                  #      - eqsel: chooses a small portion, like =
                                  #      - neqsel: large portion, like <>
                                  #      - scalar[l|g]tsel: semi-large portion before or after, like < <= or > >=
                                  #  - FUNC3 is like FUNC2 but for VAL OP VAL2. Default ones:
                                  #      - eqjoinsel: like =
                                  #      - neqjoinsel: like <>
                                  #      - scalar[l|g]tjoinsel: like < <= or > >=
                                  #      - areajoinsel: like BOX operators
                                  #      - positionjoinsel: like POINT operators
                                  #      - contjoinsel: like <@ @>
                                  #  - hashes: VAL OP VAL2 is true if hash(VAL) = hash(VAL2)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      FUNCTIONS LANGUAGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SQL FUNCTIONS ==>                 #  - execute SQL statements, returns the last one:
                                  #    - a GETCOMMAND
                                  #    - or returns void
                                  #  - either VAR or $NUM is used to refer to ARG...
                                  #    - if VAR is a COL in the current statement, use TABLE.VAR (for the COL) or FUNC.VAR (for the VAR)
                                  #    - are readonly
                                  #  - some forbidden COMMAND: transactions blocks, vacuum, etc.

PL/PGSQL ==>                      #PL/PGSQL ends with ; but structure have only one ; at end of the structure.
                                  #Arguments:
                                  #  - if VAR is a COL in the current statement, use TABLE.VAR (for the COL), LABEL.VAR (for a VAR
                                  #    declared in a block LABEL) or FUNC.VAR (for an argument)
                                  #TABLE (including COLs) must have same types accross executions because of caching. Solutions:
                                  #  - using RECORD instead of CTYPE|TABLE
                                  #  - using execute STR (doesn't cache)
GETCOMMAND ==>                    #In PL/PGSQL, also implies execute GETCOMMAND

<<LABEL>>                         #Scope block. Main function must be in a scope block. Variables are local to the block.
[declare                          #VAR must all be declared, as VAR [constant] TYPE [not null] [default VAL];...
  DECLARATIONS...]                #exception ... catches exceptions:
begin                             #  - EXCEPTION is either:
  STATEMENTS...                   #     - exception word (see appendix online or error messages) or "others" meaning any other exception.
[exception                        #     - SQLSTATE, i.e. an error code NUM typecasted to sqlstate: sqlstate '...'
  when EXCEPTIN then ...          #  - can use the local variables when the exception happened.
  ...]                            #  - can use special variables sqlstate (current SQLSTATE) and sqlerrm (error message STR)
end [LABEL]                       #  - get stacked diagnostics VAR = WORD2 ... assigns to VAR2 according to WORD2 among:
                                  #     - returned_sqlstate: like sqlstate
                                  #     - message_text: like sqlerrm
                                  #     - column|constaint|pg_datatype|table|schema_name: column|...|schema raising the exception
                                  #     - pg_exception_detail|hint: extra error messages
                                  #     - pg_exception_context: call stack
                                  #Is also a transaction block, with exceptions rollbacking the transaction.
raise [LEVEL]                     #Raises an exception.
[using VAR = STR ...]             #LEVEL can be exception (def), warning, notice, info, log or debug. Exception throw an exception, but
                                  #others don't, they just print a message.
                                  #VAR = STR provide informations among:
                                  #  - errcode: like EXCEPTION in exception block, but SQLSTATE is as STR
                                  #  - message: like sqlerrm
                                  #  - detail, hint, column|...|schema: like in get stacked diagnostics
                                  #If no using..., rethrow a currently catched exception.
raise [LEVEL] EXCEPTION
 ...                              #Like raise [LEVEL] using errcode = EXCEPTION ...
raise [LEVEL] STR_LIT,
VAL...  ...                       #Like raise [LEVEL] using message = STR ..., where STR can substitute % symbols with VAL...

return [VAL]                      #Return statement of the function.
                                  #VAL can be omitted:
                                  #  - if out ARG... are used
                                  #  - if void if the return type
return next [VAL]                 #With setof ... return type (multiple rows), add VAL (CTYPE for multiple columns) or GETCOMMAND to
return query GETCOMMAND           #the returned TABLE_VAL. Needs to be called several time, then a return; will return the whole set.
                                  #If VAL is ommitted and out ARG... are used, use the current value of those ARG... instead.

VAR := VAL                        #Assignment
SQL_COMMAND                       #Any SQL command can be performed but:
                                  #  - select ... (except select ... into) (including select VAL or select FUNC()) must be written
                                  #    perform ..., and has no return value
                                  #    - written perform (...) for a with ... query
null;                             #To do nothing (empty line), write null;
GETCOMMAND                        #Put the first row returned into VAR, which can be:
into [strict] VAR                 #  - VAR... for each column
                                  #  - CTYPE|TABLE or record for all columns
                                  #If more than one row is returned (and, if select, strict is used), an exception TOO_MANY_ROWS is
                                  #fired. If no row returned and strict, an exception NO_DATA_FOUND if fired.
                                  #select ... into is PL/PGSQL, different than the SQL select ... into

execute STR                       #Execute SQL (not PL/PGSQL) command STR.
[using VAL...]                    #STR can contain $NUM that will substituted by each VAL...:
                                  #  - must be used for all VAR coming from the FUNC ARG...
                                  #  - can only be used in select|insert|update|delete
                                  #  - cannot be used by TABLE_VAR and COL_VAR: they must be supplied as a STR concatenation
                                  #STR should be escaped:
                                  #  - $$...$$ for the command
                                  #  - use quote_* for dynamic variables

found                             #Variable containing true if last SQL command returned|manipulated at least one row.
get diagnostics                   #WORD can be either:
VAR = WORD                        #  - row_count (assign to VAR the number of rows returned|manipulated by last SQL command)
                                  #  - result_oid: OID of last row manipulated (if table has OIDs)

if BOOL then ...
[elseif BOOL then ...]
[else ...] end if;

case ... end case;                #Like SQL case ... end

[<<LABEL>>]
[while BOOL] loop ...
end loop [LABEL]                  #BOOL is true by def.

[<<LABEL>>]
for INT_VAR in [reverse]
NUM..NUM2 [by NUM3] loop
... end loop [LABEL]              #

[<<LABEL>>]                       #Iterates over rows returned by GETCOMMAND
for VAR in GETCOMMAND             #VAR can be:
loop ...                          #  - VAR... for each column
end loop [LABEL]                  #  - CTYPE|TABLE or record for all columns

[<<LABEL>>]                       #Looping through a VAR2:
foreach VAR [slice NUM]           #  - ARR
in array VAR2 loop ...            #  - CTYPE: VAR is then VAR... for each value
end loop [LABEL]                  #If NUM > 0, VAR receives an ARR2 of dimension NUM each time (for multidimensional VAR2)

exit [LABEL] [when BOOL]          #Exit a loop or block with LABEL (def: innermost one), if BOOL (def: true)
continue ...                      #Same as exit ... but skip next statements to start a new cycle.

refcursor                         #Special PL/PGSQL type to store a CURSOR.
                                  #Similar as SQL CURSOR (move and close identical). Differences are below.
                                  #Can be declared as:
                                  #  - CURSOR [scroll] cursor [(ARGS...)] for select ...:
                                  #    - like a SQL CURSOR, but:
                                  #      - not opened, needs to do open CURSOR;
                                  #      - if ARGS, can pass arguments to open CURSOR(...);
                                  #  - CURSOR refcursor:
                                  #    - same but needs to specify the query with open, either with open CURSOR [scroll] for select ...
                                  #      or open CURSOR [scroll] for execute ...
                                  #Can be returned or passed as argument:
                                  #  - pass the CURSOR as STR, casted as refcursor, e.g. FUNC(refcursor 'CURSOR')
                                  #  - to use a CURSOR in the calling FUNC, either:
                                  #     - use the names of the CURSOR.
                                  #       No need to return them, they close at end of transaction of calling function.
                                  #       If CURSOR not provided as argument, will generate a random name
                                  #     - return them, and use the return value
fetch ... into VAR                #Just like select ... into VAR (including: cannot fetch several rows)
[<<LABEL>>]
for RECORD
in REFCURSOR[(...)]
loop ... end loop [LABL]          #Loops in a REFCURSOR. Implicitely open and close the REFCURSOR

TFUNC FOR PL/PGSQL ==>            #Return type is trigger, but must return either:
                                  #Arguments are in tg_argv STR_ARR (index starts at 0 and is of size tg_nargs INT).
                                  #Can also use:
                                  #  - new|old RECORD: current row for "for each row" TFUNC (only if "for each row")
                                  #    new is absent if delete, and old is absent if insert.
                                  #  - tg_name STR: TFUNC name
                                  #  - tg_when STR: before|after|instead of
                                  #  - tg_level STR: statement|row ("for each row")
                                  #  - tg_op STR: insert|update|delete|truncate
                                  #  - tg_relid OID: of the TABLE
                                  #  - ts_table_name STR: name of the TABLE
EFUNC FOR PL/PGSQL ==>            #Return type is event_trigger, but doesn't use return.
                                  #Can also use:
                                  #  - tg_event STR: the EVENT
                                  #  - tg_tag STR: the COMMAND

PL/SH ==>                         #plsh, for any installed shell, including Bash:
                                  #  - Impossible: CTYPE, any*, out ARG, setof
                                  #  - Body: starts with correct shabang.
                                  #  - Arguments: same as the shell (for Bash: $1, "$@", $#, etc.)
                                  #  - Return value:
                                  #    - stdout, with a newline stripped
                                  #    - null if exit (exit code 0) with nothing on stdout
                                  #  - Exception:
                                  #     - printing to stderr
                                  #     - non-0 exit code
                                  #  - SQL commands: fired through psql STR command line.
                                  #  - Can use any executable, as postgres user.
                                  #  - TFUNC and EFUNC:
                                  #    - defines PLSH_TG_* like tg_* in PL/PGSQL (old|new unavailable)
                                  #    - TFUNC() est executee, mais ne modifie jamais le row courant

PL/R ==>                          #plr: for R:
                                  #  - Arguments: named arguments.
                                  #  - Return value: return(VAL)
                                  #  - Types (other than obvious):
                                  #    - dimensions (max 3):
                                  #      - 0 dimension  (TYPE)                    <-> VAL
                                  #      - 1 dimension  (setof TYPE, ARR)         <-> VALv
                                  #      - 2 dimensions (setof CTYPE, ARR(2 dim)) <-> DATA.FRAME|ARR(2dim)
                                  #        - TABLE|CTYPE as argument -> DATA.FRAME, but must use as.data.frame(DATA.FRAME) if want to
                                  #          be returned
                                  #      - 3 dimensions (ARR (3 dim))             <-> ARR(3dim)
                                  #    - null <-> NA|NULL(pref)
                                  #    - BYTEA <-> OBJECT ([un]serialize on RAW)
                                  #    - everything else <-> STR
                                  #  - TFUNC: defines pg.tg.* like tg_* in PL/PGSQL
                                  #  - WFUNC: will pass:
                                  #     - fargNUM...: other values in WINDOW (NUM starts at 1)
                                  #     - fnumrows: WINDOW size
                                  #     - prownum: offset of WINDOW
                                  #  - Global data are possible across calls, including global functions.

PL/R FUNC() ==>                   #
install_rcmd(STR)                 #Execute R code STR (e.g. a function definition).
plr_environ()                     #Returns all environment variables as TABLE_VAL with STR columns name and value.
plr_set_display(STR)              #Change the DISPLAY env variable (useful for plots)

PL/R R FUNCTIONS ==>              #
pg.spi.exec(STR)                  #Execute SQL command STR.
                                  #For select ..., returns query as DATA.FRAME (null -> NA).
                                  #For others, returns number of manipulated rows.
pg.spi.prepare                    #Like prepare in SQL
(STR, INT_ARR)                    #INT_ARR are the types oid:
                                  #  - same as in pg_type.oid
                                  #  - can use SQL FUNC() load_r_typenames() to create R variables holding types oid.
                                  #    To see those variables, use SQL FUNC() r_typenames()
                                  #  - must be NA if no arguments
                                  #Returns a PLAN.
pg.spi.execp(PLAN, LIST)          #Like pg.spi.exect(), but executing a PLAN.
                                  #LIST is unnammed:
                                  #  - must contain only NA if no argument.
                                  #  - must be NA for a NULL in SQL
pg.spi.factor(DATA.FRAM)          #Convert non-NUM columns of DATA.FRAME into FACTOR.

pg.spi.cursor(STR, PLAN           #
[, LIST])                         #Creates and returns a readonly CURSOR names STR for the command defined by PLAN and LIST.
pg.spi.fetch(CURSOR,
BOOL, INT)                        #Same as fetch forward|backward (true|false) INT in SQL
pg.spi.close(CURSOR)              #

pg.spi.lastoid()                  #If last action was an insert of a single row, returns OID of that row.
pg.thrownotice|error
(STR)                             #Like raise notice|exception in PL/PGSQL
pg.quoteliteral|ident
(STR)                             #Like quote_literal|ident in SQL

PL/R OTHERS ==>                   #
plr_modules                       #TABLE executing plr_modules.modsrc (R code as STR) at start of each session or if reload_plr_modules()
                                  #is called.
                                  #plr_modules.modseq INT is the priority/order of execution.
                                  #Needs to be created as plr_modules( modseq int4, modsrc text ). Should be readable by all, writable
                                  #only by trusted users.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         DICTIONARIES          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PRINCIPLES ==>                    #Normalize a STR document by using:
                                  #  - a REGCONF, i.e. a combination of:
                                  #     - a PARSER, that break words into tokens according to whitespaces, hyphens, etc.
                                  #       Can produce overlapping tokens (both the compound token and its parts).
                                  #       Tokens can be words, hyphen-separated token, email address, URL or part of URL (path, host,
                                  #       etc.), numbers, XML tags
                                  #     - a list of DICTIONARY, i.e. a TEMPLATE filled with arguments.
                                  #       Goal is to remove common words, change grammatical forms, etc.
                                  #  - current REGCONF is chosen with ENVVAR default_text_search_config (can be set by initdb -T STR)
                                  #    Can be returned by get_current_ts_config()
                                  #Normalized STR documents are TSVECTOR. Queries are performed using TSQUERY.

LIMITS ==>                        #  - 2KB for lexemes. Max. 2^64 by TSVECTOR. Max 256 positions by lexeme.
                                  #  - 1MB for TSVECTOR (divide a document into several TSVECTOR)
                                  #  - Max. 2^32 elements in a TSQUERY

tsvector                          #Normalized STR document.
                                  #Each word can have a position and weight, i.e. word:NUMLETTER, where NUM is the position is the
                                  #document (1 to 16383) and a weight, A, B, C to D, indicating the level (title, subtitle, etc.)
                                  #Position only serves for relative position comparison, so 16383 is fine.
tsquery                           #A STR, query combination of normalized words on a tsvector.
                                  #Words must be separated by & or | (not space)
                                  #Can include:
                                  #  - () ! & |, e.g. 'word & (word2 | word3)'
                                  #  - word:LETTER...: only words in tsvector with weight LETTER... should match
                                  #  - word:*: not whole word need to match, but only beginning
to_tsquery|vector                 #As opposed to cast( STR as tsquery|vector ), it does normalization.
( [REGCONFIG, ]STR )              #Only use the later if already normalized.
plainto_tsquery(...)              #Same as to_tsquery() but can't accept operators & | ! (), and implicitely put & between each word

TSQUERY @@ TSVECTOR               #Returns true if match.
STR @@ STR2                       #Automatically call to_tsquery|vector()
TSVECTOR || TSVECTOR2
TSQUERY && || !! TSQURY2
TSQUERY @> @< TSQUERY2            #Contains|is contained

length(TSVECTOR)                  #number of words
strip(TSVECTOR)                   #Removes weights and indexes
setweight(TSVECTOR,
'A|B|C|D' )                       #Returns TSVECTOR with given weight
numnode(TSQUERY)                  #number of words and operators
querytree(TSQUERY)                #returns TSQUERY without parts that can't be accessed (!word and common words), to detect if a TSQUERY
                                  #can be indexed
ts_rank[_cd]([REAL_ARR,]          #Computes how much TSQUERY matches TSVECTOR. Used with "order by".
TSVECTOR,TSQUERY[,UINT])          #ts_rank look at the number of occurences, and ts_rank_cd at the proximity of occurences
                                  #(smaller is better)
                                  #REAL_ARR weights occurences according to 4 REAL for the weights D, C, B, A (def: 0.1, 0.2, 0.4, 1).
                                  #UINT is a or'd flag, that computes following operations on output :
                                  #  - 0 (def): nothing
                                  #  - 1: n/(log(len(n))+1)
                                  #  - 2: n/len(n)
                                  #  - 4: n/mean between occurences (only ts_rank_cd)
                                  #  - 8: n/number of unique words
                                  #  - 16: n/(log(number of unique words)+1)
                                  #  - 32: n/(n+1)
ts_headline([REGCONF,]            #Returns a STR3 with matching results.
STR, TSQUERY[, STR2])             #To make it faster, use select ts_headline(STR, TSQUERY) from (select ...) where the subquery returns
                                  #only rows that match the TSQUERY.
                                  #STR2 have options, as 'VAR=VAL...':
                                  #  - StartSel, StopSel STR: to be put around matches (def: '<b>' and '</b>')
                                  #  - Min|MaxWords UINT: to filter matches by number of occurences
                                  #  - ShortWord UINT: length minimal of words at being and end of matches
                                  #  - HighlightAll BOOL: negates previous args
                                  #  - FragmentDelimiter STR: between each match in output
                                  #  - MaxFragments UINT: to display in output
ts_rewrite(3 TSQUERY)             #Change occurences of TSQUERY2 inside TSQUERY to TSQUERY3
ts_rewrite(TSQUERY,
TABLE_VAL)                        #Same with a TABLE_VAL with two TSQUERY columns

ts_debug([REGCONFIG, ]            #Returns result of conversion STR -> TSVECTOR, as a TABLE_VAL with COLS:
STR)                              #  - alias STR: TYPE of TOKEN
                                  #  - description STR: of TYPE of TOKEN
                                  #  - token STR: original TOKEN
                                  #  - dictionaries STR_ARR and dictionary STR (the one chosen)
                                  #  - lexemes STR_ARR: final tokens
ts_lexize(STR, STR2)              #Returns token STR2 as STR_ARR according to dictionary STR
ts_parse(STR|OID, STR2)           #Same for parser. Returns a TABLE_VAL with COLS:
                                  #  - tokid INT: token type
                                  #  - token STR
ts_token_type(STR|OID)            #Returns all token types of the parser STR|OID, as a TABLE_VAL with COLS:
                                  #  - tokid INT
                                  #  - alias STR
                                  #  - description STR
ts_stat(STR[, STR2])              #For a query STR returning a TSVECTOR, returning each word in a TABLE_VAL:
                                  #  - word STR
                                  #  - ndoc UINT: numero du TSVECTOR
                                  #  - nentry UINT: number of occurences
                                  #If STR2, only lexemes with weights STR2 will be picked.

create text search                #... can be:
configuration REGCONF             #  - parser = STR
( ... )                           #  - or copy = REGCONF2
                                  #Dictionaries are specified with alter text search configuration ...
alter text search                 #... controls DICTIONARY for specific TOKEN (as WORD) can be:
configuration REGCONF             #  - add|alter mapping for TOKEN... with DICTIONARY...
...                               #  - drop mapping [if exists] for TOKEN...
                                  #  - alter mapping [for TOKEN...] replace DICTIONARY with DICTIONARY2
                                  #Available ones:
                                  #  - pg_catalog.LANG (def: english), which uses:
                                  #     - LANG_stem DICTIONARY for words, simple DICTIONARY for rest
                                  #     - pg_catalog.default PARSER
                                  #  - pg_catalog.simple, which uses:
                                  #     - simple DICTIONARY for all
                                  #     - pg_catalog.default PARSER

create text search                #Create a DICTIONARY, which is a TEMPLATE but with args filled in ... (as VAR = VAL)
dictionary DICTIONARY             #Available ones:
( template = TEMPLATE,            #  - LANG_stem:
 ... )                            #     - snowball TEMPLATE with language = LANG and stopwords = LANG
                                  #  - simple:
                                  #     - simple TEMPLATE
                                 ## - unaccent:
                                 ##    - filtering dictionary removing accent
                                 ##    - does it according to argument rules (which can be altered), pointing to
                                 ##      SHAREDIR/tsearch_data/ARG.rules.
                                 ##      The default one can be found with unaccent.rules (works for western languages)
                                 ##    - can also use unaccent([ARG,]STR)
                                 ##    - Postgres extension 'unaccent'
                                 ## - dict_xsyn:
                                 ##    - synonym dictionary: a word is spread into several synonymous words, so that synonymous words
                                 ##      are counted together
                                 ##    - arguments (can be altered):
                                 ##       - rules, pointing to SHAREDIR/tsearch_data/ARG.rules, see example at
                                 ##         SHAREDIR/tsearch_data/xsyn_sample.rules
                                 ##       - matchorig|synonyms BOOL: the original word or synonyms are accepted as input
                                 ##         (def: true and false)
                                 ##       - keeporig|synonyms BOOL: the original word or synonyms are produced in output
                                 ##         (def: true and true)
                                 ##    - Postgres extension 'dict_xsyn'
alter text search
dictionary DICTIONARY
( ... )                           #

create text search                #Creates a TEMPLATE. Must be superuser ROLE.
template TEMPLATE                 #A TEMPLATE:
( [init = FUNC, ]                 #  - take a STR in input from the parser and can return:
 lexize = FUNC2 )                 #     - STR_ARR of the normalized words
                                  #     - empty STR_ARR if common words
                                  #     - null if word is unknown, so original STR passed to the next dictionary
                                  #     - STR, with the TSL_FILTER flag on (filtering dictionary): pass STR to the next dictionary
                                  #       dictionary
                                  #       Ex of filtering dictionary: removing accents
                                  #  - usually use FILES:
                                  #     - can be found under SHAREDIR/tsearch_data/FILE.extension (extension depends on TEMPLATE)
                                  #     - FILE must be in UTF-8
                                  #  - use arguments that are filled by DICTIONARY
                                  #Available TEMPLATE:
                                  #  - simple:
                                  #     - arguments: accept BOOL (def: true), stopwords FILE
                                  #     - FILE (extension: .stop) is one word by line
                                  #     - if input:
                                  #        - is found in FILE, returns empty STR_ARR
                                  #        - is not found:
                                  #           - if accept is true, returns word as STR_ARR
                                  #           - if accept is false, returns null
                                  #     - input is lowercased first
                                  #  - synonym:
                                  #     - arguments: casesensitive BOOL (def: false), synonyms FILE (extension: .syn)
                                  #     - FILE (extension: .syn) have two space-separated fields
                                  #     - if input:
                                  #        - is found in first field of FILE, returns second field
                                  #        - otherwise returns null
                                  #     - input is lowercased first if not casesensitive
                                  #       Second field can have * at the end, indicating to be a word:* if used as a TSQUERY
                                  #  - thesaurus:
                                  #     - arguments: dictfile FILE, dictionary DICTIONARY
                                  #     - FILE (extension: .ths) have two colon-separated fields (can contain several words)
                                  #       Can have #comment
                                  #     - if input:
                                  #        - is found in first field of FILE, returns second field
                                  #        - otherwise returns null
                                  #     - first use another DICTIONARY.
                                  #       Common words will be erased by DICTIONARY, but can still match them in first field of FILE
                                  #       with ?
                                  #  - ispell:
                                  #     - arguments: stopwords FILE, afffile FILE2, dictfile FILE3
                                  #     - FILE (extension: .stop): like stopwords in pg_catalog.simple
                                  #     - FILE2 (extension: .affix) and FILE3 (extension: .dict): see doc. for ispell
                                  #     - In short, can turn grammatical variations of a word into a single form.
                                  #  - snowball:
                                  #     - arguments: stopwords FILE, language WORD
                                  #     - like ispell, but simpler and never return null (so should be placed in the end)

create text search
parser PARSER
( start = FUNC,
gettoken = FUNC2,
end = FUNC3,                      #Creates a PARSER. Must be superuser ROLE.
lextypes = FUNC4                  #Available ones:
[, headline = FUNC5]  )           #   - 'pg_catalog.default' (def). Notion of a "letter" depends on locales.

                                 ##Following functions can be used to test similarities between STR.
                                 ##Postgres extension 'fuzzystrmatch'
levenshtein(STR, STR2
[, INS_COST, DEL_COST,
SUB_COST])                       ##Show differences, like agrep
levenshtein_less_equal
(..., INT)                       ##Same but if more than INT, returns INT+1 (faster).
metaphone(STR, INT)              ##Convert to STR to phonetic-style code, and truncate it if more than INT characters (max 255)
                                 ##For english language. Does not work well with Unicode characters.
dmetaphone(STR)                  ##Same but unlim size.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CURSORS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CURSORS ==>                       #Row iterators over a specific query (executed once when opening the cursor)
declare CURSOR                    #Creates and open a CURSOR for query TABLE_VAL (any that would also work in a from ... clause)
[scroll] cursor                   #Is closed at end of transaction (if no transaction, is immediately closed), unless with hold is used.
[with hold]                       #Using select for update|share:
for TABLE_VAL                     #  - is not compatible with with hold or scroll
                                  #  - is recommended if using a ... where current of ... to make sure rows are the same
                                  #If scroll, fetch can work backward.
                                  #Starting position is before the first row. After the last row is the end.
fetch [WORD]                      #Return rows of CURSOR according to position specified by WORD, and move it too:
from CURSOR                       #  - forward|backward [NUM|all] (def: forward): retrieves several rows if NUM
                                  #  - relative NUM: 0 for current row
                                  #  - absolute NUM (if negative, from the end): 0 for before first row, more than number of rows for
                                  #    after last row
move [WORD] from CURSOR           #Same but only moves cursor, doesn't return anything.
close CURSOR|all                  #Close a CURSOR (or all). Automatically done at end of transaction for a CURSOR without "with hold"
update|delete ... where           #Same as update|delete ... where BOOL, with BOOL being current position of CURSOR.
current of CURSOR                 #Grouping cannot be used in the TABLE_VAL of CURSOR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SCHEMAS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create schema SCHEMA
[if not exists]                   #Creates a SCHEMA (namespace inside a DATABASE).
[authorization USER]              #USER is the owner (def: current_user).

alter TYPE VAR ...
set schema SCHEMA                 #For all TYPE that can have a SCHEMA

[SCHEMA.]VAR                      #Access/write a VAR (usually TABLE but can be any VAR) for a specific SCHEMA.
                                  #If not specified, use the ENVVAR_ARR search_path, by def. {USER, public}. Searched from left to
                                  #right. If nothing found and writing operation, use the leftest SCHEMA (except USER)

pg_catalog                        #SCHEMA for all pg_* tables and builtins functions and types. It's implicit in the search path.
                                  #To override those builtins, put pg_catalog at the end of the search path

current_schema()                  #
current_schemas([BOOL])           #If true, include implicit ones (e.g. pg_catalog)
pg_is_other_temp_schema
(OID)                             #Is it a temporary schema
pg_my_temp_schema()               #Temp SCHEMA OID, 0 if none.
pg_TYPE_is_visible(OID)           #Checks if an object is accessible through current SCHEMA path.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CONCURRENCY          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SESSION ==>                       #Beginning and end of a list of commands (e.g. interactive prompt)
CONCURRENCY ==>                   #Uses Multiversion Concurrency Control (MVCC) instead of traditional locks, for best performance
                                  #while still reliable:
                                  #  - transactions create a snapshot of the current state ("database version").
                                  #  - read/write locks are "predicate locks": they don't block, they are just informative, and later on
                                  #    might cancel actions if concurrency problem.
                                  #Best to have consistent data is:
                                  #  - if several statements need to be "write all or nothing", use transactions:
                                  #      - if the state being read can be changed by other clients in the middle of the transaction
                                  #        without problems, use read committed statements
                                  #      - otherwise, use serializable statements, but then if abort, must redo the transaction.
                                  #        Client software usually notify of transaction failures: in such case, client needs to send
                                  #        the request again.
                                  #      - in all cases, put as read only if possible, and deferrable when it's a long operation.
                                  #  - otherwise, transactions are not needed: operations are atomic and ask for the relevant blocking
                                  #    locks, so no concurrency problem (are actually atomic transactions)
                                  #When asking for locks:
                                  #  - Transaction asking for locks will wait ENVVAR lock_timeout (def: 0, in ms) before cancelling,
                                  #    and ENVVAR deadlock_timeout (def: 1s) before checking if there is a deadlock (in which case it
                                  #    is cancelled)
                                  #  - all transactions cannot exceed an average of ENVVAR max_[pred_]locks_per_transaction (def: 64)
                                  #    predicate or not-predicate locks.
                                  #FUNC():
                                  #  - always a single read commited transaction, acquiring locks
                                  #     - begin block of PL/PGSQL is logic-wise, not used for concurrency
                                  #  - finer concurrency control is only at SQL level, so need wrap FUNC() call in a SQL transaction
start transaction                 #Start a transaction (ends with rollback|commit), statements are only committed to the database at the
[isolation level WORD, ]          #end of the transaction (but "appear" committed locally inside the statement)
[read write|only, ]               #A transaction that has any statement with an error will abort (rollback when committed).
[[not] deferrable]                #WORD can be read committed, repeatable read or serializable.
                                  #Effects of WORD, for W2 -> R1 or W1 (same rows, 1 and 2 are transactions)
                                    +-------------------------------------------+-------------------------------------------+
                                    |                  W2 -> R1                 |                  W2 -> W1                 |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| atomic or committed transaction 1 |                                    No problems                                        |
| atomic or committed transaction 2 |                                                                                       |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             |                                           | read committed: ignores W2                |
| ongoing transaction 2             |                                           | non read committed: aborts (will rollback)|
+-----------------------------------+                 ignores W2                +-------------------------------------------+
| atomic or committed transaction 1 |                                           |                  blocks                   |
| ongoing transaction 2             |                                           |                                           |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             | non read committed and R1 -> W2 -> R1:    | non read committed and R1 -> W2 -> W1:    |
| atomic or committed transaction 2 |  ignores W2                               |  aborts (will rollback)                   |
|                                   | else:                                     | else:                                     |
|                                   |  takes W2 into account                    |  takes W2 into account                    |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
                                  #Summary:
                                  #  - read committed (def):
                                  #    - ignores other ongoing transactions, but takes into account changes by committed transactions
                                  #      or atomic statements
                                  #    -> see state for the current statement
                                  #  - repeatable read:
                                  #    - ignores other ongoing transactions, including changes, unless it conflicts, in which case
                                  #      it aborts.
                                  #    -> see state at start of transaction (first statement after "start transaction")
                                  #  - serializable:
                                  #    - like repeatable read, but two concurrent serializable doing a W that depends on a R that is
                                  #      changed by this W: the second will abort.
                                  #      - ex: insert into TABLE(COL) select sum(COL) from TABLE; by two serializable transactions.
                                  #    -> can virtually consider successful transactions (not aborting) to happen one after the other,
                                  #       while still avoiding blocking locks to achieve it.
                                  #Others:
                                  #  - read write|only (def: read write):
                                  #     - read only allow further concurrency optimization
                                  #     - read only can still write to TEMP
                                  #  - deferrable (def: not deferrable):
                                  #     - if serializable and read only, blocks until sees no chance of being cancelled, then go on.
                                  #     - good if cancellation might take a long time to repeat the transaction (e.g. backups)
                                  #Synchronization of two TRANSACTIONs:
                                  #  - TRANSACTION must:
                                  #     - both be either repeatable read or serializable.
                                  #       If TRANSACTION2 is serializable, so must be TRANSACTION1
                                  #     - if TRANSACTION1 is readonly, so must be TRANSACTION2
                                  #     - Must be just after the "start transaction"
                                  #  - steps:
                                  #     - TRANSACTION1 does select pg_export_snapshot(), which prints a SNAPSHOT_ID
                                  #     - TRANSACTION2 does set transaction snapshot SNAPSHOT_ID, as STR
                                  #Can have extra infos with:
                                  #  - txid_current(): current TRANSACTION_ID
                                  #  - txid_current_snapshot(): current SNAPSHOT_ID
                                  #  - txid_snapshot_xid(SNAPSHOT_ID): returns current transaction ID
                                  #  - txid_snapshot_xmax|xmin(SNAPSHOT_ID)
                                  #  - txid_visible_in_snapshot(TRANSACTION_ID, SNAPSHOT_ID)
set transaction ...               #Changes isolation level, etc. (same as start transaction ...) for current transaction.
                                  #Must be just after the "start transaction"
set session
characteristics                   #Same but for all future transactions.
as transaction ...                #Same as setting ENVVAR default_transaction_isolation|read_only|deferrable
rollback|commit                   #Finishes a transaction.
                                  #For rollback, actions are actually dropped (nothing happens).
savepoint LABEL
rollback to savepoint             #Inside a transaction block, rollback to LABEL go back to the state where savepoint LABEL was (and
LABEL                             #releases all savepoint LABEL that might have been defined after it).
release savepoint LABEL           #Delete a savepoint

prepare transaction STR           #Do a two-phase commit:
                                  #  - transaction is temporary rollbacked (except the ENVVAR changes) and will only be committed once
                                  #    commit prepared STR is done
                                  #  - commit|rollback prepared STR can be done by other clients (not inside a transaction), if same ROLE
                                  #    or superuser ROLE.
                                  #    So goal is to have a single client managing the transaction of other clients
                                  #    ("transaction management system")
                                  #STR must be unique, and less than 200 bytes
                                  #Transaction must not involve notify|[un]listen, TEMP nor CURSOR with hold
                                  #ENVVAR max_prepared_transactions (def: 0, so disabled) is available. Is set, should be
                                  #max_connections * number of prepared_transactions per connection

lock TABLE ...                    #Put a blocking lock on TABLE... among several mode WORD, and release at end of current transaction:
[in WORD mode] [nowait]           #  - should do it at beginning of transaction if repeatable read or serializable.
                                  #  - locks goal is to conflict with each other
                                  #  - prefer predicate locks
                                  #If can't access lock:
                                  #  - if nowait, only emits error
                                  #  - otherwise, transaction will be rollbacked
                                  #WORD are:
                                  #  - access exclusive (commands that erase data): block everything
                                  #  - exclusive: let up to access share
                                  #  - share row exclusive: let up to row share
                                  #  - share update exclusive (commands that change schemas (clean/analyze/optimize data/alter)): let up
                                  #    to row exclusive
                                  #  - row exclusive (commands that write data)
                                  #  - row share (select for update/share)
                                  #  - access share (commands that read data)
select ... for update             #Gets an access exclusive lock on the rows in ... for TABLE (def: all), released at end of transaction
[of TABLE...] [nowait]            #(not statement). Cannot use union, intersect or except.
select ... for share
[of TABLE...] [nowait]            #Same for share row exclusive

pg_[try_]advisory_                #Gets a lock linked to a ID BIGINT:
[xact_]lock[_shared]              #  - can be "_shared" or not (block lock() attempts, but not lock_shared() attempts)
(BIGINT)                          #  - session-level (stops at unlock() or at end of session)
                                  #    - if "xact_" transaction-level (stops at end of transaction only)
                                  #  - If "try_", only returns false/true but no block if can't get the lock.
                                  #  - can lock several times (needs to unlock several times)
pg_advisory_unlock
[_shared](BIGINT)                 #Returns true if such lock existed.
pg_advisory_unlock_all()          #

set constraints                   #Change the initially deferred|immediate state of CONSTRAINT... (def: all) within the current
all|CONSTRAINT...                 #transaction.
deferred|immediate                #CONSTRAINT can be declared (including during create table ...)
                                  #  - initially immediate (def): constraints are checked at each statement.
                                  #  - initially deferred: at each end of transaction (except not null and check())
                                  #  - not deferrable (def: deferrable): cannot be initially deferred.
                                  #Includes previous statements (retroactively), so can fire at specific point in transaction.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INDEX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create [unique] index             #Create an INDEX on COL_VAL...
[concurrently] INDEX              #Cannot be directly used, but makes select|update|delete on COL_VAL... faster (used by the planner).
on TABLE( COL_VAL...              #Can't create two INDEX with same names, even on different tables.
[OPCLASS] )                       #Will be picked when the query use the same expression used in create index ...:
[using INDEXMETHOD]               #  - COL_VAL (and not COL_VAR):
[asc|desc]                        #     - "expression index" makes it possible to use INDEX when query usually use COLVAL(e.g. FUNC(COL))
[nulls first|last]                #     - if use operator, needs extra set of parenthesis, e.g. (( COL + VAL ))
[tablespace TABLESPACE]           #  - where BOOL:
[with ( VAR = VAL... )]           #     - "partial index" (only on part of TABLE), faster when query usually use where BOOL
[where BOOL]                      #     - useful when where BOOL is selective (only few rows)
                                  #  - multiple COL_VAL...: "multicolumn index":
                                  #     - should be used only if COL_VAL... are almost always queried together
                                  #     - for btree|gist, only faster when using query using COL_VAL... in the same order and connected
                                  #       with and (up to the last OP)
                                  #     - for gist, quite slow if first COL has few distinct values
                                  #     - not for spgist
                                  #  - [asc|desc] [nulls first|last]:
                                  #     - "sorted index", faster when querying using order by ...
                                  #     - for btree, useless (except with desc nulls last or asc nulls first, or with multiple
                                  #       COL_VAL... with different sort order) since it can scan in both directions and automatically
                                  #       sort.
                                  #  - using INDEXMETHOD (def: btree): see below
                                  #  - OPCLASS:
                                  #     - use a specific OPCLASS for the type of COL_VAL instead of the default one
                                  #       The underlying OP must be immutable.
                                  #Others:
                                  #  - unique:
                                  #     - only for btree
                                  #     - use it if COL... can't have duplicate records (faster)
                                  #     - as opposed to usual unique CONSTRAINT, can be done on COL_VAL (not only COL_VAR)
                                  #  - concurrently: instead of asking for an exclusive lock, wait for possible conflicting transactions
                                  #    to complete for each row. Slower but will not lock the TABLE.
                                  #    Can fail: then drop INDEX and recreate it.
                                  #  - with ( VAR = VAL... ): see create table ...
                                  #An "index-only scan" is a query that can use only the INDEX without even visiting the TABLE (faster).
                                  #A query using "... and|or ..." can use each ... INDEX separately, then combine the result.
                                  #INDEX performances:
                                  #  - makes read faster, but write slower (needs to maintain the INDEX)
                                  #  - is mostly efficient if the query is selective (chooses few rows) on a big table.
                                  #  - to know when to use an INDEX, check with explain if INDEX is chosen by the planner for most-used
                                  #    queries or skipped in favor of a sequential scan.

INDEXMETHOD ==>                   #"Index accessor method"
                                  #Structure in which indexes are conceptually stored (because in the end they are TABLE), and
                                  #functions to maintain/create those structures.
                                  #In short: way to create INDEX, linked to specific types and operators.
                                  #Are linked to specific OPFAMILY, which are groups of OPCLASS (operator class), which is a combination
                                  #of INDEXMETHOD + TYPE + FUNC...:
                                  #  - an INDEXMETHOD provides an API with few FUNCs to implement
                                  #  - to implement a TYPE, need to write them for those TYPE -> OPCLASS
                                  #  - available OPCLASS and OPFAMILY are usually called TYPE_ops (_TYPE_ops for ARR)
                                  #INDEXMETHOD available are:
                                  #  - btree (def):
                                  #      - implemented on all TYPE for < <= = >= > <> (implies in, between, etc.)
                                  #      - faster if = is used first in queries
                                  #  - gin:
                                  #      - for VAL = one of VAL...
                                  #      - implemented on most operators for:
                                  #         - TSVECTOR
                                  #         - ARR
                                  #         - INT_ARR && <@ @> @@
                                 ##        - HSTORE @> <@ ? ?& ?|
                                 ##     - for INT_ARR, use special OPCLASS gin|gist__int[big]_ops
                                 ##        - big_ops: prefer if big dataset
                                  #  - gist:
                                  #      - implemented on most operators for CIRCLE, POINT, BOX, POLYGON, RANGE, TSQUERY, LTREE
                                  #      - implemented also on TSVECTOR, INT_ARR and HSTORE (like gist):
                                  #         - gin tends to be faster read but slower write
                                  #  - spgist:
                                  #      - implemented on most operators for POINT, RANGE, and on < <= >= = > <> for STR
                                  #  - more are available by adding a row to pg_am. Some are available as EXTENSION.
create operator
class|family ...                  #Create user-defined OPCLASS|OPFAMILY. Need C functions (see doc online)
alter operator
family ...                        #

reindex index|table|              #Rebuild INDEX... of the target (system is like database but only include system catalogs)
database|system                   #Needed to reclaim space when an INDEX has shrink size a lot.
INDEX|TABLE|DATABASE              #Also needed if an INDEX is corrupted due to software bug.
                                  #If corrupted INDEX are on system catalogs, use postgres --single -P instead of postgres, otherwise it
                                  #will crash. Then use reindex and restart.
                                  #Ask for a exclusive lock

cluster [verbose] [TABL           #Makes the physical layout of TABLE rows similar to INDEXMETHOD.
[using INDEXMETHOD]]              #Goal is to speed up queries of a TABLE with an INDEX using INDEXMETHOD.
                                  #Is one-time operation (new rows are not clustered). Acquires an exclusive lock.
                                  #Good idea to run analyze afterwards.
                                  #INDEXMETHOD can be ommitted the second time, because remember it. Memory can be altered with:
                                  #  - alter table TABLE cluster on INDEX
                                  #  - alter table TABLE set without cluster
                                  #  - same for materialized views
                                  #TABLE can be ommitted to specify all TABLE having used cluster.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EFFICIENCY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARTITIONS ==>                    #Parts (sets of rows) of a TABLE, used for improved performance (when querying, altering or deleting a
                                  #whole partition (e.g. drop TABLE is faster than delete TABLE where BOOL)).
                                  #There is:
                                  #  - an empty parent TABLE
                                  #  - partitions are children TABLE... with:
                                  #    - same COL_VAR...
                                  #    - a check CONSTRAINT to restrict to a specific set of row
                                  #    - an INDEX on the COL on which the CONSTRAINT is based.
                                  #  - a trigger FUNC that redirects data inserted in the parent TABLE to the correct child
                                  #  - Partitions should not overlap, and should together cover the whole data.

prepare PREP([TYPE...])           #Parse COMMAND to make it faster to use later, if complex and often used COMMAND.
as COMMAND                        #Must be used with execute PREP(...):
execute PREP(...)                 #  - Each TYPE can be specified in COMMAND as positional argument $NUM
                                  #  - execute PREP(...) supply those arguments
                                  #  - last TYPE... can be "unknown", meaning "anyelement" (can still be used as $NUM), providing it
                                  #    can be determined runtime
                                  #  - no overloading
                                  #COMMAND must be select|insert|update|delete|values
                                  #No problem of caching: PREP will be reparsed if:
                                  #  - used TABLE (including COLs) change types
                                  #  - search_path changes
deallocate PREP|all               #Delete a PREP.
                                  #deallocate all is automatically done at end of each session.

LARGEOBJECT ==>                   #Efficient way to manipulate big VAL (based on a file):
                                  #  - can manipulate only through OID
                                  #  - can read/write/seek (see online doc) like in C
lo_import(STR)                    #Creates a LARGEOBJECT from file at path STR, and returns its OID
lo_export(OID, STR)               #Creates a file at path STR with LARGEOBJECT with OID
lo_unlink(OID)                    #Needs to unlink LARGEOBJECT after use
                                 ##Instead of unlinking manually, could create a TFUNC executing lo_manage(COL_OID) on TABLE containing
                                 ##LARGEOBJECT as OID, before update or delete, for each row. Should delete * from table before dropping
                                 ##a TABLE to keep the trigger.
                                 ##Postgres extension 'lo'
                                 ##Command-line vacuumlo DATABASE (with connection options) can also be used.

explain( [verbose, ]              #Display the query tree of COMMAND:
[analyze, [buffers, ]             #  - nodes can be:
[timing false, ]]                 #     - Sort: order by ...
[costs false, ] [format           #     - Limit: offset|fetch
text|xml|json|yaml] )             #     - Aggregate: AFUNC
COMMAND                           #     - GroupAggregate: group by
                                  #     - HashAggregate: some AFUNC
                                  #     - WindowAgg: WFUNC
                                  #     - Filter: where TEST (or having)
                                  #     - Join: can be:
                                  #        - Nested loop, with a Join Filter: normal one
                                  #        - Hash, with a Hash Cond: involves an extra Hash step (compare hashes)
                                  #        - Merge join, compare both TABLE next to each other (must be sorted)
                                  #     - bottom only:
                                  #        - sequential scan: linear read of TABLE
                                  #        - index scan: using an INDEX, then visiting the TABLE
                                  #          - index-only scan: using an INDEX, and no need to visit the TABLE to answer the query
                                  #        - bitmap scan:
                                  #           - using an INDEX with first a Bitmap Index Scan on the INDEX
                                  #           - then fetching the TABLE with a Bitmap Heap Scan
                                  #           - difference with index scan: fetch all INDEX, then all TABLE, and not row by row INDEX
                                  #             then TABLE (index scan)
                                  #           - if using several INDEX, will do a BitmapAnd or BitmapOr
                                  #        - values scan: values( ... )
                                  #  - each node include the cost|resources of its children
                                  #Options:
                                  #  - analyze: show the actual time (unless timing false is used) and memory consumed
                                  #    Actually execute the COMMAND (otherwise, it is not) (can put in a transaction with rollback if
                                  #    don't want the execution to persist)
                                  #    - buffers: show info about buffer hits, which shows which parts are I/O intensive
                                  #  - costs (def: true):
                                  #    - show the resources taken, in a cost-based approach according to ENVVAR:
                                  #       - seq_page_cost (def: 1): sequential disk page fetch (used in sequential scan)
                                  #       - random_page_cost (def: 4): non-sequential disk page fetch (used in INDEX retrieval)
                                  #         Put at 2-3 if fast disks.
                                  #       - cpu_tuple_cost (def: 0.01): CPU cost for processing a row
                                  #       - cpu_index_tuple_cost (def: 0.005): CPU cost for processing a row in an INDEX
                                  #       - cpu_operator_cost (def: 0.0025): CPU cost for processing a FUNC
                                  #    - values are:
                                  #       - cost: first is initial cost, second is final cost
                                  #       - rows: number of rows manipulated
                                  #       - width: average size (in bytes) of a row
                                  #Advice:
                                  #  - use real data close to real environment, not test ones.
                                  #  - run analyze first if lot of change since last time autovacuum did (because based on pg_statistic)

create tablespace                 #Create a TABLESPACE, i.e. a group of DATABASE, TABLE, MATERIALIZED VIEW and INDEX:
TABLESPACE [owner ROLE]           #  - located on the disk at a specific location STR (must be empty dir)
location STR                      #  - owned by ROLE (def: current ROLE)
                                  #Must be a superuser to do it.
                                  #Goal:
                                  #  - Unlike SCHEMA, can't be used for permissions.
                                  #    Only permission is create, i.e. possibility to assign a newly created VAR to TABLESPACE
                                  #  - is used to control the physical locations of database objects, in order to:
                                  #     - optimize performance, e.g. putting heavily used INDEX on fast storage
                                  #     - optimize space, e.g. put heavy space DATABASE on high-volume storage, with possibility to move
                                  #       to another is space is not enough anymore
                                  #Is cluster-wide, so can be assigned to objects of different DATABASE.
                                  #Each new VAR has as a TABLESPACE (unless explicity mentionned):
                                  #  - if cluster-wide object, pg_global
                                  #  - otherwise the default TABLESPACE of the database.
                                  #    It is inherited from its template (pg_default for template0|1), but can be overriden by:
                                  #     - if TEMP or INDEX on TEMP, ENVVAR_ARR temp_tablespace: if used as an ARR, allocate randomly
                                  #       accross TABLESPACE...
                                  #     - otherwise, ENVVAR default_tablespace
                                  #Symlinks to DIR of user-created TABLESPACE can be found in DATADIR/pg_tblspc
alter TYPE VAR ...
set tablespace TABLSPAC           #For all TYPE that can have a TABLESPACE

WRITING ==>                       #Best way to write a big amount of data fast:
                                  #  - put in only one transaction/statement
                                  #  - use copy or (if copy not possible) prepare, if possible on an empty TABLE
                                  #  - create the INDEX and foreign keys constraints after the data has been put into
                                  #     - when doing so, increasing ENVVAR maintenance_work_mem INT, max. memory used by vacuum,
                                  #       create INDEX and foreign key constraint creation (def: '16MB')
                                  #  - less checkpoints (increasing ENVVAR checkpoint_segments and checkpoint_timeout)
                                  #  - temporarily disabling WAL or replication
                                  #Should run analyze afterwards.

PERFORMANCE ==>                   #Can:
                                  #  - disable durability, by:
                                  #      - putting fsync and full_page_writes off (risky)
                                  #      - putting synchronous_commit off (more durable and almost same performance gain)
                                  #  - less checkpoints (see below)

PERFORMANCE TUNING ==>            #ENVVAR:
                                  #  - work_mem (def: 1MB): max memory used by a single command for each of its sort operations and hash
                                  #    tables, before writing temp files to disk.
                                  #    Average total memory taken will be average_number_of_hash/sort_operations_by_command *
                                  #    number_of_connections * work_mem. Should not be more than RAM taken by:
                                  #      kernel + other applications + shared_buffers + let memory for kernel buffer
                                  #  - maintenance_work_mem (def: 16MB): max memory used by vacuum, create index and add foreign key.
                                  #    Usually not a lot of those operations are run concurrently, so can be set higher (256MB is good)
                                  #  - effective_cache_size (def: '128MB'): hint of the amount of cache available for queries
                                  #    (doesn't change the actual cache size). Should be 50-75% of the available free+cached HDD memory
                                  #  - wal_buffers (def: -1, which auto-select a value): memory used for caching WAL, i.e. number of
                                  #    WAL segments (16MB) that can be cached for all sessions before flushing them.
                                  #    If high number of transactions, might consider increasing for better performance.
                                  #    Best is 16MB.
                                  #  - max_stack_depth (def: 2MB):
                                  #     - higher can provoke stack overflow (crashing the server) if higher than the kernel limit, with
                                  #       a safety margin of 2MB
                                  #       current OS limit can be seen with ulimit -s (8MB)
                                  #     - lower can cancel complex queries requiring more stack.
                                  #  - temp_file_limit (def: -1, unlim): max memory for temp files, in KB
                                  #  - max_files_per_processes (def: 1000): max opened files per session (see limit with ulimit -S|H -n)
                                  #    Def on Linux: 1024, so that's good.
                                  #  - effective_io_concurrency: when using several disks at same time (e.g. RAID), number of disks that
                                  #    can write at same time
                                  #  - shared_buffers (def: 128MB): memory for shared buffers (def: 128MB).
                                  #    Good value is 25% of RAM (if > 1GB total RAM)
                                  #Look at amount of memory taken with pgcluu or pgbadger
pgtune -i FILE                    #Checks postgresql.conf FILE, and prints an optimized version (mostly for performance ENVVAR)
                                  #Can do -o FILE2, but should pipe it to diff - FILE, to see differences.
-M NUM                            #Total memory in bytes (def: guess it)
-c NUM                            #Number of connections expected (change max_connections and work_mem)
-T WORD                           #Type of application, among DW (OLAP), OLTP, Web or Desktop. Def is Mixed (-> unspecified)
                                  #Desktop assumes lower cache, mem and connections, DW moderate, and Web and OLTP very high.

HARDWARE ==>                      #  - more RAM -> more cache
                                  #  - good hard drives. RAID0 or RAID1 is good idea
                                  #  - CPU less important, but still important for complex functions

pgbench                           #Does a benchmark, to compare machines or server conf speed.
                                  #Must first do a pgbench -i to initialize it (creates four pgbench_* tables), with following options
                                  #while initializing:
-F NUM                            #Percentage of not-null in pgbench_* tables (def: 100)
-s NUM                            #Multiply default number or rows in pgbench_* tables. Should be at least >= -c NUM
--[index-]tablespace=
TABLESPACE                        #Use a custom TABLESPACE for tables or indexes (to do if used in production)
--unlogged-tables                 #Create pgbench_* as unlogged tables
                                  #While not initializing:
                                  #  - tps is transactions per seconds.
                                  #  - has following options:
-h -p -U                          #Connection options (see psql)
-c NUM                            #Number of concurrent connections. Should be close to average in real production.
-t NUM                            #Number of transactions per client. Higher gives more precision.
                                  #Should be high enough to run few minutes
-j NUM                            #Number of threads
-n -f FILE                        #Execute SQL FILE, instead of default one (simple update, select and insert statements)
                                  #Can include commands:
                                  #  - \setrandom INTVAR MIN MAX
                                  #  - \setshell INTVAR COMMAND ARGS
-S                                #Perform only select statements
-r                                #Show execution time for clients, per statements.

OTHERS ==>                        # - TABLE fillfactor, fastupdate


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     USERS AND PRIVILEGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ROLES ==>                         #ROLE are users or group of users for a given cluster.
                                  #Difference between OS_USER (OS-specific) and ROLE (cluster-specific).
                                  #Default installation creates a OS_USER "postgres", with group "postgres", often used as owner of
                                  #clusters.
                                  #Each cluster has a superuser ROLE:
                                  #  - defined as the owner OS_USER (user that created the cluster using initdb)
                                  #  - should have OS permissions over the DATADIR of the cluster
                                  #By default, client connects as ROLE using OS_USER, but can use flags to do otherwise.
                                  #  - so anyone that can login as the owner OS_USER of a cluster on a local computer can be superuser
                                  #    ROLE too.
                                  #ROLE own VAR they create.

create role ROLE                  #Create a ROLE:
[superuser]                       #  - superuser: creates another superuser ROLE (must be superuser ROLE)
[createdb|role]                   #    Cannot drop the initial superuser ROLE.
[[encrypted] password             #    readonly ENVVAR is_superuser ('on|off') is available.
STR]                              #  - createdb|role|login|replication: gives createdb|role|login|replication privilege (see PRIVILEGE)
[login] [replication]             #  - password STR:
[connection limit INT]            #     - either plaintext or "md5STR" where STR is a md5 hash.
[noinherits]                      #     - If encrypted is specified, STR must be plaintext -> it is converted to a "md5STR".
[valid until TIMTMPTZ]            #     - can be null for no password.
[in role ROLE2...]                #     - only useful with authentication methods with passwords (i.e. "password" and "md5")
[role passwordcheck              ##     - will cancel if password is weak.
ROLE2...]                        ##       Must also put '$libdir/passwordcheck' in shared_preload_libraries.
[admin ROLE2]                    ##       Will not work on md5 hashes, only plaintext.
                                 ##       It is recommended to rebuild the module by modifying the Makefile to enable CrackLib
                                 ##       (better weak password recognition). Must be enabled at buildtime anyway.
                                 ##     - report password failure after ENVVAR auth_delay.milliseconds (def: 0), to avoid bruteforce
                                 ##       (but makes DDoD easier).
                                 ##       Must put auth_delay in shared_preload_libraries
                                 ##        - Postgres extension 'auth_delay'
                                  #  - valid until TIMESTAMPTZ: validity of the password
                                  #  - in role ROLE2...: grants ROLE as a member of ROLE2... (prefer using grant|revoke)
                                  #  - role ROLE2...: inverse
                                  #  - admin ROLE2...: same as role ROLE2, but ROLE2 are added with admin_option
                                  #  - connection limit: how many connections at same time (def: -1, unlim)
                                  #  - noinherits: see grant|revoke
alter role ROLE ...               #Can be all options of create role but:
                                  #  - can use no..., e.g. nosuperuser
                                  #  - no in role, role or admin
alter TYPE VAR ...
owner to ROLE                     #For all TYPE but extension, index, role, rule, text search parser|template, trigger
drop role [if exists]             #Drop ROLE and cluster-specific objects owned by ROLE.
ROLE...                           #Doesn't work if ROLE own database-specific objects (use reassign|drop owned by ROLE first) or
                                  #privileges.
                                  #If client connection, doesn't stop it.
drop owned by ROLE...             #Drop all objects (except cluster-specific objects) owned by ROLE, and
cascade|restrict                  #revoke privileges given to ROLE.
reassign owned
by ROL... to ROLE2                #Change ownership.


set [local] role ROLE             #Same syntax as set, but here change current ROLE
                                  #If not superuser, must be a ROLE that the session_user is member of (but not inverse, not other
                                  #members of same ROLE), directly or indirectly.
                                  #There are two types of users:
                                  #  - current user: current ROLE. Used for permission checking.
                                  #  - session user: ROLE that (usually) started the session.
                                  #    Used to switch roles with set role and set session authorization.
                                  #    So for a session, can switch back and forth between same possible ROLE (as long as set
                                  #    session authorization is not called)
                                  #VAR session_user (one that started the session) and current_user are available.
                                  #If ROLE is none, reset to current session_user.
                                  #ROLE can be written as STR
                                  #  - local: same as set local ...
set [local] session               #Same but:
authorization ROLE                #  - for both session_user and current_user
                                  #  - must be superuser
                                  #  - use default to reset to initial session_user

grant PRIVILEGE...                #Gives permissions PRIVILEGE... on VAR... to ROLE...:
on [TYPE] VAR...                  #  - TYPE:
to ROLE...                        #     - is table (def)|sequence|database|domain|function|language|schema|tablespace|type|
[with grant option]               #       large object|foreign data wrapper|foreign server
                                  #         - table include VIEW and FOREIGNTABLE
                                  #     - [TYPE] VAR can be all TYPEs in schema SCHEMA for table|sequence|function
                                  #  - PRIVILEGE:
                                  #     - can be:
                                  #        - on table:
                                  #           - insert (a): insert or copy from
                                  #           - delete (d): delete
                                  #           - truncate (D): truncate
                                  #           - references (x): needed on both TABLE to use foreign key
                                  #           - trigger (t): create trigger
                                  #        - on table|sequence|large object:
                                  #           - select (r):
                                  #              - select, copy or using VAR in update|delete
                                  #              - sequence: same + currval()
                                  #              - large object: being read
                                  #           - update (w)
                                  #              - update, select for share|update
                                  #              - sequence: same + nextval|setval()
                                  #              - large object: being written
                                  #        - on sequence|domain|foreign data wrapper|foreign server|language|schema|type:
                                  #           - usage (U):
                                  #              - language:
                                  #                - create function for this PL/* language
                                  #                - careful if functions can access OS (e.g. Bash)
                                  #              - schema: reading VAR in SCHEMA (without permission can still look up VAR names in
                                  #                SCHEMA)
                                  #              - sequence: currval() + nextval()
                                  #              - type|domain: using it in creation of any VAR (including FUNC() and TABLE)
                                  #              - foreign data wrapper: create server using FDW
                                  #              - foreign server: create foreign table using FSERVER
                                  #        - on database:
                                  #           - connect (c): can start client session
                                  #           - temp (T): create temp
                                  #        - on database|schema|tablespace:
                                  #           - create (C):
                                  #             - database: create schema
                                  #             - schema: create any VAR inside SCHEMA, and rename (must be owner too)
                                  #             - tablespace: create table|index|temp in it, and use TABLESPACE in create database
                                  #        - on function:
                                  #           - execute (X): executing *FUNC() (including TFUNC())
                                  #        - on all:
                                  #           - all privileges
                                  #     - Other PRIVILEGE which can be obtained differently:
                                  #        - drop|alter:
                                  #           - no way to grant them to others:
                                  #              - drop or alter definition of VAR:
                                  #        - grant|revoke:
                                  #           - grant PRIVILEGE ... with grant|admin option:
                                  #              - grant for PRIVILEGE for the specific PRIVILEGE and TYPE
                                  #              - admin for ROLE membership
                                  #        - createrole:
                                  #           - create|alter role ... createrole:
                                  #              - create role (or drop|alter)
                                  #              - To do so on superuser ROLE, must be superuser.
                                  #              - implies admin option
                                  #              - can create roles with higher permissions or memberships, so can be dangerous
                                  #        - createdb:
                                  #           - create|alter role ... createdb:
                                  #              - create database (or drop|alter)
                                  #        - login:
                                  #           - create|alter role ... login:
                                  #              - ROLE can initiate a client session (e.g. with psql)
                                  #                Without it (def) ROLE can only be assigned with set role ROLE
                                  #        - replication:
                                  #           - create|alter role ... login:
                                  #             - ROLE can use pg_basebackup
                                  #     - PRIVILEGE can be PRIVILEGE( COL_VAR... ) for select|insert|update|references
                                  #     - Def. PRIVILEGE is all for TABLE owner and superuser.
                                  #       They can revoke their own privileges though.
                                  #     - Permissions are optimistic: has privileges in following cases:
                                  #        - granted at table-level but revoked at column-level
                                  #        - granted to ROLE2 to which is member, but revoked to the specific ROLE
                                  #     - SQL only has VAR, not VAR...
                                  #  - ROLE:
                                  #     - if ROLE2 is a member of ROLE (directly or indirectly), it is targeted too:
                                  #         - generally ROLE is just a group name, and ROLE2 real users, or subgroups
                                  #         - but could also copy and extend|restrict permissions of another user
                                  #         - inherit:
                                  #           - by def., ROLE2 inherit privileges of ROLE (except "Other PRIVILEGE")
                                  #           - with noinherits, can still use set role ROLE (but then loses its inital PRIVILEGE)
                                  #     - can be public:
                                  #        - meaning all current and future ROLE.
                                  #        - cannot be member of a ROLE.
                                  #        - has default privilege: connect, temp, usage and create on SCHEMA public, execute, usage on
                                  #          LANGUAGES. Can be revoked.
                                  #     - If current user is superuser or can grant thanks to membership or "with grant option", looks
                                  #       like PRIVILEGE has been granted by VAR owner
                                  #Access controls:
                                  #  - are recorded in the database objects they are attached (using system catalogs) as ACLITEM_ARR,
                                  #    i.e. [ROLE]=LETTER.../ROLE2, where ROLE is the granted (def: public), ROLE2 the granter and
                                  #    LETTER shown above between parenthesis for each PRIVILEGE.
                                  #  - Each LETTER can be followed by * for "with grant option"
                                  #  - don't show default PRIVILEGE
revoke [grant|admin               #Inverse.
option for] PRIVILEGE...          #"with grant|admin option" is revoked too. If "grant|admin option for", only "with grant|admin option"
on [TYPE] VAR...                  #is removed.
from ROLE...                      #Revoke privileges granted to others by ROLE2 too: if restrict, command will fail if there are some
cascade|restrict                  #ROLE can only revoke ROLE2 for PRIVILEGE it previously personnally granted the other or a ROLE the
                                  #other is member of.

alter default
privileges                        #Change default PRIVILEGE for ROLE2 (in ...) of all VAR (write TYPEs, not TYPE VAR) that will be
[for ROLE...]                     #created in the future by ROLE (must be current user (def) or a ROLE3 that ROLE is member of), in
[in schema SCHEMA]                #SCHEMA (if specified)
grant|revoke...                   #Only for TABLE, SEQUENCE, FUNC or TYPE.

grant ROLE... to ROL2...          #
[with admin option]               #
revoke [admin option
for] ROL... from ROL2...          #Grant|revoke ROLE2... as members of ROLE...
cascade|restrict                  #Gives|revokes associated PRIVILEGE too.

security label
[for PROVIDER]
on VAR is STR                     #Used to implement SE-Linux (see online doc)

has_TYPE_privilege                #TYPE can also be:
([ROLE, ]VAR, PRIVILEG)           #  - any_column
                                  #  - column: adds a COL_VAR arg after VAR
pg_has_role([ROLE, ]
ROLE2, PRIVILEG)                  #With membership


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        AUTHENTICATION         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_hba.conf                       #File under a cluster DATADIR controlling:
                                  #  - who can connect (host address, OS_USER)
                                  #  - to which database
                                  #  - with which connection method
                                  #  - under which ROLE
                                  #Have five whitespaces-separated fields (can contain whitespace if double-quoted) (with #Comment):
                                  #  - type (protocol):
                                  #    - local: local connection (Unix sockets)
                                  #    - host: TCP/IP
                                  #    - host[no]ssl: TCP/IP with|without SSL. For SSL:
                                  #       - must set ENVVAR ssl to "on" (def: "off") at server start
                                  #       - must have been enable when installing|building PostgreSQL
                                  #  - database:
                                  #    - connect to which DATABASE... (comma-separated)
                                  #    - sameuser means DATABASE with same name as ROLE
                                  #    - samerole means DATABASE with same name as ROLE or a member of ROLE
                                  #    - replication: special DATABASE used by pg_basebackup
                                  #    - @FILE... means list of DATABASE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                                  #    - can be all
                                  #  - user:
                                  #    - under which ROLE... (comma-separated) can connect
                                  #    - +ROLE means any member of ROLE
                                  #    - @FILE... means list of ROLE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                                  #    - can be all
                                  #  - address:
                                  #    - which IPv4|6 addresses (along the netmask range) can connect
                                  #    - can also be a hostname (can be slow)
                                  #      - including a .DOMAINNAME for hosts under DOMAINNAME
                                  #    - can be all
                                  #    - samehost: any of the machine own IPs
                                  #    - samenet: any of the machine own subnet
                                  #  - netmask (optional):
                                  #    - IPv4|v6 netmask
                                  #  - auth. method:
                                  #    - trust|reject: always accept|refuse
                                  #    - ident [map=MAP]:
                                  #      - use MAP in pg_ident.conf (ENVVAR ident_file) to determine which ROLE correspond to OS_USER
                                  #      - def: ROLE = OS_USER
                                  #    - password|md5: ask for password (md5 hashes it but not crypto-secure)
                                  #      - libpq variable password:
                                  #         - To use if is demanded. Def: PGPASSWORD
                                  #         - Can also use a FILE PAGPASSFILE (def: ~/.pgpass), which should contain lines with format:
                                  #             host:port:database:user:password (first four field can be *)
                                  #           Permission must be 0600
                                  #    - SSL:
                                  #      - libpq variables:
                                  #        - sslmode: priority of SSL over non-SSL (def: PGSSLMODE):
                                  #          - disable: non-SSL
                                  #          - prefer (def): first SSL, then non-SSL
                                  #          - require: SSL. If root CA, verify certificate
                                  #          - verify-ca: SSL. Always verify certificate
                                  #          - verify-full: SSL. Verify certificate, and that hostname match in certificate
                                  #        - sslcert: certificate FILE (def: ~/.postgresql/postgresql.crt or PGSSLCERT)
                                  #        - sslkey:
                                  #          - Secret key (def: ~/.postgresql/postgresql.key or PGSSLKEY).
                                  #          - Can also be OpenSSL engines, as ENGINE:KEY
                                  #        - sslrootcert: CA certificate FILE (def: ~/.postgresql/root.crt or PGSSLROOTCERT)
                                  #        - sslcrl: CA revocation list FILE (def: ~/.postgresql/root.crl or PGSSLCRL)
                                  #    - krb5: Kerberos5
                                  #      - libpq variable krbsrvname: Kerberos server name (def: PGREALM or PGKRBSRVNAME)
                                  #    - GSSAPI:
                                  #      - libpq variable gsslib: on Windows, set to "gssapi" to use GSSAPI instead of SSPI
                                  #        (def: PGGSSLIB)
                                  #  - auth. method options (optional):
                                  #    - as VAR=VAL ...
                                  #    - see above
                                  #Default settings:
                                  #  - host and local connections: initdb --auth-host|local=STR (or -A STR for both)
                                  #  - default password: initdb -W|--pwfile=FILE, from stdin or from first line of FILE (if using
                                  #    password authentication)
                                  #Only the first matching line is chosen. Put most likely first for efficiency.
                                  #Location can be changed by ENVVAR hba_file
                                  #Read on server startup, or when receiving a SIGHUP.

requirekeeper                     #LIBPQ variable: OS_USER behind the server process must match STR.
                                  #Avoid another OS_USER starting the server while legit one is rebooting it.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         LOCALIZATION          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENCODING ==>                      #Sets within a cluster with initdb -E STR, which sets client_encoding ENVVAR, encoding of templates:
                                  #  - can be many, among 'LATIN1','UTF8' and 'SQL_ASCII'
                                  #  - Def. is locale (UTF8 in Linux) or SQL_ASCII if no locale.
                                  #If client has different encoding, a conversion is performed according to pg_conversion
                                  #(see online doc).
                                  #Available conversions from UTF8 should be enough. Otherwise can do:
                                  #  - create conversion CONVERSION for STR to STR2 from FUNC
                                  #Encoding is sometimes specified as INT: use pg_encoding_to_char(INT) to get it as a STR
                                  #server_encoding ENVVAR is available

LOCALES ==>                       #Use ENVVAR lc_*, that are usually set at cluster creation with initdb --lc-*=LOCALE.
                                  #lc_collate|ctype (unless using COLLATION) and encoding can't be changed after database creation
                                  #Def. is "", i.e. locale (en_US.UTF-8 e.g.) or SQL_ASCII if no locale.
                                  #Locales are server-dependent, not client-dependent.
                                  #Non-C locales allow local-dependent on some operations:
                                  #  - sorting: order by, < > >= <=
                                  #  - case: upper, lower, initcap, regexps
                                  #  - [[:...:]] in regexps
                                  #  - to_char()
                                  #C locale is faster and the only one that can use INDEX on like operator.

COLLATION ==>                     #Combination of lc_collate and lc_ctype, specified as "lang_LANG[.ENCODING]":
                                  #  - are used by doing:
                                  #     - VAL collate "..." (e.g. "fr_FR")
                                  #     - TYPE collate "..."
                                  #  - cannot mix different COLLATION in same statement
                                  #  - prefer without ENCODING, which then let pg_collation use current ENCODING
                                  #Creating COLLATION:
                                  #  - available can be seen with pg_collation
                                  #  - populated by initdb with available locales on the OS
                                  #  - using create collation ...
create collation COLLATION
( [locale = LOCALE]
[, lc_collate|ctype =
LOCALE] )                         #LOCALE is a shortcut for both lc_collate and lc_ctype
create collation
COLLATION from COLLATION2         #

collation for (STR)               #Returns COLLATION

convert(STR, STR2,STR3)           #Encoding conversion. STR is string to convert.
convert_to|from(STR,
STR2)                             #Same, but assumes the dest|original encoding to be the current system's encoding
pg_client_encoding()              #Current encoding


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EXTENSIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create extension                  #Activate an EXTENSION, i.e. a collection of objects (library), except DATABASE, ROLE, INDEX and
[if not exists] EXTENSIN          #TABLESPACE, for a specific DATABASE.
[schema SCHEMA]                   #STR is the version (def: the one in the default_version)
[version STR]                     #SCHEMA is for objects defined in EXTENSION (default: current SCHEMA)
                                  #Some EXTENSIONs are already present in the default compilation of PostgreSQL, or personalized
                                  #compilations (but usually not activated).
                                  #Permissions are the ones required in executing the underlying SQL file.
                                  #Files must already be present in SHAREDIR/extensions/:
                                  #  - a SQL file "EXTENSION--VERSION.sql" defining new CTYPE, FUNC, etc.:
                                  #     - implicitly inside a transaction, so cannot use transaction statements
                                  #     - do select pg_extension_config_dump(STR, STR2); for a TABLE named STR used for users to
                                  #       personalize the extension.
                                  #       It will make the TABLE backupable, as opposed to rest of EXTENSION.
                                  #       STR is a filter, e.g. 'where BOOL' or ''.
                                  #     - should include protective lines:
                                  #       -- complain if script is sourced in psql, rather than via CREATE EXTENSION
                                  #       \echo Use "CREATE EXTENSION pgrowlocks" to load this file. \quit
                                  #  - an ASCII text file "EXTENSION.control" defining metadata as VAR = VAL ..., with #comment possible:
                                  #     - default_version STR: current version
                                  #     - relocatable BOOL (def: false): if SCHEMA can be changed after create extension
                                  #     - comment STR: description
                                  #     - directory STR: DIR of the SQL file (def: same as *.control file)
                                  #     - encoding STR (def: client's encoding): to be defined in no ASCII characters
                                  #     - module_pathname STR:
                                  #       - can be used as MODULE_PATHNAME in the *.sql file
                                  #       - can use $libdir, e.g. '$libdir/myextension'
                                  #     - requires STR...: other extensions this one depends on
                                  #     - superuser BOOL (def: true): if only superuser can create extension
                                  #     - schema STR: def. SCHEMA (only if non-relocatable)
                                  #  - additional EXTENSION--VERSION.control with same format but for a specific VERSION can be defined
                                  #  - additional EXTENSION--VERSION--VERSION2.sql are executed to go through VERSION to VERSION2 with
                                  #    alter extension update
alter extension EXTNSIN
add|drop TYPE VAR                 #

load STR                          #Loads a shared library located at STR, in order:
                                  #  - an absolute path
                                  #  - or look into ENVVAR dynamic_library_path, which a colon-separated list (def: $libdir),
                                  #    where items can start with $libdir (def: /usr/lib/postgresql/9.3/lib/)
                                  #    (can be seen with pg_config --pkglibdir)
                                  #If not superuser, must be inside $libdir/plugins/
                                  #STR path convention is OS-specific.
                                  #Not useful if only functions definitions because they are loaded automatically with create function...
                                  #Can also use ENVVAR shared_preload_libraries (comma-separated list), without the extension .so

create foreign data               #Creates a FDW, i.e. functions that permits using another DBMS inside PostgreSQL.
wrapper FDW                       #Can be slow and not optimized.
[handler FUNC]                    #Based on a standard implemented by other DBMS, including noSQL.
[validator FUNC2]                 #FUNC and FUNC2: see doc. on how to create them.
[options ( VAR STR ...)]          #Must be superuser.
                                  #Available ones (as EXTENSION):
                                  #  - postgres_fdw: for PostgreSQL to other DBMS
                                  #     - FSERVER options (can also use libpq variables)
                                  #        - host STR
                                  #        - dbname STR
                                  #        - port STR
                                  #     - user mapping options:
                                  #        - user STR
                                  #        - password STR
                                  #        - client_encoding STR (def: ENVVAR client_encoding)
                                  #     - FTABLE options:
                                  #        - schema|table|column_name STR: if name is different
                                  #     - FSERVER or FTABLE options!
                                  #        - updatable BOOL (def: true): read-write
                                  #     - details:
                                  #        - transaction read committed -> repeatable read
                                  #  - file_fdw: for CSV file (or other formats of copy COMMAND), read-only:
                                  #     - FTABLE options:
                                  #        - filename STR: absolute path
                                  #        - ...: same options as copy COMMAND, except force* and oids
alter foreign data
wrapper FDW
[no handler|handler FNC]          #
[no validator|validator           #
FUNC]                             #
options ( [add|set|drop]          #
VAR VAL ... )                     #

create server FSERVER
[type STR]
[version STR2]                    #Creates a FSERVER, i.e. a connection to a specific DATABASE using a FDW.
foreign data wrapper FDW          #STR are all FDW-dependent options. They usually specify the connection details
[options ( VAR STR3...)]          #(host and port, database name, etc.)
alter server FSERVER
[version STR]
[options ([add|set|drop]
VAR STR2... )]                    #

create user mapping
for ROLE|current_user
server FSERVER                    #Specify authentication details on FSERVER for ROLE.
[options ( VAR STR ...)]          #STR are all FDW-dependent options.
alter user mapping ...
[options
( [add|set|drop] ... )]           #

create foreign table
[if not exists] FTABLE
( COL_VAR TYPE
[options ( VAR STR ... )]
 CONSTRAINT|not null|
 default VAL ... )                #Creates a FTABLE (can be used as a TABLE) while connected to a FSERVER.
server FSERVER                    #TABLE, COL, TYPE, CONSTRAINT, etc. should be same as on FSERVER, unless specified in the options.
[options (VAR STR2 ...)]          #But can only select some of the foreign COL.

alter foreign table FTABL
rename COLVAR to COLVAR2          #

alter foreign table FTABL
add COL_ARG                       #

alter foreign table FTABL
drop [if exists] COL_VAR
cascade|restrict                  #

alter foreign table FTABL
alter COL_VAR type TYPE           #

alter foreign table FTABL
alter COL_VAR set|drop
not null|default [VAL]            #Same as alter table ...

alter foreign table FTABL
[alter COL_VAR]
options( [add|set|drop]
VAR VAL ... )                     #

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      SYSTEM COLS/TABLES       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SYSTEM COLS ==>                   #Hidden COL... defined for every TABLE
                                  #ctid, etc. are used for MVCC
TABLE.tableoid                    #OID of the TABLE
TABLE.oid                         #ID row, doesn't change with update, but created by insert.
                                  #Table-specific: cross join with tableoid to have real IDs.
                                  #Creation:
                                  #  - ENVVAR default_with_oids = true (def: false)
                                  #  - create table ... with[out] oids
                                  #  - alter table TABLE set with[out] oids
                                  #Is actually a cycling number incrementing from 1, database-wise.
TABLE.ctid                        #Same but change avec committed update or insert
                                  #Is actually a cycling number incrementing from 1, table-wise.
TABLE.cmin, TABLE.cmax            #Same but change avec uncommitted update or insert (put back to 0 when ctid changes)
                                  #Is actually the numero of the statement inside the transaction.
TABLE.xmin                        #ID of the transaction inserting (table-wise, not client-wise)
TABLE.xmax                        #ID of the transaction deleting or updating this row by another non committed transaction.

SYSTEM CATALOGS ==>               #TABLE used by the system. Can also be VIEW (system views)
                                  #Are sometimes readonly. Anyway, should not modify them directly unless good reasons (e.g. creating a
                                  #new INDEXMETHOD).
                                  #Are database-specific unless on cluster-specific VAR
INFORMATION SCHEMA ==>            #Same but is SQL standard: more portable but does not contain all information.
                                  #Is in SCHEMA information_schema.*, written is_* below
                                  #Most of them not written here, look at online doc if needed

pg_database                       #All DATABASE, including:
                                  #  - datistemplate BOOL: can be used in create database template DATABASE
                                  #  - datallowconn BOOL: if false, no one can connect to it

pg_class                          #All TABLE-like elements: TABLE, VIEW, INDEX, SEQUENCE, MVIEW and CTYPE, including:
                                  #  - relisshared BOOL: if cluster-wide
pg_tables                         #All TABLE
pg_views                          #All VIEW
pg_matviews                       #All MVIEW
pg_index                          #All INDEX
pg_indexes                        #Same but with other infos, like the query that created it.
is.sequences                      #All SEQUENCE
pg_attribute                      #All COL, including:
                                  #  - attisdropped BOOL: dropped COL but physically kept
pg_attrdef                        #Default values for COL that have some
pg_constraint                     #All CONSTRAINT
pg_inherits                       #Inheritance between TABLE

pg_[sh]depend                     #Relations between [cluster-wide] objects, to avoid conflicts when dropping an object.
pg_locks                          #All currently held locks

pg_proc                           #All FUNC
pg_aggregate                      #All AFUNC
pg_trigger                        #All TFUNC
pg_event_trigger                  #All EFUNC
pg_operator                       #All OPERATOR
pg_cast                           #All CAST, including:
                                  #  - oid OID
                                  #  - castsource|casttarget TYPE: from and to, refers to pg_type.OID
                                  #  - castfunc:
                                  #      - FUNC(TYPE1_VAL[, INT[, BOOL]]) -> TYPE2_VAL used for conversion
                                  #         - INT is the pg_attribute.ATTTYPEMOD
                                  #         - BOOL is true if explicit cast
                                  #      - refers to pg_proc.OID
                                  #      - If TYPE1 = TYPE2, this is a "sizing cast", which is used for things like size-checking or padding.
                                  #  - castcontext STR: indicates when to cast:
                                  #     - 'e': only in explicit cast
                                  #     - 'a': in explicity + implicity only when casting to COL type
                                  #     - 'i': always
                                  #  - castmethod STR: indicates how to cast:
                                  #     - 'f' use castfunc
                                  #     - 'i' use C-level I/O functions
                                  #     - 'b' doesn't do anything (types are binary-equivalent)
pg_rewrite                        #All RULE
pg_rules                          #Same, but more for user-defined RULE

pg_type                           #All TYPE, including:
                                  #  - typispreffered BOOL: if true, is used as the TYPE for the whole typecategory with
                                  #    overloading functions (def: text for STR, and real for numeric)
pg_enum                           #All ENUM
pg_range                          #All RANGE
is.domains                        #All DOMAIN

pg_authid|roles                   #All ROLE, including:
                                  #  - rolcatupdate BOOL (def: false): if false, all ROLE, including superuser ROLE, cannot write on
                                  #    system catalogs.
                                  #pg_roles blanks out the password
pg_auth_members                   #ROLE memberships, including:
                                  #  - roleid|member|grantor OID: means ROLE2 is a member of ROLE, which has been granted by ROLE3.
                                  #    Refer to pg_authid.oid
pg_default_acl                    #All default privileges.

pg_namespace                      #All SCHEMA
pg_tablespace                     #All TABLESPACE

pg_extension                      #Activated EXTENSION
pg_available_extensions           #Possible EXTENSION for their current version
pg_available_extension_
versions                          #Possible EXTENSION for all versions
pg_language                       #All activated LANGUAGE, including:
                                  #  - lanpltrusted BOOL: can not access filesystem (so non-superuser can create function)
                                  #    Is false for C, R and SH but true to SQL and PL/PGSQL
pg_pltemplate                     #All LANGUAGE

pg_ts_config[_map]                #All REGCONF
pg_ts_dict                        #All DICTIONARY
pg_ts_template                    #All TEMPLATE
pg_ts_parser                      #All PARSER

pg_collation                      #All COLLATION
pg_conversion                     #All CONVERSION

pg_cursors                        #All CURSOR
pg_prepared_statements            #All PREP
pg_prepared_xacts                 #All "prepare transaction STR"

pg_foreign_data_wrapper           #All FDW
pg_foreign_server                 #All FSERVER
pg_foreign_table                  #All FTABLE
pg_user_mapping[s]                #All USERMAPPING. With s, leaves out the option field when user should not see it

pg_am                             #All INDEXMETHOD: what they support (e.g. in create index ...) and the FUNC they use
pg_opclass                        #All OPCLASS
pg_opfamily                       #All OPFAMILY
pg_amop                           #OPERATOR used by OPFAMILY
pg_amproc                         #FUNC used by OPFAMILY

pg_largeobject
[_metadata]                       #All LARGEOBJECT

pg_[sh]description                #All COMMENT [on cluster-wide objects]

pg_settings                       #All ENVVAR, with also when is readonly, type, min|max value, unit
pg_db_role_setting                #All role and/or database-specific ENVVAR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATABASE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


MAIN STRUCTURE ==>                #  - Cluster: set of DATABASEs, managed by a single instance of a server daemon (postgres).
                                  #      - linked to a DATADIR
                                  #      - cluster-wide ENVVARs
                                  #      - cluster-specific objects:
                                  #         - DATABASE, including templates
                                  #         - ROLE
                                  #         - TABLESPACE
                                  #         - LANGUAGE
                                  #         - system catalogs on cluster-specific objects
                                  #  - Database: set of SCHEMAs. A client can connect only to one DATABASE at once.
                                  #  - Schema: set of SQL objects (TABLE, FUNC, etc.)
                                  #  - Session: client connection to the server for a specific database, e.g. interactive prompt session.
                                  #  - Transaction: set of statements that should executed all-or-nothing (concurrency)
                                  #  - Statement: SQL command individually sent to the server by the client.

create database DATABAS           #Create a DATABASE
[owner ROLE]                      #  - ROLE is current one by def
[template DATABASE2]              #  - DATABASE2 is the initial state of DATABASE:
[encoding WORD]                   #     - It is a default DATABASE2 called template1 by def. (created by initdb)
[lc_collate|ctype WORD]           #     - defines locales (see initdb)
[connection limit INT]            #     - template0 is the same as template1, but template1 can be modified to have specific initial
[tablespace TABLESPACE]           #       state of DATABASE, while keeping template0 as the initial template if needed.
                                  #     - DATABASE2 cannot be accessed just before and during create database
                                  #  - encoding|lc_collate|ctype WORD: overrided default defined by template. Should be used only if
                                  #    DATABASE2 is template0.
                                  #  - connection limit: number of concurrent non-superuser client sessions max (def: -1, i.e. unlim)
                                  #Cannot be inside a transaction.
alter database DATABASE
connection limit INT              #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SERVER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INSTALLATION ==>                  #  - Server: postgresql
                                  #  - Client: postgresql-client, pgadmin3
                                  #  - Extensions: postgresql-contrib, postgresql-EXTENSION
                                  #  - Extension building: postgresql-server-dev
pg_config                         #Show compilation-time conf
--bin|doc|html|include|
pkginclude|lib|pkglib|            #Print DIR. By default, useful DIR (so without headers, docs, etc.) are:
locale|man|share|                 #  - /usr/lib/postgresql/VERSION/: bin and lib
sysconfdir                        #  - /usr/share/postgresql{,-common}/VERSION/
--includedir-server|pgxs          #  - /etc/postgresql-common/: few conf files
--configure                       #Print all configuration flags (usually to mimic another installation)
--cc|cppflags|
cflags[_sl]|
ldflags[_ex|sl]|libs              #Print makefile variables

REGRESSION ==>                    #Running the regression tests (to unsure installation is fine):
                                  #  - download source, configure and build it
                                  #     - after building, before installation: make check (must not be root)
                                  #     - after installation: make installcheck[-parallel] (localhost, unless PGPORT or PGHOST is used)
                                  #  - when failed:
                                  #     - check src/test/regress/expected/*.out to see the test and its expected output, and
                                  #       compare with diff on src/test/regress/results/*.out
                                  #     - or directly look at src/test/regress/regression.diffs
UPGRADING ==>                     #Only needs to care about upgrade incompatibilities for major upgrades (e.g. 9.3 to 9.4), every year.
                                  #How:
                                  #  - use pg_dumpall backup
                                  #  - use pg_upgrade -b OLDBINDIR -B NEWBINDIR -d OLDDATADIR -D NEWDATADIR. Flags are:
                                  #     -u USER
                                  #     -c dry-run
                                  #     -j NUM: multiprocess run
                                  #     -k: use hard links instead of copies. Faster.
                                  #     -p|P NUM: old|new port number (can also use PGPORTOLD|NEW)
                                  #    NEWBINDIR and NEWDATADIR must be a new install, with similar settings.
                                  #    OLD*DIR must be erased afterwards.
                                  #    Connects to OLDDATADIR -> might want to put authentication as trust temporarily
                                  #    Both servers must be down.
                                  #    Must use NEWBINDIR/pg_upgrade, not OLDBINDIR.
                                  #    Cannot upgrade a log shipping standby.

STARTING ==>                      #If DATADIR (e.g. /usr/local/pgsql/data), as postgres user:
                                  #  - initdb -D DATADIR: creates cluster
                                  #  - postgres -D DATADIR 2> LOG &: starts the server
                                  #  - createdb DATABASE (default "postgres" DATABASE could be used though)
                                  #  - psql [DATABASE]
                                  #Autostart at boot time are system-specific.
                                  #Have a lot of memory PC to avoid memory crash if big data.
STOPPING ==>                      #Terminate server:
                                  #  - Can send signals:
                                  #    - SIGTERM (15): doesn't accept new connections, but let current ones finish.
                                  #       - server process stops, but will redo uncaught statements when restarting
                                  #    - SIGINT (2, CTRL-C): close all connections with a SIGTERM: abort current statements but close
                                  #      server itself propertly
                                  #    - SIGQUIT (3, CTRL-\): close all connections with a SIGQUIT: abort statements and server.
                                  #  - to send a signal on Windows, use pg_ctl kill ...
                                  #  - PID of a cluster can be found in DATADIR/postmaster.pid (ENVVAR external_pid_file) (first line)
                                  #    or with pg_backend_pid().
                                  #Terminate a client session (superuser or same member, but not same user):
                                  #  - pg_terminate_backend(PID)
                                  #  - send SIGTERM (15)
                                  #  - can see client PID with pg_stat_activity (pid)
                                  #Terminate a client current statement (superuser or same member, but not same user):
                                  #  - pg_cancel_backend(PID)
                                  #  - send INT (2, CTRL-C)
LOCATIONS ==>                     #  - Each cluster has a DATADIR (must have permissions 0700)
                                  #  - SHAREDIR, e.g. /usr/share/postgresql/9.3/

initdb [-D DATADIR]               #Creates a cluster, i.e.:
                                  #  - populates or create DATADIR. Def: PGDATA
                                  #     - directory under which all data of the cluster are stored.
                                  #     - /usr/local/pgsql/data/ or /var/lib/pgsql/data/ are often used.
                                  #  - creates template0|1 and an empty database "postgres"
                                  #  - create cluster-specific system catalogs
                                  #  - initialize cluster-wide ENVVARs
                                  #User:
                                  #  - should have right to write in DATADIR
                                  #  - will use current OS_USER as the superuser ROLE (will create the ROLE in the cluster)
                                  #     - good idea to use a OS_USER which doesn't have any permissions on the filesystem apart from
                                  #       the directories used by the server.
                                  #        - A OS_USER "postgres" is created by def. installation for this purpose.
                                  #  - can't be root
-U ROLE                           #Name of the superuser ROLE (by def. the current OS_USER).
                                  #Change the ROLE name but not:
                                  #  - the fact that OS_USER will be superuser
                                  #  - nor the name of the default database "postgres" (but could rename it)
-E STR                            #See encoding in this doc.
--lc-collate|ctype|
messages|monetary|
numeric|time=LOCALE               #See locales in this doc.
--locale=LOCALE                   #Same but for all
--auth-host|local=STR             #
-A STR                            #
-W
--pwfile=FILE                     #Default authentification, see pg_hba.conf
-X DIR                            #Log DIR

LIBPQ VARIABLES ==>               #Used in several places in this doc.
host[addr]                        #Def is localhost or PGHOST[ADDR]. Can be an absolute path to a Unix socket.
                                  #With addr:
                                  #  - specifies IP address
                                  #  - avoid host name lookup, faster.
                                  #  - Can't be used with Kerberos, GSSAPI, SSPI or verify-full SSL
port                              #Def is PGPORT (5432)
dbname                            #Def: OS_USER or PGDATABASE
user                              #ROLE to use. Def: OS_USER or PGUSER
connect_timeout                   #In seconds. Def: 0 (unlim). Should not be <2. Def: PGCONNECT_TIMEOUT
client_encoding                   #Def: "auto" (defaults OS locales). Def: PGCLIENT_ENCODING
options                           #Set postgres flags at runtime, client-side. Def: PGOPTIONS
                                  #Can use -c ENVVAR=VAL to set ENVVAR.
[fallback_]                       #Name of the application (psql, pg_dump, etc.)
application_name                  #Used in logs (title of the client connection). Can be env. variable PGAPPNAME too.
                                  #If application_name or PGAPPNAME is blank, defaults to fallback_* (let users override it)
keepalives                        #1 to enable it (def). If connection seems lost, sends packet to check it actually is.
keepalives_                       #Send max *count packets, every *interval seconds. Start sending after non-activity from server for
[idle|iterval|count]              #*idle packets.
                                  #Can also use ENVVAR tcp_keepalives_*
OTHERS ==>                        #See Authentication section
service                           #Additional parameters (def: PGSERVICE[FILE])

postgres [-D DATADIR]             #Starts the server daemon for a specific cluster.
                                  #To start postgres on several clusters on same machine, use different ports.
                                  #Only one server can be launched for a given cluster. Each client session spawns a new process.
                                  #Should be logged as the same OS_USER who used initdb. It should have permissions to access files that
                                  #will be used:
                                  #   - DIR of TABLESPACE
                                  #   - FILE referenced by copy from|to, load STR or create function ... as STR
                                  #DATADIR is PGDATA by def (unset by def). Can also use ENVVAR data_directory, and PGDATA or -D DATADIR
                                  #will point to the directory containing the postgresql.conf.
                                  #Print log messages on stderr. Should be launched in the background.
                                  #Can call version(), ENVVAR server_version[_num] and also a file DATADIR/PG_VERSION.
                                  #Commands that started the server can be found under DATADIR/postmaster.opts
                                  #Connections are done using either:
                                  #  - Unix sockets ("local"), on a socket at DIR/.s.PSQL.PORT (created at server start), where
                                  #    DIR is designated by ENVVAR unix_socket_directories (def: /tmp or /var/lib/postgresql/),
                                  #    comma-separated list by order of preference. Can be "" to disallow "local" connections.
                                  #    OS_USER must (to avoid spoofing) have:
                                  #      - permissions on DIR, preferably only that user
                                  #      - socket itself will be owned by OS_USER and OS_GROUP_USER (can be changed with ENVVAR
                                  #        unix_socket_group)
                                  #      - socket itself will have permission ENVVAR unix_socket_permissions (def: 0777). Could set to
                                  #        0770 or 0700 (only OS_GROUP_USER+OS_USER or only OS_USER can connect)
                                  #    Client must connect by using DIR as host
                                  #  - TCP/IP ("host"), which uses socket designated by IP addresses specified in ENVVAR
                                  #    listen_addresses (comma-separated) with IPv4|6 addresses (def: "localhost"). Should be set at
                                  #    server start.
                                  #    "*" means all IPv4|6, "0.0.0.0" all IPv4, "::" all IPv6
                                  #If crash, sessions will restart automatically if ENVVAR restart_after_crash (def: on)
-p NUM                            #Port number (def: 5432).
                                  #Can also use ENVVAR port at server start.
-k STR                            #Sets ENVVAR unix_socket_directories.
-h STR                            #Sets ENVVAR listen_addresses
-i                                #Same as -h "*"
-N                                #Sets ENVVAR max_connections (def: 100), which is the number of superuser + normal users max
                                  #connections. The last ENVVAR superuser_reserved_connections (def: 3) are kept for superuser only.
                                  #Look at work_mem to see estimate of memory used by this configuration.
                                  #Lower max_connections means more connections denied, higher means more chance to crash server.
                                  #Look at real life usage to set this setting.
                                  #Can also sets database-wise with connection limit (see create database)
--single                          #Single-user mode: start both the daemon and a superuser client session
                                  #Must be put before -D DATADIR
                                  #Quits with EOF (C-D). No readline.
                                  #By default use newlines instead of semicolons. Use -j to use EOF instead (then there will be only one
                                  #command).
-l                                #Enable SSL connections

pg_isready                        #Returns (exit code) according to server up or down status:
                                  #  - 0: OK
                                  #  - 1: refusing connections
                                  #  - 2: no server response
                                  #  - 3: could not send request
-d -h -p -U -w|W                  #Connection options (see psql)
-t NUM                            #Timeout (in sec, def 3, 0 to disable)
-q                                #quiet

ENVVAR ==>                        #Environment variables. Can be specified with:
                                  #  - cluster-specific:
                                  #     - postgres -c VAR=VAL or postgres --VAR=VAL
                                  #     - editing DATADIR/postgresql.conf (ENVVAR config_file)
                                  #        - has lines VAR = VAL, and #comment
                                  #        - can have include[_if_exists] 'FILE' or include_dir 'DIR' (includes DIR/*.conf, by
                                  #        - alphabetical order)
                                  #        - can use pg_reload_conf() or send SIGHUP to server daemon postgres
                                  #        - can see load time with pg_conf_load_time()
                                  #     - PGOPTIONS
                                  #  - role-specific:
                                  #     - alter role ROLE ... set ENVVAR to VAL | from current
                                  #                           reset ENVVAR|all
                                  #  - database-specific:
                                  #     - alter role all in database DATABASE ... set ENVVAR from current
                                  #                                               reset ENVVAR|all
                                  #  - role-database-specific:
                                  #     - alter role ROLE in database DATABASE ... set ENVVAR to VAL | from current
                                  #                                                reset ENVVAR|all
                                  #  - session-specific:
                                  #     - set ...
                                  #     - setting PGOPTIONS with '-c VAR=VAL ...' before launching the client command
                                  #  - function-specific:
                                  #     - do create function ... set ENVVAR to VAL | from current:
                                  #        - from current: use current VAL as VAL
                                  #     - inside a function:
                                  #        - set local (if FUNC() used create function ... set ...)
                                  #  - transaction-specific:
                                  #     - set local ...
                                  #Some require superuser ROLE to write.
                                  #Some cannot be session-specific.
                                  #Are always STR. BOOL actually use 'on' and 'off'.
set [local] ENVAR                 #VAL can be default
to VAL                            #For an ARR, write VAL...
                                  #Can also use set_config(ENVVAR_STR, VAL_STR, BOOL) (BOOL is is_local)
reset ENVVAR|all                  #Same as set ... to default
show ENVVAR|all                   #Print their values.
                                  #Can also use current_setting(ENVVAR_STR)
                                  #Can also use postgres ... -C ENVVAR, which is done against a running server
alter TYPE VAR ...
set ENVVAR
to VAL|from current               #
alter TYPE VAR ...
reset ENVVAR|all                  #For database, function or role

discard plans|temp|all            #Remove session-specific information:
                                  #  - plans: cached explain plans
                                  #  - temp: TEMP
                                  #  - all: cached explain plans, TEMP, session-specific ENVVAR, PREP, CURSOR, unlisten *,
                                  #    pg_advisory_unlock_all(), put session_user and current_user to default

pg_postmaster_start_
time()                            #
current_database()                #Can also use information_schema.information_schema_catalog_name.catalog_name
current_query()                   #Current executing statement
inet_client|server_
addr|port()                       #

statement_timeout                 #ENVVAR (in ms) after which client requests fail (def: 0).

FILESYSTEM ACCESS ==>             #
pg_ls_dir(STR)                    #Returns COL_STR. Must be relative path (.. not allowed)
pg_read[_binary]_file
(STR)                             #Returns as STR|BYTEA
pg_stat_file(STR)                 #Returns as a TABLE with one row with COL size, access (atime), modification (mtime), change (ctime),
                                  #creation (creation time, only Windows) and isdir.
pg_file_write(STR,
STR2, BOOL)                      ##BOOL is append
                                 ##Postgres extension 'adminpack'
pg_file_read(STR,STR2,
BOOL)                            ##BOOL is append
pg_file_rename(STR,
STR2)                            ##
pg_file_unlink(STR)              ##
pg_file_length(STR)              ##

DATADIR ==>                       #  - base/: databases main data.
                                  #    Querying database|tables oids:
                                  #      - utility oid2name:
                                  #         - without option: database oid -> name
                                  #         - -H -p -U -P: usual connection flags
                                  #         - -d DATABASE: show table oid -> name
                                  #           - -S: include system catalogs, views and TOAST
                                  #         - -f|o NUM: for tables with filenode|oid NUM
                                  #         - -t STR: same with name
                                  #         - -i: include SEQUENCE and INDEX
                                  #         - -s: info about TABLESPACE
                                  #      - pg_relation_filenode|filepath(OID|STR)
                                  #    6MB for an empty database. Each file is a table with optionally:
                                  #      - a FILE_vm: Visibility Map
                                  #      - a FILE_fsm: Free Space Map
                                  #    TOAST are special subfiles when a file is too big.
                                  #  - global/: cluster-wide data.
                                  #  - pg_xlog/: WAL
                                  #  - pg_log/: logs (see log_directory)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CLIENT             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


psql                              #Starts interactive commands while connecting to a DATABASE
                                  #Read from standard input, so <<<"STR" or <FILE are possible.
                                  #Use readline so can use a .inputrc
                                  #The five first options are connection-related, and shared by other commands. When available, can
                                  #also always use a STR argument instead with either:
                                  #  - VAR=VAL... as space-separated libpq variables
                                  #  - postgres://[USER:PASSWORD@][HOST][:PORT]/[DATABASE][?VAR=VAL] (VAR are libpq variables)
-d DATABASE                       #Def: "postgres"
-h HOST                           #Def: PGHOST or (if absent) local host.
-p PORT                           #Def: PGPORT or (if absent) default port.
-U USER                           #Def: PGUSER or (if absent) current OS user.
-w|W                              #Force to not use or use a password

-c STR
-f FILE                           #Redirect input from STR or FILE (non-interactive shell)
-o FILE                           #Redirect output to FILE
-L FILE                           #Prints queries to FILE (logging)

-X                                #Do not read init files (files read at session start containing any command that PSQL understands)
                                  #Init file can be:
                                  #  - systemwide (/etc/postgresql-common/psqlrc)
                                  #  - user-specific (shell variable PSQLRC, by def HOME/.psqlrc)
                                  #Can append -NUM[.NUM2[.NUM3]] to target only specific PostgreSQL versions.
-n                                #Don't use readline (useful when pasting)
-s                                #Ask for confirmation before each command
-1                                #Wrap commands in a transaction block. Commands should not contain transaction blocks themselves.
                                  #Disable all EFUNCs
-l                                #Does \list then exits

\COMMAND                          #psql can use special commands. Finished by newline not semicolon.
                                  #Can use \n \t \r \000 \x00 in 'STR'. Can do shell substitution with `COMMAND`
                                  #Can appear anywhere a SQL command can.
                                  #Some commands use SREGEXP:
                                  #  - . only means SCHEMA separation (otherwise current schema).
                                  #    For all objects outside current SCHEMA, do *.*
                                  #  - $ not present
                                  #  - ? and * are globbing
                                  #  - case-insensitive unless ""

\q                                #Exits
\h [COMMAND]                      #Help on SQL commands
\?                                #Help on PSQL commands
\! [COMMAND]                      #Execute a shell command according to the shell variable SHELL
\timing [on|off]                  #Show time taken by commands

\pset VAR [VAL]                   #If VAL, sets printing options, if not either toggle or print current value.
                                  #VAR [VAL] can be:
format STR                        #How things are printed. Can be unaligned, aligned (def), wrapped, html, latex[-long table], troff-ms.
                                  #  - Wrapped: like aligned but wrap long lines according to columns INT (if 0, only on terminal
                                  #    output, and uses shell env variable COLUMNS or detected screen size)
                                  #  - unaligned: can control separators with field|recordsep STR or field|recordsep_zero (use \0)
x [on|off|auto]                   #Put in expanded mode (switch cols and rows, good for table with long width). If auto, do it only for
                                  #wide tables.
t                                 #No footer nor headers
footer [on|off]                   #footer is command tag, etc.
title STR                         #Print STR in front of all tables
border INT                        #Column width
linestyle ascii|unicode           #
null STR                          #What to print for null strings (def: "")
numericlocale on|off              #Locale-specific NUM
pager on|off|always               #Using env variable PAGER (def: less)
tableattr STR                     #In HTML output format, HTML attributes of <table>

END OF LINE ==>                   #The seven following commands should be put at the end of a command (instead of ;):
\w FILE| |COMMAND                 #Print current input to FILE or pipe to COMMAND (don't execute it)
\p                                #Same as \w |cat
\g [FILE| |COMMAND]               #Redirect output like \o, but for current input only.
\r                                #Clears current input.
\e [FILE]                         #Edit current command (or if FILE, FILE) with editor (shell variable [PSQL_]EDITOR, i.e. vim), which
                                  #becomes the new command.
                                  #New command is only executed if terminated by ;
\ef [FUNC[(...)]]                 #Same but the command is a "create or replace function FUNC". If no FUNC, creates a empty definition.
\watch INT                        #Execute current input every INT seconds, until interrupted (or error).

\o [FILE| |COMMAND ]              #Redirect stdout (not stderr) (def: to stdout)
\i[r] FILE                        #Execute command in FILE.
                                  #If r and non-interactive, relative path to script DIR, otherwise relative to PWD)
\copy ...                         #Like SQL copy but performed locally, not on the server.
                                  #Can use pstdin|out to avoid any stdin|stdout redirection (i.e. use terminal input|output)
\lo_export OID STR
\lo_import STR                    #Like lo_ex|import(...), but performed locally, not on the server.

\[q]echo [-n] VAL                 #Prints VAL (use q if \o has been used), without trailing newline if -n
                                  #Can appear anywhere in a SQL command.
\setenv VAR [VAL]                 #Sets shell environment variables

\d...[S][+] [SREGEXP]             #Show info about VAR specified in ... (S for also system ones, + for more info) among:
                                  #  - nothing (all table-like), t (table), v (view), i (index), m (materialized view),
                                  #    s (sequence)
                                  #  - u (ROLE), dp (default user privilege), p (all table-like with privileges)
                                  #  - d (constraint, operator*, rule, trigger)
                                  #  - n (SCHEMA)
                                  #  - b (tablespace)
                                  #  - L (LANGUAGE), x (EXTENSION)
                                  #  - T (TYPE), D (domain), C (cast)
                                  #  - O (COLLATION), c (conversion)
                                  #  - E|et (ftable), es (foreign server), eu (user mapping), ew (foreign data wrapper)
                                  #  - f[n|a|w|t] (func, afunc, wfunc, tfunc), y (efunc), o (OPERATOR)
                                  #  - F (REGCONF), Fd (DICTIONARY), Fp (PARSER), Ft (TEMPLATE)
                                  #  - l (large objects)
                                  #  - rds: ENVVAR, ROLE-specific (SREGEXP) and optionally database-specific too (use a second SREGEXP)
                                  #  - \l[+] (not \d): DATABASE
\sf [FUNC[(...)]]                 #Prints definition of FUNC

\c [DATABASE [USER]
[HOST] [PORT]]                    #Connect to different database. Def is current
\conninfo                         #Print current connection info
\cd [DIR]                         #Change PWD (def: HOME)

\[un]set INTVAR [VAL]             #Internal VAR. INTVAR is case-sensitive. Are not ENVVAR.
                                  #Can also use psql -v INTVAR[=[VAL]]
                                  #Can be created INTVAR. Substitution is done with:
                                  #  - :INTVAR: macro expansion of INTVAR
                                  #  - :'INTVAR': same but surround with '', unless already present (do with STR)
                                  #  - :"INTVAR": same with "" (do with VAR)
\gset [WORD]                      #Put at end of command (like \p, \r, etc.)
                                  #Command output is redirected to new INTVAR (one by column) called [WORD_]COL_VAR.
                                  #Columns must be named. Null give unset variables, failing commands don't change variables
\prompt [-f] [STR]
INTVAR                            #Prompt for INTVAR, with message STR. Use terminal (no -f) or stdin/stdout (-f)

DBNAME                            #
HOST                              #
PORT                              #
USER                              #Connections info

ON_ERROR_STOP                     #When set, errors terminate script (non-interactive) or line (interactive)
                                  #ENVVAR exit_on_error is also available, where errors terminate whole session.
ON_ERROR_ROLLBACK                 #If on, errors in transactions are just ignored. If off (def), they abort the whole transaction.
on|interactive|off                #Interactive means on for interactive and off for non-interactive.
IGNOREEOF INT                     #Number of EOF (C-D) to send to terminate a session (def: 1)

PROMPT1|2|3                       #Prompt. Literal STR with possible sequences escaped by %: M (full host), m (short host), > (port),
                                  #n (user), / (database), ~ (database, but ~ if default one), # (# if superuser, > otherwise),
                                  #R (= if normal, ! if disconnected), x (transaction block), NNN (octal), `command`, :VAR:,
                                  #[ and ] (ansi escaping sequences). Def is %/%#
                                  #PROMPT1 is normal, PROMPT2 when continuing on a new line, PROMPT3 when reading from stdin
COMP_KEYWORD_CASE
[preserve-]lower|upper            #Completion case. If preserve, tries to keep current word case.
QUIET                             #Don't print welcome message
ECHO                              #When set to '' (def), do nothing.
                                  #When 'queries', print all input to output.
                                  #When 'all', same but only for non-interactive input.
ECHO_HIDDEN                       #When set, prints commands behind \ commands

HISTFILE                          #Def: ~/.psql_history. Written at exit.
                                  #Can for example use user, database-specific, etc. hist files
                                  #Can also use shell variable PSQL_HISTORY
                                  #Can also use \s [FILE] (always relative to PWD) (def: print to stdout)
HISTSIZE                          #Number max of commands (def: 500)
HISTCONTROL
ignoredups|space|both             #Like in Bash

AUTOCOMMIT on|off                 #By def (on), each individual command is wrapped in a single transaction. When off, a start
                                  #transaction is implicitly fired but needs to manually commit it
LASTOID                           #OID of last written object
FETCH_COUNT                       #Number of max rows in memory at once

PGADMIN3 ==>                      #GUI client. Can do almost anything that can be done with psql.
                                  #Installing/launching:
                                  #  - MaintenanceDB:
                                  #     - DATABASE where pgAdmin connects first (should use postgres)
                                  #     - should install adminpack extension
                                  #  - Should use the same OS_USER we would use for a normal psql session
                                  #Usage:
                                  #  - Is not refreshed automatically: needs to refresh it manually.
                                  #  - Some types of objects are hidden by default (e.g. AFUNC or TYPE): can change in options
                                  #  - Servers have names, and can be grouped.
                                  #Useful tools:
                                  #  - Easy access to objects, with properties and statistics, and conf files
                                  #  - Can open a psql session in a console
                                  #  - Edit/View data: simple spreadsheet.
                                  #    Read-only if no primary key.
                                  #    blank is null (needs to write '' for '')
                                  #  - Server status: current clients, locks, log (needs to edit path in options), transactions
                                  #Less useful tools:
                                  #  - Grant wizard (available when on a SCHEMA): generate SQL grant statements with GUI
                                  #  - Reports: generating HTML files for objects properties, statistics, dependencies.
                                  #  - Query tool: useless, use the console (unless needs a SQL debugger, which needs to be installed)

PGAGENT ==>                       #  - SQL "cron", inside pgadmin3
                                  #  - to install on a cluster:
                                  #     - connect to postgres
                                  #     - install plpgsql
                                  #     - as superuser ROLE, execute pgagent.sql (i.e. in pgadmin3 sharedir).
                                  #       It will create TABLE and FUNC used for storing/manipulating the jobs and schedules, in SCHEMA
                                  #       pagent
                                  #     - launch pgagent LIBPQ_STR
                                  #        - this daemon will look at those tables and determine if need to launch job
                                  #        - run as root
                                  #        - options:
                                  #           -s LOG_FILE
                                  #           -l1 (best verbosity)
                                  #           -t (poll time, def: 10 sec)
                                  #        - do no put password in LIBPQ_STR (use passfile instead)
                                  #        - automatically in background and detached from current terminal tab
                                  #        - should automatically launch it at startup
                                  #  - configure jobs using pgadmin, under "Jobs":
                                  #     - steps are:
                                  #         - SQL commands (will be executed as superuser ROLE)
                                  #         - or "batch" (shell commands, run as root, must start with #!/bin/bash)
                                  #     - schedules are when it is done
                                  #  - monitor with check_postgres

TEAMPOSTGRESQL ==>                #Alternative to pgAdmin3. Fewer functions, but is a web interface instead of a dekstop app.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              IPC              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


notify WORD[, STR_LIT]            #Sends a message STR_LIT (def: "") on channel WORD. Also communicates the server PID.
                                  #Any client connected to the same server and listening to WORD will get a notification.
                                  #How this notification is handled depends on the client:
                                  #  - psql: print to stderr
                                  #If sends twice the same WORD + STR_LIT, might notify only once.
                                  #STR_LIT is max 8KB
                                  #pg_notify(STR, STR2) is also available: same but don't need to use constant STR_LIT.
[un]listen WORD                   #WORD can be * for unlisten
                                  #listening is session-wise: unlisten * is executed at end of each session
pg_listening_channels()           #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LOGGING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOGGING ==>                       #Controlled by several ENVVARs
log_destination                   #Where to put log messages (comma-separated list):
                                  #  - stderr (def)
                                  #  - csvlog:
                                  #     - with logging_collector, will output as CSV files
                                  #     - goal is to import them in tables with copy from
                                  #  - syslog:
                                  #     - prefer using the logging_collector
                                  #     - Needs to put local0.* /var/log/postgresql/ in syslog conf file
                                  #     - Can use ENVVAR syslog_facility, syslog_ident and event_source
logging_collector                 #When on, use the logging facility:
                                  #  - redirect stderr to file in ENVVAR log_directory too (can be relative to DATADIR) (def: pg_log).
                                  #      - files are named according to ENVVAR log_filename, which can use %... (data escape)
                                  #        (def: "postgresql-%Y-%m-%d_%H%M%S.log") for file creation time
                                  #      - a new file is created every ENVVAR log_rotation_age (def: 1d)
                                  #        or every time the file is more than ENVVAR log_rotation_size (def: 10MB)
                                  #        or when pg_rotate_file() is called
                                  #    If csvlog, will be in CSV format and use name ENVVAR application_name.
                                  #  - Files have permission ENVVAR log_file_mode (def: 0600, only server owner can read/write)

log|client_min_messages           #Between debug5-1,log,notice,warning,error,fatal,panic (def: notice for client, warning for log) for
                                  #either client messages or logging.
                                  #postgres -d NUM can set log_min_messages (from 0 to 5, def: 0), for DEBUG5-1
log_error_verbosity               #Verbosity of messages: default, verbose (include SQLSTATE error code) or terse (no defail, hint,
                                  #query nor context)
log_min_error_statement           #Same as log_min_message, but for logging the statements themselves (def: error).
log_statement                     #Which statements to log among none (def), all, ddl or mod (include ddl)
log_min_duration_                 #Logs time of statement execution, when it is higher than this limit (in ms, 0 to log all, -1 not to
statement                         #log it (def))
log_statement_stats               #If on, server prints to stderr the statements executed and the time it took
log_line_prefix                   #What to put in beginning of each log line. Can include %-escapes:
                                  #  - a: application_name
                                  #  - u: user
                                  #  - d: database
                                  #  - r: host+port
                                  #  - h: host
                                  #  - p: PID
                                  #  - t|m: timestamp (to seconds|ms). Timezone is controlled by ENVVAR log_timezone (def: 'localtime')
                                  #  - s: process start time
                                  #  - c: session id (process start time + PID)
                                  #  - i: command
                                  #  - e: error code
                                  #  - l: log number, session-wise
                                  #  - x|v: [virtual] transaction ID
log_checkpoints                   #Logs checkpoints (def: off)
log_[dis]connections              #Logs [dis]connections attempts (def: off)
log_lock_waits                    #Logs deadlocks (see deadlock_timeout)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MONITORING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


check_postgres                    #Performs several sanity monitoring tests, and outputs warnings.
-db -H -u -p                      #Connection options. Some actions requires several clusters/databases: then use STR...
--dbpass=STR...                   #Can also use STR... for any action to perform the check separately on several clusters/databases
                                  #(will return problem if any of them is wrong), or on master/slave
--output=STR                      #Output format (this doc only talks about nagios):
                                  #  - nagios (def):
                                  #     - compatible with NAGIOS (server network monitoring application)
                                  #     - one line with test name, then OK|WARNING|CRITICAL|UNKNOWN (and exit code 0 to 3 accordingly),
                                  #       followed by colon and description.
                                  #       UNKNOWN is when test cannot be performed, and WARNING|CRITICAL are set up according to
                                  #       -w|c VAL ("thresholds", depends on action). If warning = critical, turn off warnings.
                                  #     - can use option:
                                  #       --showperf=1|0: show performance at end of line (def: 1)
                                  #           --perflimit=NUM: limit --showperf to NUM items (def: 0, i.e. unlim)
                                  #           --showtime=1|0: show queries execution time (def: 1)
                                  #  - mrtg
                                  #     - compatible with MRTG (traffic load monitoring application)
                                  #     - four lines: NUM (usually main info), description (usually 0), blank and DATABASE
                                  #       (only when relevant)
                                  #     - can use option --mrtg=VAL to pass arguments to MRTG
                                  #     - usually don't issue warnings|critical
                                  #  - simple
                                  #     - like mrtg, but only first line (NUM)
                                  #     - can be followed by unit, e.g. --output=simple,MB
--action=STR...                   #Checks to perform, among:
                                  #Connections:
                                  #  - connection:
                                  #    Checks if server is up.
                                  #    CRITICAL + psql error description if no, OK + server version if yes.
                                  #  - backends:
                                  #    Checks if number of connections is more than threshold (NUM or % of max_connections) or if more
                                  #    than threshold connections are left (-NUM).
                                  #    Can only select --noidle connections.
                                  #    Look at --include below for how to specify per DATABASE.
                                  #  - pgbouncer_backends:
                                  #    Same but with pgBouncer (max_client_conn)
                                  #  - pgb_pool_maxwait|cl_active|waiting|sv_active|idle|used|tested|login:
                                  #    Checks if any pgBouncer pool has more than thresholds (see show pools in pgBouncer)
                                  #Space usage (look at --include below):
                                  #  - disk_space:
                                  #    Checks if any partition used by any data in the cluster (DATADIR, tablespaces, WAL dir, log dir)
                                  #    is using more than thresholds of memory (can use percentage, "MB", etc. units, and "and|or")
                                  #  - database|relation|index|table_size:
                                  #    Checks if any DATABASE|TABLE|INDEX is more than thresholds (can include "MB", etc.).
                                  #    Prints size in bytes (first line), and name (last line) of biggest one.
                                  #  - bloat:
                                  #    Checks if there are more than threshold (NUM (unit: 'KB', etc.) or % of TABLE size) dead rows
                                  #    in any TABLE|INDEX (only consider ones with > 10|15 pages)
                                  #  - wal_files:
                                  #    Checks if there are more than thresholds WAL files.
                                  #    Number of WAL files is usually comprised in a given range, unless there is a malfunction
                                  #    (long transaction, wrong archive_command, etc.), creating disk space usage risk.
                                  #Unusual state:
                                  #  - pgagent_jobs:
                                  #    Checks if all pgagent jobs since threshold (unit 's|m|h|d') have an exit code of 0
                                  #  - logfile:
                                  #    Checks if redirection to log file is happening correctly.
                                  #    Must provide log full path with --logfile=STR (can use "%Y|%m|%d|%H").
                                  #    Does not work if redirection to stderr without logging collector on.
                                  #  - commitratio:
                                  #    Checks if commit ratio (non-rollbacked transactions/transactions) is lower than thresholds.
                                  #  - disabled_triggers:
                                  #    Checks if number of disabled triggers is >= thresholds.
                                  #  - locks:
                                  #    Checks if number of locks held >= threshold.
                                  #  - txn_idle:
                                  #    Checks if there are more than thresholds idle current transactions (waiting for locks), and if
                                  #    any has lasted more than threshold (unit 's|m|h|d')
                                  #  - prepared_txns:
                                  #    Checks max. age of prepared transactions (not prepared statements).
                                  #Corruption:
                                  #  - sequence:
                                  #    Checks if sequence has been used more than threshold (%)
                                  #  - txn_wraparound:
                                  #    Checks if more than thresholds transactions have not been vacuumed, creating risk for
                                  #    xid wraparound. Wraparound happends every 2e9, so value should be e.g. 1.5e9
                                  #  - autovac_freeze:
                                  #    Checks if number of old transactions is more than threshold (%) of autovacuum_freeze_max_age
                                  #Performance (look at --include below):
                                  #  - query_runtime:
                                  #    Checks if queries specified by --queryname=STR (VIEW or FUNC) runs in more than time specified
                                  #  - txn|query_time:
                                  #    Same for running transactions|queries
                                  #  - hitratio:
                                  #    Checks if cache hit ratio is lower than thresholds.
                                  #  - last_[auto]analyze|vacuum:
                                  #    Checks if has been run (auto only checks autovacuum|analyze, other checks all) since threshold
                                  #    (in s|m|h|d, def: 1d for vacuum, 2d for analyze).
                                  #    Should exclude tables with no dead rows.
                                  #  - dbstats:
                                  #    For each DATABASE, print one line with backends (number of processes), commits|rollbacks
                                  #    (number since beginning), read|hit (number of blocks since beginning), ret|fetch|ins|upd|del
                                  #    (number of rows), dbname, idx..., seq...
                                  #    Cannot use alternate outputs.
                                  #Comparison:
                                  #  - same_schema:
                                  #    Compares two or more databases, schema-wise (not data-wise).
                                  #    If only one host: make a time-based comparaison: next time it will be executed, will compare
                                  #    with previous version.
                                  #    To do so, create a file at ./check_postgres.audit.port.PORT.db.DATABASE:
                                  #      - Use --replace to overwrite it.
                                  #      - can add .STR to the filename with suffix=STR
                                  #    Can exclude objects with:
                                  #      --filter=nouser|schema|table|view|index|sequence|constraint|trigger|perm|funcbody|
                                  #        function[=RGXSTR]
                                  #      --filter=noposition: don't compare columns positions
                                  #  - settings_checksum:
                                  #    Compares two settings (ENVVAR...) for a given user.
                                  #    First use -c 0 to get checksum, then do -w|c=CHECKSUM
                                  #  - pgbouncer_checksum:
                                  #    Same but with pgBouncer
                                  #  - timesync:
                                  #    Checks if local time diff >= threshold (in sec., should not be <5)
                                  #Standbies (can all test standby mode with --assume-standby|prod-mode):
                                  #  - hot_standby_delay:
                                  #    Checks if delay between current database (master) and slave >= threshold (number of WAL lines)
                                  #  - replicate_row:
                                  #    Checks that updates of a single row takes no more than threshold to replicate using replication.
                                  #    Should choose column to change (should pick one not likely to be changed by another process),
                                  #    with --repinfo=TABLE,PKEY,PKEY_VAL(to select row),COL_VAR,OLD_VAL,NEW_VAL
                                  #  - checkpoint:
                                  #    Checks if last checkpoint was run more than threshold ago (unit: 's|m|h|d').
                                  #    Meant to be run on a slave. Must supply --datadir DATADIR
                                  #Upgrades:
                                  #  - new_version_bc|cp|pg:
                                  #    Checks if new version of Bucardo|check_postgres|PostgreSQL is available.
                                  #    Only nagios. UNKNOWN if binary not here, CRITICAL is revision upgrade, WARNING is major upgrade.
                                  #  - version:
                                  #    Checks that server version is at least threshold
                                  #Custom:
                                  #  - custom_query:
                                  #    Checks a custom --query=STR, which returns a single column called "result", if any row value,
                                  #    depending on type of -w|c VAL:
                                  #      - NUM: >= NUM
                                  #      - NUM[KB,etc.]: >= NUM
                                  #      - STR's|m|h|d': older or same as STR
                                  #      - STR: same as STR
--in|exclude=STR...               #Limit the objects checked:
                                  #  - DATABASE: for backends, database_size, locks, query_time, txn_idle, txn_time
                                  #  - TABLE|INDEX: for bloat, index|table|relation_size, last_[auto]vacuum|analyze
                                  #  - FILESYSTEM: disk_space
                                  #include alone means "include only", but not alone means "include also" (to reinstate objects that
                                  #have been excluded with --exclude).
                                  #STR:
                                  #  - ending with . matches a schema
                                  #  - starting with ~ is a REGEXP (otherwise full VAR name)
--in|excludeuser=STR...           #Same for objects owned by ROLE_STR...
                                  #Works for relation|database_size, query|txn_time, last_[auto]vacuum|analyze.
-t NUM                            #Timeout (in secs, def: 10) after which returns UNKNOWN status, per cluster.
-v ...                            #Verbosity. Do several times to increase verbosity.
--debugoutput=LETTER...           #Prints also the psql output for a (all), c (critical), w (warning), o (ok), u (unknown)
--PGBINDIR=STR                    #psql directory (see man page on precautions to use)

pgbadger FILE[...]                #FILE... are the log files (stderr, csvlog (need Text::csv_xs module) or syslog format).
                                  #FILE can be - for stdin (not for csvlog).
                                  #Recognize compressed files from extensions .gz, .bz2 or .zip
                                  #Should put:
                                  #  - log_statement to 'none' (do not enable it)
                                  #  - log_min_duration_statement to 0
                                  #  - log_checkpoints|[dis]connections|lock_waits to 'on'
                                  #  - log_temp_files to 0
                                  #  - lc_messages to 'C'
                                  #If stderr:
                                  #  - log_line_prefix to '%t [%p]: [%l-1] user=%u,db=%d,host=%h,application=%a'
                                  #    Use pgbadger -p '%t ...' (same as above) -f stderr
                                  #Use latest release (3.3 is not)
                                  #Needs to put as much as possible in logs to get all graphs.
                                  #Can zoom it with shift button.
-f stderr|csvlog|syslog           #Def: stderr
                                  #For syslog:
                                  #  -i STR: Program name used as ident for syslog
-o FILE                           #Output file and format (among .html, .txt and .tsung). Def is output.html
                                  #Can also use -x text|html|tsung. Tsung is <sessions> tag for XML config file with most usual session.
-q                                #Quiet

-c HOST
-d DATABASE
-u USER
-N APPLICATION_NAME               #Filter for only this parameter (can be used several times)
-U USER                           #Filter for excluding USER (can be used several times)
-b|e DATE                         #Start|end time to be processed.
-l FILE                           #Only use logs starting from this log file.

-a NUM                            #Step (in min, def: 5) for the average number of query per second.
-s NUM                            #Number of sample queries (def: 3)
-t NUM                            #Number of top queries (def: 20)
--pie-limit NUM                   #Minimum percentage for pie chart slices

-S                                #Only analyze select queries
--exclude-query STR               #Exclude queries matching regexp STR
--exclude-file FILE               #Same but regexps are in FILE
--include-...                     #Inverse: include only.
-T                                #HTML <title> (def: "pgBadger")
-C                                #Remove /*comment*/ from queries
--disable-error|hourly|
type|session|temporary|
connection|query|lock|
autovacuum|checkpoint             #Remove a specific part in the report

-j|J NUM                          #Multiprocessing. Cannot be used with compressed files, csvlog or on Windows.
                                  #j is number of jobs/log file, J is number of log files in same time.

pgcluu_collectd DIR               #GUI that gives info on resource and space usage (similar to pgbadger, but gives some different stats).
                                  #pgcluu_collectd is the daemon collecting stats, pgcluu the tool creating reports
                                  #Should be run as the OS_USER owning the cluster, on a DIR owned by this OS_USER.
                                  #Good idea is to put inside DATADIR, with same permissions as other folders.
                                  #psql, sar (from package sysstat) should be installed. Their path should be given with -P|s STR if not
                                  #in /usr/bin/
                                  #Can find a sar file and several CSV files in DIR/
-d -h -p -U                       #Connection options
-D                                #Run as daemon. Can be killed with pgcluu_collectd -k
-i NUM                            #Frequency in seconds (def: 60)
-f FILE                           #PID FILE (def: /tmp/pgcluu_collectd.pid)
--stat-type all                   #Includes also system catalogs stats.
-m STR                            #Restrict data collection with a comma-separated list of metrics to perform (list can be found with
                                  #pgcluu_collectd --list-metric)
--pgbouncer-args=STR              #If pgbouncer (connection pooling utility) is used, arguments to pass to it (e.g. connection options)

pgcluu DIR                        #Creates report. DIR is the pgcluu_collectd DIR
                                  #Should be run as same OS_USER as pgcluu_collectd
                                  #sadf (from package sysstat) should be installed. Its path should be given with -s STR if not
                                  #in /usr/bin/
                                  #Can zoom in graphs
-b|e DATETIME                     #Begin|end time when to report.
-d DATABASE                       #Filter for only DATABASE (can be used several times)
-T TABLE                          #Same for TABLE (don't seem to work)
-t                                #Per table stats (don't seem to work)
-p DEVICE                         #Filter I/O info for only DEVICE (can be used several times)
-o DIR                            #DIR to create the HTML files (def: $PWD)

pg_top [NUM]                      #Show info about running PostgreSQL clients and servers in realtime, tables|indexes read|write.
                                  #psql is shown as "postgresql" command.
                                  #Must be run as the OS_USER owning the server.
                                  #If NUM, only show NUM first processes.
                                  #Accepts the following keystrokes:
                                  #  C-L: refresh
                                  #  R|X: switch with table|index stats
                                  #    t: show cumulative, not instant stats
                                  #  i: toggle display of idle processes
                                  #  k: kill
                                  #  o: change sorting
                                  #  Q: show current query
                                  #  u: show only specific user
                                  #Also available for smartphones/tablets.
-I                                #Do not display idle processes.
-o FIELD                          #Sorts according to FIELD
-z USER                           #Filter for only USER
-x [NUM]                          #Prints NUM first processes (def: "all"), then exits.
-c                                #Show command name instead of full command line
-s NUM                            #Delay in seconds (def: 5)
-r                                #Connects to a remote database. Needs to use -h -p -U -W connection options.

DETAILED MONITORING ==>           #Usually not needed, because there are higher-level monitoring tools:
                                  #  - ps auxww | grep ^postgres: see individual processes and description:
                                  #     - postgres master process
                                  #     - several master background processes: checkpoints, WAL, autovacuum, statistics collector
                                  #     - each client connection has one process with description showing CLIENT DATABASE HOST ACTIVITY
                                  #       (autoupdate can be turned on|off by ENVVAR update_process_title)
                                  #  - statistic collector:
                                  #     - daemon that fill in pg_stat* system views
                                  #     - used to collect statistics on activity
                                  #     - controlled by several ENVVAR BOOL:
                                  #        - track_activities (def: 'on') (COMMAND executed and time of execution)
                                  #           - track_activity_query_size (size of tracks in track_activities, def: 1024). Can only be
                                  #             set at server start.
                                  #        - track_counts (def: 'on') (general activity). Also allows explain.
                                  #        - track_io_timing (def: 'off') (I/O timing). Can be slow.
                                  #        - track_functions (def: 'none') (FUNC calls). Can also be 'pl' (PL/*) or 'all' (PL/*, SQL
                                  #          and C functions)
                                  #     - temp stats are stored in ENVVAR DATADIR/stats_temp_directory (def: 'pg_stat_tmp').
                                  #       Putting it in a RAM disk can improve performance.

pg_stat_activity                  #Server processes, with names, usernames, start|last time, addresses and command activity.
pg_stat_bgwriter                  #Background writer process's activity, e.g. for checkpoints.
pg_stat_replication               #WAL sender processes.
pg_stat_database                  #DATABASE, with number of servers/clients, transactions, temp files, tuples manipulated
                                  #(fetch|select|insert|update|delete), blocks read|hits, time spent on I/O read|write, deadlocks.
pg_stat_database_
conflicts                         #DATABASE, with query cancelled due to recovery on standby servers.
pg_stat[io]_[xact_]               #TABLE, with:
all|sys|user_tables               #  - not io: number of sequential|indexed scans (and tuples they fetched), tuples manipulated
                                  #    (insert|update|delete), number of rows (live|dead), [auto]vacuum|analyze activity
                                  #  - io: disk read|hits for all, index-only, TOAST and TOAST index
                                  #Can be for only system catalogs (sys) or not (user).
                                  #If xact_, take the current transaction into account.
pg_stat[io]_                      #INDEX, with:
all|sys|user_indexes              #  - not io: number of scan (with tuples fetched: bitmap + simple index scan, or simple index scan
                                  #    only)
                                  #  - io: index blocks read|hits (efficient if low read/hits %)
pg_statio_all_sequences           #SEQUENCE, with number of blocks read|hits
pg_stat_[xact_]user_              #FUNC, with number of calls and total time (only FUNC, or FUNC called by it too).
functions                         #ENVVAR track_functions must be on.

pg_stat_statements                #VIEW for all queries (query, time, number of rows, I/O).
                                  #Must put pg_stat_statements in ENVVAR shared_preload_libraries and use EXTENSION pg_stat_statements.
                                  #Can be reset with pg_stat_statements_reset().
                                  #Can use ENVVAR pg_stat_statementsmax (def: 1000)

OBJECT SIZES ==>                  #Can also look at pg_class.relpages (a page is 8KB)
pg_column_size(VAL)               #
pg_database_size
(OID|STR)                         #
pg_tablespace_size
(OID|STR)
pg_indexes_size(OID|STR)
pg_relation_size                  #Size in bytes. Can be a TABLE, INDEX or TOAST.
(OID|STR[, STR2])                 #STR2 can be 'main' (def), 'vm' of 'fsm'
pg_total_relation_size            #Same but with INDEX included
(OID|STR)
pg_table_size(OID|STR)            #Same with INDEX excluded and only for TABLE
pg_size_pretty(UINT)              #Convert a bytes size into human readable STR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         MAINTAINANCE          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


vacuum([full,][freeze,]           #Options:
[verbose, ] [analyze])            #  - analyze: do analyze ... too
[TABLE[( COL_VAR... )]]           #  - full:
                                  #     - make database size shrink to its minimum (while non-full keep same size, but let rows space
                                  #       be used by further writes)
                                  #     - require exclusive lock
                                  #     - slower and should be avoided
                                  #  - verbose: print details
                                  #Goal:
                                  #  - With MVCC deleted rows are actually hidden so that other transactions can read them.
                                  #    Once they are not used anymore, those are "dead rows".
                                  #    Delete them, so optimize disk storage and thus speed (garbage-collection)
                                  #  - update visibility map: 1 and 0 map for rows that are not committed yet.
                                  #    Used to optimize index-only scans queries.
                                  #  - protect losing very old data due to xid (xmin or xmax) wraparound:
                                  #     - rows that will be wraparound but are still not committed can be "frozen" (frozenxid at the
                                  #       TABLE-level), so that their xid is skipped in the wraparound
                                  #        - vacuum freeze rows that are older than ENVVAR vacuum_freeze_max_age transactions or
                                  #          2e9 - ENVVAR vacuum_freeze_min_age (def: 5e7) transactions
                                  #            - if problems of wraparound, increase vacuum_freeze_max_age or decrease
                                  #              vacuum_freeze_min_age
                                  #        - force vacuuming freeze rows if "freeze" is used (can violate MVCC)
                                  #        - TABLE with no "dead rows" will not be checked for frozenxid removal unless it is older
                                  #          than ENVVAR vacuum_freeze_table_age (def: 1.5e8)
                                  #     - 2e9 xid, so need to vacuum once every 1e9 transactions
                                  #     - if there are wraparound risk, the server will not accept any new command
                                  #Def. TABLE is all TABLE that current ROLE has permissions. Def. COL_VAR is all COL_VAR.
                                  #Cannot be done inside a transaction.
                                  #Requires share update exclusive lock, and makes database slower during vacuuming.

AUTOVACUUM ==>                    #Daemon automatically doing vacuum [analyze] on individual TABLE... according to how many rows are
                                  #changed. Better than doing it manually.
                                  #Enabled by ENVVAR autovacuum (def: 'on')
                                  #  - do vacuum every time:
                                  #     - a vacuum freeze is needed
                                  #       (can set its own ENVVAR autovacuum_freeze_* to override vacuum_freeze_*)
                                  #     - or ENVVAR autovacuum_vacuum_threshold (def: 50) +
                                  #       (number of inserted|deleted|updated rows) * ENVVAR autovacuum_vacuum_scale_factor (def: 0.2)
                                  #  - do vacuum analyze similary according to ENVVAR autovacuum_analyze_threshold (def: 50)
                                  #    and autovacuum_analyze_scale_factor (def: 0.1)
                                  #  - ENVVAR:
                                  #     - autovacuum_naptime INT: min. time (def: '1min') * number of databases before two launches
                                  #     - autovacuum_max_workers: numbers of DATABASEs that can be vacuumed at same time (def: 3)
                                  #  - doesn't vacuum TEMP
                                  #Can set autovacuum_* at the TABLE-level with create table ... with ( autovacuum_* = VAL )

analyze [verbose]                 #Fill in pg_statistic.
[TABLE[( COL_VAR... )]]           #Parameters are like for vacuum.
                                  #Requires only an access share lock.
                                  #FOREIGNTABLE are analyzed only when explicitly specified, and not always supported.

pg_statistic                      #Stats used by the planner to optimize queries (only indexed COL).
                                  #Not exact stats, because only a random sample of the rows is chosen for efficiency purpose.
                                  #Example of statistics: number of entries, distinct entries, histograms, size (number of disk blocks)
                                  #ENVVAR default_statistics_target (def: 100, from 0 to 10000) decides the sample size.
                                  #Can be set column-wise with alter table TABLE alter COL_VAR set statistics INT (-1 means default)
                                  #(same for FTABLE and materialized views).
                                  #  - starelid OID: of the TABLE. Refers to pg_class.oid
                                  #  - staatnum UINT: COL index. Refers to pg_attribute.attnum.
                                  #  - stainherit BOOL: all COLs have false + (if inherited COL) a row with true, with inherited
                                  #    version of the TABLE
                                  #  - stanullfrac FLOAT: percentage of null values
                                  #  - stawidth INT: average size of null values
                                  #  - stadistinct FLOAT: number of repetitions: -NUM if repetitions (UNIQUE/TOTAL, closer to 0 if
                                  #    many repetitions), +NUM means no repetitions (UNIQUE), 0 means unknown
                                  #  - for NUM statistics:
                                  #     - stakindNUM INT: subtype of the statistic (code number)
                                  #     - staopNUM OID: FUNC used. Refers to pg_operator.oid
                                  #     - stanumbersNUM FLOAT_ARR: stats as FLOAT, or null if COL is not numerical
                                  #     - stavaluesNUM ARR: stats as the same type as the COL
pg_stats                          #User-friendly version of pg_statistic


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      RECOVERY & BACKUPS       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DURABILITY ==>                    #PostgreSQL has durability: write operations will succeed even after a crash.
                                  #A crash will lead to:
                                  #  1) an inconsistent state if transaction was half written because of:
                                  #     - cache:
                                  #        - ENVVAR fsync (def: 'on'): don't use cache, i.e. flush WAL records as they are written.
                                  #          Can be disabled at server startup with postgres -F
                                  #        - ENVVAR wal_sync_method tells which OS command to use to flush cache (when using fsync):
                                  #          open_datasync, fdatasync, fsync, fsync_writethrough or open_sync (def: fdatasync). Best one
                                  #          can be determined with:
                                  #             pg_test_fsync -f FILE (FILE must be on same filesystem than DATADIR)
                                  #        - HDD cache on Linux:
                                  #           - queried with: hdparm -I /dev/FILE | grep "Write cache" (* at beginning if cache enabled)
                                  #           - disabled with: hdparm -W 0 /dev/FILE
                                  #        - Filesystem caching through journaling: disable it with mount options (e.g. data=writeback
                                  #          on ext3).
                                  #     - partial writes, controlled by ENVVAR full_page_writes (def: 'on').
                                  #       Putting to 'off' can improve performance.
                                  #  2) lost transactions (but no inconsistency) because of:
                                  #     - no synchronous commit: doesn't wait for the WAL to be written to report success of operation.
                                  #       ENVVAR synchronous_commit can be activated (local|remote_write|on (def)) or not ('off')
                                  #     - WAL is written after ENVVAR commit_delay (def: 0ms) in hope several operations will happen in
                                  #       the delay, which will then use a single flush. Best value is half the time of a single
                                  #       8kB write, as reported by last line of pg_test_fsync
                                  #       Only happens when min. ENVVAR commit_siblings (def: 5) transactions are currently opened.
                                  #     - ENVVAR wal_writer_delay (INT in ms, def: 200) too high: delay between each WAL writes
                                  #1) is dangerous: database could not be restared without a restore.
                                  #2) gives similar performance gain without that problem.
                                  #Putting DATADIR in a RAM disk is hardcore non-durable, and limits space to RAM space, but highly
                                  #efficient.

BACKUP VS HIGH                    #Backup strategy (saving data) is different from, but should be combined with high availability
AVAILABILITY ==>                  #strategy (quick restore of system if a node falls down)

BACKUPS ==>                       #Either:
                                  #  - pg_dump: more stable from one PostgreSQL version to another, or from one architecture to another.
                                  #  - WAL archiving: provides continuous archiving and PITR
                                  #Best: do both too (do a pg_dumpall after each base backup).
                                  #Good idea to compress backups
                                  #Backups methods are all hot backups.

pg_dump [DATABASE]                #Def DATABASE: PGDATABASE or OS_USER
                                  #Must be connect as superuser ROLE (for both backups and restores).
                                  #Unless "all", doesn't backup cluster-specific information. Remember then to restore them before
                                  #psql <FILE.
                                  #template1-specific information are backuped too: remember to restore from template0.
                                  #INTVAR ON_ERROR_STOP should be set.
                                  #Recommendations for restore:
                                  #  - psql --single can be used to make everything rollback if error
                                  #  - Make sure tablespace DIR are good
                                  #  - should analyze restored databases.
-F p|c|d|t                        #Format of the output:
                                  #  - p (def): sql command in text format
                                  #  - c: custom compressed format (must be restored with pg_restore)
                                  #  - d -f DIR: put in a directory DIR (must be restored with pg_restore) with one file by TABLE, and a
                                  #    table of content file
                                  #  - t -f DIR: same but use tar (not compressed). Limit of GB per table.
-a                                #Don't save SCHEMAs (save only data)
-s                                #Save only SCHEMAs (not data)
-n SREGEXP                        #Only VAR... in SCHEMA matching SREGEXP (see psql). Can be specified several times.
                                  #Caution: doesn't dump VAR of other SCHEMA2... that SCHEMA might depend on.
-N                                #Inverse: SCHEMA not matching SREGEXP
-t SREGEXP                        #Same for TABLE matching SREGEXP. Incompatible with -n or -N
-T SREGEXP                        #Inverse: TABLE not matching SREGEXP.
--exclude-table-data=
SREGEXP                           #Same but only exclude TABLE data, not definition
-b                                #Include large objects, which is the default unless -n, -s or -t is used
-o                                #Includes OID
-O                                #Don't save ROLE ownership (with -F p). Will be able to restore backups without being superuser, but
                                  #restorer will get ownership of all objects.
-x                                #Don't save privileges (grant/revoke)
--no-tablespaces                  #Save everything in same, default tablespace.
--no-unlogged-table-data          #Don't save content of unlogged TABLEs
--no-security-labels              #Don't save seLINUX labels (when using it)
-c                                #Put cleaning commands first (drop VAR before trying to create it)
-C                                #Create the database in the beginning (otherwise need to create it).
--[column-]inserts                #Use insert instead of copy (slower). With column, put COL_VAR names instead of using positions. Is
                                  #much slower.
--serializable-                   #Execute command in a serializable deferrable transaction (useful only when dumping to clone to
deferrable                        #another machine)
--disable-dollar-quoting          #Use standard SQL quoting ' ' instead of $$ $$
--disable-triggers                #Create commands (with -F p) which disable triggers on tables before dump is restored.
-E STR                            #Encoding (def: PGCLIENTENCODING)
-j NUM                            #Use several threads in same time (faster but uses more resources). Make sure ENVVAR max_connections
                                  #is high enough. Doesn't work if any exclusive lock is being requested meanwhile.
--lock-wait-timeout=NUM           #Wait for NUM seconds when asking for locks (def: unlim)
-d -h -p -U -w|W                  #Connection options (see psql)
--role=ROLE                       #ROLE when getting the dump data

pg_dumpall                        #Same as pg_dump, but for the whole cluster (except template0)
                                  #Use same options as pg_dump, except ones that are irrelevant, and selection options (like -T).
                                  #All databases must already exist.
-g                                #Only saves cluster-wide objects
-t                                #Only saves tablespaces
-r                                #Only saves ROLE

pg_restore [FILE]                 #Restore a backup produced by pg_dump -F (except for normal format, which should be restored with psql).
                                  #Def FILE is stdin. If no -d DATABASE is specified, print a text version of the restoration instead.
-a
-c
-C
-F c|d|t
-j NUM
-n SCHEMA
-O
-s
-S ROLE
-t SREGEXP
-x
--disable-triggers
--no-tablespaces
--no-security-labels
-d -h -p -U -w|W                  #Like pg_dump
-e                                #Sets ENVVAR exit_on_error
-1                                #Put in only one transaction
-L FILE                           #Restore only objects present in FILE (can be produced with pg_restore -l, then manipulated)

WAL ==>                           #Write-ahead logs. Logs that store every operation on the cluster before they are performed.
                                  #Goal:
                                  #  - when starting the server, if the last operations of the WAL have not been applied to the data
                                  #    (i.e. if the DATADIR data don't match the WAL), last operations are performed.
                                  #    Goal is to recover from crash.
                                  #  - can also be used for backups (see below)
                                  #Only WAL log are garanteed to be flushed (faster), not real operations, to ensure durability.
                                  #Structure:
                                  #  - Use 16MB segments. A log "line" is a "record".
                                  #    Every write on the cluster adds a new record on the last segment.
                                  #    New segments are automatically added and rotated.
                                  #    Are in DATADIR/pg_xlog/ but could be moved to a faster storage using symlinks.
                                  #Checkpoints are when operations recorded by WAL are flushed to the disk (as opposed to flushing
                                  #the WAL itself, which is controlled by fsync, etc.):
                                  #  - last one is where to restart in crash recovery
                                  #  - are performed at min. time between ENVVAR checkpoint_segments (number of segments, def 3) and
                                  #    checkpoint_timeout (time between checkpoints, def '5min').
                                  #    Increasing it will improve performance but increase crash recovery time (values between 32 to 256
                                  #    are often used for checkpoint_segments, and checkpoint_timeout can be one day)
                                  #    If ENVVAR checkpoint_warning (def: '30s') is less than the time between checkpoints, but more
                                  #    than checkpoint_timeout, a warning will be issued to the server log.
                                  #  - can also issue SQL command checkpoint to do it
                                  #  - flushes performed by a checkpoints are spread to the next checkpoint. The spread is
                                  #    ENVVAR checkpoint_completion_target, i.e. percentage of size spread for the free time allowed
                                  #    between checkpoints (def: 0.5, best is 0.9). Can go up to 0.9 will improve performance, but
                                  #    increase recovery time. Can only be set at server start.
pg_xlogdump [FILE]                #Show a WAL file in human readable format
                                  #When in DATADIR, can also use FILE FILE2 to go from FILE to FILE2
pg_resetxlog                      #To use when WAL is corrupted. Look at online doc
pg_controldata DATADIR            #Show debug info for WAL

WAL ARCHIVING /                   #  - goals:
ONLINE BACKUP ==>                 #     - "continuous archiving". Just need to archive new WAL segments.
                                  #     - point in time recovery (PITR): instead of single snapshots, can recover to specific time in
                                  #       past
                                  #  - enabled by ENVVAR wal_level to 'archive|hot_standby' (def: 'minimal') and archive_mode to 'on'
                                  #  - backing up WAL segments continuously, and DATADIR at regular times:
                                  #     - backup in different folders, let's call them DIR1 and DIR2
                                  #     - events since the last DATADIR since the crash are then restored thanks to the archived WAL
                                  #       segments
                                  #  - backup of WAL segments:
                                  #     - each time a new WAL segment is about to be erased (because of rotation), ENVVAR
                                  #       archive_command STR is fired to back it up:
                                  #        - can include %p for its path and %f for its filename, e.g.:
                                  #            '[ ! -f "DIR1/%f" ] && cp -a "%p" "DIR1/%f"'
                                  #        - should give exit code != 0 if error, so that it retries it
                                  #        - should not allow overwritting files
                                  #        - on Linux, use sh, not Bash
                                  #        - should be faster than the speed at which WAL segments appear
                                  #        - check permissions of server daemon to execute command
                                  #     - new segments are automatically made. But can be created manually by:
                                  #        - running pg_switch_xlog()
                                  #        - can be made every max. every ENVVAR archive_timeout (def: 0, in seconds). Should not be
                                  #          under 60s.
                                  #          Goal is for databases with low traffic: new segments are rarely created but still want to
                                  #          archive the little traffic.
                                  #     - archived WAL segments before the last "base backup" can be erased to save space, up until when
                                  #       we want to do a PITR
                                  #     - Can also use command pg_receivexlog -D DIR, which archive WAL segments to DIR, according to
                                  #       connection options (see psql) -d -h -p -U -w|W
                                  #  - backup of DATADIR ("base backups"):
                                  #     - manually, steps are:
                                  #        - connect to any DATABASE of the cluster and fire pg_start_backup(STR) as superuser.
                                  #          STR should be the number of this unique backup
                                  #           - creates a text file DATADIR/pg_xlog/FILE.*.backup, where FILE is the last WAL segment
                                  #             archived, with information used by the recovery process (e.g. last WAL segment of
                                  #             current DATADIR)
                                  #           - creates DATADIR/backup_label, which is a very similar file
                                  #        - backup DATADIR with any command (such as cp -a) to DIR2:
                                  #           - don't include postmaster.* nor pg_xlog/*
                                  #           - don't forget directories that might be elsewhere, e.g. tablespaces or directories using
                                  #             symlinks postgresql.conf, pg_hba.conf, pg_ident.conf could also be put somewhere else
                                  #             with ENVVAR config|hba|ident_file
                                  #           - copy might issue warnings because DATADIR files change on the fly (since cluster is
                                  #             running): it's fine
                                  #        - fire pg_stop_backup() as superuser.
                                  #           - removes backup_label file
                                  #        - utilities (not necessarily needed):
                                  #           - pg_is_in_backup(), pg_backup_start_time()
                                  #           - pg_start|stop_backup() returns the WAL segment as STR: to translate into filenames:
                                  #              - pg_xlogfile_name[_offset](STR)
                                  #              - pg_xlog_location_diff(STR, STR2)
                                  #     - pg_basebackup:
                                  #        - automate all this. Options are:
                                  #            -h -p -U -w      Connection options
                                  #            -D DIR           DIR to copy to. Can be - (stdout) for tar mode
                                  #            -F p|t           If p, do a simple copy. Files pointed by symlinks (such as tablespaces),
                                  #                             will be copied to the destination using the same absolute path
                                  #                             If t, will tar it under the filename base.tar (symlinks files are tar'd
                                  #                             too, under their abs. path)
                                  #                             Can also use -z to gzip it and -Z 1-9 for the compression level (def: 6)
                                  #            -R               Put a recovery.conf sample if the backup
                                  #            -X s             Includes first WAL segment in the backup (def: doesn't include any WAL
                                  #                             segment).
                                  #                             Will use two clients in max_wal_senders
                                  #            -l STR           Label used in backup_label (def: 'pg_basebackup base backup')
                                  #            -c fast|spread   Change the checkpoint_completion_target (def: spread)
                                  #            -P               Progress bar
                                  #        - use same privileges as streaming replication (max_wal_senders, replication privilege, etc.)
                                  #     - in all cases, need to be done regularly, e.g. with a cron script
                                  #        - more regular base backups require more storage, but make faster recoveries
                                  #  - recovery:
                                  #     - steps:
                                  #        - stop server
                                  #        - replace DATADIR by DIR2, but keeping the WAL segments:
                                  #           - move DATADIR/* to temporary DIR3 (including tablespaces, etc., see above)
                                  #           - copy DIR2/* to DATADIR (including tablespaces, etc., see above), with right ownership
                                  #             and permissions
                                  #           - replace DATADIR/pg_xlog/* by DIR3/pg_xlog/*, with right ownership|permissions
                                  #             (in case some WAL segments were not archived but still present in DATADIR)
                                  #        - copy archived WAL segments from DIR1 to DATADIR:
                                  #           - create DATADIR/recovery.conf (its presence instructs server start to be in recovery mode)
                                  #              - can copy template SHAREDIR/recovery.conf.sample
                                  #              - must set variables:
                                  #                 - restore_command STR: just like archive_command, but to copy the WAL segments from
                                  #                   DIR1 to DATADIR/pg_xlog/
                                  #                   Should overwrite existing ones.
                                  #                   Will emit warnings because try to copy files that might not exist.
                                  #                   Ex: 'cp -a "DIR1/%f" "%p"'
                                  #              - can recover to a specific time (PITR):
                                  #                 - by setting (in recovery.conf) any of:
                                  #                    - recovery_target_time TIMESTAMP
                                  #                    - recovery_target_xid STR: the transaction ID
                                  #                    - recovery_target_name STR: STR is a restore point, which must have been
                                  #                      previously created by pg_create_restore_point(STR)
                                  #                 - time must be after the creation time of DIR2/*
                                  #                 - recover just before|after according to variable (in recovery.conf)
                                  #                   recovery_target_inclusive (def: true, i.e. after)
                                  #                 - will stop (unless variable pause_at_recovery_target is set to false or if
                                  #                   hot_standby mode), so we can check if the state is fine. Can resume by firing
                                  #                   pg_xlog_replay_resume()
                                  #                 - must remove WAL segments that have been archived after that time, to restart
                                  #                   archiving them normally
                                  #           - start the server in single user mode
                                  #           - recovery will happen: when done, recovery.conf will be recovery.done
                                  #        - make sure everything is ok, then restart the server normally
                                  #     - timelines:
                                  #        - each time a recovery suceeds, it increments the first number of the WAL segment files, e.g
                                  #          00...00100..0034 to 00...00200..0034
                                  #        - the first number is the timeline ID. Goal it that following WAL archives doesn't overwrite
                                  #          previous WAL archives created between the recovery and the crash, in case we want to come
                                  #          back to that point.
                                  #        - by default, recover to the timeline that was used during the base backup, but can specify
                                  #          recovery_target_timeline STR with "latest" in recovery.conf, or with the specified
                                  #          timeline ID

HIGH AVAILABILITY ==>             #Can use:
                                  #  - log shipping: master ships WAL to a DIR, then standby gets it from DIR
                                  #  - streaming replication: ships directly WAL from master to slave. Probably better.
                                  #     - async. (better performance) or sync. (better availability)
                                  #Any standby can also be a hot standby (makes more sense for a streaming replication one) to improve
                                  #load balancing (watch out precautions)

LOG SHIPPING /                    #  - Goal: not backup (but can be combined with backup) but to maintain a copy of the master server, so
WARM STANDBY ==>                  #    a switchover to the standby can happen quickly if there is a problem with the master
                                  #  - Idea: the standby machine keeps on reading the WAL archive (master must do WAL archiving) and
                                  #    applies them right away.
                                  #  - How:
                                  #     - start a cluster with a base backup, with a recovery.conf file in it.
                                  #       recovery.conf variable standby_mode should be on.
                                  #     - will continuously call recovery.conf variable restore_command (same format as archive_command)
                                  #       to copy WAL archive DIR1 to its own pg_xlog/, e.g. 'cp -a "DIR1/%f" "%p"'
                                  #        - will show error messages for next WAL segment, and .history file -> it's normal
                                  #     - Put recovery_target_timeline to "latest" (to stay sync. with the timeline chosen by the master)
                                  #     - If don't want to use DIR1 for backup purpose, clean every WAL archive that has been copied by
                                  #       setting variable archive_cleanup_command:
                                  #        - %r is the filename (not path) of the first WAL file to keep
                                  #        - pg_archivecleanup is a command line often used:
                                  #            - pg_archivecleanup "DIR" "%r"
                                  #            - flags are -d (verbose), -x STR (use it if WAL segments have this extension,
                                  #              e.g. -x .gz) and -n (dry-run)
                                  #     - Can stop standby mode and become a master:
                                  #        - by creating file specified by recovery.conf variable trigger_file, or firing
                                  #          pg_ctl promote.
                                  #           - change recovery.conf to recovery.done
                                  #        - never two masters at same time:
                                  #           - should turn off former master shortly before
                                  #           - before restarting, former master should become the new slave
                                  #        - good idea to prepare already the slave to become a master by setting up WAL archiving, etc.
                                  #        - recovery_end_command STR will be fired (%r is the same as archive_cleanup_command)
                                  #        - automatic failover is only possible using external packages.
                                  #  - Precautions:
                                  #     - DIR1 should not be on the master machine.
                                  #     - WAL segments are sent async (don't wait for shipping to execute), so there's a window for data
                                  #       loss, that can be reduce by lowering archive_timeout
                                  #     - standby and master should have similar config:
                                  #        - logically, e.g. symlinks (including table spaces)
                                  #        - software-wise
                                  #        - hardware wise. CPU architecture must be same.
                                  #     - switchover is manual: should have own mechanism to notify when the primary server is down, and
                                  #       to automatically failover

ASYNC. STREAMING                  #  - Goal: like log shipping, but smaller delay between master and slave state (still small one)
REPLICATION ==>                   #  - Idea: like log shipping, but doesn't use WAL archive DIR1 (nor restore_command,
                                  #    recovery_target_timeline, archive_cleanup_command), but directly get WAL from the server (over
                                  #    TCP connection).
                                  #  - How:
                                  #     - Set recovery.conf variable primary_conninfo (as "VAR=VAL ...", using libpq variables) for how
                                  #       to connect to the master.
                                  #     - Same as above for recovery_target_timeline
                                  #     - Must have privileges:
                                  #        - to connect to "replication" virtual DATABASE (in pg_hba.conf)
                                  #        - replication and login privileges (better to create a ROLE than to set up as superuser).
                                  #        - max number of connections is ENVVAR max_wal_senders (def: 0).
                                  #     - slave must keep up with the pace:
                                  #        - can increase ENVVAR wal_keep_segments on the master (number of segments that should be
                                  #          recycled but are kept, def: 0)
                                  #        - can use log shipping in parallel.
                                  #        - If fall behind, can redo a base backup.
                                  #        - Can tell by:
                                  #           - comparing pg_current_xlog_[insert_]location() on the master (current WAL),
                                  #             pg_last_xlog_receive|replay_location|timestamp() on the slave
                                  #           - use pg_stat_replication system view
                                  #        - Connection waits only for ENVVAR wal_receiver_timeout (def:60s) from slave to master, and
                                  #          wal_sender_timeout (def: 0, turned off) from master to slave.
                                  #     - Cascading replication:
                                  #        - Just use replication from downstream to upstream servers.
                                  #        - Goal: to reduce cost for master, but introduces more delay for other standbies.
                                  #        - sync. replication doesn't work for downstream servers.

SYNC. STREAMING                   #  - Goal: like async. streaming replication, but reduces data loss window to nothing (at expense of
REPLICATION ==>                   #    performance): every write transaction returns only after WAL is sent to standby ("2-safe
                                  #    replication").
                                  #  - How:
                                  #     - Master must set ENVVAR synchronous_standby_names with standbies:
                                  #        - comma-separated-list, only picks the first connected in the list
                                  #        - names must match application_name in primary_conninfo
                                  #           - can be * for any application_name
                                  #        - def is walreceiver
                                  #     - Actually waits according to ENVVAR synchronous_commit:
                                  #        - on (received and flushed to disk on slave), remote_write (only received) or local|off
                                  #          (nothing).
                                  #        - Makes it possible to set synchronous_commit specific values for databases, users or
                                  #          transactions, for different durability/performance tradeoff.
                                  #  - Precautions:
                                  #     - If last standby loses connection, will wait forever
                                  #        - if last standby needs to be down, must first put synchronous_commit to off in a
                                  #          pg_start|stop_backup() block

HOT STANDBY ==>                   #  - Goal: use a standby server (streaming replication or log shipping) for readonly queries (load
                                  #    balancing).
                                  #  - How:
                                  #     - Must set ENVVAR hot_standby to on on standby and ENVVAR wal_level to hot_standby for master
                                  #     - Must start with a new base backup (if switching from non hot standby to hot standby)
                                  #  - Precautions:
                                  #     - Watch out for the delay between master write and ability to read it in standby. If an arriving
                                  #       WAL archive is conflicting with a current query (e.g. if master dropped a table while standby
                                  #       is querying it), it will wait ENVVAR max_standby_archive|streaming_delay (for streaming
                                  #       replication mode or not) (def: 30000 ms, -1 for unlim) then cancel
                                  #        - low value provokes more cancels, but standby and master are more in sync: good if goal is
                                  #          more High availability, bad if goal is more load balancing
                                  #        - could be set at approx max time of queries.
                                  #        - Can also increase ENVVAR vacuum_defer_cleanup_age if lot of vacuum-related conflicts.
                                  #          Cancels can be seen on system view pg_stat_database_conflicts.
                                  #     - Hot standby stops at startup when standby tries to catch up servers WAL segments. During that
                                  #       period, there might be seemingly weird behavior.
                                  #       pg_is_in_recovery() will return true.
                                  #     - those ENVVAR must be superior or equal on the standby than the master: max_connections,
                                  #       max_prepared_transactions, max_locks_per_transaction
                                  #     - advisory locks can't be shared between master and slave
                                  #     - isolation level serializable not available


repmgr                            #Must have rsync, pg_ctl and pg_config in $PATH. Must be installed from source (see online doc).
                                  #Actions can be:
                                  #  - standby clone NODE_VAR: make it possible to put as standby (do a base backup).
                                  #  - master|standby register: put as master|standby (master should be done first)
                                  #  - standby promote|follow: in case of a failover, automatical new master to promoted, and followers
                                  #    will replicate from it. Automatical or manual???
-d -h -p -U                       #Connection options
-D DATADIR                        #Cluster to target
-f DIR                            #repmgr.conf DIR (def: same as executable).
                                  #repmgr has three lines: cluster STR, node number INT, libpq_conninfo STR
--force                           #Do with standby clone when a master is up again after having being down, to get back the changes
                                  #since then from the new master.

repmgrd                           #Daemon doing automatic failover.
                                  #Needs to do all the standby register first.
-f DIR                            #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     CLUSTERING AND POOLING    :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pgbouncer PGBFILE                 #Does connection pooling: maintains a single connection to be reused for each DATABASE+USER pair.
                                  #Goal is to lower connection time.
                                  #Can be much faster when connection time is important (small sessions).
                                  #Disadvantages:
                                  #  - requires more fds, so might needs lower max_client_conn than PostgreSQL's server
                                  #  - hides original host+port information in logs (all traffic goes though pgbouncer)
                                  #  - cannot implement all authentication method
                                  #Act as layer of abstraction: connection to pgbouncer DATABASE can be redirected to DATABASE of
                                  #other names, or of different clusters/machines.
                                  #Should be run as same OS_USER as the server.
                                  #Should be installed:
                                  #  - on database server, if lot of web servers connect to it (because it is the center of all
                                  #    connections that should be pooled)
                                  #  - on web server, if connects to lot of database servers
                                  #Each pool (DATABASE+USER) has:
                                  #  - clients (to pgBouncer) and servers (connection of pgBouncer to PostgreSQL).
                                  #    1 client = 0|1 server: 0 server when client just connected (session pool_mode) or is not issuing
                                  #    a request (transaction pool_mode)
                                  #  - cl_active (from show pool, see below) is number of clients on a pool.
                                  #    If more than pool size, clients will be cl_waiting instead until cl_active is lower.
                                  #    Pool size is determined by:
                                  #      - [default_]pool_size: cluster-wide and database-specific pool size.
                                  #      - min_pool_size: at first client, opens at least NUM idle servers, to make it more responsive
                                  #        in the first requests.
                                  #      - reserver_pool_size: extra pool size used for clients waiting (cl_waiting) for more than
                                  #        reserve_pool_timeout
                                  #  - max_client_conn is max number of cl_active for all pools together.
                                  #    When reached, clients don't wait, they crash.
                                  #  - Optimize limits:
                                  #     - number of file descriptors used = 2 + 1 per client (max_client_conn) + 1 per server
                                  #       (pool_size * number of users * number of databases)
                                  #         - pool_size should be at max without creating more servers than PostgreSQL's max_connections
                                  #         - max_client_conn should be max number of servers + expected number of idle clients
                                  #         - total should not exceed max number of file descriptors
                                  #Pool mode:
                                  #  - when server is not used anymore, returns back to pool (sv_active -> sv_idle|used)
                                  #  - it is done according to pool_mode, either after each session (def), transaction or query (avoid).
                                  #    transaction doesn't support session states, i.e. [re]set ENVVAR, listen|notify, with hold CURSOR,
                                  #    PREP, load, user-defined volatile FUNC
                                  #    Use transaction if lot of idle times in sessions, or if long queries.
                                  #Look at check_postgres for monitoring.
-d                                #Run in background.
                                  #Needs to give pidfile = FILE in [pgbouncer] in PGBFILE (FILE is created with the PID)
-R                                #Online restart: closes current running pgbouncer and inherits its connections without interrupting
                                  #anything (current running pgbouncer will be closed). Useful to upgrade without interrupting anything.
-q                                #Quiet mode
-v                                #Verbosity (can do several times)

PGBFILE ==>                       #Usually called pgbouncer.ini
                                  #Has two parts, each started with [databases], then [pgbouncer] on a single line, and separated by
                                  #blank line.
                                  #Each part has VAR = VAL ... (STR don't have any quoting)

                                  #[databases]:
DATABASE                          #STR, libpq string, but with only:
                                  #  - dbname: def. is same as DATABASE
                                  #  - host: def. is using Unix socket.
                                  #  - port: def. 5432
                                  #  - user: def. is same user
                                  #  - password
                                  #When client asks PgBouncer to connect on DATABASE, will use STR to connect to server.
                                  #Can also specify:
                                  #  - pool_size: Per-database pool size
                                  #  - connect_query: query done at connection start.
                                  #    For connection end (not DATABASE-specific), use server_reset_query (def: "discard all")
                                  #Can use * DATABASE to mean "any other database"

                                  #[pgbouncer]:
listen_port                       #Proxy port (which client should connect to in order to reach server)
listen_addr                       #Same for proxy address. Can be *
unix_socket_dir|mode|             #Like PostgreSQL ENVVAR unix_socket_directories|permissions|group
group                             #Def. are /tmp, 0777 and ""
auth_type                         #Similar as in pg_hba.conf. Can be md5 (def), plain (like password in pg_hba.conf), trust or any.
                                  #any is like trust, except that users are not even remembered which means:
                                  #  - all DATABASE must specify user=VAL in their libpq STR
                                  #  - control with admin_users is not effective
auth_file                         #"USER" "PASSWORD" ...
                                  #Necessary (only USER with a line in it will be able to connect)
admin_users                       #USER... (pgBouncer USER) allowed to connect to pgBouncer and issue statements on it.
stats_users                       #Same but can only use show ENVVAR (except show fds)

logfile                           #Redirect stderr to FILE, without stopping stderr
log_[dis]connections              #Logs them (def: 1)
log_pooler_errors                 #Logs errors sent to client (def: 1)
stats_period                      #Logs stats every NUM seconds (def: 60)
syslog[_ident|facility]           #

pool_mode                         #See above (def: session)
                                  #If transaction, server_reset_query should be ""
max_client_conn                   #(def: 100)
default_pool_size                 #(def: 20)
min_pool_size                     #(def: 0)
reserve_pool_size|
timeout                           #(def: 0 and 5 seconds)

server_check_delay                #After NUM seconds (def: 30), goes from sv_idle to sv_used, i.e. run sanity check query on server
                                  #connections when going from idle to active.
server_lifetime                   #Closes server connections opened for more than NUM seconds (def: 3600)
server_idle_timeout               #Same but for idle server connections (def: 600)
server_connect_timeout            #Same but for connecting time (def: 15)
client_login_timeout              #If client connects but does not login before NUM seconds (def: 60), drops it.
autodb_idle_timeout               #Closes pools (using * in [databases]) that have been unused for more than NUM seconds (def: 3600)

server_login_retry                #Waits NUM seconds (def: 15) after each failed authentification.
dns_max_ttl                       #DNS (host resolution) cache time in seconds (def: 15)
max_packet_size                   #Max packet size between PostgreSQL and pgBouncer, in bytes (def: 2GB)

server_round_robin                #If 0 (def), reuse connections in LIFO manner. If 1, in a random manner (better if TCP round-robin
                                  #distributing load between servers)

pgbouncer DATABASE ==>            #Virtual DATABASE, where only show ENVVAR is allowed, with some commands:
reload                            #Reload PGBFILE (can also use SIGHUP)
pause [DATABASE]                  #Safest way to stop pgBouncer: wait for clients to complete. Can also use SIGINT (CTRL-C)
shutdown                          #Like pause, but exit pgBouncer completely. Can also use SIGTERM
suspend                           #Drop clients, but flush buffers
kill DATABASE                     #Least saft way to stop DATABASE
resume [DATABASE]                 #Resume from pause|resume

                                  #Can also show the following ENVVAR:
lists                             #Snapshot of all other info
databases                         #DATABASE: connection+pool_size
stats                             #DATABASE:
servers                           #
clients                           #
pools                             #
fds                               #File descriptors
users                             #All users in auth_file
config                            #PGBFILE info
dns_hosts
dns_zones                         #Host resolution


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            TESTING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PGTAP UNITS ==>                  ##Needs to be installed.
                                 ##Tested database must enable PL/PGSQL.
                                 ##Idea is to create assertions (see below for list) in a separate test unit SQL file. Usually put in a
                                 ##tests/ folder
                                 ##Usually do test manipulation on the database between assertions, so in the end should rollback:
                                 ##  - put in a transaction block that rollbacks.
                                 ##  - put INTVAR ON_ERROR_ROLLBACK and ON_ERROR_STOP
                                 ##For best formatting:
                                 ##  - put INTVAR ECHO to nothing, QUIET to 1
                                 ##  - \pset format unaligned, pager on, t true
                                 ##  - if using psql, use -X to bypass init files
                                 ##Each assertion produce a TAP format line (common text format for test results, that is runned and
                                 ##parsed by "test harnesses")
                                 ##Postgres extension 'pgtap'
select plan(NUM)                 ##Starts a unit with exactly NUM assertions. If not sure use select * from no_plan() instead (avoid).
select * from finish()           ##Completes a test unit

PGTAP XUNIT TESTS ==>            ##Same as normal pgTap units (same output and assertions) but:
                                 ##  - use functions, not files. Functions just execute the assertions functions, and returns them as a
                                 ##    STR_ARR:
                                 ##      create or replace function SCHEMA.FUNC()
                                 ##      returns setof text as
                                 ##      $$begin
                                 ##          return next ASSERTION_FUNC(ARGS)...
                                 ##        end$$
                                 ##      language pgsql
                                 ##  - don't need anything to start|finish the unit but needs to run the unit with:
                                 ##      select * from runtests( [STR][, STR2] )
                                 ##    where STR is SCHEMA, and STR2 is a regular expression to match the FUNC to choose.
                                 ##    Since STR and STR2 are both optional (def: 'public' and '^test') cast to name if using only STR
                                 ##     - Use transaction on all tests as a whole, then on each individual test (including fixture
                                 ##       functions)
                                 ##  - can use fixture functions (same definition as above), special FUNC used at specific moments.
                                 ##    Each set run in alphabetical order. FUNC name must start with the name of the phase:
                                 ##     - startup, once before all tests
                                 ##     - setup, once before each test
                                 ##     - teardown|shutdown: same as setup|startup, but after tests
                                 ##    Watch out to exclude the fixture functions in runtests(). Good practice is to use a SCHEMA, and
                                 ##    '^test' for tests.

pg_prove [DIR|FILE...]           ##Test harness for pgTap (executes tests).
                                 ##Like doing psql, but better output, and don't need to set \pset and INTVAR
                                 ##(but should still put in a transaction block)
                                 ##For DIR, recognize test files according to extension ".pg"
                                 ##Same for xUnit-style tests, but:
                                 ##  - don't use FILE... but -R, which fires runtests()
                                 ##  - use -s SCHEMA and -x REGEXP to specify arguments to runtests(STR, STR2)
-d -U -h -p                      ##Connection options
-P|S VAR=VAL                     ##Does \[p]set VAR VAL
--ext STR                        ##With DIR, use extension STR, not ".pg"
-r                               ##With DIR, recursive

--shuffle|reverse                ##Modify test execution order
--state STR,...                  ##Which tests to run:
                                 ##  - last: same as last time
                                 ##  - all: in normal order
                                 ##  - failed|passed: only ones that failed|passed
                                 ##  - hot: most recent failure first
                                 ##  - todo: only test with todos
                                 ##  - slow|fast: in speed order
                                 ##  - new|old: in mtime order
                                 ##  - fresh: only the ones that have been modified
                                 ##  - save: save state in a file ./.pg_prove (must be done first to be able to do --state next time)

-f                               ##Print failed tests
-q|Q                             ##Quiet, or even quieter
--verbose                        ##Outputs full TAP format
--no-comments                    ##Don't show diag() messages
--directives                     ##Only show skip() messages and todo() tests
-D                               ##Dry-run
-t                               ##Show time of execution of each test

-j NUM                           ##Number of jobs in parallel
-b FILE                          ##PSQL location

PGTAP ASSERTIONS ==>             ##They are FUNC that all come with an optional (but recommended) last arg STR for error message
                                 ##(def: '').
                                 ##They print the result of the assertion as a STR

GENERAL ASSERTS ==>              ##
ok( BOOL )                       ##Asserts that BOOL is true.
                                 ##Prefer other function when possible, e.g. is(VAL, VAL2) over ok(VAL = VAL2), because more descriptive
                                 ##output.
pass|fail()                      ##Like ok( true|false ) (avoid them)
is[nt]( VAL, VAL2 )              ## is [not] distinct from
[i]matches(VAL,VAL2)             ## ~[*]
doesnt_[i]match(...)             ## !~[*]
[un][i]alike(VAL,VAL2)           ## [not] [a]like
cmp_ok(VAL,OP_STR,VL2)           ## VAL OP VAL2
isa_ok(VAL,STR)                  ## pg_type(VAL) = STR

SQL QUERIES RESULTS==>           ##Asserts results of sql select ...:
SQL_STR                          #Means SQL statement STR (either as is, or name of a PREP (recommended)).
                                 ##A PREP with arguments needs to be written as is, i.e. not 'PREP' but 'execute PREP(ARGS)'
throws_ok(SQL_STR                ##Asserts that SQL_STR throws an exception, with errcode STR2 and errmessage STR3 (each can be null
[, STR2 [, STR3]])               ##(def) for all errcode|errmessage)
lives_ok(SQL_STR)                ##Inverse of throws_ok(SQL_STR)
throws_[i]like|
matching(SQL_STR
[, STR2])                        ##Same as throws_ok(SQL_STR, null, STR2), but STR2 needs to match with [i]like or ~[*], not = <>
performs_ok(SQL_STR,
INT)                             ##Asserts that SQL_STR performs in less than INT ms.
results_eq|ne                    ##Asserts that both queries compare with = <>
(SQL_STR|CURSOR,                 ##Is row-wise, so make sure they are ordered the same.
SQL_STR2|ARR|CURSOR2)            ##CURSOR iterates over all rows (must be STR casted as refcursor)
                                 ##ARR represents a single-column (values ... could also be used for several columns)
bag|set_eq|ne(SQL_STR,           ##Same but compares not row-wise, but the whole set of values together (so order doesn't matter).
SQL_STR2|ARR)                    ##set removes duplicates, bag doesn't.
bag|set_has[nt]
( SQL_STR, SQL_STR2)             ##Same as bag|set_eq|ne, but only for subset, i.e. asserts that SQL_STR includes SQL_STR2
is[nt]_empty(SQL_STR)            ##Asserts number of rows = <> 0
row_eq(SQL_STR, CTYPE)           ##Same as results_eq, but for a single row. CTYPE can't be a CTYPE_LIT (using row())

SCHEMA CONFORMANCE ==>           ##Asserts that current variables are exactly this.
schemas|tablespaces|
roles|languages|
casts_are(STR_ARR)               ##
tables|views|sequences
|functions|opclasses|
types|domains|enums|
operators_are                    ##Can restrict to a SCHEMA, otherwise use search_path.
([SCHEMA_STR],STR_ARR)           ##Functions are only the name, without arguments.
columns|indexes|
triggers|rules_are(
[SCHEMA_STR],
TABLE_STR, STR_ARR)              ##

SCHEMA EXISTENCE ==>             ##Asserts that variable exist.
has[nt]_schema|role|
language(STR)                    ##
has[nt]_table|view|
sequence|foreign_table
|type|composite|domain
|enum|opclass|relation
( [SCHEMA_STR, ]STR )            ##relation is table|view|sequence|ctype
has[nt]_index
( [SCHEMA_STR, ]
TABLE_STR, INDEX_STR,
[COL_STR[_ARR]] )                ##COL_STR[_ARR] not with hasnt.
has[nt]_trigger|rule
( [SCHEMA_STR, ]
TABLE_STR, STR )                 ##
has[nt]_function
( [SCHEMA_STR, ]
FUNC_STR
[, ARGSTYPE_STR_ARR] )           ##
has[nt]_cast(TYPE_STR,
TYPE2_STR[,SCHEMA_STR]
[, FUNC_STR] )                   ##
has_operator(TYPE_STR
[, SCHEMA_STR],OP_STR,
TYPE2_STR
[, RETURNTYPE_STR] )             ##
has_left|rightop
( [SCHEMA_STR],OP_STR,
TYPE_STR
[, RETURNTYPE_STR] )             ##
has[nt]_tablespace
(STR[, STR2])                    ##Can use a STR2 as tablespace location (not with hasnt).

COL ATTRIBUTES ==>               ##
has[nt]_column(
[SCHEMA_STR,]TABLESTR,
COL_STR)                         ##
col_not|is_null|
has[nt]_default|pk|fk|
unique|check(
[SCHEMA_STR,]TABLESTR,
COL_[ARR_]STR)                   ##pk is primary key, fk foreign key constraint.
is_clustered|
index_is_unique|
primary( [SCHEMA_STR,]
[TABLE_STR,]INDEX_STR)           ##Asserts properties for an index COL
has[nt]_unique|check|
pk|fk( [SCHEMA_STR,]
TABL_STR)                        ##TABLE has at least those constraints.
col_default_is(
[SCHEMA_STR,]TABLESTR,
COL_STR, VAL)                    ##
fk_ok( [SCHEMA_STR, ]
TABLE_STR,
COL[_ARR]_STR,
TABLE2_STR,
COL2[_ARR]_STR )                 ##Asserts that COL references COL2

TYPES ==>                        ##
col_type_is(
[SCHEMA_STR,]TABLESTR,
COL_STR,[SCHEMA2_STR,]
TYPE_STR)                        ##
index_is_type(
[SCHEMA_STR, ]
[TABLE_STR,]INDEX_STR,
TYPE_STR)                        ##TYPE_STR is 'btree', 'hash', etc.
domain_type_is[nt](
[SCHEMA_STR, ]
DOMAIN_STR,
[,SCHEMA2_STR]TYP_STR)           ##TYPE_STR is 'btree', 'hash', etc.
enum_has_labels(
[SCHEMA_STR,]ENUM_STR,           ##
VAL_STR_ARR)

FUNCTIONS ==>                    ##
can( [SCHEMA_STR, ]
FUNC_ARR_STR )                   ##Same as has_function, but without ARGSTYPE_STR_ARR, and with FUNC_ARR
function_lang_is(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
LANGUAGE_STR )                   ##
function_returns(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
TYPE_STR )                       ##
volatility_is(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
STR )                            ##
function_is_definer|
strict|aggregate(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR] )           ##
cast_context_is(
TYPE_STR,TYP2_STR,STR)           ##STR can be 'implicit', 'assignment', 'explicit'
trigger_is
( [SCHEMA_STR, ]
TABLE_STR, TFUNC_STR
[,SCHEMA2_STR]FUNCSTR)           ##Asserts that TFUNC executes FUNC
rule_is_instead(
[SCHEMA_STR, ]
TABLE_STR, RULE_STR)             ##
rule_is_on(
[SCHEMA_STR, ]
TABLE_STR, RULE_STR,
EVENT_STR )                      ##

ROLES AND SECURITY ==>
db|schema|tablespace|
language_owner_is
(STR, ROLE_STR)                  ##
table|view|sequence|
composite|
foreing_table|relation
|opclass|type_owner_is
([SCHEMA_STR, ]STR,
ROLE_STR)                        ##
index_owner_is(
[SCHEMA_STR,]TABL_STR,
INDEX_STR, ROLE_STR)             ##
function_owner_is(
[SCHEMA_STR,]FUNC_STR,
ARGSTYPE_STR_ARR,
ROLE_STR)                        ##
*_privs_are(...,                 ##Same as *_owner_is(...), but asserts PRIVILEGE[_ARR] for ROLE. Differences:
PRIVILEGE_[ARR_]STR)             ##  - db -> database
                                 ##  - no relation, view, composite, foreign_table, index, opclass, type
                                 ##  - there is also:
                                 ##     - column*( ..., TABLE_STR, COL_STR, ... ) and any_column*( ..., TABLE_STR )
                                 ##     - fdw|server( FDW|FSERVER_STR, ... )

is[nt]_superuser(ROLE)           ##
is_member_of(ROLE_STR,
ROLE2[_ARR]_STR)                 ##
language_is_trusted(
LANGUAGE_STR)

UTILITIES ==>                    ##
diag( STR... )                   ##Returns STR (separated by newline), in front of a #, to add comments to the output.
skip(STR[, INT])                 ##Skip the next INT (def: 1) PGTAP functions, with explanation STR.
                                 ##To put in a conditional branch (e.g. case when ...) when a test might provoke the whole unit test
                                 ##to throw an exception (language or function not available).
collect_tap(                     ##Do several PGTAP assertions functions at once.
ASSERT_FUNC(...)... )            ##Useful when can't be put several COMMAND; but only one, for example in a SQL case when
todo(STR[, INT])                 ##Same, but instead of skipping, just declares that tests are expected to fail, because still on the
                                 ##todo list.
todo_start|end(STR)              ##Do todo() for all tests between start and end.
in_todo()                        ##Returns true if in a todo_start|end block.
os_name()                        ##e.g. 'linux'

OWN ASSERTION_FUNC ==>           ##Just create a plpgsql function that returns text, with a last optional text argument, and which
                                 ##returns ok() if test passes, or returns error message if not.

datafiller.py [FILE]              #Script printing commands filling randomly some TABLE...
                                  #FILE (def: stdin) is a list of DDL commands creating the TABLE.
                                  #Hints on how to fill are provided with --comments:
                                  #  - syntax:
                                  #     -- df [MACRO]: VAR[=VAL]
                                  #       - with MACRO, can do elsewhere use=DIRECTIVE to repeat all the VAR[=VAL]...
                                  #          - some predefined MACRO:
                                  #             words: word=/etc/dictionaies-common/words
                                  #       - VAL can use '' for STR and TIMESTAMP
                                  #     - can also use -- df T=TABLE A=COL_VAR: ... to target a TABLE or COL_VAR on a separate line
                                  #       after it. TABLE cannot use skip=FLOAT.
                                  #     - to specify a VAR, I write $VAR, but it should be written VAR
                                  #  - supported VAR:
                                  #     - all TABLE (put comment on a line by itself)
                                  #        - size, offset, mangle, null, seed: see below
                                  #     - TABLE (put comment after the opening parenthesis of creation):
                                  #        - size=INT: number of tuples to fill.
                                  #          Can only be on TABLE, not COL (except for gen=serand)
                                  #        - mult=INT: multiply $size for this TABLE
                                  #          mult (def: 1) should be done on each TABLE (relative size with each other)
                                  #          size (def: 100) only once for all TABLE (to scale it)
                                  #        - skip=FLOAT: divide $size for this TABLE.
                                  #          As opposed to mult, actually produce the rows, but randomly don't output them
                                  #        - nogen, null=FLOAT: same as below
                                  #     - COL (put comment after it):
                                  #        - all:
                                  #           - type=TYPE: generate another TYPE, then casted to the actual type
                                  #           - nogen: no random data (use only default values)
                                  #           - null=FLOAT: percentage of nulls
                                  #           - seed=INT: set random seed (def: use OS (usually depends on current time))
                                  #        - BOOL:
                                  #           - rate=FLOAT: percentage of true (def: 0.5)
                                  #        - INT (integer, not int)|DATE|TIMESTAMP|INTERVAL|INET|CIDR|MAC:
                                  #           - gen=STR: distribution, among:
                                  #              - serial: counter, increments $step (def: 1, must not be divider of $size) from
                                  #                $shift (def:0), then modulo $size, then adds $offset (def: 1)
                                  #                If $mangle, choose random $shift and $step
                                  #              - uniform: uniform distribution, from $offset to $offset + $size - 1
                                  #              - serand: serial up to $size1, then uniform to $size2 - $size1 ($size1 and $size2
                                  #                are the COL-level, and TABLE-level $size)
                                  #              - for other distributions: just use type=float, then use float distributions
                                  #           - offset, shift, step, size, mangle: see above
                                  #        - FLOAT:
                                  #           - gen=WORD: distribution, among:
                                  #              - uniform, gauss|norm, log (lognormal), beta, gamma, weibull, vonmises: use $alpha and
                                  #                $beta
                                  #              - exp, pareto: use $alpha
                                  #           - alpha|beta: see above
                                  #        - STR, followed by:
                                  #           - nothing: prefix followed by repetition of number, separated by _
                                  #              - prefix=STR (def: COL_VAR)
                                  #              - length|lenvar=NUM: average length and diff from average of STR (def: 12 and 3)
                                  #           - chars=STR: choose random characters among a dictionary built with random words using
                                  #             characters in STR
                                  #              - cgen=MACRO: specifies INT parameters (to be used like use=) to specify how selection
                                  #                is done
                                  #              - length and lenvar: see above
                                  #           - text: can use INT parameters, choose from list of words
                                  #              - word=FILE|:STR,...: list of words, of 'size' words
                                  #              - length and lenvar: see above
                                  #           - word=FILE|:STR,...: same as above, but length to 1 and lenvar to 0. Can support unique.
                                  #        - DATE|TIMESTAMP|INTERVAL:
                                  #           - start|end=...
                                  #           - prec=NUM: in days for DATE, in seconds for TIMESTAMP
                                  #        - TIMESTAMP:
                                  #           - tz=STR
                                  #        - INTERVAL:
                                  #           - unit=s|m|h|d|mon|y (def: s)
                                  #        - BYTEA:
                                  #           - length and lenvar: see above
                                  #        - INET|CIDR:
                                  #           - network=STR
                                  #        - MAC
                                  #COL can't be unique for FLOAT, STR chars|text and BYTEA.
                                  #Uniqueness is tried 10 times (can be changed with datafiller.py --tries=NUM)
--[no-]filter                     #Output also FILE (commands creating the TABLE...), before commands filling the TABLE...
--drop                            #Output also commands dropping the TABLE...
--truncate                        #Sale for truncating
--size|offset|seed INT
--null FLOAT
--mangle                          #Sets VAR
-T                                #Put in a single transaction (normal isolation level)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            OTHERS             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


comment on ... is STR             #Add a comment STR, that can be read with:
                                  #  - col_description(TABLEOID, NUM)
                                  #  - [sh]obj_description(OID, DATABASE): shobj is for cluster-wide objects
                                  #  - a psql command \d* under "description"
                                  #... is TYPE followed by the definition, e.g. table TABLE, trigger TFUNC on TABLE, column TABLE.COL
                                  #cast ( TYPE as TYPE2 ), aggregate AFUNC(...), etc.
                                  #TYPE can be any object that can be used with create TYPE ...
                                  #If STR is null, remove the comment.
                                  #To write must be owner or superuser

TO DO ==>                         #In online doc:
                                  #  - Clients all rely on a backend protocol.
                                  #  - libpq is a C library that implement it and often used by clients.
                                  #    Large objects are a specific type handled by libpq.
                                  #  - Embedded SQL is a preprocessor that allows to write SQL in C code (then processed to libpq)
                                  #  - C can be used to write functions: see online doc, including background worker processes, SPI,
                                  #    creating new types, writing a PL/* handler, writing a FDW


      
   PG  
      


VERSION ==>                       #3.6.2

PG                                #Installation requires libpq

new PG.Client( [STR|OBJ],         #Connect to the database specified by STR|OBJ, then fire FUNC.
FUNC( ERROR, CLIENT,              #FUNC2 must be fired when all operations are done to close connection.
FUNC2(ERROR)))                    #STR is "[connectionname://][user[:password]@][host[:port]][/database]"
                                  #(all defaults if no first arg) or a IPC socket folder path.
                                  #Connectionname can be anything, it just differentiate sessions. OBJ has members :
                                  #  - user (def: process.env.USER)
                                  #  - database (def: process.env.USER)
                                  #  - password (def: null)
                                  #  - port (def: 5432)
                                  #  - host (def: null): if not URL, use DIR/.s.PGSQL.PORT
                                  #  - ssl (def: false)
                                  #Defaults are in PG.defaults.VAR
                                  #Other defaults:
                                  #  - PG.defaults.parseInt8:
                                  #     - PSQL bigint (such as result of count()) is too big for JavaScript INT.
                                  #     - If false (def), bigint -> STR. If true, bigint -> INT
                                  #Returns CLIENT, but should only be used for events. Use CLIENT in callback for connect|end()
CLIENT.connect
([FUNC(ERROR, CLIENT)])           #
CLIENT.end()                      #


                                       /=+===============================+=\
                                      /  :                               :  \
                                      )==:            QUERIES            :==(
                                      \  :_______________________________:  /
                                       \=+===============================+=/


CLIENT.query(STR[, VAL_ARR]       #Returns a QUERY from PostgreSQL command STR.
[,FUNC(ERROR, OBJ)])              #If FUNC, also execute it and event handlers of QUERY cannot be used (so QUERY is useless then).
                                  #OBJ is same as in QUERY end event handler.
                                  #If VAL_ARR, each "$1", "$2", etc. in STR will be replaced by those VAL, providing it does not
                                  #point to a TABLE, a COL or a SCHEMA. It is slower but it prevents SQL injections (VAL_ARR are
                                  #properly escaped instead of using risky STR concatenation).
CLIENT.query(OBJ[, FUNC])         #Same but OBJ can have members :
                                  #  - text: same as STR
                                  #  - values: same as VAL_ARR
                                  #  - name STR3:
                                  #     - make it PREP: but using PSQL Extended Protocol, so same effect (skip parsing phase when
                                  #       calling came query with same name (will use same text|values)), but not actual PREP
                                  #     - parsing is only done when values VAL_ARR is used, so only useful then

QUERY.on
("row", FUNC( OBJ ))              #Execute QUERY and fire event handler for each row OBJ: { VAR: VAL }...
QUERY.on("end", FUNC( OBJ ) )     #Execute QUERY and fire event handler for all rows. OBJ has members :
                                  #  - command STR : SQL command
                                  #  - rowCount UINT
                                  #  - oid DOUBLE
                                  #  - rows OBJ_ARR: { VAR: VAL }...
                                  #  - fields OBJ_ARR:
                                  #     - name STR
                                  #     - format TYPE_STR
                                  #     - tableID DOUBLE
                                  #     - columnID DOUBLE
                                  #     - dataTypeID DOUBLE
                                  #     - dataTypeSize UINT
                                  #     - dataTypeModifier UINT
QUERY.on("error", FUNC(ERROR))    #

CLIENT.query
(new PG-QUERY-STREAM(STR))        #Like CLIENT.query(STR) but returns as ISTREAM.
CLIENT.query
(new PG-CURSOR(STR)[,VAL_ARR])    #Like CLIENT.query(STR[, VAL_ARR]) but returns a CURSOR (version 0.2.0).
CURSOR.read
(UINT, FUNC(ERROR, OBJ_ARR))      #OBJ_ARR is { VAR: VAL }... or [] if no more rows
CLIENT.copyFrom|To( STR )         #COPY...FROM|TO statement must use this instead of CLIENT.query().
                                  #Returns a I|OSTREAM (must execute I|OSTREAM.end()).
                                  #Can use stdin (not stdout) in STR if writing|reading from I|OSTREAM
CLIENT.pause|resumeDrain()        #Stops|resumes emission of drain events (useful when async operations need to complete first)


                                       /=+===============================+=\
                                      /  :                               :  \
                                      )==:       OTHER OPERATIONS        :==(
                                      \  :_______________________________:  /
                                       \=+===============================+=/


CLIENT.on("drain", FUNC())        #Fired each time all queries have been executed
CLIENT.on("error",FUNC(ERROR))    #
CLIENT.on                         #Fired with listen/notify SQL statements
("notification", FUNC(OBJ))       #OBJ:
                                  #  - name "notification"
                                  #  - channel STR
                                  #  - payload STR
                                  #  - length NUM
                                  #  - processId NUM
CLIENT.on("notice", FUNC(STR))    #Fired with warning messages (otherwise printer in stdout)


                                       /=+===============================+=\
                                      /  :                               :  \
                                      )==:            POOLING            :==(
                                      \  :_______________________________:  /
                                       \=+===============================+=/


PG.pools.getOrCreate([OBJ])       #Returns POOL (from GENERIC-POOL) of CLIENT that has extra method:
                                  #  - connect(FUNC(ERROR, CLIENT, FUNC2(ERROR2))): acquire a CLIENT and fires FUNC()
                                  #Created with params (either OBJ or PG.defaults):
                                  #  - name: OBJ stringified
                                  #  - max <- poolSize
                                  #  - idleTimeoutMillis <- poolIdleTimeout
                                  #  - reapIntervalMillis <- reapIntervalMillis
                                  #  - log <- poolLog
                                  #Other OBJ passed to new PG.CLIENT(OBJ)
                                  #If no POOL used, would use one new connection for each query.
PG.pools.all                      #POOL_OBJ

PG.connect(OBJ, FUNC)             #Like new PG.CLIENT(OBJ, FUNC).connect() but uses PG.pools.getOrCreate(OBJ)
PG.on
("error",FUNC(ERROR, CLIENT))     #
PG.end()                          #Close all CLIENT, even if currently querying.
