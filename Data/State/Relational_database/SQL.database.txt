
       
   SQL  
       


Relationships:
  - can be 0, 0|1, many or 0|many on each side
  - 1-to-1: foreign key
  - all others: third table with foreign keys for both sides
Enum:
  - prefer small tables when possible
How to check that foreign key cover all the primary key values?
Primary key:
  - should always create surrogate key as primary key, even when natural keys are candidate keys
  - should then put non-primary candidate keys with unique constraint
  - check article stackoverflow answered in StackOverflow



NOTATION ==>             #Standard
POSTGRESQL ==>          ##PostgreSQL-specific
                     STR###PostgreSQL extension STR

TO FINISH ==>           ##  - SSH, GPG
                        ##  - repmgr
                        ##  - Chapter 17, 19, and 18.3.2, and postgres -l
                        ##  - pgcrypto
                        ##  - 39.1. Installing procedural languages
                        ##  - Postgres-XC


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MAIN TASKS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DATABASE DESIGN ==>     ##Modelling (pgModeler)
                        ##  - create tables with right columns, constraints (pkey, fkey, default, check, not null, unique, exclude) and properties (inherits)
                        ##  - using:
                        ##     - normal types (NUM, BOOL, STR, BSTR, BYTEA, DATE|TIME, ENUM) and arrays, ranges
                        ##     - special types (net, dictionary, xml, json, hstore, ltree, geometry) and created types (ctype, domain)
                        ##     - id vs uuid
                        ##     - views for encapsulation, or security restriction per column
                        ##     - [event] triggers
                        ##     - sequences
                        ##     - schemas
                        ##     - foreign tables (including file_fdw for CSV files)
                        ##     - listen|notify for clients communication
                        ##Security:
                        ##  - roles and privileges
                        ##  - authentication
                        ##  - unix_socket_directories
                        ##  - FUNC definition (leakproof, security definer)
                        ##Multithread-safety (transactions, locks)
                        ##Watch out for:
                        ##  - null possibility in queries
                        ##  - SQL injection when concatenating SQL_STR (use quote_*() or format())

PERFORMANCE ==>         ##  - on design, check using:
                        ##     - materialized views instead of views
                        ##     - rules instead of triggers
                        ##     - index
                        ##     - cursors
                        ##     - partitions
                        ##     - prepared statements
                        ##     - large objects
                        ##     - tablespaces
                        ##  - ENVVAR tunning:
                        ##     - disabling durability, decreasing checkpoints frequency, using RAM disks
                        ##     - setting right resources needed for [maintenance_]work_mem, effective_cache_size, wal_buffers, max_stack_depth, temp_file_limit,
                        ##       max_files_per_processes, effective_io_concurrency, shared_buffers, max_connections, statement_timeout
                        ##     - using pgtune
                        ##  - optimizing queries with explain
                        ##  - using pgbench
                        ##  - for big data write, see below best practices
                        ##  - use connection pooling (pgBouncer)
                        ##  - upgrading hardware
                        ##  - FUNC definition (volatility, cost, rows)
                        ##  - TABLE fillfactor, fastupdate
                        ##  - autovacuum tunning

SETUP FOR END USERS     ##  - create [A|W]FUNC (possibily from PL/* languages), prepared statements, comments
AND FUTURE MAINTENANCE  ##  - logging
 ==>                    ##  - [hot] standby with [a]sync. streaming replication
                        ##  - use pgagent for regular tasks:
                        ##     - pgbadger and pgcluu reports creation
                        ##     - check_postgres
                        ##        - good idea to merge pgbadger, pgcluu and check_postgres into one HTML file with a script
                        ##     - pg_dumpall
                        ##  - if durability, check proper cache usage (wal_sync_method, HDD|filesystem cache)

TESTING ==>             ##  - random filling (datafiller.py)
                        ##  - unit testing (pgTap)
                        ##  - load testing (Tsung)

MAINTENANCE ==>         ##  - monitoring:
                        ##     - use pgadmin, with Server status window, and opening a psql within pgadmin (create proper .psqlrc), or use teamPostgreSQL
                        ##     - resource (should not exceed max_connections and work_mem), space usage or other problems:
                        ##        - pgbadger and pgcluu
                        ##        - pg_top
                        ##        - check_postgres
                        ##  - data update (should create functions):
                        ##     - partitionning
                        ##     - copy TABLE from
                        ##  - cluster/reindex (ask for exclusive lock)
                        ##  - check PostgreSQL upgrades, and use pg_upgrade
                        ##  - create restore points with pg_create_restore_point(STR) after critical operations


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          POSTGRESQL           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ARCHITECTURE ==>        ##Conform to SQL 2011 for most of it
                        ##Client (such as psql, pgadmin) / server (postgres) architecture
                        ##RDBMS with focus on standard compliance and extensibility.
                        ##Version of this doc is 9.3.5

LIMITS ==>              ## - database: unlim
                        ## - table: 32 TB
                        ## - no of col: 250 to 1600 depending on type (but might mean bad design)
                        ## - no of rows: unlim
                        ## - cell: 1GB

NUMBER OF FDS ==>       #By def. 1024 but can be increased without any problem.
                        #How:
                        #  - /etc/sysctl.conf: add line
                        #      fs.file-max = 500000 (def: 20000)
                        #  - /etc/security/limits.conf: add lines
                        #      * soft nofile 60000
                        #      * hard nofile 60000
                        #  - apply changes by rebooting
                        #Be careful of file descriptors leak (can check with lsof [| wc -l])


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SYNTAX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COMMAND;                #Case-insensitive for keywords, case-sensitive for variables only if written "VAR" or U&"VAR2"
                        #Whitespaces don't matter
                        #  - "statements" are COMMANDS
                        #  - "clauses" are part of COMMANDS attached to a set of keywords (e.g. WHERE BOOL)
--COMMENT
/* COMMENT */


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             TYPES             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TYPES ==>               #  - NUM:
                        #    - smallint (SHORT): integer
                        ##   - bigint (LONGLONG)
                        #    - real: double precision
                        #      - can be 'NaN' or '[-]Infinity'
                        #    - numeric|decimal(NUM, NUM2): fixed precision, where NUM2 is number of decimals and NUM total number of digits
                        #    - money: like bigint divided by 100, so with two decimals. To use with money (as opposed to real)
                        #  - bool:
                        #    - true, false or null
                        #    - operators:
                        #      - or and not
                        #      - BOOL is [not] BOOL2: like = <>, except if BOOL|BOOL2 null (see below)
                        #  - STR:
                        #     - [var]char(NUM):
                        #       - NUM is max number of chars
                        #       - if no var, padds rest with spaces (prefer varchar(), faster)
                        #       - written '...' :
                        #         - '' to escape a '
                        #         - '...' + newline + '...' is same as '...' || '...'
                        #         - '...' can include Unicode characters, but U&'...' can also use \NNNN Unicode point) (UTF-8)
                        ##        - E'...' to use backslash escape sequences:
                        ##            \b, \f, \n, \r, \t, \NNN, \xNN, (only for UTF-8:) \uNNNN, \UNNNNNNNN
                        ##        - $[TAG]$...$[TAG]$ is same but:
                        ##           - escape newline, ' and \
                        ##           - safer when injecting SQL commands
                        ##    - text: like varchar, but unlimited length
                        ##    - unknown:
                        ##       - when a STR doesn't have a specific type (like varchar or text) and is not coerced to a type (e.g. select STR;)
                        ##       - will be coerced to a true STR type when needed
                        #  - bit [varying]([NUM]) BSTR:
                        #    - like [var]char, but bits binary string
                        ##   - def NUM: unlim
                        #    - written B'10011' or X'a45f0e'
                        ## - bytea:
                        ##   - like varchar, but byte binary string
                        ##   - use E'\\xNNNN...'
                        #  - date/time:
                        #    - date, e.g. '2011-05-03'
                        #    - time, e.g. '15:51:36[.NNNNNN]'
                        #    - timetz, e.g. '15:51:36-2' or '15:51:36 GMT+2'
                        #    - timestamp[tz], e.g. DATE + space + TIME[TZ]
                        #    - interval ..., e.g. '[YYYY-][MM-][DD] [HH:[MM:]...]'
                        #       - ... can restrict which fields to use: year, month, ... [to year, month, ..., second]
                        #       - two identical INTERVAL might be printed differently (e.g. 24 hours and 1 day)
                        #       - can have two formats according to ENVVAR intervalstyle: 'postgres' (def), 'sql_standard' or 'ISO_8601'
                        #    - special:
                        #       - DATE|TIMESTAMP: 'epoch', '[-]infinity', 'today', 'tomorrow', 'yesterday' (midnight)
                        #       - TIME: allballs (midnight)
                        #       - DATE|TIME|TIMESTAMP: 'now'
                        #    - timezones:
                        #       - use either:
                        #          - zone name (e.g. 'America/New_York'). Possible values are in pg_timezone_names.
                        #          - zone abbrev (e.g. 'PST'). Possible values are in pg_timezone_abbrevs.
                        #       - must set ENVVAR timezone
                        ## - ENUM:
                        ##    - manipulation:
                        ##       - create type ENUM as enum(STR...)
                        ##       - alter type ENUM add value [if not exists] STR [before|after STR2]
                        ##    - then use ENUM as a type and STR... as values
                        ##    - ordering in STR... is use for < <= >= >
                        ## - geometry:
                        ##    - point '(NUM,NUM2)'
                        ##    - line '((NUM,NUM2),(NUM3,NUM4))'
                        ##    - lseg: same as line but segment not line
                        ##    - box: same as line but rectangle not line. With array, noted (BOX;BOX...), not (BOX,BOX...)
                        ##    - path: '((NUM,NUM2)...)' for closed path, '[(NUM,NUM2)...]' for open path
                        ##    - polygon: like closed path
                        ##    - circle: '<(NUM,NUM2),NUM3>'
                   cube###    - n-dimensional cube: designated by diagonal '(NUM...), (NUM2...)'
                        ##    - a lot of operators for shift, rotation, scaling, getting points|distance like center, positions, etc. exist: cf online doc (including for cube)
                        ## - net :
                        ##    - macaddr 'NN:NN:NN:NN:NN:NN'
                        ##    - inet IPv4 or IPv6
                        ##    - cidr: same as inet, but can write 10 instead of 10.0.0.0 for example
                        ## - xml:
                        ##    - created by xmlparse(document|content STR)
                        ##       - must be well-formed.
                        ##       - content is for fragments
                        ##    - converted back by xmlserialize(document|content XML as [var]char|text)
                        ##    - look at online doc for operators/functions
                        ## - json :
                        ##    - written like STR.
                        ##    - must be well-formed.
                  hstore###- hstore:
                        ###   - key-value store
                        ###   - '"VAR"=>"VAL",...' ("" used to escape whitespace , = > in VAR|VAL)
                        ###      - VAL can be null
                   ltree###- ltree:
                        ###   - 'VAR.VAR2....'
                        ###      - tree of VAR ([[:alpha:]_], max 256 characters), which could be VAR pointing to value elsewhere
                        ###- lquery:
                        ###   - globbing-like query against a LTREE (in whole)
                        ###   - looks like a LTREE but can contain (can be combined):
                        ###     *[{[NUM],[NUM2]}] instead of a VAR, e.g. *.VAR.*
                        ###                       NUM[2]specifies how many VAR can match (def: {1,})
                        ###     VAR*              Any suffix
                        ###     VAR%[*]           Any suffix starting with _
                        ###                       With *, also any suffix at end of _-separated words in VAR
                        ###     VAR@              Case insensitive
                        ###     VAR|VAR2          Or
                        ###     !VAR              Not
                        ###-ltxtquery:
                        ###  - like lquery, but combination of VAR with tsquery-like & | !, matching any VAR inside LTREE
                        ###  - e.g.: VAR* & VAR2@
               uuid-ossp###- uuid:
                        ###  - 'nnnnnnnn-nnnn-nnnn-nnnn-nnnnnnnnnnnn', 'nnnn-nnnn-nnnn-nnnn-nnnn-nnnn-nnnn-nnnn' or
                        ###    'nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn'
                        ###  - functions:
                        ###     - uuid_generate_v1[mc]():
                        ###        - returns UUID version 1 (MAC address + time)
                        ###        - if mc, uses a random multicast MAC address
                        ###     - uuid_generate_v3|5(uuid_ns_*(), STR): returns UUID version 3|5 (STR with md5|SHA1),
                        ###       uuid_ns_*() where * chooses the type of STR among dns, url, oid or x500.
                        ###     - uuid_generate_v4(): returns UUID version 4 (random)
                        ## - oid:
                        ##   - ID of a VAR.
                        ##   - 32 bits, so can't be use reliably for uniqueness with more than 1000 (TABLE.oid are TABLE-specific), so deprecated.
                        ##   - Can be cast to UINT.
                        ##   - other system types include tid, xid and cid (tuple, transaction and command).
                        ##   - oid aliases but for semantic purposes include (can all be casted to oid):
                        ##     - regproc (FUNC)
                        ##     - regprocedure (FUNC(...))
                        ##     - regoper (+)
                        ##     - regoperator (+(...))
                        ##     - regclass (TABLE)
                        ##     - regtype (TYPE)
                        ##     - regconfig (REGCONF)
                        ##     - regdictionary
pg_typeof(VAL)         ##Returns type as STR

IMPLICIT CASTING ==>    #How:
                        #  - for a FUNC arg or the TYPE of a COL
                        #  - it looks at pg_cast to see if a casting function is available. Examples:
                        #    - between NUM
                        #    - between date/time and STR
                        #    - no casting between STR and NUM
cast(VAL as TYPE)       #Explicit casting
TYPE STR                ##Same but shorter and must provide VAL as STR

create cast
(TYPE as TYPE2)
with function FUNC(...)
[as assignment|         #Creates a type casting FUNC with a 'f' castmethod (see pg_cast.castfunc for FUNC(...))
as implicit]            ##assignment|implicit designates the 'a' and 'i' castcontext (def: 'e')
create cast ...
with inout ...          #Same but with a 'i' castmethod (see pg_cast)
create cast ...
without function  ...   #Same but with a 'b' castmethod (see pg_cast)

null                    #Represents a missing data. Error-prone, to avoid.
                        #  - is of type unknown until it resolves to an actual type (any TYPE can have null values)
                        #  - all OP and FUNC with null returns null, including:
                        #     - NUM OP null, STR OP null
                        #     - FUNC(null)
                        #     - null = <> < null, case VAL when null
                        #     - exists(), CTYPE_VAL in TABLE_VAL, etc.
                        #  - exceptions:
                        #     - null = null for anything about duplicates, except unique
                        #       (group by, partition by, union|intersect|except [all], distinct)
                        #     - BOOL:
                        #        - null or true -> true, null and false -> false
                        #        - null -> false in where BOOL, but null -> true in check (BOOL)
                        #  - working around nulls:
                        #     - VAL is [not] distinct from VAL2: like = <>, but treats null as any other value.
                        #       - null is [not] null -> same as is [not] distinct from null
                        #     - concat(STR...): same STR || ..., but treats null as ""
                        #     - nullif(VAL, VAL2): same as case VAL when VAL2 then null else VAL end
                        #     - coalesce(VAL...): returns the first VAL that is not null
                        #  - AFUNC() removes nulls first, except count(*)
                        #  - foreign keys (COL2 is the primary key, COL the foreign key):
                        #     - null COL value match any COL2 (but not inverse)
                        #     - for multicolumn COL..., match if at least one (match simple (def)) or all (match full) null COL value
                        #  - best to do:
                        #     - use not null constraints when possible
                        #     - if not:
                        #       - append "or|and VAL is [not] null" to BOOL
                        #       - always think about possibility of null is expressions

TABLE                   #Name of a TABLE
TABLE_VAL               #TABLE or table expression (expression that returns a TABLE_VAL), including subqueries
COL_VAR                 #Name of a COL. Must be TABLE.COL or COL(TABLE) if several TABLE.
VAR                     #Can be max 63 characters, [[:alnum:]_]+
"VAR"                   #Like VAR, but cannot be mistaken as keyword (e.g. "select" as a VAR), and is case-sensitive
VAL                     #Either a COL or scalar VAL, treated as a COL of size 1
                        #When VAL OP VAL2 with different sizes, the smaller one is being repeated.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           OPERATORS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


operator([SCHEMA.]OP)   #Another way to write OP (default SCHEMA for OP is pg_catalog)

OPERATORS ==>           #Operators shared by all type, except geometry, json, xml (must cast to STR)
VAL = <> > >= < <= VAL2 #true > false.
VAL between [symmetric]
VAL2 and VAL3           #Same as VAL >= VAL2 and VAL <= VAL3. If symmetric and VAL3 < VAL2, swap them.
greatest|least(VAL...)  #Use > >= < <=
VAL OP any|all (ARRAY)  #True if VAL OP any|all ARRAY_VAL
VAL [not] in (VAL2...)  #Same as VAL =|<> any ARRAY, but with VAL2...

exists(TABLE_VAL)       #Returns true if TABLE_VAL has at least one row
                        #Often:
                        #  - TABLE_VAL will not care about the VAL in select VAL ..., so write select 1 ...
                        #  - used row-wise in a where clause
                        #     - e.g. select COL from TABLE where exists(select 1 from TABLE2 where TABLE2.COL2 = TABLE.COL)
CTYPE_VAL OP any|all    #Returns true if any|all of the rows of TABLE_VAL OP CTYPE_VAL
TABLE_VAL               #Often used row-wise in a where clause.
CTYP_VL [not] in TBL_VAL#Same as CTYPE_VAL =|<> any TABLE_VAL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            NUMBERS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


NUM - + - / * NUM2      #
NUM & | # << >> NUM2
~ NUM                   #Bitwise operations (# is xor) (only real numbers)
power(NUM, NUM2)
NUM ^ NUM2              #Same
mod(NUM, NUM2)
NUM % NUM2              #Same
div(NUM, NUM2)          #trunc(NUM/NUM2)
|/ NUM                  #√NUM
cbrt(NUM)
||/ NUM                 #Same
NUM!
!! NUM                  #Factorial (only integers)
exp(NUM)                #
ln(NUM)                 #
log(NUM[, NUM2])        #Def: 10
abs(NUM)
@ NUM                   #Same
sign(NUM)               #-1, 0 or 1
ceil|floor(NUM)         #
round|trunc(NUM[, NUM2])#Def: 0

[a]cos|sin|tan(NUM)     #
cot(NUM)                #
atan2(NUM, NUM2)        #
degrees|radians(NUM)    #Conversion
pi()                    #

width_bucket(4 NUM)     #In a histogram from NUM2 to NUM3 with NUM4 buckets, returns in which bucket number would fall NUM1.
generate_series
(INT, INT2[, INT3])     #Returns a TABLE_VAL from INT to INT2, with step INT3 (def: 1)

random()                #From 0 to 1 (cycle of 3e14 numbers, PRNG not crypto-secure)
setseed(NUM)            #NUM is from 0 to 1
normal_rand(INT,
REAL, REAL2) tablefunc###Generate INT random variables following N(REAL, REAL2) as TABLE_VAL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            STRINGS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


STR || STR2             #Concatenation
concat(STR...)          #Same but null are ignored, instead of producing null as a result
concat_ws(STR, STR2...) #Same but with separator STR
[bit|char|octet_]length
(STR)                   #Def is char
repeat(STR, UINT)
reverse(STR)
lower|upper(STR)        #
initcap(STR)            #upper() to first letters, lower() to rest. Use locales.

position(STR in STR2)   #Returns index of STR inside STR2 (0 if not found)
substring(STR from UINT
[to UINT2])             #Returns STR from character UINT to UINT2 (def: end)
left|right(STR, UINT)   #Same from left|right
overlay(STR placing STR2
from UINT[to UINT2])    #Replace character UINT to UINT2 (def: end) of STR by STR2
trim([trailing|leading|
both] [STR from] STR2)  #Remove (def: both) characters among STR (def: ' ') from STR2
l|rpad(STR, UINT[,STR2])#Pads STR2 repeatingly from left|right of STR, until it has length UINT (if inferior, truncate STR).
translate(STR,STR2,STR3)#Like Unix command tr


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PATTERNS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SQL GLOBBING ==>        #Use % and _, same as normal globbing * and .
REGEXP ==>              #Also with some different syntaxes:
                        #  - QUANTIFIER* -> QUANTIFIER?
                        #  - \< \> -> \m \M
                        #  - \b \B -> \y \Y
                        #  - no \u \l
                        #Use locales for case sensitivity and [[:...:]]
STR [not] [i]like STR2  #Same as STR = STR2, but can use SQL globbing (i is case-insensitive)
[escape STR3]           #STR3 is escape character (def: '\').
STR [!]~[*] STR2        #Same but with regexps. STR2 needs to match only part of STR. * means case-insensitive.
substring(STR, STR2)    #Returns the part of STR that matches the regexp STR2, or null if no match.
                        #If regexp contains parenthesis, only returns the part in the first outer set of parenthesis.
regexp_matches(STR,     #Same except :
STR2[, STR3])           #  - returns as a TABLE_VAL, containing ARRAY for each match for a set of parenthesis
                        #    (if no parenthesis, the whole match)
                        #  - if several matches and flag 'g', returns several rows.
                        #  - If no match, returns no row. So output rows don't always match input rows, unless it is a
                        #    subquery crossjoined and no flag 'g', e.g. select COL, (select regexp_matches(COL2, STR2))
regexp_replace(STR,     #Same as sed 's/STR2/STR3/[STR4]' <<<STR
STR2, STR3[, STR4])     #STR3 can contain \1, \&, etc.
replace(STR, STR2, STR3)#Same but without regexps (faster)
regexp_split_to_array   #Returns the split of STR, using delim STR2, with flags STR3, as an ARRAY.
(STR, STR2[, STR3])     #If delim not to be found, returns only STR in ARRAY.
regexp_split_to_table() #Same but returns as a TABLE_VAL, with one row for each element.
                        #Same thing as regexp_matches() pour le crossjoin.
split_part(STR STR2,INT)#Same but without regexps, and returns the field number INT.

quote_ident|literal|    #Returns an escape STR, when constructing SQL commands as STR.
nullable(VAL)           #ident quote adds extra "" when needed (for COL or VAR), literal extra ''.
                        #nullable is same as literal, but if VAL is null, returns 'NULL', not null
                        #Prefer format()
format(STR, ...)        #Similar to sprintf() in C
                        #Types can be %I (quote_ident), %L (quote_nullable) or %s (quote_literal without the extra '')

ascii(STR)
chr(UINT)               #Conversion from and a to a single character. Can return Unicode point.
to_hex(NUM)             #Returns a STR with hexa representation of NUM

md5(STR)                #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            BINARY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


encode(BYTEA, STR)      #Returns BYTEA as a textual form, according to STR among "base64" or "hex"
decode(STR, STR2)       #Inverse
trim(...)
md5(...)                #Same as STR, but for BYTEA

BYTEA|BSTR || BYTEA|BSTR
bit|octet_length(...)
overlay(...)
position(...)
substring(...)          #Same as STR, but for BYTEA and BSTR
get_byte(BYTEA,UINT)    #
set_byte(BYTEA,UINT,    #
UINT2)
get_bit(BYTEA|BSTR,UINT)#
set_bit(BYTEA|BSTR,UINT,#
UINT2)

BSTR & | # << >> BSTR2
~ BSTR                  #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATE/TIME           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


to_char(TIMESTMP|INTRVL,#Prints TIMESTAMP as TEXT according to format (cf online doc), e.g. "HH24:MI:SS"
TEXT)                   #Use locales.
to_date(TEXT, TEXT2)    #Inverse, but with DATE
                        #Uses ENVVAR DateStyle: when reading ambiguous DATE (day is from 1 to 12), how to read it: "DMY" (def), "MDY" or "YMD"
to_timestamp(DOUBLE)    #DOUBLE is seconds since epoch.
to_char(NUM, TEXT)
to_number(TEXT, TEXT)   #Same but for NUM

DATETIME + - DATETIME2  #Anywhere it makes sense
-INTERVAL
INTERVAL / * NUM
current_date
local|current_time
[stamp]                 #From beginning of transaction
transaction_timestamp() #Same as current_timestamp, but from beginning of this statement
TIME[STAMP][TZ]
at time zone STR        #
extract(WORD from       #WORD can be millenium, century, decade, [iso]year, quarter, month, week, day, [iso]dow (day of week),
INTERVAL|TIME[STAMP])   #doy, epoch, hour, minute, [micro|milli]seconds, timezone[_hour|minute]
                        #Returns UINT
date_trunc(STR,
TIMESTAMP|INTERVAL)     #STR is same as WORD in extract() (for most of it). Truncates the date until STR.
isfinite(DATE|TIMESTAMP|
INTERVAL)               #
justify_days|hours|
interval(INTERVAL)      #Adjust an INTERVAL so that extra hours become days, extra days become months, or both
generate_series(TIMSTMP,
TIMESTAMP2, INTERVAL)   #Returns a TABLE_VAL from TIMESTAMP to TIMESTAMP2, with step INTERVAL
pg_sleep(DOUBLE)        #Sleeps DOUBLE seconds (0.01 sec. resolution)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ENUM              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


enum_first|last         #Returns first|last ENUM elem
enum_range(ENUM[,ENUM2])#Returns ENUM_ARR de l'element ENUM à ENUM2 (def: fin d'ENUM)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INET              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INET >> >>= << <<= INET2#Containing|contained within [or equal]
INET|MAC | & INET2|MAC  #
~INET|MAC               #Bitwise operations
INET + - INET|UINT      #From last to first field.
abbrev(INET|CIDR)       #Show as STR
text(INET)              #Same but longer version
broadcast(INET)         #
host(INET)              #IP address without mask
hostmask(INET)          #Only part of IP addres with mask
netmask(INET)           #Mask as INET
masklen(INET)           #Mask as NUM
set_masklen(INET|CIDR,
UINT)                   #Modifies mask
network(INET)           #Part of IP without last number
family(INET)            #4|6 for ipv4|6
trunc(MAC)              #Put second half as 00


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             JSON              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


JSON->UINT|STR          #Same as OBJ[UINT|STR]
JSON->>UINT|STR         #Same but returns as STR
JSON#>ARRAY
JSON#>>ARRAY            #Same but with several successive indexes

to_json(VAL)            #If a cast function exists, return cast(VAL as json), otherwise, converts VAL to STR first.
array_to_json(ARRAY)    #Returns ARRAY as JSON { ARRAY }
json_array_length(JSON) #Length of ARRAY in JSON { ARRAY }
row_to_json(CTYPE)      #Returns CTYPE as JSON { "f1": ..., "f2": ... ... }
json_each[_text](JSON)  #Returns JSON as a TABLE_VAL (each first dimension VAR is a row, and its VAL the second column)
                        #text convert to STR after.
json_object_keys(JSON)  #Same but with only the first column (the VARs)
json_populate_record    #Same as json_each(), but horizontal, and only keeps VAR that have a matching COL in TABLE.
(null::TABL,JSON[,BOOL])#If true (def: false), will convert to false too.
json_populate_recordset
(null::TABL,JSON[,BOOL])#Same but from a JSON array, and do one row for each OBJ of the array.
json_array_elements(JSN)#Returns a JSON array as a TABLE_VAL with one column, and a row for each VAL.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            HSTORE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


HSTORE->STR
HSTORE->STR_ARR   hstore###Gets VAL[_ARR] whose key is STR[_ARR], as STR
HSTORE || HSTORE2       ###Concatenation
HSTORE - STR[_ARR]      ###Returns HSTORE without keys specified in STR[_ARR] or keys+values specified in HSTORE2
HSTORE - HSTORE2        ###STR must be of type text, not unknown.
slice(HSTORE, STR_ARR)  ###Returns HSTORE with keys specified in STR_ARR (if keys are absent, not an error)
TABLE_VAL #= HSTORE     ###Returns TABLE_VAL with changes specified by HSTORE (key is column name, value is new value)
                        ###To use TABLE_VAL, needs to do:
                        ###  select (ALIAS).* from (select TABLE_VAL #= HSTORE as ALIAS from TABLE) ALIAS2;

HSTORE ? STR
HSTORE ?& ?| STR_ARR    ###True if HSTORE contains key STR, or all|one of key in STR_ARR
defined(HSTORE, STR)    ###True if HSTORE contains key STR, and value is not null
HSTORE @> <@ HSTORE2    ###Contains/is contained

%% %# HSTORE            ###Converts to STR_ARR[_ARR].
hstore(STR_ARR[_ARR])   ###Inverse
hstore(STR_ARR,STR2_ARR)###Converts to HSTORE, where STR_ARR are the keys and STR2_ARR the values.
hstore(TABLE_VAL)       ###Converts to HSTORE (with name of columns, or fNUM if anonymous CTYPE_VAL)
                        ###Ex: select hstore(TABLE) from TABLE;
a|skeys|vals(HSTORE)    ###Gets keys or values of HSTORE as a STR_ARR (a) or TABLE_VAL (s)
hstore_to_json[_loose]
(HSTORE)                ###If loose, NUM and BOOL will have this type in JSON (otherwise, will stay as STR)
each(HSTORE)            ###Converts to TABLE_VAL, with two columns key and value


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             LTREE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LTREE < <= >= > <> =
LTREE2             ltree###
LTREE || LTREE2         ###Concatenation
LTREE[_ARR] ~ LQUERY
LTREE[_ARR] ? LQUERY_ARR
LTREE[_ARR] @ LTXTQUERY ###Does it match LQUERY[_ARR]|LTXTQUERY [any of]
LTREE[_ARR] <@ @>
LTREE2[_ARR]            ###Contained|contains [any of]
LTREE_ARR ?@> ?<@ ?~ ?@
...                     ###Returns the first LTREE in LTREE_ARR that returns true to @> <@ ~ @ ... (null if none)

nlevel(LTREE)           ###Number of VAR
index(LTREE, LTREE2     ###Position of first VAR starting LTREE2 inside LTREE, from INT (def: 0, can be negative to specify
[, INT])                ###from end) (-1 if not found)
subltree(LTREE,INT,INT2)###Slice it from VAR numero INT to numero INT2-1
subpath(LTRE,INT[,INT2])###Same but from numero INT (can be negative to be from end) with length INT2 (can be negative to
                        ###specify how many left on the right) (def: all)
lca(LTREE_ARR|LTREE...) ###Number of VAR matching at beginning of each LTREE ('' if none)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ARRAY             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TYPE array              #Array for any TYPE (can be used as any TYPE).
TYPE[]                  #Dimensions are defined at write-time.
array[VAL...]           #ARRAY_LIT. VAL can be a subarray, i.e. [VAL...] or {VAL...} for multidimensional arrays.
'{VAL...}'              #STR must be double-quoted inside '{...}'.
'[INT1:INT2]...={VAL..}'#Same but with specified lower bounds.
array(TABLE_VAL)        #TABLE_VAL must have only one column, and not be the result of a FUNC() (unless written as a subquery (select FUNC()))

ARRAY[UINT]...          #Accessing ARRAY. Index starts at 1. Returns null if any index is out-of-bound.
ARRAY[UINT1:UINT2]...   #Accessing slices of ARRAY.
                        #Subsequent dimensions must use slices too (but can take a single element of the slice).
                        #Returns null if one index is out-of-bound, and empty ARRAY if both are out-of-bound.
array_ndims(ARRAY)      #Returns number of dimensions as UINT
array_dims(ARRAY)       #Returns number of dimensions as STR, e.g. '[1:3][1:4]'
array_lower|upper
(ARRAY, UINT)           #Returns the lower and upper bound of dimension UINT, as UINT2.
array_length(ARRAY,UINT)#Same as array_upper - array_lower
generate_subscripts
(ARRAY, UINT)           #Returns the indexes of dimension UINT, as a TABLE_VAL

ARRAY = ARRAY_LIT       #Any expression can be used to write an ARRAY or part of it. If index is out-of-bound and only one
ARRAY[...]=VAL|ARRAY_LIT#dimension, size of ARRAY is extended (if less than 0, modifies lower bound).
VAL|ARRAY || ARRAY2
ARRAY2|| VAL|ARRAY      #If VAL|ARRAY has dimension n-1 and ARRAY2 dimension n, appends|prepends. If same dimension, concatenate
array_append|cat(ARRAY,
VAL|ARRAY2)             #Same as ||, but throw error if not VAL|ARRAY2
ARRAY @> ARRAY2         #True if ARRAY2 is contained within ARRAY (all elements of ARRAY are in ARRAY2)
ARRAY <@ ARRAY2         #Inverse
ARRAY && ARRAY2         #True if ARRAY overlaps ARRAY2 (any element of ARRAY is in ARRAY2)
any(ARRAY) OP VAL       #Same as ARRAY[1] OP VAL or ARRAY[2] OP VAL or ... Must be on the right operand.
and(ARRAY)              #Same with and

array_fill(VAL, ARRAY
[, ARRAY2])             #Returns an ARRAY filled with VAL, of dimensions ARRAY and lower bound ARRAY2 (def: {1...})
array_remove(ARRAY, VAL)#Returns (one-dimensionned) ARRAY without any element = VAL
array_replace(ARRAY,VAL,
VAL2)                   #Same but replace with VAL2
array_to_string(ARRAY,
STR2[, STR3])           #Returns concatenation d'ARRAY as STR, with STR2 as delim, and STR3 replacing null values (def: removes them)
string_to_array(STR,
STR2[, STR3])           #Inverse. If delim is null, each character is separated with next one. If "", returns whole STR as one element ARRAY
unnest(ARRAY)           #Flattens ARRAY, and returns it as a TABLE_VAL with one column and one row for each VAL.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        INTEGER ARRAYS         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INT ARRAY     intarray###All following will not work if contains null. ARR must be one-dimensional.
INT_ARR | INT2[_ARR]  ###Union
INT_ARR & INT2_ARR    ###Intersection
INT_ARR && INT2_ARR   ###Overlaps.
INT_ARR <@ | @>
INT2_ARR              ###Is contained | contains
INT_ARR @@ TSQUERY    ###Matches TSQUERY, containing NUM

INT_ARR +|- INT_ARR   ###
INT_ARR +|- INT       ###Append or remove entries matching INT

icount(INT_ARR)       ###Number of elements
sort(INT_ARR[,'desc'])###
uniq(INT_ARR)
idx(INT_ARR, INT2)    ###Index of first element matching INT2
subarray(INT_ARR,
INT2, INT3)           ###

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         COMPOSED TYPE         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create type CTYPE as    #Composed type (array with different types). Can be used as any TYPE.
(VAR TYPE...)           #create table TABLE creates a CTYPE with same name, which means TABLE === CTYPE:
                        #  - as TYPE (strip their constraint though)
                        #  - but not as a VAR nor TABLE_VAL
alter type CTYPE
add attribute VAR TYPE
cascade|restrict        #  - cascade|restrict: when CTYPE is of a TABLE, modify the TABLE too
alter type CTYPE
drop attribute
[if exists] VAR TYPE
cascade|restrict        #
alter type CTYPE
alter attribute
VAR type TYPE
cascade|restrict       ##
alter type CTYPE
rename attribute
VAR to VAR2
cascade|restrict       ##

[row](VAL...)           #CTYPE_LIT. null is specified by writting nothing, e.g. (1,,3). row is necessary only if only one VAL.
(CTYPE_VAL).VAR         #Accessing single types of composite types.
                        #Sometimes an extra set of parenthesis is needed around CTYPE_VAL (e.g. if produced by a FUNC())
                        #CTYPE < <= > >= CTYPE2 compares the first element first, etc.
VAR(CTYPE_VAL)          #Other notation


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       USER-DEFINED TYPE       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create domain TYPE as   #Creates a user-defined TYPE, interchangeable with TYPE2, but with:
TYPE2 [default VAL]     #  - default VAL (def: null, can be default)
[constraint CONSTRAINT] #  - constraint: BOOL can use "value", which refers to the values used.
[not null|check( BOOL )]#    CONSTRAINT just specify the name.
                        #Ex: email adresses

alter domain TYPE
set default VAL         #
alter domain TYPE
set|drop not null       #
alter domain TYPE add
[constraint CONSTRAINT]
[not null|check( BOOL)]
[not valid]            ##not valid: like alter table ... not valid
alter domain TYPE
drop constraint
[if exists] CONSTRAINT
restrict|cascade        #
alter domain TYPE
rename constraint
CONSTRAINT to CONSTRANT2#

create type TYPE
( input = FUNC,
output = FUNC2, ... )  ##Creates user-defined TYPE, based on C functions (see online doc)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             RANGE             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


RANGE ==>              ##Like an ARRAY of size 2, but with semantic indicating start and end.
                       ##TYPE can be :
                       ##  - int4range (integer)
                       ##  - int8range (bigint)
                       ##  - numrange (numeric)
                       ##  - tsrange (timestamp)
                       ##  - tstzrange
                       ##  - daterange
create type TYPE as
range(subtype = TYPE2,
subtypediff = FUNC
[, canonical = FUNC2]
[, subtype_opclass =
OPCLASS]               ##Personalized RANGE. FUNC(x,y) -> x-y (as TYPE2)
[, collation=COLATION])##FUNC2(x) -> x is done first. To use TYPE in FUNC2, must use a dummy create type TYPE; first before this call.

'[VAL, VAL2]'
'[VAL, VAL2)'          ##RANGE_LIT. Square brackets include, parenthesis exclude.
'(VAL, VAL2]'          ##Def of VAL|VAL 2 is [-]infinity. null is equivalent to [-]Infinity
'(VAL, VAL2)'          ##For discrete types, ] is automatically converted to ) and 1 added to VAL2.
RANGE_TYPE(VAL, VAL2
[,STR])                ##Same. STR is e.g. '[)' (def) or '()'

RANGE @> VAL|RANGE2    ##True if VAL|RANGE2 is contained within RANGE
VAL|RANGE <@ RANGE2    ##Inverse
RANGE && RANGE2        ##True if RANGE overlap RANGE2.
                       ##Can be used with exclude ( RANGE with && ) to make RANGE not overlap in one COL.
RANGE << >> RANGE2     ##True if end|begin of RANGE is before begin|end of RANGE2
RANGE &< &> RANGE2     ##True if begin and end of RANGE is before begin and end of RANGE2 (or inverse for &>)
RANGE -|- RANGE2       ##True if end of RANGE is adjacent to RANGE2
RANGE + RANGE2         ##Returns union (error if not contiguous)
RANGE - RANGE2         ##Returns [RANGE.begin, RANGE2.begin]. If RANGE2.begin < RANGE.begin: if RANGE2.end > RANGE.end,
                       ##returns RANGE, otherwise returns empty
RANGE * RANGE2         ##Returns intersection RANGE3 (empty if no intersection)

lower|upper(RANGE)     ##Returns VAL or VAL2 of the RANGE
lower|upper[_inc](RANG)##True whether bounds are inclusive or not (brackets or parenthesis)
lower|upper[_inf](RANG)##True whether bounds are [-]Infinity or null
'empty'                ##Special RANGE where [VAL,VAL2), and VAL = VAL2 = null
isempty(RANGE)         ##


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           STRUCTURE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


case [VAL]
  when VAL2a then
    VAL2b
  [when VAL3a then
    VAL4b]...
  [else                 #Switch statement. Substitutes to VAL|VAL2|VAL3
    VAL4]               #Def VAL is true, i.e. if VAL*a are BOOL, is like an if statement
end                     #Use =, e.g. VAL = VAL2a, so can't compare to null (should use "case when VAL is null" if can be null)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              DDL              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PGMODELER ==>          ##GUI modelling tool:
                       ##  - goal is to create/modify the DDL of a database, using a GUI.
                       ##     - can import DDL from existing database (of objects the user has permissions to query).
                       ##  - outputs SQL commands (or send to a database) or PNG image.
                       ##  - can validate DDL and issue warnings (requires connection to a database)
                       ##  - most DDL is available except foreign wrapper, etc., materialized views, event triggers,
                       ##    dictionaries, unlogged|temp tables, reference to VIEW COL
                       ##  - relationships: generalization is inherits, copy is create table like, others are foreign keys (with proper uniqueness)
                       ##  - there is a tree to go through object on the right panel

drop WORD [concurrently]#All create WORD have a related drop WORD.
[if exists] ...         #  - if exists (all): no error if not existing
[on TABLE]              #  - cascade|restrict:
cascade|restrict        #     - drop also objects that depend on it (cascade) or not restrict (def, should always be mentioned)
                        #     - doesn't work for database and tablespace: need to remove all objects manually first
                        #  - concurrently (index):
                        #     - like create index concurrently ...
                        #     - needs to be outside a transaction, restrict and only one INDEX
                        #... is VAR except:
                        #  - VAR... (domain|table|sequence|[materialized ]view|schema|extension|index|foreign table|type)
                        #  - VAR(...) (aggregate, function)
                        #  - VAR(none|TYPE, none|TYPE2) (operator)
                        #  - VAR on TABLE (trigger, rule)
                        #  - ( TYPE as TYPE2 ) (cast)
                        #  - VAR using INDEXMETHOD (operator class|family)
                        #  - for ROLE|current_user server SERVER (user mapping)
                        #drop table ... cascade|restrict also remove related INDEX, RULE, TFUNC and CONSTRAINT

alter TYPE VAR         ##Every create TYPE VAR (except cast) has a corresponding alter TYPE VAR ... to change options after creation
[if exists] ...        ##if exists is available if create ... if not exists is available.
                       ##Other alter ... are documented in this doc
alter TYPE VAR ...
rename to VAR2         ##For all TYPE but extension, operator

create [temp|unlogged]  #Creates a TABLE containing ARG columns, which can be:
table [if not exists]   # - COL_VAR TYPE [constraint CONSTRAINT] ... [initially deferred|immediate] [[not] deferrable]:
TABLE [of CTYPE](ARG...)#    - ... force the COL to respond true to a condition
[with ( VAR = VAL ... )]#    - CONSTRAINT is the variable name
[on commit              #    - [initially deferred|immediate] [[not] deferrable]: see concurrency chapter
delete rows|drop]       #    - ... can be:
[inherits ( TABLE2... )]#       - check(BOOL) [no inherit]:
[tablespace TABLESPACE] #          - def for CONSTRAINT is TABLE_COL_check
[as TABLE_VAL           #       - not null:
with [no] data]         #          - same as check(COL_VAR [is] not null)
                        #          - cannot have a name CONSTRAINT
                        #       - unique [with (VAR = VAL...)] [using index tablespace TABLESPACE]:
                        #          - same as check("no duplicate in COL_VAR")
                        #          - creates a btree INDEX called CONSTRAINT_VAR
                        #          - def for CONSTRAINT_VAR is TABLE_COL_key
                        #          - VAR = VAL...: storage options, cf below
                        #       - primary key [with (VAR = VAL...)] [using index tablespace TABLESPACE]:
                        #          - same as not null unique ..., but :
                        #             - there must be only one primary key
                        #             - def for CONSTRAINT_VAR is TABLE_pkey
                        #       - references TABLE2(COL2_VAR) [match full|simple] [on delete WORD] [on update WORD]:
                        #          - creates a foreign key:
                        #             - 1) inserted COL values must be one of the COL2 values
                        #             - 2) dropped COL2 values must not be referenced by COL values
                        #             - 3) 4) same but with update ... set
                        #          - COL2 must be unique
                        #          - on update|delete:
                        #             - doesn't change 1) or 3) behavior
                        #             - but WORD is the action when 2) (delete) or 4) (update) happens:
                        #                - no action (def): emits an error
                        #                - restrict: same but cannot be deferrable
                        #                - cascade: drop|change COL values too
                        #                - set default|null: set COL values to null|default
                        #          - match simple|full: when inserting nulls in COL: see null
                        #          - def for CONSTRAINT_VAR is TABLE_COL_fkey
                        #       - default VAL:
                        #          - default value when insert into omet COL_VAR
                        #          - VAL is evaluated at insertion, not table creation time
                        #          - cannot have a named CONSTRAINT
                        #  - like TABLE|CTYPE [including defaults|constraints|indexes|storage|comments|all]
                        #     - like putting the definitions of all the COL of TABLE|CTYPE. Can be used among other ARG.
                        #     - by def. only include COL_VAR TYPE + not null. Other things can be included with including ...
                        #        - indexes is for INDEX, primary key and unique
                        #        - foreign key cannot be copied
                        #  - [small|big]serial (with no TYPE):
                        #     - same as integer not null default nextval('TABLE_COL_SEQ'), after creating a TABLE_COL_SEQ, owned by COL
                        #     - can be followed by other constaints, i.e. unique, primary key, references or check
                        #  - multicolumn constraint:
                        #    - same syntax as usual ARG, but without COL_VAR TYPE, and constraint can use several COL_VAR...
                        #      (including for primary|foreign key) of the TABLE:
                        #       - e.g. unique ... -> unique(COL_VAR...) ...
                        #    - exceptions:
                        #       - references TABLE(COL2_VAR...) -> foreign key(COL_VAR...) references TABLE(COL2_VAR...)
                        #       - not for not null nor default
                       ##    - can also use this constraint:
                       ##       - exclude [using INDEXMETHOD] (COL_VAR [OPCLASS] [asc|desc] [nulls first|last] with OP ...)
                       ##         [where BOOL] [using index tablespace TABLESPACE]:
                       ##          - make new ROW that returns true with ROW OP (any already existing) ROW2, not be inserted.
                       ##          - filter for only ROW that match BOOL
                       ##          - OP must be commutative
                       ##          - has similar goal as unique, but more advanced. Could also use create domain TYPE.
                       ##          - cree CONSTRAINT TABLE_COL_excl
                       ##          - based on an INDEX:
                       ##             - INDEXMETHOD cannot be gin
                       ##             - can specify a TABLESPACE for it
                       ##             - [OPCLASS] [asc|desc] [nulls first|last]: passed to the create index ...
                        #Others:
                       ##  - inherits:
                       ##     - TABLE becomes child of TABLE2...:
                       ##        - i.e. TABLE inherits COL... of TABLE2 (but keeps its own ARG...)
                       ##        - manipulation on TABLE2 include COL... of TABLE unless "only (TABLE2)" is used:
                       ##           - the tableoid of the row can show which TABLE actually owns each row
                       ##           - commands like reindex, vacuum, etc. don't support inheritance
                       ##     - TABLE:
                       ##        - doesn't inherit unique, primary key, foreign key constraints (not inherited, and
                       ##          references TABLE implicitly means "only (TABLE)"), indexes, rules and triggers
                       ##            -> makes inherits mostly useless and dangerous
                       ##        - but inherits check() (unless no inherit is used) and not null
                        #  - temp:
                        #     - TABLE will be dropped at end of session
                        #        - should manually run analyze on it (no autovacuum)
                        #     - with "on commit", at the end of a successful transaction:
                        #        - delete rows: TABLE will be truncated
                        #        - drop: TABLE will be dropped
                        #     - max buffers increment size used is ENVVAR temp_buffers (def: 8MB)
                        #  - unlogged:
                        #     - WAL doesn't take TABLE into account, which makes it faster, but also not crash-safe, and cannot use replication to standby servers
                        #  - of CTYPE:
                        #     - declare the TYPE of the COL_VAR according to CTYPE
                        #        - should use COL_VAR with options ... instead of COL_VAR TYPE ...
                        #        - CTYPE must not be a TABLE, but a "real" CTYPE created with create type CTYPE
                        #     - TABLE will be dropped by drop CTYPE cascade
                        #     - cannot use inherits
                       ##  - with (VAR = VAL...):
                       ##     - fillfactor INT (10 to 100 (def)):
                       ##        - percentage of non-free space on the page in which the TABLE is stored
                       ##        - if lot of updates, more efficient to have lower than 100.
                       ##     - autovacuum_*: set ENVVAR autovacuum_* at the table-level
                       ##     - buffering on|off|auto
                       ##        - use a cache to speed up row inserts
                       ##        - def: auto, i.e. on when INDEX size > ENVVAR effective_cache_size
                       ##        - for gist INDEX only
                       ##     - fastupdate:
                       ##        - def: on
                       ##        - speed up writes, but can slow queries
                       ##        - for gin INDEX only
                       ##     - alter TYPE VAR ... set (VAR = VAL...) and alter TYPE VAR ... reset (VAR...) are also available for all TYPE that support with (VAR = VAL...)
                       ##        - can also do it at the COL-level: alter table TABLE alter COL_VAR [re]set ... (same for materialized views)
                        #  - as TABLE_VAL:
                        #     - creates TABLE as a copy of the subquery TABLE_VAL
                        #     - ARG... can only be COL_VAR..., and is optional
                        #     - of CTYPE, if not exists, inherits are not allowed
                        #     - with no data (def: with data): only copies the TYPE of TABLE_VAL, not the content

alter table TABLE
rename CONSTRAINT
to CONSTRAINT2         ##

alter table TABLE
rename COL_VAR
to COL_VAR2            ##

alter table TABLE
add COL_ARG             #Same as the ARG in create table TABLE( ARG )

alter table TABLE
drop [if exists]
COL_VAR restrict|cascade#

alter table TABLE
alter COL_VAR type TYPE #
[using VAL]            ##If VAL, convert VAL instead of the COL_VAL... (VAL usually perform an operation on the COL_VAL...)

alter table TABLE
alter COL_VAR set|drop
not null|default [VAL] ##

alter table TABLE
[no] inherit TABLE2    ##

alter table TABLE
[not] of CTYPE         ##

alter table TABLE
add constraint CONSTRNT##CONSTRAINT can be deffered with "not valid". Will be checked when doing:
... [not valid]        ##  alter table TABLE validate constraint CONSTRAINT

alter table TABLE
drop constraint
[if exists] CONSTRAINT
cascade|restrict       ##

create [or replace]     #Store the TABLE_VAL result of COMMAND (select ...) in a VIEW.
[temp] [recursive]      #  - COL_VAR...: rename (and not select) COL...
view VIEW[(COL_VAR...)] #  - temp: like for TABLE (def if TABLE is temp)
[with(security_barrier)]#  - recursive: like with (...)
as COMMAND              #Is readonly in following cases of COMMAND:
                        #  - join TABLE_VAL...
                        #  - top-level with, distinct, group by, limit|offset
                        #  - union|intersect|except
                        #  - COL_VAL not being simple COL_VAR
                        #  - with (security_barrier)
                        #To make it read|write in such cases, use TFUNC or RULE.
                        #Goal is encapsulation, with triggers to:
                        #  - manipulate underlying more complicated tables while maintaining simple unique interface
                        #  - restrict user permission (privileges on VIEW are not also on underlying TABLE):
                        #    - per column
                        #    - per row with where BOOL. Caution: hidden rows from TABLE in VIEW are securely hidden only if
                       ##      using create view VIEW with (security_barrier)
                        #      Cover channel attacks are still possible to infer size of hidden rows for example
                        #      (from explain query plans, time of queries, etc.).
                        #  - include metadata (timestamp, etc.)
                       ##or replace is not standard
alter view VIEW
alter COL_VAR
drop|set default [VAL] ##Can have different default than the underlying TABLE, when writing default values (not reading)

create materialized    ##Just like create table as ..., but can be manually updated with:
view TABLE[(COLVAR...)]##  refresh materialized view TABLE [with [no] data]
[with (VAR = VAL...)]  ##Does denormalization: manipulated like a VIEW but faster (doesn't query COMMAND, but write it).
[tablespace TABLESPACE]##Others:
as COMMAND             ##  - COL_VAR: like create view
[with [no] data]       ##  - with [no] data: controls whether the VIEW should forbid querying (def: with data (no forbid))
                       ##  - with (VAR = VAL...): like create table
alter materialized view
TABLE rename
COL_VAR to COL_VAR2    ##

with [recursive] TABLE ##Makes it possible to refer to the TABLE returned by GETCOMMAND inside COMMAND2 or inside further TABLE as (GETCOMMAND).
[(COL_VAR...)]         ##Is a prettier shortcut that avoids nesting subqueries (CTE, "Common Table Expressions"), similar to assigning subqueries to variables
as (GETCOMMAND),...    ##GETCOMMAND is done before COMMAND2, and is executed only once, even if COMMAND2 references it several times.
COMMAND2               ##If GETCOMMAND is not a select, COMMAND2 needs to be top-level (not a subquery), but subqueries can see TABLE from superqueries anyway.
                       ##COL_VAR... define new column name for the return TABLE.
                       ##If recursive, TABLE can be used inside GETCOMMAND:
                       ##  - usually this is a:
                       ##      select VAL union select ... from TABLE ... where BOOL (recursion stops when BOOL is false)
                       ##  - it usually also use (COL_VAR...) so the calling recursion can select the columns of the last recursion (which is just select VAL).
                       ##  - only select ... will be possible (with possible union, etc.) in GETCOMMAND

create [temp] sequence  #Creates a SEQUENCE, which is a special TABLE-like (can be used as TABLE_VAL) storing a streaming BIGINT algorithm
SEQUENCE                #Columns are:
[increment by BIGINT]   #  - sequence_name STR
[min|maxvalue BGNT]     #  - is_called BOOL (def: false): whether setval() or nextval() have been called
[start BIGINT]          #  - last_value BIGINT (def: 1): current value
[cache BIGINT] [cycle]  #  - start_value BIGINT (def: 1): first value (if not is_called)
[owned by TABL.COL]     #  - increment_by BIGINT (def: 1): how much to increment after each nextval()
                        #  - min|max_value BIGINT (def: 1 to bigintmax)
                        #  - cache_value (def: 1):
                        #     - how many numbers to cache in advance
                        #     - if more than 1 and concurrency, still works, but cache reserve the numbers, which produces "holes" (but numbers are still unique)
                        #  - is_cycled BOOL (def: false): when reaching end, either go to start, or produce error
                        #If temp, session-only, and can't use any SCHEMA.
                       ##If owned by TABLE.COL, if COL is dropped, SEQUENCE is dropped. Must be in same SCHEMA and same owner.
alter sequence SEQUENCE #Same ... as create sequence ... but:
...                     #  - can use no min|maxvalue, owned by none, no cycle
                        #  - restart BIGINT makes the cycle start again too

nextval(SEQUENCE_STR)  ##Go to next value, then returns current value. To keep case-sensitivity, write '"SEQUENCE"'
currval(SEQUENCE_STR)  ##Returns current value
lastval()              ##Returns current value of the last SEQUENCE called.
setval(SEQUENCE_STR,
BIGINT[, BOOL])        ##Change last_value and is_called (def: true)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              DML              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


select [distinct        #Returns COL_VAL... of TABLE_VAL, with records filtered by BOOL
[on (COL_VAL...)]]      #VAL can be [TABLE.]* to mean all columns (bad practice in production)
VAL [[as] COL_ALIAS]... #ALIAS:
[from TABLE_VAL]        #  - goal:
[[ as] TABLE_ALIAS]...  #     - shorter version
[where BOOL]            #     - can give name to temporary VAL (e.g. AFUNC(COL) or COL + VAL)
[order by COL_VAL...    #     - necessary for subqueries
[asc|desc|using OP]     #  - can be:
[nulls first|last]]     #     - TABLE_ALIAS: TABLE_ALIAS[( COL_ALIAS... )]
[group by COL_VAL       #     - COL_ALIAS
[having BOOL]]          #  - ALIAS is the new name (former VAR will not be usable anymore in current query)
[offset UINT]           #  - "as" is recommended
[fetch first UINT2 rows #VAL... can be:
only]                   #  - COL_VAL
                        #  - any VAL (e.g. select NUM;)
                        #  - FUNC()
                        #from TABLE_VAL:
                        #  - can use ... join (see below)
                       ##  - can use a FUNC() returning a TABLE_VAL.
                        #    In standard SQL, from FUNC(...) -> from lateral (select FUNC(...)) ALIAS
                       ##  - can be ommitted, e.g. select VAL;
                        #distinct:
                        #  - remove rows will all entries duplicate.
                       ##  - on (COL_VAL...): only targets COL_VAL..., which must be among the leftmost COL_VAL... in order by ...
                        #where BOOL:
                        #  - filter rows according to BOOL
                        #group by COL_VAL:
                        #  - all VAL... in select ... (except COL_VAR if group by COL_VAR) must be scalar values, typically through a AFUNC()
                        #  - can also be NUM like order by
                        #  - only non-duplicate entries of COL_VAL are selected, and COL_VAL2 are aggregated according to groups of duplicate COL_VAL, using AFUNC() if provided.
                        #having BOOL:
                        #  - like where BOOL, but on the groups created by group by
                        #  - must use scalar values, typically though a AFUNC()
                        #order by:
                        #  - sort according to COL_VAL... (def: asc, and nulls first for desc, nulls last for asc)
                        #  - using OP: using < is like asc, etc.
                        #  - can also be NUM, to be the VAL... number NUM
                        #offset and fetch:
                        #  - select only the first UINT2 rows, or according to an offset UINT
                        #  - order by should also be used to be consistent
                        #Order of execution:
                        #  - select 6 from 2, (1) where 3 group by 4 having 5 order by 7 offset|fetch 8 (1 is a subquery)

TABLE_VAL [QUAL] join   #Produces a TABLE_VAL from two TABLE_VAL, for a select ... from TABLE2_VAL:
TABLE2_VAL              #  - can use all TABLE_VAL in the other clauses. Should qualify all COL by using TABLE_VAL.COL
[on BOOL|               #Conditions are impossible for cross join, necessary for others:
using(COL_VAR...)]      #  - on BOOL: filter according to BOOL
                        #  - using(COL...) and natural: avoid them (can do same with on BOOL)
                        #QUAL can be :
                        #  - cross:
                        #     - uses the cartesian product of the TABLE_VAL...:
                        #        - all combinations of values, starting from first TABLE
                        #        - i.e. a TABLE_VAL with length = product of lengths of all TABLE_VAL
                        #     - can also be written TABLE_VAL, TABLE2_VAL
                        #  - inner (def): same (but can use on BOOL)
                        #  - left:
                        #     - same but instead of filtering, fill TABLE2_VAL with null, and remove duplicate rows that have been filled this way.
                        #     - i.e. every TABLE_VAL value will be present, with unfiltered TABLE2_VAL combinations, null if none
                        #  - right: same but fill with null in TABLE_VAL
                        #  - full: left outer + right outer
SELF JOIN ==>           #Are used to output twice the same column.
                        #Must use ALIAS to produce virtual copies of the same TABLE.
                        #E.g.
                        #  - replacing TABLE.COL and TABLE.COL2 (foreign keys refering to TABLE2.COL), with TABLE2.COL2
                        #      select ALIAS.COL2, ALIAS2.COL2 from TABLE, TABLE2 ALIAS, TABLE2 ALIAS2 where TABLE.COL = ALIAS.COL and TABLE.COL2 = ALIAS.COL

values (VAL...)...
order by ...            #Produce a TABLE_VAL with constant VAL...
offset ... fetch ...    #Arguments: see select

(select ...)            #Subquery, returning a TABLE_VAL. If in select VAL, must have a TABLE_ALIAS[( COL_ALIAS... )]
(values ...)            #() are needed only when there might be confusion. Otherwise useless.
                        #TABLE (and TABLE_ALIAS) are visible from superqueries to subqueries (but not inverse):
                        #  - in a from ... clause:
                        #     - only if preceded by lateral
                        #        - ex: select * from TABLE, lateral (select TABLE.COL, TABLE2.COL from TABLE2)
                        #     - for a FUNC(TABLE), must be after TABLE in the from ... clause
                        #        - ex: select * from TABLE, FUNC(TABLE)
                        #  - otherwise: makes a crossjoin ("correlated subquery")

select ...
union|intersect|except  #Both select ... must have same COL... with same types.
[all] select ...        #If not "all", removes duplicates afterwards.
[order by ...]          #order by must be a NUM or a COL_VAR, not a COL_VAL

insert into TABLE       #Adds TABLE_VAL records to COL... of TABLE
(COL_VAR...)            #Omitted COL get default value
TABLE_VAL|default values#If TABLE_VAL is values ... (often the case), can use default as VAL.

update TABLE            #Assign VAL... to COL... records (filtered by BOOL)
[[as] TABLE_ALIAS]      #To reference other TABLE, either:
set ... [from ...]      #  - use with ...
[where BOOL]           ##  - use from ... (same as select ... from ..., makes a cross join)  (try to avoid it)
                        #  - use a subquery in BOOL, e.g. where COL in (select ... from TABLE2 where BOOL2)
                        #set ... can be either:
                        #  - set COL_VAR = VAL ...
                        #  - set ( COL_VAR... ) = ( VAL... ) ...
                        #VAL can be default
                        #TABLE is implicit in the query. TABLE.COL should not be used, but COL.

delete from TABLE
[[as] TABLE_ALIAS]
[using ...]             #Erases records of TABLE according to BOOL (by default all records)
[where BOOL]            #using ...: same as from ... in update.

GETCOMMAND ==>         ##insert|update|delete can be finished by returning ...:
                       ##  - ... is same as select ..., but points to the rows inserted|updated|deleted
                       ##  - make those commands return like a select
                       ##  - cannot be used in a subquery
                       ##GETCOMMAND refer to any of them, or to select

truncate table TABLE...#Like delete from TABLE with no BOOL (faster and more vacuum-friendly than delete from TABLE;)
[restart identity]     ##  - restart identity: sequences are set to start_value
cascade|restict        ##  - cascade: truncates TABLE2... depending on TABLE too
                       #Is not concurrency-safe.

copy TABLE[(COLVAR...)]##Append TABLE to a FILE_STR or from a FILE_STR, for COL...
from|to ...            ##Copy TABLE_VAL to is also possible
[ with OPTIONS...]     ##VIEW is only possible through copy (select * from VIEW)
                       ##... can be:
                       ##  - FILE_STR (must use absolute path)
                       ##  - stdin|stdout
                       ##  - program STR, which use input|ouput piping to shell command STR
                       ##OPTIONS are space-separated and can be:
                       ##  - text(def)|csv|binary
                       ##  - oids: copies OIDS too
                       ##  - delimiter STR (def in text '\t', in csv ',')
                       ##  - null STR (def in text '\N', in csv ''): STR used to represent null
                       ##  - header (only in csv): first CSV row === field names
                       ##  - quote STR (def '"')
                       ##  - escape STR (only in csv, def: '\')
                       ##  - force quote( COL_VAR... )|* (only in csv, and copy to): quote all values, except null
                       ##  - force not null( COL_VAR... ) (only in csv, and copy from)
                       ##  - encoding STR (def: client_encoding)
                       ##  - freeze: force a vacuum freeze first
                       ##For portability, ENVVAR DateStyle should be 'ISO' for copy to, and IntervalStyle should not be 'sql_standard'


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           FUNCTIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


AGGREGATE FUNCTION ==>  #Transform a COL_VAL to a scalar VAL.
                        #If AFUNC(VAL), VAL is converted to COL_VAL.
                        #WFUNC are performed after AFUNC, and AFUNC after all the later clauses (e.g. BOOL).
                        #To include a former in a later, use a command substitution, e.g. instead of AFUNC(COL_VAL):
                        #  (select AFUNC(COL_VAL) [from COL_VAL])
AFUNC([distinct] ...    #For all AFUNC. Same semantic as select ...
[order by ...])         #Nulls are ignored
sum(COL_VAL)
count(COL_VAL)          #Can do count(*)
avg(COL_VAL)
min|max(COL_VAL)
bit_and|or(COL_VAL)     #Bitwise and|or on all values
bool_and|or(COL_VAL)    #Boolean and|or on all values
exists(COL_VAL)         #Returns false if COL has length 0
array|json_agg(COL_VAL) #Returns as an ARRAY or JSON array.
string_agg(COL_VAL, STR)#Returns as a STR delimited by STR

corr(COL_VAL, COL_VAL2) #r
regr_r2(COL_VAL,
COL_VAL2)               #r₂
stddev|var|covar_
pop_samp(COL_VAL,
COL_VAL2)               #Standard deviation, variance, covariance, / (n-1) or /n
regr_count(COL_VAL,
COL_VAL2)               #Exclude row with one null
regr_avgx|y(COL_VAL,
COL_VAL2)               #sum(VAL[2]/regr_count(VAL, VAL2))
regr_intercept|slope
([TABL]_VAL,[TABL_]VAL2)#
regr_sxx|syy|sxy        #In order:
([TABL]_VAL,[TABL_]VAL2)#  - sxx: sum(VAL^2) - sum(VAL)^2/regr_count(VAL,VAL2)
                        #  - syy: sum(VAL2^2) - sum(VAL2)^2/regr_count(VAL,VAL2)
                        #  - sxy: sum(VAL*VAL2) - sum(VAL)*sum(VAL2)/regr_count(VAL,VAL2)

AFUNC (COL_VAL) over    #Window function WFUNC :
([partition by COL_VAR] #  - the whole expression returns a COL of length n (not 1)
 [order by ...]         #  - for each element, AFUNC() is fired on a moving WINDOW, for each COL group. Groups are defined as:
 [rows|range between    #     - if "partition by": according to identical VAL in COL_VAR
 WORD and WORD2])       #     - if no "partition by":
                        #        - if "order by": same, but each group also include previous groups
                        #        - if no "order by": only one group
                        #  - WINDOW is:
                        #     - inside each group, between WORD and WORD2, which can be:
                        #        - unbounded|VAL preceding|following
                        #        - current row: according to current element
                        #     - def is between unbounded preceding and unbounded following, which means AFUNC() est effectue sur l'ensemble du groupe à chaque fois
                        #     - if there is a group of identical VAL..., current row means:
                        #        - if range: for each VAL, the first one for WORD, and the last one for WORD2
                        #        - if rows: for each VAL, that VAL
                        #  - l'output est trie selon COL2, au sein de chaque groupe
AFUNC([COL_VAL]) over
WINDOW,
AFUNC2([TABLE2_VL]) over#If multiple AFUNC in a statement use the same WINDOW, it can use an alias WINDOW, then put a later
WINDOW ...              #clause, as followed :
window WINDOW as ( ... )#
                        #Quelques AFUNC ne sont possible qu'en tant que WFUNC :
row_number()            #Order au sein de la WINDOW
cume_dist()             #Row number au sein de la WINDOW, divided by number of rows for the group
first|last_value(COL_VR)#Returns first|last element of the WINDOW
nth_value(COL_VAR,UINT) #Returns element numero UINT of the WINDOW (null if not existing)
lag|lead(COL_VAR,UINT
[, VAL])                #Returns the element UINT times before|after in the WINDOW (VAL (def: null) if not existing)
rank()                  #Classement numerique, according to "order by", au sein de WINDOW
dense_rank()            #Same but if ex-aequo, next rank doesn't jump
percent_rank()          #Same but from 0 to 1
ntile(UINT)             #Returns 1..UINT, starting from lower to upper value, with equal (if possible) number of values in
                        #each 1..UINT (buckets)

create aggregate AFUNC ##Creates an AFUNC(COL_VAL...):
(TYPE...) (sfunc =FUNC,##  - TYPE are type of each COL.
stype = TYPE2          ##  - each VAL... of each COL is passed inside successive FUNC(VAL2, VAL...)
[, finalfunc = FUNC2]  ##     - VAL2 is the return value of the last FUNC(), and its type is TYPE2
[, initcond = STR]     ##     - its value in the first FUNC() is TYPE2 STR (typecasting from STR) (def: null)
)                      ##  - last FUNC can be different if FUNC2(VAL2) is defined: takes only one ARG, and can return any type.
                       ##  - last VAL2 is returned
                       ##  - by convention, null should be ignored in FUNC, unless there are only null, where null should be
                       ##    returned

create [or replace]    ##Creates a user-defined fonction, that can used anywhere.
function FUNC( [[VAR]  ##  - or replace: if already existing, will be overwritten (fails if different types)
TYPE[ default VAL]...])##  - arguments:
returns TYPE2          ##    - VAL is the default value
as STR_LIT             ##    - TYPE can be any but also:
language LANG          ##      - CTYPE|TABLE:
[window] [immutable|   ##        - TABLE_VAL are converted to CTYPE
stable|volatile]       ##        - calling FUNC(TABLE_VAL) needs a reference to TABLE. Examples:
[leakproof] [strict]   ##           - select FUNC(TABLE_VAL) from TABLE
[security              ##           - select * from TABLE, FUNC(TABLE_VAL)
invoker|definer]       ##        - FUNC(TABLE|CTYPE) can also be called TABLE|CTYPE.FUNC, so it is unwise to name FUNC same
[cost FLOAT]           ##          as a member of TABLE|CTYPE
[rows UINT]            ##      - record:
                       ##        - like CTYPE of any*
                       ##      - pseudo-type:
                       ##         - any[element|[non]array|enum|range]:
                       ##           - polymorphic types.
                       ##           - Must be all of the same type (args and return value), for a specific call.
                       ##           - anyarray|range must have same type as other any*
                       ##           - if return value is polymorphic, at least one arg must be too.
                       ##           - $0 is a VAR with the polymorphic type, initialized to null.
                       ##             Can be used to store return value or do computations.
                       ##         - cstring (null-terminated, for C only)
                       ##    - variadic ARG (must be ARRAY):
                       ##      - elements of ARR must then be of same TYPE
                       ##      - can fire it with FUNC(..., variadic ARRAY) or FUNC(..., ...)
                       ##      - is not optional by default, needs to add e.g. variadic VAR TYPE default array[]::TYPE[]
                       ##    - overloading is possible. If multiple choice of casting, the pg_type.typeispreferred is used.
                       ##    - can call by name : FUNC( VAR := VAL )
                       ##  - return value:
                       ##    - always returns a TABLE_VAL (TYPE actually means TABLE_TYPE_VAL)
                       ##    - returns first row only, unless setof TYPE is used
                       ##    - can also be in the function args instead of returns ...:
                       ##      - as out ARG...:
                       ##        - ARG... can be assigned inside FUNC instead of the usual way to return a value
                       ##          (in language that permits assigning to VAR, e.g. not SQL but PL/SQL)
                       ##        - if several ARG...:
                       ##          - returns as a multicolumn TABLE_VAL in a from clause
                       ##          - otherwise, returns as one CTYPE_COL_VAL
                       ##        - to return more than first row, put also returns setof TYPE (record if several ARG...)
                       ##      - inout ARG is same as ARG out ARG (both ARG and return value)
                       ##    - TYPE2 is like TYPE, but can also be:
                       ##      - void: if no return value
                       ##      - CTYPE|TABLE or table( COL TYPE ... ): like specifying several out ARG...
                       ##      - record: FUNC must be called with FUNC(...) as [ALIAS](COL_VAR TYPE ...) to precise types of
                       ##        RECORD
                       ##  - VAR%type: TYPE of a VAR, including TABLE.COL. Can be a VAR from the FUNC too
                       ##    (but not for an ARG from another ARG)
                       ##  - body:
                       ##     - STR. Better to use $$...$$ notation
                       ##  - can be written in different LANG (see below):
                       ##     - SQL
                       ##     - pl*: procedural languages, faster than SQL functions, among:
                       ##       - already in PostgreSQL (just need create extension pl*)
                       ##         - plpgsql
                       ##         - plpython
                       ##         - plperl
                       ##         - pltcl
                       ##     - C, faster than pl*
                       ##     - internal:
                       ##       - builtins functions. Can only refer to them by putting the C function name in the body STR
                       ##       - possible (only) use: renaming
                       ##       - actually C functions, but compiled in (not loaded with a shared library)
                       ##  - window:
                       ##     - means it is a WFUNC (only in C and some PL/*)
                       ##       As such, it passes some extra arguments about the current window.
                       ##  - immutable|stable|volatile: used for performance optimization. Can be:
                       ##     - immutable: doesn't read|write global state (including the database or the pg_catalog.*)
                       ##     - stable: doesn't write global state
                       ##     - volatile (def): write|read global state
                       ##  - leakproof: used for performance and security optimization. Means it is immutable, and
                       ##    doesn't give informations about arguments VAL aside from return value (e.g. does not throw errors
                       ##    for some return values but not others, prints arguments VAL, etc.)
                       ##  - strict:
                       ##     - returns automatically null if an arg is null.
                       ##     - for VARIADIC arg, only works if whole VARIADIC is null, not only part of it.
                       ##  - privilege are the ones of:
                       ##      - security invoker (def): the caller
                       ##      - security definer: the user creating the FUNC. Forbids using set role.
                       ##        Should also define fonction-specific ENVVAR search_path by adding a temporary SCHEMA at the
                       ##        end (to avoid malicious VAR shadowing)
                       ##  - cost FLOAT: CPU cost for the planner (cpu_operation_cost) (def: 1 for C FUNC, 100 for others)
                       ##  - rows UINT: average number of rows returned, for the planner (def: 1000). Only if return value
                       ##    returns several rows.
alter function FUNC(...)
...                     #... is any of the options after language LANG. Can use not leakproof.


do [language LANGUAGE]
STR                    ##Execute an anonymous function STR (body of function), from LANGUAGE (def: plpgsql)

create [constraint]     #Execute a FUNC for a specific EVENT on TABLE or (not standard) VIEW.
trigger TFUNC           #Multiple TFUNC are fired alphabetically.
before|after|instead    #  - EVENT can be insert|delete|truncate, or update [of COL_VAR...]
of EVENT [or EVENT2...] #    COL_VAR... means COL_VAR or COL_VAR2, etc.
on TBL|VIEW [from TABL2]#    Truncate only on before|after and on TABLE.
[not deferrable]        #  - before|after|instead of is for the constraint checking + the operation itself (row-wise).
[initially immediate|   #    So before fired even if constraint fails, but not after.
deferred] [for each row]#    If COMMAND is the one triggering EVENT "for each row":
[when ( BOOL )]         #       - before|instead of are executed row-wise according to COMMAND (so next rows in TFUNC see
execute procedure       #         changes of previous rows by COMMAND)
FUNC(STR...)            #       - after is executed table-wise according to COMMAND (so rows in TFUNC see changes of all rows
                        #         by COMMAND)
                        #    before|instead of are executed on all rows
                        #    instead of can only be on VIEW and cannot use when BOOL. Usually used to modify the underlying
                        #    TABLE so users can modify VIEW.
                       ##    Can also use or EVENT2...
                        #  - FUNC: any language (e.g. PL/* or C) but not SQL. See doc about TFUNC for those languages.
                        #    Must take no arguments (arguments are passed with special variables) and with a trigger return
                        #    type, but actually returning:
                        #     - "before" + "for each row":
                        #        - null: skip further TFUNC and cancel statement (for that row)
                        #        - CTYPE|TABLE with same structure as new|old (including them):
                        #          - modified row: modify new and return it.
                        #          - unmodified row: return old|new according to the operation
                        #     - otherwise: return null
                        #  - for each row: fired for each manipulated row (not fired if no manipulated row), and not once for
                        #    all rows.
                        #    Is necessary for contraint trigger and instead of.
                        #    Is impossible on non-truncate VIEW.
                       ##  - constraint: makes it possible to:
                       ##      - change not deferrable, etc. with set constraints TFUNC deferred|immediate (def: immediate)
                       ##      - use not deferrable, etc. (same as for CONSTRAINT).
                       ##    Must be "after" and "for each row".
                       ##    Can be used to simulate a constraint, in which case an exception should be raised.
                       ##    Can also use set constraints to fire trigger at specific point during current transaction.
                        #  - when ( BOOL ):
                        #     - can use old|new like trigger functions
                        #     - BOOL cannot be a subquery.
                        #A TFUNC can fire another one ("cascading triggers"), including recursively.
                        #Can see current TFUNC depth with pg_trigger_depth()
                    tcn###To create a TFUNC notifying of each modification:
                       ###  triggered_change_notification([STR]) is a function to use to do notify STR, for each row,
                       ###  after insert and/or update and/or delete, with payload explaining the modification.
create event trigger   ##Like TFUNC but EVENT can be DDL not DML:
EFUNC on EVENT         ##  - ddl_command_start|end: before|after a create, alter or drop (except for cluster-wide objects
[when tag in (STR...)] ##    and TFUNC|EFUNC)
execute procedure FUNC()#  - sql_drop: same but only for drop
                       ##     - FUNC() can use pg_event_trigger_dropped_objects() which returns a TABLE_VAL with one row for
                       ##       each dropped objects and COL...:
                       ##        - classid: DATABASE OID
                       ##        - objid: object OID
                       ##        - objsubid: e.g. for columns the attnum
                       ##        - object_type STR
                       ##        - schema_name STR
                       ##        - object_name STR
                       ##        - object_identity STR: SCHEMA.OBJECT name
                       ##STR... are commands (e.g. 'drop table') to filter EFUNC.
                       ##FUNC must return have a event_trigger return type, and takes no arguments.
                       ##Must be superuser.
alter event trigger
disable|enable
[replica|always]       ##
alter table TABLE      ##all|user:
enable|disable         ##  - not for RULE
[replica|always]       ##  - same but all include system TRIGGER (enforcing CONSTRAINT), not user
rule|trigger           ##replica|always:
RULE|TRIGGER|all|user  ##  - def: affects only non-replication sessions (ENVVAR session_replication_role "origin" (def) or
                       ##    "local")
                       ##  - replica: affects only replication sessions (session_replication_role "replica")
                       ##  - always: affects all sessions

create [or replace]    ##Creates a macro that modifies a command matching EVENT into COMMANDS (add or replace according to
rule RULE as on EVENT  ##"instead"), for TABLE, when BOOL:
to TABLE [where BOOL]  ##  - EVENT:
do [instead] COMMANDS  ##     - select|insert|update|delete
                       ##     - select must be "instead", non-"where BOOL" and with select COMMAND
                       ##       (similar to create a VIEW)
                       ##  - BOOL:
                       ##     - can refer to old|new.COL (like triggers)
                       ##COMMANDS:
                       ##  - nothing: with "instead", original EVENT is dropped
                       ##  - COMMAND or ( COMMAND;...):
                       ##     - Must be select|insert|update|delete|notify.
                       ##     - For insert, the original EVENT is performed before COMMAND, for others after.
                       ##     - "instead" non-"where BOOL" COMMAND insert|update|delete must be GETCOMMAND
                       ##       so that calling command can use returning itself.
                       ##When to use rules:
                       ##  - VIEW are better than RULE with select EVENT.
                       ##  - rules are similar to TFUNC, often faster but harder and less flexible.
                       ##    Prefer triggers unless performance is critical, in which case check if actually faster.

create operator OP     ##Creates an OP, i.e. a FUNC using a sign. Can have one or two arguments.
( left|rightarg = TYPE,##Can be an already existing OP, with different TYPE (overloading)
procedure = FUNC       ##OP is [+-*/<>=~!@#%^&|`?]+
[, commutator = OP2]   ##Other args are for optimization (hints the planner in rewriting queries so they match an INDEX):
[, negator = OP3]      ##  - OP2 if VAL OP VAL2 is equivalent to VAL2 OP2 VAL
[, restrict = FUNC2]   ##  - OP3 if VAL OP VAL2 is equivalent to not VAL2 OP3 VAL, or OP VAL to not OP3 VAL
[, join = FUNC3]       ##  - FUNC2 tells when using VAL OP VAL2_LIT (returning BOOL), how much portion of the COL is likely to
[hashes]               ##    be chosen.
                       ##    Can choose among default ones:
                       ##      - eqsel: chooses a small portion, like =
                       ##      - neqsel: large portion, like <>
                       ##      - scalar[l|g]tsel: semi-large portion before or after, like < <= or > >=
                       ##  - FUNC3 is like FUNC2 but for VAL OP VAL2. Default ones:
                       ##      - eqjoinsel: like =
                       ##      - neqjoinsel: like <>
                       ##      - scalar[l|g]tjoinsel: like < <= or > >=
                       ##      - areajoinsel: like BOX operators
                       ##      - positionjoinsel: like POINT operators
                       ##      - contjoinsel: like <@ @>
                       ##  - hashes: VAL OP VAL2 is true if hash(VAL) = hash(VAL2)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      FUNCTIONS LANGUAGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SQL FUNCTIONS ==>      ##  - execute SQL statements, returns the last one:
                       ##    - a GETCOMMAND
                       ##    - or returns void
                       ##  - either VAR or $NUM is used to refer to ARG...
                       ##    - if VAR is a COL in the current statement, use TABLE.VAR (for the COL) or FUNC.VAR (for the VAR)
                       ##    - are readonly
                       ##  - some COMMAND interdites: transactions blocks, vacuum, etc.

PL/PGSQL ==>           ##PL/PGSQL ends with ; but structure have only one ; at end of the structure.
                       ##Arguments:
                       ##  - if VAR is a COL in the current statement, use TABLE.VAR (for the COL), LABEL.VAR (for a VAR
                       ##    declared in a block LABEL) or FUNC.VAR (for an argument)
                       ##TABLE (including COLs) must have same types accross executions because of caching. Solutions:
                       ##  - using RECORD instead of CTYPE|TABLE
                       ##  - using execute STR (doesn't cache)
GETCOMMAND ==>         ##In PL/PGSQL, also implies execute GETCOMMAND

<<LABEL>>              ##Scope block. Main function must be in a scope block. Variables are local to the block.
[declare               ##VAR must all be declared, as VAR [constant] TYPE [not null] [default VAL];...
  DECLARATIONS...]     ##exception ... catches exceptions:
begin                  ##  - EXCEPTION is either:
  STATEMENTS...        ##     - exception word (see appendix online or error messages) or "others" meaning any other exception.
[exception             ##     - SQLSTATE, i.e. an error code NUM typecasted to sqlstate: sqlstate '...'
  when EXCEPTIN then ...#  - can use the local variables when the exception happened.
  ...]                 ##  - can use special variables sqlstate (current SQLSTATE) and sqlerrm (error message STR)
end [LABEL]            ##  - get stacked diagnostics VAR = WORD2 ... assigns to VAR2 according to WORD2 among:
                       ##     - returned_sqlstate: like sqlstate
                       ##     - message_text: like sqlerrm
                       ##     - column|constaint|pg_datatype|table|schema_name: column|...|schema raising the exception
                       ##     - pg_exception_detail|hint: extra error messages
                       ##     - pg_exception_context: call stack
                       ##Is also a transaction block, with exceptions rollbacking the transaction.
raise [LEVEL]          ##Raises an exception.
[using VAR = STR ...]  ##LEVEL can be exception (def), warning, notice, info, log or debug. Exception throw an exception, but
                       ##others don't, they just print a message.
                       ##VAR = STR provide informations among:
                       ##  - errcode: like EXCEPTION in exception block, but SQLSTATE is as STR
                       ##  - message: like sqlerrm
                       ##  - detail, hint, column|...|schema: like in get stacked diagnostics
                       ##If no using..., rethrow a currently catched exception.
raise [LEVEL] EXCEPTION
 ...                   ##Like raise [LEVEL] using errcode = EXCEPTION ...
raise [LEVEL] STR_LIT,
VAL...  ...            ##Like raise [LEVEL] using message = STR ..., where STR can substitute % symbols with VAL...

return [VAL]           ##Return statement of the function.
                       ##VAL can be omitted:
                       ##  - if out ARG... are used
                       ##  - if void if the return type
return next [VAL]      ##With setof ... return type (multiple rows), add VAL (CTYPE for multiple columns) or GETCOMMAND to
return query GETCOMMAND##the returned TABLE_VAL. Needs to be called several time, then a return; will return the whole set.
                       ##If VAL is ommitted and out ARG... are used, use the current value of those ARG... instead.

VAR := VAL             ##Assignment
SQL_COMMAND            ##Any SQL command can be performed but:
                       ##  - select ... (except select ... into) (including select VAL or select FUNC()) must be written
                       ##    perform ..., and has no return value
                       ##    - written perform (...) for a with ... query
null;                  ##To do nothing (empty line), write null;
GETCOMMAND             ##Put the first row returned into VAR, which can be:
into [strict] VAR      ##  - VAR... for each column
                       ##  - CTYPE|TABLE or record for all columns
                       ##If more than one row is returned (and, if select, strict is used), an exception TOO_MANY_ROWS is
                       ##fired. If no row returned and strict, an exception NO_DATA_FOUND if fired.
                       ##select ... into is PL/PGSQL, different than the SQL select ... into

execute STR            ##Execute SQL (not PL/PGSQL) command STR.
[using VAL...]         ##STR can contain $NUM that will substituted by each VAL...:
                       ##  - must be used for all VAR coming from the FUNC ARG...
                       ##  - can only be used in select|insert|update|delete
                       ##  - cannot be used by TABLE_VAR and COL_VAR: they must be supplied as a STR concatenation
                       ##STR should be escaped:
                       ##  - $$...$$ for the command
                       ##  - use quote_* for dynamic variables

found                  ##Variable containing true if last SQL command returned|manipulated at least one row.
get diagnostics        ##WORD can be either:
VAR = WORD             ##  - row_count (assign to VAR the number of rows returned|manipulated by last SQL command)
                       ##  - result_oid: OID of last row manipulated (if table has OIDs)

if BOOL then ...
[elseif BOOL then ...]
[else ...] end if;

case ... end case;     ##Like SQL case ... end

[<<LABEL>>]
[while BOOL] loop ...
end loop [LABEL]       ##BOOL is true by def.

[<<LABEL>>]
for INT_VAR in [reverse]
NUM..NUM2 [by NUM3] loop
... end loop [LABEL]   ##

[<<LABEL>>]            ##Iterates over rows returned by GETCOMMAND
for VAR in GETCOMMAND  ##VAR can be:
loop ...               ##  - VAR... for each column
end loop [LABEL]       ##  - CTYPE|TABLE or record for all columns

[<<LABEL>>]            ##Looping through a VAR2:
foreach VAR [slice NUM]##  - ARRAY
in array VAR2 loop ... ##  - CTYPE: VAR is then VAR... for each value
end loop [LABEL]       ##If NUM > 0, VAR receives an ARRAY2 of dimension NUM each time (for multidimensional VAR2)

exit [LABEL] [when BOOL]#Exit a loop or block with LABEL (def: innermost one), if BOOL (def: true)
continue ...           ##Same as exit ... but skip next statements to start a new cycle.

refcursor              ##Special PL/PGSQL type to store a CURSOR.
                       ##Similar as SQL CURSOR (move and close identical). Differences are below.
                       ##Can be declared as:
                       ##  - CURSOR [scroll] cursor [(ARGS...)] for select ...:
                       ##    - like a SQL CURSOR, but:
                       ##      - not opened, needs to do open CURSOR;
                       ##      - if ARGS, can pass arguments to open CURSOR(...);
                       ##  - CURSOR refcursor:
                       ##    - same but needs to specify the query with open, either with open CURSOR [scroll] for select ...
                       ##      or open CURSOR [scroll] for execute ...
                       ##Can be returned or passed as argument:
                       ##  - pass the CURSOR as STR, casted as refcursor, e.g. FUNC(refcursor 'CURSOR')
                       ##  - to use a CURSOR in the calling FUNC, either:
                       ##     - use the names of the CURSOR.
                       ##       No need to return them, they close at end of transaction of calling function.
                       ##       If CURSOR not provided as argument, will generate a random name
                       ##     - return them, and use the return value
fetch ... into VAR     ##Just like select ... into VAR (including: cannot fetch several rows)
[<<LABEL>>]
for RECORD
in REFCURSOR[(...)]
loop ... end loop [LABL]#Loops in a REFCURSOR. Implicitely open and close the REFCURSOR

TFUNC FOR PL/PGSQL ==> ##Return type is trigger, but must return either:
                       ##Arguments are in tg_argv STR_ARR (index starts at 0 and is of size tg_nargs INT).
                       ##Can also use:
                       ##  - new|old RECORD: current row for "for each row" TFUNC (only if "for each row")
                       ##    new is absent if delete, and old is absent if insert.
                       ##  - tg_name STR: TFUNC name
                       ##  - tg_when STR: before|after|instead of
                       ##  - tg_level STR: statement|row ("for each row")
                       ##  - tg_op STR: insert|update|delete|truncate
                       ##  - tg_relid OID: of the TABLE
                       ##  - ts_table_name STR: name of the TABLE
EFUNC FOR PL/PGSQL ==> ##Return type is event_trigger, but doesn't use return.
                       ##Can also use:
                       ##  - tg_event STR: the EVENT
                       ##  - tg_tag STR: the COMMAND

PL/SH ==>              ##plsh, for any installed shell, including Bash:
                       ##  - Impossible: CTYPE, any*, out ARG, setof
                       ##  - Body: starts with correct shabang.
                       ##  - Arguments: same as the shell (for Bash: $1, "$@", $#, etc.)
                       ##  - Return value:
                       ##    - stdout, with a newline stripped
                       ##    - null if exit (exit code 0) with nothing on stdout
                       ##  - Exception:
                       ##     - printing to stderr
                       ##     - non-0 exit code
                       ##  - SQL commands: fired through psql STR command line.
                       ##  - Can use any executable, as postgres user.
                       ##  - TFUNC and EFUNC:
                       ##    - defines PLSH_TG_* like tg_* in PL/PGSQL (old|new unavailable)
                       ##    - TFUNC() est executee, mais ne modifie jamais le row courant

PL/R ==>               ##plr: for R:
                       ##  - Arguments: named arguments.
                       ##  - Return value: return(VAL)
                       ##  - Types (other than obvious):
                       ##    - dimensions (max 3):
                       ##      - 0 dimension  (TYPE)                    <-> VAL
                       ##      - 1 dimension  (setof TYPE, ARR)         <-> VALv
                       ##      - 2 dimensions (setof CTYPE, ARR(2 dim)) <-> DATA.FRAME|ARR(2dim)
                       ##        - TABLE|CTYPE as argument -> DATA.FRAME, but must use as.data.frame(DATA.FRAME) if want to
                       ##          be returned
                       ##      - 3 dimensions (ARR (3 dim))             <-> ARR(3dim)
                       ##    - null <-> NA|NULL(pref)
                       ##    - BYTEA <-> OBJECT ([un]serialize on RAW)
                       ##    - everything else <-> STR
                       ##  - TFUNC: defines pg.tg.* like tg_* in PL/PGSQL
                       ##  - WFUNC: will pass:
                       ##     - fargNUM...: other values in WINDOW (NUM starts at 1)
                       ##     - fnumrows: WINDOW size
                       ##     - prownum: offset of WINDOW
                       ##  - Global data are possible across calls, including global functions.

PL/R FUNC() ==>        ##
install_rcmd(STR)      ##Execute R code STR (e.g. a function definition).
plr_environ()          ##Returns all environment variables as TABLE_VAL with STR columns name and value.
plr_set_display(STR)   ##Change the DISPLAY env variable (useful for plots)

PL/R R FUNCTIONS ==>   ##
pg.spi.exec(STR)       ##Execute SQL command STR.
                       ##For select ..., returns query as DATA.FRAME (null -> NA).
                       ##For others, returns number of manipulated rows.
pg.spi.prepare         ##Like prepare in SQL
(STR, INT_ARR)         ##INT_ARR are the types oid:
                       ##  - same as in pg_type.oid
                       ##  - can use SQL FUNC() load_r_typenames() to create R variables holding types oid.
                       ##    To see those variables, use SQL FUNC() r_typenames()
                       ##  - must be NA if no arguments
                       ##Returns a PLAN.
pg.spi.execp(PLAN, LIST)#Like pg.spi.exect(), but executing a PLAN.
                       ##LIST is unnammed:
                       ##  - must contain only NA if no argument.
                       ##  - must be NA for a NULL in SQL
pg.spi.factor(DATA.FRAM)#Convert non-NUM columns of DATA.FRAME into FACTOR.

pg.spi.cursor(STR, PLAN##
[, LIST])              ##Creates and returns a readonly CURSOR names STR for the command defined by PLAN and LIST.
pg.spi.fetch(CURSOR,
BOOL, INT)             ##Same as fetch forward|backward (true|false) INT in SQL
pg.spi.close(CURSOR)   ##

pg.spi.lastoid()       ##If last action was an insert of a single row, returns OID of that row.
pg.thrownotice|error
(STR)                  ##Like raise notice|exception in PL/PGSQL
pg.quoteliteral|ident
(STR)                  ##Like quote_literal|ident in SQL

PL/R OTHERS ==>        ##
plr_modules            ##TABLE executing plr_modules.modsrc (R code as STR) at start of each session or if reload_plr_modules()
                       ##is called.
                       ##plr_modules.modseq INT is the priority/order of execution.
                       ##Needs to be created as plr_modules( modseq int4, modsrc text ). Should be readable by all, writable
                       ##only by trusted users.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         DICTIONARIES          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PRINCIPLES ==>         ##Normalize a STR document by using:
                       ##  - a REGCONF, i.e. a combination of:
                       ##     - a PARSER, that break words into tokens according to whitespaces, hyphens, etc.
                       ##       Can produce overlapping tokens (both the compound token and its parts).
                       ##       Tokens can be words, hyphen-separated token, email address, URL or part of URL (path, host,
                       ##       etc.), numbers, XML tags
                       ##     - a list of DICTIONARY, i.e. a TEMPLATE filled with arguments.
                       ##       Goal is to remove common words, change grammatical forms, etc.
                       ##  - current REGCONF is chosen with ENVVAR default_text_search_config (can be set by initdb -T STR)
                       ##    Can be returned by get_current_ts_config()
                       ##Normalized STR documents are TSVECTOR. Queries are performed using TSQUERY.

LIMITS ==>             ##  - 2KB for lexemes. Max. 2^64 by TSVECTOR. Max 256 positions by lexeme.
                       ##  - 1MB for TSVECTOR (divide a document into several TSVECTOR)
                       ##  - Max. 2^32 elements in a TSQUERY

tsvector               ##Normalized STR document.
                       ##Each word can have a position and weight, i.e. word:NUMLETTER, where NUM is the position is the
                       ##document (1 to 16383) and a weight, A, B, C to D, indicating the level (title, subtitle, etc.)
                       ##Position only serves for relative position comparison, so 16383 is fine.
tsquery                ##A STR, query combination of normalized words on a tsvector.
                       ##Words must be separated by & or | (not space)
                       ##Can include:
                       ##  - () ! & |, e.g. 'word & (word2 | word3)'
                       ##  - word:LETTER...: only words in tsvector with weight LETTER... should match
                       ##  - word:*: not whole word need to match, but only beginning
to_tsquery|vector      ##As opposed to cast( STR as tsquery|vector ), it does normalization.
( [REGCONFIG, ]STR )   ##Only use the later if already normalized.
plainto_tsquery(...)   ##Same as to_tsquery() but can't accept operators & | ! (), and implicitely put & between each word

TSQUERY @@ TSVECTOR    ##Returns true if match.
STR @@ STR2            ##Automatically call to_tsquery|vector()
TSVECTOR || TSVECTOR2
TSQUERY && || !! TSQURY2
TSQUERY @> @< TSQUERY2 ##Contains|is contained

length(TSVECTOR)       ##number of words
strip(TSVECTOR)        ##Removes weights and indexes
setweight(TSVECTOR,
'A|B|C|D' )            ##Returns TSVECTOR with given weight
numnode(TSQUERY)       ##number of words and operators
querytree(TSQUERY)     ##returns TSQUERY without parts that can't be accessed (!word and common words), to detect if a TSQUERY
                       ##can be indexed
ts_rank[_cd]([REAL_ARR,]#Computes how much TSQUERY matches TSVECTOR. Used with "order by".
TSVECTOR,TSQUERY[,UINT])#ts_rank look at the number of occurences, and ts_rank_cd at the proximity of occurences
                       ##(smaller is better)
                       ##REAL_ARR weights occurences according to 4 REAL for the weights D, C, B, A (def: 0.1, 0.2, 0.4, 1).
                       ##UINT is a or'd flag, that computes following operations on output :
                       ##  - 0 (def): nothing
                       ##  - 1: n/(log(len(n))+1)
                       ##  - 2: n/len(n)
                       ##  - 4: n/mean between occurences (only ts_rank_cd)
                       ##  - 8: n/number of unique words
                       ##  - 16: n/(log(number of unique words)+1)
                       ##  - 32: n/(n+1)
ts_headline([REGCONF,] ##Returns a STR3 with matching results.
STR, TSQUERY[, STR2])  ##To make it faster, use select ts_headline(STR, TSQUERY) from (select ...) where the subquery returns
                       ##only rows that match the TSQUERY.
                       ##STR2 have options, as 'VAR=VAL...':
                       ##  - StartSel, StopSel STR: to be put around matches (def: '<b>' and '</b>')
                       ##  - Min|MaxWords UINT: to filter matches by number of occurences
                       ##  - ShortWord UINT: length minimal of words at being and end of matches
                       ##  - HighlightAll BOOL: negates previous args
                       ##  - FragmentDelimiter STR: between each match in output
                       ##  - MaxFragments UINT: to display in output
ts_rewrite(3 TSQUERY)  ##Change occurences of TSQUERY2 inside TSQUERY to TSQUERY3
ts_rewrite(TSQUERY,
TABLE_VAL)             ##Same with a TABLE_VAL with two TSQUERY columns

ts_debug([REGCONFIG, ] ##Returns result of conversion STR -> TSVECTOR, as a TABLE_VAL with COLS:
STR)                   ##  - alias STR: TYPE of TOKEN
                       ##  - description STR: of TYPE of TOKEN
                       ##  - token STR: original TOKEN
                       ##  - dictionaries STR_ARR and dictionary STR (the one chosen)
                       ##  - lexemes STR_ARR: final tokens
ts_lexize(STR, STR2)   ##Returns token STR2 as STR_ARR according to dictionary STR
ts_parse(STR|OID, STR2)##Same for parser. Returns a TABLE_VAL with COLS:
                       ##  - tokid INT: token type
                       ##  - token STR
ts_token_type(STR|OID) ##Returns all token types of the parser STR|OID, as a TABLE_VAL with COLS:
                       ##  - tokid INT
                       ##  - alias STR
                       ##  - description STR
ts_stat(STR[, STR2])   ##For a query STR returning a TSVECTOR, returning each word in a TABLE_VAL:
                       ##  - word STR
                       ##  - ndoc UINT: numero du TSVECTOR
                       ##  - nentry UINT: number of occurences
                       ##If STR2, only lexemes with weights STR2 will be picked.

create text search     ##... can be:
configuration REGCONF  ##  - parser = STR
( ... )                ##  - or copy = REGCONF2
                       ##Dictionaries are specified with alter text search configuration ...
alter text search      ##... controls DICTIONARY for specific TOKEN (as WORD) can be:
configuration REGCONF  ##  - add|alter mapping for TOKEN... with DICTIONARY...
...                    ##  - drop mapping [if exists] for TOKEN...
                       ##  - alter mapping [for TOKEN...] replace DICTIONARY with DICTIONARY2
                       ##Available ones:
                       ##  - pg_catalog.LANG (def: english), which uses:
                       ##     - LANG_stem DICTIONARY for words, simple DICTIONARY for rest
                       ##     - pg_catalog.default PARSER
                       ##  - pg_catalog.simple, which uses:
                       ##     - simple DICTIONARY for all
                       ##     - pg_catalog.default PARSER

create text search     ##Create a DICTIONARY, which is a TEMPLATE but with args filled in ... (as VAR = VAL)
dictionary DICTIONARY  ##Available ones:
( template = TEMPLATE, ##  - LANG_stem:
 ... )                 ##     - snowball TEMPLATE with language = LANG and stopwords = LANG
                       ##  - simple:
                       ##     - simple TEMPLATE
               unaccent### - unaccent:
                       ###    - filtering dictionary removing accent
                       ###    - does it according to argument rules (which can be altered), pointing to
                       ###      SHAREDIR/tsearch_data/ARG.rules.
                       ###      The default one can be found with unaccent.rules (works for western languages)
                       ###    - can also use unaccent([ARG,]STR)
              dict_xsyn### - dict_xsyn:
                       ###    - synonym dictionary: a word is spread into several synonymous words, so that synonymous words
                       ###      are counted together
                       ###    - arguments (can be altered):
                       ###       - rules, pointing to SHAREDIR/tsearch_data/ARG.rules, see example at
                       ###         SHAREDIR/tsearch_data/xsyn_sample.rules
                       ###       - matchorig|synonyms BOOL: the original word or synonyms are accepted as input
                       ###         (def: true and false)
                       ###       - keeporig|synonyms BOOL: the original word or synonyms are produced in output
                       ###         (def: true and true)
alter text search
dictionary DICTIONARY
( ... )                ##

create text search     ##Creates a TEMPLATE. Must be superuser ROLE.
template TEMPLATE      ##A TEMPLATE:
( [init = FUNC, ]      ##  - take a STR in input from the parser and can return:
 lexize = FUNC2 )      ##     - STR_ARR of the normalized words
                       ##     - empty STR_ARR if common words
                       ##     - null if word is unknown, so original STR passed to the next dictionary
                       ##     - STR, with the TSL_FILTER flag on (filtering dictionary): pass STR to the next dictionary
                       ##       dictionary
                       ##       Ex of filtering dictionary: removing accents
                       ##  - usually use FILES:
                       ##     - can be found under SHAREDIR/tsearch_data/FILE.extension (extension depends on TEMPLATE)
                       ##     - FILE must be in UTF-8
                       ##  - use arguments that are filled by DICTIONARY
                       ##Available TEMPLATE:
                       ##  - simple:
                       ##     - arguments: accept BOOL (def: true), stopwords FILE
                       ##     - FILE (extension: .stop) is one word by line
                       ##     - if input:
                       ##        - is found in FILE, returns empty STR_ARR
                       ##        - is not found:
                       ##           - if accept is true, returns word as STR_ARR
                       ##           - if accept is false, returns null
                       ##     - input is lowercased first
                       ##  - synonym:
                       ##     - arguments: casesensitive BOOL (def: false), synonyms FILE (extension: .syn)
                       ##     - FILE (extension: .syn) have two space-separated fields
                       ##     - if input:
                       ##        - is found in first field of FILE, returns second field
                       ##        - otherwise returns null
                       ##     - input is lowercased first if not casesensitive
                       ##       Second field can have * at the end, indicating to be a word:* if used as a TSQUERY
                       ##  - thesaurus:
                       ##     - arguments: dictfile FILE, dictionary DICTIONARY
                       ##     - FILE (extension: .ths) have two colon-separated fields (can contain several words)
                       ##       Can have #comment
                       ##     - if input:
                       ##        - is found in first field of FILE, returns second field
                       ##        - otherwise returns null
                       ##     - first use another DICTIONARY.
                       ##       Common words will be erased by DICTIONARY, but can still match them in first field of FILE
                       ##       with ?
                       ##  - ispell:
                       ##     - arguments: stopwords FILE, afffile FILE2, dictfile FILE3
                       ##     - FILE (extension: .stop): like stopwords in pg_catalog.simple
                       ##     - FILE2 (extension: .affix) and FILE3 (extension: .dict): see doc. for ispell
                       ##     - In short, can turn grammatical variations of a word into a single form.
                       ##  - snowball:
                       ##     - arguments: stopwords FILE, language WORD
                       ##     - like ispell, but simpler and never return null (so should be placed in the end)

create text search
parser PARSER
( start = FUNC,
gettoken = FUNC2,
end = FUNC3,           ##Creates a PARSER. Must be superuser ROLE.
lextypes = FUNC4       ##Available ones:
[, headline = FUNC5]  )##   - 'pg_catalog.default' (def). Notion of a "letter" depends on locales.

          fuzzystrmatch###Following functions can be used to test similarities between STR:
levenshtein(STR, STR2
[, INS_COST, DEL_COST,
SUB_COST])             ###Show differences, like agrep
levenshtein_less_equal
(..., INT)             ###Same but if more than INT, returns INT+1 (faster).
metaphone(STR, INT)    ###Convert to STR to phonetic-style code, and truncate it if more than INT characters (max 255)
                       ###For english language. Does not work well with Unicode characters.
dmetaphone(STR)        ###Same but unlim size.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CURSORS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CURSORS ==>            ##Row iterators over a specific query (executed once when opening the cursor)
declare CURSOR         ##Creates and open a CURSOR for query TABLE_VAL (any that would also work in a from ... clause)
[scroll] cursor        ##Is closed at end of transaction (if no transaction, is immediately closed), unless with hold is used.
[with hold]            ##Using select for update|share:
for TABLE_VAL          ##  - is not compatible with with hold or scroll
                       ##  - is recommended if using a ... where current of ... to make sure rows are the same
                       ##If scroll, fetch can work backward.
                       ##Starting position is before the first row. After the last row is the end.
fetch [WORD]           ##Return rows of CURSOR according to position specified by WORD, and move it too:
from CURSOR            ##  - forward|backward [NUM|all] (def: forward): retrieves several rows if NUM
                       ##  - relative NUM: 0 for current row
                       ##  - absolute NUM (if negative, from the end): 0 for before first row, more than number of rows for
                       ##    after last row
move [WORD] from CURSOR##Same but only moves cursor, doesn't return anything.
close CURSOR|all       ##Close a CURSOR (or all). Automatically done at end of transaction for a CURSOR without "with hold"
update|delete ... where##Same as update|delete ... where BOOL, où BOOL designe current position of CURSOR.
current of CURSOR      ##Grouping cannot be used in the TABLE_VAL of CURSOR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SCHEMAS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create schema SCHEMA    #Creates a SCHEMA (namespace inside a DATABASE).
[if not exists]         #USER is the owner (def: current_user).
[authorization USER]   ##if not exists is not standard

alter TYPE VAR ...
set schema SCHEMA      ##For all TYPE that can have a SCHEMA

[SCHEMA.]VAR            #Access/write a VAR (usually TABLE but can be any VAR) for a specific SCHEMA.
                        #If not specified, use the ENVVAR_ARR search_path, by def. {USER, public}. Searched from left to
                        #right. If nothing found and writing operation, use the leftest SCHEMA (except USER)

pg_catalog             ##SCHEMA for all pg_* tables and builtins functions and types. It's implicit in the search path.
                       ##To override those builtins, put pg_catalog at the end of the search path

current_schema()       ##
current_schemas([BOOL])##If true, include implicit ones (e.g. pg_catalog)
pg_is_other_temp_schema
(OID)                  ##Is it a temporary schema
pg_my_temp_schema()    ##Temp SCHEMA OID, 0 if none.
pg_TYPE_is_visible(OID)##Checks if an object is accessible through current SCHEMA path.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CONCURRENCY          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SESSION ==>             #Beginning and end of a list of commands (e.g. interactive prompt)
CONCURRENCY ==>         #Uses Multiversion Concurrency Control (MVCC) instead of traditional locks, for best performance
                        #while still reliable:
                        #  - transactions create a snapshot of the current state ("database version").
                        #  - read/write locks are "predicate locks": they don't block, they are just informative, and later on
                        #    might cancel actions if concurrency problem.
                        #Best to have consistent data is:
                        #  - if several statements need to be "write all or nothing", use transactions:
                        #      - if the state being read can be changed by other clients in the middle of the transaction
                        #        without problems, use read committed statements
                        #      - otherwise, use serializable statements, but then if abortion, must redo the transaction.
                        #        Client software usually notify of transaction failures: in such case, client needs to send
                        #        the request again.
                        #      - in all cases, put as read only if possible, and deferrable when it's a long operation.
                        #  - otherwise, transactions are not needed: operations are atomic and ask for the relevant blocking
                        #    locks, so no concurrency problem (are actually atomic transactions)
                        #When asking for locks:
                        #  - Transaction asking for locks will wait ENVVAR lock_timeout (def: 0, in ms) before cancelling,
                        #    and ENVVAR deadlock_timeout (def: 1s) before checking if there is a deadlock (in which case it
                        #    is cancelled)
                        #  - all transactions cannot exceed an average of ENVVAR max_[pred_]locks_per_transaction (def: 64)
                        #    predicate or not-predicate locks.
                        #FUNC():
                        #  - always a single read commited transaction, acquiring locks
                        #     - begin block of PL/PGSQL is logic-wise, not used for concurrency
                        #  - finer concurrency control is only at SQL level, so need wrap FUNC() call in a SQL transaction
start transaction       #Start a transaction (ends with rollback|commit), statements are only committed to the database at the
[isolation level WORD, ]#end of the transaction (but "appear" committed locally inside the statement)
[read write|only, ]     #A transaction that has any statement with an error will abort (rollback when committed).
[[not] deferrable]      #WORD can be read committed, repeatable read or serializable.
                        #Effects of WORD, for W2 -> R1 or W1 (same rows, 1 and 2 are transactions)
                                    +-------------------------------------------+-------------------------------------------+
                                    |                  W2 -> R1                 |                  W2 -> W1                 |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| atomic or committed transaction 1 |                                    No problems                                        |
| atomic or committed transaction 2 |                                                                                       |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             |                                           | read committed: ignores W2                |
| ongoing transaction 2             |                                           | non read committed: aborts (will rollback)|
+-----------------------------------+                 ignores W2                +-------------------------------------------+
| atomic or committed transaction 1 |                                           |                  blocks                   |
| ongoing transaction 2             |                                           |                                           |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
| ongoing transaction 1             | non read committed and R1 -> W2 -> R1:    | non read committed and R1 -> W2 -> W1:    |
| atomic or committed transaction 2 |  ignores W2                               |  aborts (will rollback)                   |
|                                   | sinon:                                    | sinon:                                    |
|                                   |  takes W2 into account                    |  takes W2 into account                    |
+-----------------------------------+-------------------------------------------+-------------------------------------------+
                        #Summary:
                        #  - read committed (def):
                        #    - ignores other ongoing transactions, but takes into account changes by committed transactions
                        #      or atomic statements
                        #    -> see state for the current statement
                        #  - repeatable read:
                        #    - ignores other ongoing transactions, including changes, unless it conflicts, in which case
                        #      it aborts.
                        #    -> see state at start of transaction (first statement after "start transaction")
                        #  - serializable:
                        #    - like repeatable read, but two concurrent serializable doing a W that depends on a R that is
                        #      changed by this W: the second will abort.
                        #      - ex: insert into TABLE(COL) select sum(COL) from TABLE; by two serializable transactions.
                        #    -> can virtually consider successful transactions (not aborting) to happen one after the other,
                        #       while still avoiding blocking locks to achieve it.
                        #Others:
                        #  - read write|only (def: read write):
                        #     - read only allow further concurrency optimization
                        #     - read only can still write to TEMP
                       ##  - deferrable (def: not deferrable):
                       ##     - if serializable and read only, blocks until sees no chance of being cancelled, then go on.
                       ##     - good if cancellation might take a long time to repeat the transaction (e.g. backups)
                       ##Synchronisation of two TRANSACTION:
                       ##  - TRANSACTION must:
                       ##     - both be either repeatable read or serializable.
                       ##       If TRANSACTION2 is serializable, so must be TRANSACTION1
                       ##     - if TRANSACTION1 is readonly, so must be TRANSACTION2
                       ##     - Must be just after the "start transaction"
                       ##  - steps:
                       ##     - TRANSACTION1 does select pg_export_snapshot(), which prints a SNAPSHOT_ID
                       ##     - TRANSACTION2 does set transaction snapshot SNAPSHOT_ID, as STR
                       ##Can have extra infos with:
                       ##  - txid_current(): current TRANSACTION_ID
                       ##  - txid_current_snapshot(): current SNAPSHOT_ID
                       ##  - txid_snapshot_xid(SNAPSHOT_ID): returns current transaction ID
                       ##  - txid_snapshot_xmax|xmin(SNAPSHOT_ID)
                       ##  - txid_visible_in_snapshot(TRANSACTION_ID, SNAPSHOT_ID)
set transaction ...     #Changes isolation level, etc. (same as start transaction ...) for current transaction.
                        #Must be just after the "start transaction"
set session
characteristics         #Same but for all future transactions.
as transaction ...      #Same as setting ENVVAR default_transaction_isolation|read_only|deferrable
rollback|commit         #Finishes a transaction.
                        #For rollback, actions are actually dropped (nothing happens).
savepoint LABEL
rollback to savepoint   #Inside a transaction block, rollback to LABEL go back to the state where savepoint LABEL was (and
LABEL                   #releases all savepoint LABEL that might have been defined after it).
release savepoint LABEL #Supprime a savepoint

prepare transaction STR##Do a two-phase commit:
                       ##  - transaction is temporary rollbacked (except the ENVVAR changes) and will only be committed once
                       ##    commit prepared STR is done
                       ##  - commit|rollback prepared STR can be done by other clients (not inside a transaction), if same ROLE
                       ##    or superuser ROLE.
                       ##    So goal is to have a single client managing the transaction of other clients
                       ##    ("transaction management system")
                       ##STR must be unique, and less than 200 bytes
                       ##Transaction must not involve notify|[un]listen, TEMP nor CURSOR with hold
                       ##ENVVAR max_prepared_transactions (def: 0, so disabled) is available. Is set, should be
                       ##max_connections * number of prepared_transactions per connection

lock TABLE ...         ##Put a blocking lock on TABLE... among several mode WORD, and release at end of current transaction:
[in WORD mode] [nowait]##  - should do it at beginning of transaction if repeatable read or serializable.
                       ##  - locks goal is to conflict with each other
                       ##  - prefer predicate locks
                       ##If can't access lock:
                       ##  - if nowait, only emits error
                       ##  - otherwise, transaction will be rollbacked
                       ##WORD are:
                       ##  - access exclusive (commands that erase data): block everything
                       ##  - exclusive: let up to access share
                       ##  - share row exclusive: let up to row share
                       ##  - share update exclusive (commands that change schemas (clean/analyze/optimize data/alter)): let up
                       ##    to row exclusive
                       ##  - row exclusive (commands that write data)
                       ##  - row share (select for update/share)
                       ##  - access share (commands that read data)
select ... for update  ##Gets an access exclusive lock on the rows in ... for TABLE (def: all), released at end of transaction
[of TABLE...] [nowait] ##(not statement). Cannot use union, intersect or except.
select ... for share
[of TABLE...] [nowait] ##Same for share row exclusive

pg_[try_]advisory_      #Gets a lock linked to a ID BIGINT:
[xact_]lock[_shared]    #  - can be "_shared" or not (block lock() attempts, but not lock_shared() attempts)
(BIGINT)                #  - session-level (stops at unlock() or at end of session)
                        #    - if "xact_" transaction-level (stops at end of transaction only)
                        #  - If "try_", only returns false/true but no block if can't get the lock.
                        #  - can lock several times (needs to unlock several times)
pg_advisory_unlock
[_shared](BIGINT)       #Returns true if such lock existed.
pg_advisory_unlock_all()#

set constraints         #Change the initially deferred|immediate state of CONSTRAINT... (def: all) within the current
all|CONSTRAINT...       #transaction.
deferred|immediate      #CONSTRAINT can be declared (including during create table ...)
                        #  - initially immediate (def): constraints are checked at each statement.
                        #  - initially deferred: at each end of transaction (except not null and check())
                        #  - not deferrable (def: deferrable): cannot be initially deferred.
                        #Includes previous statements (retroactively), so can fire at specific point in transaction.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             INDEX             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create [unique] index  ##Create an INDEX on COL_VAL...
[concurrently] INDEX   ##Cannot be directly used, but makes select|update|delete on COL_VAL... faster (used by the planner).
on TABLE( COL_VAL...   ##Can't create two INDEX with same names, even on different tables.
[OPCLASS] )            ##Will be picked when the query use the same expression used in create index ...:
[using INDEXMETHOD]    ##  - COL_VAL (and not COL_VAR):
[asc|desc]             ##     - "expression index" makes it possible to use INDEX when query usually use COLVAL(e.g. FUNC(COL))
[nulls first|last]     ##     - if use operator, needs extra set of parenthesis, e.g. (( COL + VAL ))
[tablespace TABLESPACE]##  - where BOOL:
[with ( VAR = VAL... )]##     - "partial index" (only on part of TABLE), faster when query usually use where BOOL
[where BOOL]           ##     - useful when where BOOL is selective (only few rows)
                       ##  - multiple COL_VAL...: "multicolumn index":
                       ##     - should be used only if COL_VAL... are almost always queried together
                       ##     - for btree|gist, only faster when using query using COL_VAL... in the same order and connected
                       ##       with and (up to the last OP)
                       ##     - for gist, quite slow if first COL has few distinct values
                       ##     - not for spgist
                       ##  - [asc|desc] [nulls first|last]:
                       ##     - "sorted index", faster when querying using order by ...
                       ##     - for btree, useless (except with desc nulls last or asc nulls first, or with multiple
                       ##       COL_VAL... with different sort order) since it can scan in both directions and automatically
                       ##       sort.
                       ##  - using INDEXMETHOD (def: btree): see below
                       ##  - OPCLASS:
                       ##     - use a specific OPCLASS for the type of COL_VAL instead of the default one
                       ##       The underlying OP must be immutable.
                       ##Others:
                       ##  - unique:
                       ##     - only for btree
                       ##     - use it if COL... can't have duplicate records (faster)
                       ##     - as opposed to usual unique CONSTRAINT, can be done on COL_VAL (not only COL_VAR)
                       ##  - concurrently: instead of asking for an exclusive lock, wait for possible conflicting transactions
                       ##    to complete for each row. Slower but will not lock the TABLE.
                       ##    Can fail: then drop INDEX and recreate it.
                       ##  - with ( VAR = VAL... ): see create table ...
                       ##An "index-only scan" is a query that can use only the INDEX without even visiting the TABLE (faster).
                       ##A query using "... and|or ..." can use each ... INDEX separately, then combine the result.
                       ##INDEX performances:
                       ##  - makes read faster, but write slower (needs to maintain the INDEX)
                       ##  - is mostly efficient is the query is selective (chooses few rows) on a big table.
                       ##  - to know when to use an INDEX, check with explain if INDEX is chosen by the planner for most-used
                       ##    queries or skipped in favor of a sequential scan.

INDEXMETHOD ==>        ##"Index accessor method"
                       ##Structure in which indexes are conceptually stored (because in the end they are TABLE), and
                       ##functions to maintain/crate those structures.
                       ##In short: way to create INDEX, linked to specific types and operators.
                       ##Are linked to specific OPFAMILY, which are groups of OPCLASS (operator class), which is a combination
                       ##INDEXMETHOD + TYPE + FUNC...:
                       ##  - an INDEXMETHOD provides an API with few FUNC to implement
                       ##  - to implement a TYPE, need to write them for those TYPE -> OPCLASS
                       ##  - available OPCLASS and OPFAMILY are usually called TYPE_ops (_TYPE_ops for ARRAY)
                       ##INDEXMETHOD available are:
                       ##  - btree (def):
                       ##      - implemented on all TYPE for < <= = >= > <> (implies in, between, etc.)
                       ##      - faster if = is used first in queries
                       ##  - gin:
                       ##      - for VAL = one of VAL...
                       ##      - implemented on most operators for:
                       ##         - TSVECTOR
                       ##         - ARRAY
                       ##         - INT_ARR && <@ @> @@
                 hstore###        - HSTORE @> <@ ? ?& ?|
               intarray###     - for INT_ARR, use special OPCLASS gin|gist__int[big]_ops
                       ###        - big_ops: prefer if big dataset
                       ##  - gist:
                       ##      - implemented on most operators for CIRCLE, POINT, BOX, POLYGON, RANGE, TSQUERY, LTREE
                       ##      - implemented also on TSVECTOR, INT_ARR and HSTORE (like gist):
                       ##         - gin tends to be faster read but slower write
                       ##  - spgist:
                       ##      - implemented on most operators for POINT, RANGE, and on < <= >= = > <> for STR
                       ##  - more are available by adding a row to pg_am. Some are available as EXTENSION.
create operator
class|family ...       ##Create user-defined OPCLASS|OPFAMILY. Need C functions (see doc online)
alter operator
family ...             ##

reindex index|table|   ##Rebuild INDEX... of the target (system is like database but only include system catalogs)
database|system        ##Needed to reclaim space when an INDEX has shrink size a lot.
INDEX|TABLE|DATABASE   ##Also needed if an INDEX is corrupted due to software bug.
                       ##If corrupted INDEX are on system catalogs, use postgres --single -P instead of postgres, otherwise it
                       ##will crash. Then use reindex and restart.
                       ##Ask for a exclusive lock

cluster [verbose] [TABL##Makes the physical layout of TABLE rows similar to INDEXMETHOD.
[using INDEXMETHOD]]   ##Goal is to speed up queries of a TABLE with an INDEX using INDEXMETHOD.
                       ##Is one-time operation (new rows are not clustered). Acquires an exclusive lock.
                       ##Good idea to run analyze afterwards.
                       ##INDEXMETHOD can be ommitted the second time, because remember it. Memory can be altered with:
                       ##  - alter table TABLE cluster on INDEX
                       ##  - alter table TABLE set without cluster
                       ##  - same for materialized views
                       ##TABLE can be ommitted to specify all TABLE having used cluster.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EFFICIENCY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PARTITIONS ==>          #Parts (sets of rows) of a TABLE, used for improved performance (when querying, altering or deleting a
                        #whole partition (e.g. drop TABLE is faster than delete TABLE where BOOL)).
                        #There is:
                        #  - an empty parent TABLE
                        #  - partitions are children TABLE... with:
                        #    - same COL_VAR...
                        #    - a check CONSTRAINT to restrict to a specific set of row
                        #    - an INDEX on the COL on which the CONSTRAINT is based.
                        #  - a trigger FUNC that redirects data inserted in the parent TABLE to the correct child
                        #  - Partitions should not overlap, and should together cover the whole data.

prepare PREP([TYPE...])##Parse COMMAND to make it faster to use later, if complex and often used COMMAND.
as COMMAND             ##Must be used with execute PREP(...):
execute PREP(...)      ##  - Each TYPE can be specified in COMMAND as positional argument $NUM
                       ##  - execute PREP(...) supply those arguments
                       ##  - last TYPE... can be "unknown", meaning "anyelement" (can still be used as $NUM), providing it
                       ##    can be determined runtime
                       ##  - no overloading
                       ##COMMAND must be select|insert|update|delete|values
                       ##No problem of caching: PREP will be reparsed if:
                       ##  - used TABLE (including COLs) change types
                       ##  - search_path changes
deallocate PREP|all    ##Supprime a PREP.
                       ##deallocate all is automatically done at end of each session.

LARGEOBJECT ==>        ##Efficient way to manipulate big VAL (based on a file):
                       ##  - can manipulate only through OID
                       ##  - can read/write/seek (see online doc) like in C
lo_import(STR)         ##Creates a LARGEOBJECT from file at path STR, and returns its OID
lo_export(OID, STR)    ##Creates a file at path STR with LARGEOBJECT with OID
lo_unlink(OID)         ##Needs to unlink LARGEOBJECT after use
                    lo###Instead of unlinking manually, could create a TFUNC executing lo_manage(COL_OID) on TABLE containing
                      ###LARGEOBJECT as OID, before udpate or delete, for each row. Should delete * from table before dropping
                      ###a TABLE to keep the trigger.
                      ###Command-line vacuumlo DATABASE (with connection options) can also be used.

explain( [verbose, ]   ##Display the query tree of COMMAND:
[analyze, [buffers, ]  ##  - nodes can be:
[timing false, ]]      ##     - Sort: order by ...
[costs false, ] [format##     - Limit: offset|fetch
text|xml|json|yaml] )  ##     - Aggregate: AFUNC
COMMAND                ##     - GroupAggregate: group by
                       ##     - HashAggregate: some AFUNC
                       ##     - WindowAgg: WFUNC
                       ##     - Filter: where TEST (or having)
                       ##     - Join: can be:
                       ##        - Nested loop, with a Join Filter: normal one
                       ##        - Hash, with a Hash Cond: involves an extra Hash step (compare hashes)
                       ##        - Merge join, compare both TABLE next to each other (must be sorted)
                       ##     - bottom only:
                       ##        - sequential scan: linear read of TABLE
                       ##        - index scan: using an INDEX, then visiting the TABLE
                       ##          - index-only scan: using an INDEX, and no need to visit the TABLE to answer the query
                       ##        - bitmap scan:
                       ##           - using an INDEX with first a Bitmap Index Scan on the INDEX
                       ##           - then fetching the TABLE with a Bitmap Heap Scan
                       ##           - difference with index scan: fetch all INDEX, then all TABLE, and not row by row INDEX
                       ##             then TABLE (index scan)
                       ##           - if using several INDEX, will do a BitmapAnd or BitmapOr
                       ##        - values scan: values( ... )
                       ##  - each node include the cost|resources of its children
                       ##Options:
                       ##  - analyze: show the actual time (unless timing false is used) and memory consumed
                       ##    Actually execute the COMMAND (otherwise, it is not) (can put in a transaction with rollback if
                       ##    don't want the execution to persist)
                       ##    - buffers: show info about buffer hits, which shows which parts are I/O intensive
                       ##  - costs (def: true):
                       ##    - show the resources taken, in a cost-based approach according to ENVVAR:
                       ##       - seq_page_cost (def: 1): sequential disk page fetch (used in sequential scan)
                       ##       - random_page_cost (def: 4): non-sequential disk page fetch (used in INDEX retrieval)
                       ##         Put at 2-3 if fast disks.
                       ##       - cpu_tuple_cost (def: 0.01): CPU cost for processing a row
                       ##       - cpu_index_tuple_cost (def: 0.005): CPU cost for processing a row in an INDEX
                       ##       - cpu_operator_cost (def: 0.0025): CPU cost for processing a FUNC
                       ##    - values are:
                       ##       - cost: first is initial cost, second is final cost
                       ##       - rows: number of rows manipulated
                       ##       - width: average size (in bytes) of a row
                       ##Advice:
                       ##  - use real data close to real environment, not test ones.
                       ##  - run analyze first if lot of change since last time autovacuum did (because based on pg_statistic)

create tablespace      ##Cree un TABLESPACE, i.e. a group of DATABASE, TABLE, MATERIALIZED VIEW and INDEX:
TABLESPACE [owner ROLE]##  - located on the disk at a specific location STR (must be empty dir)
location STR           ##  - owned by ROLE (def: current ROLE)
                       ##Must be a superuser to do it.
                       ##Goal:
                       ##  - Unlike SCHEMA, can't be used for permissions.
                       ##    Only permission is create, i.e. possibility to assign a newly created VAR to TABLESPACE
                       ##  - is used to control the physical locations of database objects, in order to:
                       ##     - optimize performance, e.g. putting heavily used INDEX on fast storage
                       ##     - optimize space, e.g. put heavy space DATABASE on high-volume storage, with possibility to move
                       ##       to another is space is not enough anymore
                       ##Is cluster-wide, so can be assigned to objects of different DATABASE.
                       ##Each new VAR has as a TABLESPACE (unless explicity mentionned):
                       ##  - if cluster-wide object, pg_global
                       ##  - otherwise the default TABLESPACE of the database.
                       ##    It is inherited from its template (pg_default for template0|1), but can be overriden by:
                       ##     - if TEMP or INDEX on TEMP, ENVVAR_ARR temp_tablespace: if used as an ARR, allocate randomly
                       ##       accross TABLESPACE...
                       ##     - otherwise, ENVVAR default_tablespace
                       ##Symlinks to DIR of user-created TABLESPACE can be found in DATADIR/pg_tblspc
alter TYPE VAR ...
set tablespace TABLSPAC##For all TYPE that can have a TABLESPACE

WRITING ==>            ##Best way to write a big amount of data fast:
                       ##  - put in only one transaction/statement
                       ##  - use copy or (if copy not possible) prepare, if possible on an empty TABLE
                       ##  - create the INDEX and foreign keys constraints after the data has been put into
                       ##     - when doing so, increasing ENVVAR maintenance_work_mem INT, max. memory used by vacuum,
                       ##       create INDEX and foreign key constraint creation (def: '16MB')
                       ##  - less checkpoints (increasing ENVVAR checkpoint_segments and checkpoint_timeout)
                       ##  - temporarily disabling WAL or replication
                       ##Should run analyze afterwards.

PERFORMANCE ==>        ##Can:
                       ##  - disable durability, by:
                       ##      - putting fsync and full_page_writes off (risky)
                       ##      - putting synchronous_commit off (more durable and almost same performance gain)
                       ##  - less checkpoints (see above)

PERFORMANCE TUNING ==> ##ENVVAR:
                       ##  - work_mem (def: 1MB): max memory used by a single command for each of its sort operations and hash
                       ##    tables, before writing temp files to disk.
                       ##    Average total memory taken will be average_number_of_hash/sort_operations_by_command *
                       ##    number_of_connections * work_mem. Should not be more than RAM taken by:
                       ##      kernel + other applications + shared_buffers + let memory for kernel buffer
                       ##  - maintenance_work_mem (def: 16MB): max memory used by vacuum, create index and add foreign key.
                       ##    Usually not a lot of those operations are run concurrently, so can be set higher (256MB is good)
                       ##  - effective_cache_size (def: '128MB'): hint of the amount of cache available for queries
                       ##    (doesn't change the actual cache size). Should be 50-75% of the available free+cached HDD memory
                       ##  - wal_buffers (def: -1, which auto-select a value): memory used for caching WAL, i.e. number of
                       ##    WAL segments (16MB) that can be cached for all sessions before flushing them.
                       ##    If high number of transactions, might consider increasing for better performance.
                       ##    Best is 16MB.
                       ##  - max_stack_depth (def: 2MB):
                       ##      - higher can provoke stack overflow (crashing the server) if higher than the kernel limit, with
                       ##        a safety margin of 2MB
                       ##        current OS limit can be seen with ulimit -s (8MB)
                       ##      - lower can cancel complex queries requiring more stack.
                       ##  - temp_file_limit (def: -1, unlim): max memory for temp files, in KB
                       ##  - max_files_per_processes (def: 1000): max opened files per session (see limit with ulimit -S|H -n)
                       ##    Def on Linux: 1024, so that's good.
                       ##  - effective_io_concurrency: when using several disks at same time (e.g. RAID), number of disks that
                       ##    can write at same time
                       ##  - shared_buffers (def: 128MB): memory for shared buffers (def: 128MB).
                       ##    Good value is 25% of RAM (if > 1GB total RAM)
                       ##Look at amount of memory taken with pgcluu or pgbadger
pgtune -i FILE         ##Checks postgresql.conf FILE, and prints an optimized version (mostly for performance ENVVAR)
                       ##Can do -o FILE2, but should pipe it to diff - FILE, to see differences.
-M NUM                 ##Total memory in bytes (def: guess it)
-c NUM                 ##Number of connections expected (change max_connections and work_mem)
-T WORD                ##Type of application, among DW (OLAP), OLTP, Web or Desktop. Def is Mixed (-> unspecified)
                       ##Desktop assumes lower cache, mem and connections, DW moderate, and Web and OLTP very high.

HARDWARE ==>           ##  - more RAM -> more cache
                       ##  - good hard drives. RAID0 or RAID1 is good idea
                       ##  - CPU less important, but still important for complex functions

pgbench                ##Does a benchmark, to compare machines or server conf speed.
                       ##Must first do a pgbench -i to initialize it (creates four pgbench_* tables), with following options
                       ##while initializing:
-F NUM                 ##Percentage of not-null in pgbench_* tables (def: 100)
-s NUM                 ##Multiply default number or rows in pgbench_* tables. Should be at least >= -c NUM
--[index-]tablespace=
TABLESPACE             ##Use a custom TABLESPACE for tables or indexes (to do if used in production)
--unlogged-tables      ##Create pgbench_* as unlogged tables
                       ##While not initializing:
                       ##  - tps is transactions per seconds.
                       ##  - has following options:
-h -p -U               ##Connection options (see psql)
-c NUM                 ##Number of concurrent connections. Should be close to average in real production.
-t NUM                 ##Number of transactions per client. Higher gives more precision.
                       ##Should be high enough to run few minutes
-j NUM                 ##Number of threads
-n -f FILE             ##Execute SQL FILE, instead of default one (simple update, select and insert statements)
                       ##Can include commands:
                       ##  - \setrandom INTVAR MIN MAX
                       ##  - \setshell INTVAR COMMAND ARGS
-S                     ##Perform only select statements
-r                     ##Show execution time for clients, per statements.

OTHERS ==>             ## - TABLE fillfactor, fastupdate

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     USERS AND PRIVILEGES      :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ROLES ==>               #ROLE are users or group of users for a given cluster.
                        #Difference between OS_USER (OS-specific) and ROLE (cluster-specific).
                        #Default installation creates a OS_USER "postgres", with group "postgres", often used as owner of
                        #clusters.
                        #Each cluster has a superuser ROLE:
                        #  - defined as the owner OS_USER (user that created the cluster using initdb)
                        #  - should have OS permissions over the DATADIR of the cluster
                        #By default, client connects as ROLE using OS_USER, but can use flags to do otherwise.
                        #  - so anyone that can login as the owner OS_USER of a cluster on a local computer can be superuser
                        #    ROLE too.
                        #ROLE own VAR they create.

create role ROLE        #Create a ROLE:
[superuser]            ##  - superuser: creates another superuser ROLE (must be superuser ROLE)
[createdb|role]        ##    Cannot drop the initial superuser ROLE.
[[encrypted] password  ##    readonly ENVVAR is_superuser ('on|off') is available.
STR]                   ##  - createdb|role|login|replication: gives createdb|role|login|replication privilege (see PRIVILEGE)
[login] [replication]  ##  - password STR:
[connection limit INT] ##     - either plaintext or "md5STR" where STR is a md5 hash.
[noinherits]           ##     - If encrypted is specified, STR must be plaintext -> it is converted to a "md5STR".
[valid until TIMTMPTZ] ##     - can be null for no password.
[in role ROLE2...]     ##     - only useful with authentication methods with passwords (i.e. "password" and "md5")
[role passwordcheck   ###     - will cancel is password is weak.
ROLE2...]             ###       Must also put '$libdir/passwordcheck' in shared_preload_libraries.
[admin ROLE2]         ###       Will not work on md5 hashes, only plaintext.
                      ###       It is recommended to rebuild the module by modifying the Makefile to enable CrackLib
                      ###       (better weak password recognition). Must be enabled at buildtime anyway.
            auth_delay###     - report password failure after ENVVAR auth_delay.milliseconds (def: 0), to avoid bruteforce
                      ###       (but makes DDoD easier).
                      ###       Must put auth_delay in shared_preload_libraries
                       ##  - valid until TIMESTAMPTZ: validity of the password
                       ##  - in role ROLE2...: grants ROLE as a member of ROLE2... (prefer using grant|revoke)
                       ##  - role ROLE2...: inverse
                       ##  - admin ROLE2...: same as role ROLE2, but ROLE2 are added with admin_option
                       ##  - connection limit: how many connections at same time (def: -1, unlim)
                       ##  - noinherits: see grant|revoke
alter role ROLE ...    ##Can be all options of create role but:
                       ##  - can use no..., e.g. nosuperuser
                       ##  - no in role, role or admin
alter TYPE VAR ...
owner to ROLE          ##For all TYPE but extension, index, role, rule, text search parser|template, trigger
drop role [if exists]   #Drop ROLE and cluster-specific objects owned by ROLE.
ROLE...                 #Doesn't work if ROLE own database-specific objects (use reassign|drop owned by ROLE first) or
                        #privileges.
                        #If client connection, doesn't stop it.
drop owned by ROLE...   #Drop all objects (except cluster-specific objects) owned by ROLE, and
cascade|restrict        #revoke privileges given to ROLE.
reassign owned
by ROL... to ROLE2      ##Change ownership.


set [local] role ROLE   #Same syntax as set, but here change current ROLE
                        #If not superuser, must be a ROLE that the session_user is member of (but not inverse, not other
                        #members of same ROLE), directly or indirectly.
                        #There are two types of users:
                        #  - current user: current ROLE. Used for permission checking.
                        #  - session user: ROLE that (usually) started the session.
                        #    Used to switch roles with set role and set session authorization.
                        #    So for a session, can switch back and forth between same possible ROLE (as long as set
                        #    session authorization is not called)
                        #VAR session_user (one that started the session) and current_user are available.
                        #If ROLE is none, reset to current session_user.
                       ##ROLE can be written as STR
                       ##  - local: same as set local ...
set [local] session     #Same but:
authorization ROLE      #  - for both session_user and current_user
                        #  - must be superuser
                        #  - use default to reset to initial session_user

grant PRIVILEGE...      #Gives permissions PRIVILEGE... on VAR... to ROLE...:
on [TYPE] VAR...        #  - TYPE:
to ROLE...              #     - is table (def)|sequence|database|domain|function|language|schema|tablespace|type|
[with grant option]     #       large object|foreign data wrapper|foreign server
                        #         - table include VIEW and FOREIGNTABLE
                        #     - [TYPE] VAR can be all TYPEs in schema SCHEMA for table|sequence|function
                        #  - PRIVILEGE:
                        #     - can be:
                        #        - on table:
                        #           - insert (a): insert or copy from
                        #           - delete (d): delete
                        #           - truncate (D): truncate
                        #           - references (x): needed on both TABLE to use foreign key
                        #           - trigger (t): create trigger
                        #        - on table|sequence|large object:
                        #           - select (r):
                        #              - select, copy or using VAR in update|delete
                       ##              - sequence: same + currval()
                        #              - large object: being read
                        #           - update (w)
                        #              - update, select for share|update
                       ##              - sequence: same + nextval|setval()
                        #              - large object: being written
                        #        - on sequence|domain|foreign data wrapper|foreign server|language|schema|type:
                        #           - usage (U):
                       ##              - language:
                       ##                - create function for this PL/* language
                       ##                - careful if functions can access OS (e.g. Bash)
                        #              - schema: reading VAR in SCHEMA (without permission can still look up VAR names in
                        #                SCHEMA)
                       ##              - sequence: currval() + nextval()
                        #              - type|domain: using it in creation of any VAR (including FUNC() and TABLE)
                        #              - foreign data wrapper: create server using FDW
                        #              - foreign server: create foreign table using FSERVER
                        #        - on database:
                        #           - connect (c): can start client session
                        #           - temp (T): create temp
                       ##        - on database|schema|tablespace:
                       ##           - create (C):
                       ##             - database: create schema
                       ##             - schema: create any VAR inside SCHEMA, and rename (must be owner too)
                       ##             - tablespace: create table|index|temp in it, and use TABLESPACE in create database
                        #        - on function:
                        #           - execute (X): executing *FUNC() (including TFUNC())
                        #        - on all:
                        #           - all privileges
                        #     - Other PRIVILEGE which can be obtained differently:
                        #        - drop|alter:
                        #           - no way to grant them to others:
                        #              - drop or alter definition of VAR:
                        #        - grant|revoke:
                        #           - grant PRIVILEGE ... with grant|admin option:
                        #              - grant for PRIVILEGE for the specific PRIVILEGE and TYPE
                        #              - admin for ROLE membership
                        #        - createrole:
                        #           - create|alter role ... createrole:
                        #              - create role (or drop|alter)
                        #              - To do so on superuser ROLE, must be superuser.
                        #              - implies admin option
                        #              - can create roles with higher permissions or memberships, so can be dangerous
                        #        - createdb:
                        #           - create|alter role ... createdb:
                        #              - create database (or drop|alter)
                        #        - login:
                        #           - create|alter role ... login:
                        #              - ROLE can initiate a client session (e.g. with psql)
                        #                Without it (def) ROLE can only be assigned with set role ROLE
                        #        - replication:
                        #           - create|alter role ... login:
                        #             - ROLE can use pg_basebackup
                        #     - PRIVILEGE can be PRIVILEGE( COL_VAR... ) for select|insert|update|references
                        #     - Def. PRIVILEGE is all for TABLE owner and superuser.
                        #       They can revoke their own privileges though.
                        #     - Permissions are optimistic: has privileges in following cases:
                        #        - granted at table-level but revoked at column-level
                        #        - granted to ROLE2 to which is member, but revoked to the specific ROLE
                       ##     - SQL only has VAR, not VAR...
                        #  - ROLE:
                        #     - if ROLE2 is a member of ROLE (directly or indirectly), it is targeted too:
                        #         - generally ROLE is just a group name, and ROLE2 real users, or subgroups
                        #         - but could also copy and extend|restrict permissions of another user
                        #         - inherit:
                        #           - by def., ROLE2 inherit privileges of ROLE (except "Other PRIVILEGE")
                        #           - with noinherits, can still use set role ROLE (but then loses its inital PRIVILEGE)
                        #     - can be public:
                        #        - meaning all current and future ROLE.
                        #        - cannot be member of a ROLE.
                        #        - has default privilege: connect, temp, usage and create on SCHEMA public, execute, usage on
                        #          LANGUAGES. Can be revoked.
                        #     - If current user is superuser or can grant thanks to membership or "with grant option", looks
                        #       like PRIVILEGE has been granted by VAR owner
                        #Access controls:
                        #  - are recorded in the database objects they are attached (using system catalogs) as ACLITEM_ARR,
                        #    i.e. [ROLE]=LETTER.../ROLE2, where ROLE is the granted (def: public), ROLE2 the granter and
                        #    LETTER shown above between parenthesis for each PRIVILEGE.
                        #  - Each LETTER can be followed by * for "with grant option"
                        #  - don't show default PRIVILEGE
revoke [grant|admin     #Inverse.
option for] PRIVILEGE...#"with grant|admin option" is revoked too. If "grant|admin option for", only "with grant|admin option"
on [TYPE] VAR...        #is removed.
from ROLE...            #Revoke privileges granted to others by ROLE2 too: if restrict, command will fail if there are some
cascade|restrict        #ROLE can only revoke ROLE2 for PRIVILEGE it previously personnally granted the other or a ROLE the
                        #other is member of.

alter default
privileges             ##Change default PRIVILEGE for ROLE2 (in ...) of all VAR (write TYPEs, not TYPE VAR) that will be
[for ROLE...]          ##created in the future by ROLE (must be current user (def) or a ROLE3 that ROLE is member of), in
[in schema SCHEMA]     ##SCHEMA (if specified)
grant|revoke...        ##Only for TABLE, SEQUENCE, FUNC or TYPE.

grant ROLE... to ROL2...#
[with admin option]     #
revoke [admin option
for] ROL... from ROL2...#Grant|revoke ROLE2... as members of ROLE...
cascade|restrict        #Gives|revokes associated PRIVILEGE too.

security label
[for PROVIDER]
on VAR is STR           #Used to implement SE-Linux (see online doc)

has_TYPE_privilege     ##TYPE can also be:
([ROLE, ]VAR, PRIVILEG)##  - any_column
                       ##  - column: adds a COL_VAR arg after VAR
pg_has_role([ROLE, ]
ROLE2, PRIVILEG)       ##With membership

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        AUTHENTICATION         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


pg_hba.conf            ##File under a cluster DATADIR controlling:
                       ##  - who can connect (host address, OS_USER)
                       ##  - to which database
                       ##  - with which connection method
                       ##  - under which ROLE
                       ##Have five whitespaces-separated fields (can contain whitespace if double-quoted) (with #Comment):
                       ##  - type (protocol):
                       ##    - local: local connection (Unix sockets)
                       ##    - host: TCP/IP
                       ##    - host[no]ssl: TCP/IP with|without SSL. For SSL:
                       ##       - must set ENVVAR ssl to "on" (def: "off") at server start
                       ##       - must have been enable when installing|building PostgreSQL
                       ##  - database:
                       ##    - connect to which DATABASE... (comma-separated)
                       ##    - sameuser means DATABASE with same name as ROLE
                       ##    - samerole means DATABASE with same name as ROLE or a member of ROLE
                       ##    - replication: special DATABASE used by pg_basebackup
                       ##    - @FILE... means list of DATABASE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                       ##    - can be all
                       ##  - user:
                       ##    - under which ROLE... (comma-separated) can connect
                       ##    - +ROLE means any member of ROLE
                       ##    - @FILE... means list of ROLE is available under DATADIR/FILE (if relative) or FILE (if abs.)
                       ##    - can be all
                       ##  - address:
                       ##    - which IPv4|6 addresses (along the netmask range) can connect
                       ##    - can also be a hostname (can be slow)
                       ##      - including a .DOMAINNAME for hosts under DOMAINNAME
                       ##    - can be all
                       ##    - samehost: any of the machine own IPs
                       ##    - samenet: any of the machine own subnet
                       ##  - netmask (optional):
                       ##    - IPv4|v6 netmask
                       ##  - auth. method:
                       ##    - trust|reject: always accept|refuse
                       ##    - ident [map=MAP]:
                       ##      - use MAP in pg_ident.conf (ENVVAR ident_file) to determine which ROLE correspond to OS_USER
                       ##      - def: ROLE = OS_USER
                       ##    - password|md5: ask for password (md5 hashes it but not crypto-secure)
                       ##      - libpq variable password:
                       ##         - To use if is demanded. Def: PGPASSWORD
                       ##         - Can also use a FILE PAGPASSFILE (def: ~/.pgpass), which should contain lines with format:
                       ##             host:port:database:user:password (first four field can be *)
                       ##           Permission must be 0600
                       ##    - SSL:
                       ##      - libpq variables:
                       ##        - sslmode: priority of SSL over non-SSL (def: PGSSLMODE):
                       ##          - disable: non-SSL
                       ##          - prefer (def): first SSL, then non-SSL
                       ##          - require: SSL. If root CA, verify certificate
                       ##          - verify-ca: SSL. Always verify certificate
                       ##          - verify-full: SSL. Verify certificate, and that hostname match in certificate
                       ##        - sslcert: certificate FILE (def: ~/.postgresql/postgresql.crt or PGSSLCERT)
                       ##        - sslkey:
                       ##          - Secret key (def: ~/.postgresql/postgresql.key or PGSSLKEY).
                       ##          - Can also be OpenSSL engines, as ENGINE:KEY
                       ##        - sslrootcert: CA certificate FILE (def: ~/.postgresql/root.crt or PGSSLROOTCERT)
                       ##        - sslcrl: CA revocation list FILE (def: ~/.postgresql/root.crl or PGSSLCRL)
                       ##    - krb5: Kerberos5
                       ##      - libpq variable krbsrvname: Kerberos server name (def: PGREALM or PGKRBSRVNAME)
                       ##    - GSSAPI:
                       ##      - libpq variable gsslib: on Windows, set to "gssapi" to use GSSAPI instead of SSPI
                       ##        (def: PGGSSLIB)
                       ##  - auth. method options (optional):
                       ##    - as VAR=VAL ...
                       ##    - see above
                       ##Default settings:
                       ##  - host and local connections: initdb --auth-host|local=STR (or -A STR for both)
                       ##  - default password: initdb -W|--pwfile=FILE, from stdin or from first line of FILE (if using
                       ##    password authentication)
                       ##Only the first matching line is chosen. Put most likely first for efficiency.
                       ##Location can be changed by ENVVAR hba_file
                       ##Read on server startup, or when receiving a SIGHUP.

requirekeeper          ##LIBPQ variable: OS_USER behind the server process must match STR.
                       ##Avoid another OS_USER starting the server while legit one is rebooting it.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         LOCALIZATION          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ENCODING ==>            #Sets within a cluster with initdb -E STR, which sets client_encoding ENVVAR, encoding of templates:
                        #  - can be many, among 'LATIN1','UTF8' and 'SQL_ASCII'
                        #  - Def. is locale (UTF8 in Linux) or SQL_ASCII if no locale.
                        #If client has different encoding, a conversion is performed according to pg_conversion
                        #(see online doc).
                        #Available conversions from UTF8 should be enough. Otherwise can do:
                       ##  - create conversion CONVERSION for STR to STR2 from FUNC
                        #Encoding is sometimes specified as INT: use pg_encoding_to_char(INT) to get it as a STR
                        #server_encoding ENVVAR is available

LOCALES ==>             #Use ENVVAR lc_*, that are usually set at cluster creation with initdb --lc-*=LOCALE.
                        #lc_collate|ctype (unless using COLLATION) and encoding can't be changed after database creation
                        #Def. is "", i.e. locale (en_US.UTF-8 e.g.) or SQL_ASCII if no locale.
                        #Locales are server-dependent, not client-dependent.
                        #Non-C locales allow local-dependant on some operations:
                        #  - sorting: order by, < > >= <=
                        #  - case: upper, lower, initcap, regexps
                        #  - [[:...:]] in regexps
                        #  - to_char()
                        #C locale is faster and the only one that can use INDEX on like operator.

COLLATION ==>           #Combination of lc_collate and lc_ctype, specified as "lang_LANG[.ENCODING]":
                        #  - are used by doing:
                        #     - VAL collate "..." (e.g. "fr_FR")
                        #     - TYPE collate "..."
                        #  - cannot mix different COLLATION in same statement
                        #  - prefer without ENCODING, which then let pg_collation use current ENCODING
                        #Creating COLLATION:
                        #  - available can be seen with pg_collation
                        #  - populated by initdb with available locales on the OS
                        #  - using create collation ...
create collation COLATIN
( [locale = LOCALE]
[, lc_collate|ctype =
LOCALE] )              ##LOCALE is a shortcut for both lc_collate and lc_ctype
create collation
COLATION from COLATION2##

collation for (STR)    ##Returns COLLATION

convert(STR, STR2,STR3)##Encoding conversion. STR is string to convert.
convert_to|from(STR,
STR2)                  ##Same, but assumes the dest|original encoding to be the current system's encoding
pg_client_encoding()   ##Current encoding


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          EXTENSIONS           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


create extension       ##Activate an EXTENSION, e.g. a collection of objects (library), except DATABASE, ROLE, INDEX and
[if not exists] EXTENSIN#TABLESPACE, for a specific DATABASE.
[schema SCHEMA]        ##STR is the version (def: the one in the default_version)
[version STR]          ##SCHEMA is for objects defined in EXTENSION (default: current SCHEMA)
                       ##Some EXTENSION are already present in the default compilation of PostgreSQL, or personalized
                       ##compilations (but usually not activated).
                       ##Permissions are the ones required in executing the underlying SQL file.
                       ##Files must already be present in SHAREDIR/extensions/:
                       ##  - a SQL file "EXTENSION--VERSION.sql" defining new CTYPE, FUNC, etc.:
                       ##     - implicitly inside a transaction, so cannot use transaction statements
                       ##     - do select pg_extension_config_dump(STR, STR2); for a TABLE named STR used for users to
                       ##       personalize the extension.
                       ##       It will make the TABLE backupable, as opposed to rest of EXTENSION.
                       ##       STR is a filter, e.g. 'where BOOL' or ''.
                       ##     - should include protective lines:
                       ##       -- complain if script is sourced in psql, rather than via CREATE EXTENSION
                       ##       \echo Use "CREATE EXTENSION pgrowlocks" to load this file. \quit
                       ##  - an ASCII text file "EXTENSION.control" defining metadata as VAR = VAL ..., with##comment possible:
                       ##     - default_version STR: current version
                       ##     - relocatable BOOL (def: false): if SCHEMA can be changed after create extension
                       ##     - comment STR: description
                       ##     - directory STR: DIR of the SQL file (def: same as *.control file)
                       ##     - encoding STR (def: client's encoding): to be defined in no ASCII characters
                       ##     - module_pathname STR:
                       ##       - can be used as MODULE_PATHNAME in the *.sql file
                       ##       - can use $libdir, e.g. '$libdir/myextension'
                       ##     - requires STR...: other extensions this one depends on
                       ##     - superuser BOOL (def: true): if only superuser can create extension
                       ##     - schema STR: def. SCHEMA (only if non-relocatable)
                       ##  - additional EXTENSION--VERSION.control with same format but for a specific VERSION can be defined
                       ##  - additional EXTENSION--VERSION--VERSION2.sql are executed to go through VERSION to VERSION2 with
                       ##    alter extension update
alter extension EXTNSIN
add|drop TYPE VAR      ##

load STR               ##Loads a shared library located at STR, in order:
                       ##  - an absolute path
                       ##  - or look into ENVVAR dynamic_library_path, which a colon-separated list (def: $libdir),
                       ##    where items can start with $libdir (def: /usr/lib/postgresql/9.3/lib/)
                       ##    (can be seen with pg_config --pkglibdir)
                       ##If not superuser, must be inside $libdir/plugins/
                       ##STR path convention is OS-specific.
                       ##Not useful if only functions definitions because they are loaded automatically with create function...
                       ##Can also use ENVVAR shared_preload_libraries (comma-separated list), without the extension .so

create foreign data     #Creates a FDW, i.e. functions that permits using another DBMS inside PostgreSQL.
wrapper FDW             #Can be slow and not optimized.
[handler FUNC]          #Based on a standard implemented by other DBMS, including noSQL.
[validator FUNC2]      ##FUNC and FUNC2: see doc. on how to create them.
[options ( VAR STR ...)]#Must be superuser.
                        #Available ones (as EXTENSION):
                        #  - postgres_fdw: for PostgreSQL to other DBMS
                        #     - FSERVER options (can also use libpq variables)
                        #        - host STR
                        #        - dbname STR
                        #        - port STR
                        #     - user mapping options:
                        #        - user STR
                        #        - password STR
                        #        - client_encoding STR (def: ENVVAR client_encoding)
                        #     - FTABLE options:
                        #        - schema|table|column_name STR: if name is different
                        #     - FSERVER or FTABLE options!
                        #        - updatable BOOL (def: true): read-write
                        #     - details:
                        #        - transaction read committed -> repeatable read
                        #  - file_fdw: for CSV file (or other formats of copy COMMAND), read-only:
                        #     - FTABLE options:
                        #        - filename STR: absolute path
                        #        - ...: same options as copy COMMAND, except force* and oids
alter foreign data
wrapper FDW
[no handler|handler FNC]#
[no validator|validator##
FUNC]                  ##
options ( [add|set|drop]#
VAR VAL ... )           #

create server FSERVER
[type STR]
[version STR2]          #Creates a FSERVER, i.e. a connection to a specific DATABASE using a FDW.
foreign data wrapper FDW#STR are all FDW-dependent options. They usually specify the connection details
[options ( VAR STR3...)]#(host and port, database name, etc.)
alter server FSERVER
[version STR]
[options ([add|set|drop]
VAR STR2... )]          #

create user mapping
for ROLE|current_user
server FSERVER          #Specify authentication details on FSERVER for ROLE.
[options ( VAR STR ...)]#STR are all FDW-dependent options.
alter user mapping ...
[options
( [add|set|drop] ... )] #

create foreign table
[if not exists] FTABLE
( COL_VAR TYPE
[options ( VAR STR ... )]
 CONSTRAINT|not null|    #Creates a FTABLE (can be used as a TABLE) while connected to a FSERVER.
 default VAL ... )       #TABLE, COL, TYPE, CONSTRAINT, etc. should be same as on FSERVER, unless specified in the options.
server FSERVER           #But can only select some of the foreign COL.
[options (VAR STR2 ...)]##not null|default VAL are not standard

alter foreign table FTABL
rename COLVAR to COLVAR2##

alter foreign table FTABL
add COL_ARG              #

alter foreign table FTABL
drop [if exists] COL_VAR
cascade|restrict         #

alter foreign table FTABL
alter COL_VAR type TYPE  #

alter foreign table FTABL
alter COL_VAR set|drop
not null|default [VAL]  ##Same as alter table ...

alter foreign table FTABL
[alter COL_VAR]
options( [add|set|drop]
VAR VAL ... )           ##

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      SYSTEM COLS/TABLES       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SYSTEM COLS ==>        ##Hidden COL... defined for every TABLE
                       ##ctid, etc. are used for MVCC
TABLE.tableoid         ##OID of the TABLE
TABLE.oid              ##ID row, doesn't change with update, but created by insert.
                       ##Table-specific: cross join with tableoid to have real IDs.
                       ##Creation:
                       ##  - ENVVAR default_with_oids = true (def: false)
                       ##  - create table ... with[out] oids
                       ##  - alter table TABLE set with[out] oids
                       ##Is actually a cycling number incrementing from 1, database-wise.
TABLE.ctid             ##Same but change avec committed update or insert
                       ##Is actually a cycling number incrementing from 1, table-wise.
TABLE.cmin, TABLE.cmax ##Same but change avec uncommitted update or insert (put back to 0 when ctid changes)
                       ##Is actually the numero of the statement inside the transaction.
TABLE.xmin             ##ID de la transaction ayant insere (table-wise, not client-wise)
TABLE.xmax             ##ID de la transaction deleting or updating this row by another non committed transaction.

SYSTEM CATALOGS ==>    ##TABLE used by the system. Can also be VIEW (system views)
                       ##Are sometimes readonly. Anyway, should not modify them directly unless good reasons (e.g. creating a
                       ##new INDEXMETHOD).
                       ##Are database-specific unless on cluster-specific VAR
INFORMATION SCHEMA ==> ##Same but is SQL standard: more portable but does not contain all information.
                       ##Is in SCHEMA information_schema.*, written is_* below
                       ##Most of them not written here, look at online doc if needed

pg_database            ##All DATABASE, including:
                       ##  - datistemplate BOOL: can be used in create database template DATABASE
                       ##  - datallowconn BOOL: if false, no one can connect to it

pg_class               ##All TABLE-like elements: TABLE, VIEW, INDEX, SEQUENCE, MVIEW and CTYPE, including:
                       ##  - relisshared BOOL: if cluster-wide
pg_tables              ##All TABLE
pg_views               ##All VIEW
pg_matviews            ##All MVIEW
pg_index               ##All INDEX
pg_indexes             ##Same but with other infos, like the query that created it.
is.sequences           ##All SEQUENCE
pg_attribute           ##All COL, including:
                       ##  - attisdropped BOOL: dropped COL are physically kept
pg_attrdef             ##Default values for COL that have some
pg_constraint          ##All CONSTRAINT
pg_inherits            ##Inheritance between TABLE

pg_[sh]depend          ##Relations between [cluster-wide] objects, to avoid conflicts when dropping an object.
pg_locks               ##All currently held locks

pg_proc                ##All FUNC
pg_aggregate           ##All AFUNC
pg_trigger             ##All TFUNC
pg_event_trigger       ##All EFUNC
pg_operator            ##All OPERATOR
pg_cast                ##All CAST, including:
                       ##  - oid OID
                       ##  - castsource|casttarget TYPE: from and to, refers to pg_type.OID
                       ##  - castfunc:
                       ##      - FUNC(TYPE1_VAL[, INT[, BOOL]]) -> TYPE2_VAL used for conversion
                       ##         - INT is the pg_attribute.ATTTYPEMOD
                       ##         - BOOL is true if explicit cast
                       ##      - refers to pg_proc.OID
                       ##      - If TYPE1 = TYPE2, this is a "sizing cast", which is used for things like size-checking or padding.
                       ##  - castcontext STR: indicates when to cast:
                       ##     - 'e': only in explicit cast
                       ##     - 'a': in explicity + implicity only when casting to COL type
                       ##     - 'i': always
                       ##  - castmethod STR: indicates how to cast:
                       ##     - 'f' use castfunc
                       ##     - 'i' use C-level I/O functions
                       ##     - 'b' doesn't do anything (types are binary-equivalent)
pg_rewrite             ##All RULE
pg_rules               ##Same, but more for user-defined RULE

pg_type                ##All TYPE, including:
                       ##  - typispreffered BOOL: if true, is used as the TYPE for the whole typecategory with
                       ##    overloading functions (def: text for STR, and real for numeric)
pg_enum                ##All ENUM
pg_range               ##All RANGE
is.domains             ##All DOMAIN

pg_authid|roles        ##All ROLE, including:
                       ##  - rolcatupdate BOOL (def: false): if false, all ROLE, including superuser ROLE, cannot write on
                       ##    system catalogs.
                       ##pg_roles blanks out the password
pg_auth_members        ##ROLE memberships, including:
                       ##  - roleid|member|grantor OID: means ROLE2 is a member of ROLE, which has been granted by ROLE3.
                       ##    Refer to pg_authid.oid
pg_default_acl         ##All default privileges.

pg_namespace           ##All SCHEMA
pg_tablespace          ##All TABLESPACE

pg_extension           ##Activated EXTENSION
pg_available_extensions##Possible EXTENSION for their current version
pg_available_extension_
versions               ##Possible EXTENSION for all versions
pg_language            ##All activated LANGUAGE, including:
                       ##  - lanpltrusted BOOL: can not access filesystem (so non-superuser can create function)
                       ##    Is false for C, R and SH but true to SQL and PL/PGSQL
pg_pltemplate          ##All LANGUAGE

pg_ts_config[_map]     ##All REGCONF
pg_ts_dict             ##All DICTIONARY
pg_ts_template         ##All TEMPLATE
pg_ts_parser           ##All PARSER

pg_collation           ##All COLLATION
pg_conversion          ##All CONVERSION

pg_cursors             ##All CURSOR
pg_prepared_statements ##All PREP
pg_prepared_xacts      ##All "prepare transaction STR"

pg_foreign_data_wrapper##All FDW
pg_foreign_server      ##All FSERVER
pg_foreign_table       ##All FTABLE
pg_user_mapping[s]     ##All USERMAPPING. With s, leaves out the option field when user should not see it

pg_am                  ##All INDEXMETHOD: what they support (e.g. in create index ...) and the FUNC they use
pg_opclass             ##All OPCLASS
pg_opfamily            ##All OPFAMILY
pg_amop                ##OPERATOR used by OPFAMILY
pg_amproc              ##FUNC used by OPFAMILY

pg_largeobject
[_metadata]            ##All LARGEOBJECT

pg_[sh]description     ##All COMMENT [on cluster-wide objects]

pg_settings            ##All ENVVAR, with also when is readonly, type, min|max value, unit
pg_db_role_setting     ##All role and/or database-specific ENVVAR

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATABASE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


MAIN STRUCTURE ==>     ##  - Cluster: set of DATABASE, managed by a single instance of a server daemon (postgres).
                       ##      - linked to a DATADIR
                       ##      - cluster-wide ENVVAR
                       ##      - cluster-specific objects:
                       ##         - DATABASE, including templates
                       ##         - ROLE
                       ##         - TABLESPACE
                       ##         - LANGUAGE
                       ##         - system catalogs on cluster-specific objects
                       ##  - Database: set of SCHEMA. A client can connect only to one DATABASE at once.
                       ##  - Schema: set of SQL objects (TABLE, FUNC, etc.)
                       ##  - Session: client connection to the server for a specific database, e.g. interactive prompt session.
                       ##  - Transaction: set of statements that should executed all-or-nothing (concurrency)
                       ##  - Statement: SQL commande individually sent to the server by the client.

create database DATABAS##Cree une DATABASE
[owner ROLE]           ##  - ROLE is current one by def
[template DATABASE2]   ##  - DATABASE2 is the initial state of DATABASE:
[encoding WORD]        ##     - It is a default DATABASE2 called template1 by def. (created by initdb)
[lc_collate|ctype WORD]##     - defines locales (see initdb)
[connection limit INT] ##     - template0 is the same as template1, but template1 can be modified to have specific initial
[tablespace TABLESPACE]##       state of DATABASE, while keeping template0 as the initial template if needed.
                       ##     - DATABASE2 cannot be accessed just before and during create database
                       ##  - encoding|lc_collate|ctype WORD: overrided default defined by template. Should be used only if
                       ##    DATABASE2 is template0.
                       ##  - connection limit: number of concurrent non-superuser client sessions max (def: -1, i.e. unlim)
                       ##Cannot be inside a transaction.
alter database DATABASE
connection limit INT   ##


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SERVER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


INSTALLATION ==>       ##  - Server: postgresql
                       ##  - Client: postgresql-client, pgadmin3
                       ##  - Extensions: postgresql-contrib, postgresql-EXTENSION
                       ##  - Extension building: postgresql-server-dev
pg_config              ##Show compilation-time conf
--bin|doc|html|include|
pkginclude|lib|pkglib| ##Print DIR. By default, useful DIR (so without headers, docs, etc.) are:
locale|man|share|      ##  - /usr/lib/postgresql/VERSION/: bin and lib
sysconfdir             ##  - /usr/share/postgresql{,-common}/VERSION/
--includedir-server|pgxs#  - /etc/postgresql-common/: few conf files
--configure            ##Print all configuration flags (usually to mimic another installation)
--cc|cppflags|
cflags[_sl]|
ldflags[_ex|sl]|libs   ##Print makefile variables

REGRESSION ==>         ##Running the regression tests (to unsure installation is fine):
                       ##  - download source, configure and build it
                       ##     - after building, before installation: make check (must not be root)
                       ##     - after installation: make installcheck[-parallel] (localhost, unless PGPORT or PGHOST is used)
                       ##  - when failed:
                       ##     - check src/test/regress/expected/*.out to see the test and its expected output, and
                       ##       compare with diff on src/test/regress/results/*.out
                       ##     - or directly look at src/test/regress/regression.diffs
UPGRADING ==>          ##Only needs to care about upgrade incompatibilities for major upgrades (e.g. 9.3 to 9.4), every year.
                       ##How:
                       ##  - use pg_dumpall backup
                       ##  - use pg_upgrade -b OLDBINDIR -B NEWBINDIR -d OLDDATADIR -D NEWDATADIR. Flags are:
                       ##     -u USER
                       ##     -c dry-run
                       ##     -j NUM: multiprocess run
                       ##     -k: use hard links instead of copies. Faster.
                       ##     -p|P NUM: old|new port number (can also use PGPORTOLD|NEW)
                       ##    NEWBINDIR and NEWDATADIR must be a new install, with similar settings.
                       ##    OLD*DIR must be erased afterwards.
                       ##    Connects to OLDDATADIR -> might want to put authentication as trust temporarily
                       ##    Both servers must be down.
                       ##    Must use NEWBINDIR/pg_upgrade, not OLDBINDIR.
                       ##    Cannot upgrade a log shipping standby.

STARTING ==>           ##If DATADIR (e.g. /usr/local/pgsql/data), as postgres user:
                       ##  - initdb -D DATADIR: creates cluster
                       ##  - postgres -D DATADIR 2> LOG &: starts the server
                       ##  - createdb DATABASE (default "postgres" DATABASE could be used though)
                       ##  - psql [DATABASE]
                       ##Autostart at boot time are system-specific.
                       ##Have a lot of memory PC to avoid memory crash if big data.
STOPPING ==>           ##Terminate server:
                       ##  - Can send signals:
                       ##    - SIGTERM (15): doesn't accept new connections, but let current ones finish.
                       ##       - server process stops, but will redo uncaught statements when restarting
                       ##    - SIGINT (2, CTRL-C): close all connections with a SIGTERM: abort current statements but close
                       ##      server itself propertly
                       ##    - SIGQUIT (3, CTRL-\): close all connections with a SIGQUIT: abort statements and server.
                       ##  - to send a signal on Windows, use pg_ctl kill ...
                       ##  - PID of a cluster can be found in DATADIR/postmaster.pid (ENVVAR external_pid_file) (first line)
                       ##    or with pg_backend_pid().
                       ##Terminate a client session (superuser or same member, but not same user):
                       ##  - pg_terminate_backend(PID)
                       ##  - send SIGTERM (15)
                       ##  - can see client PID with pg_stat_activity (pid)
                       ##Terminate a client current statement (superuser or same member, but not same user):
                       ##  - pg_cancel_backend(PID)
                       ##  - send INT (2, CTRL-C)
LOCATIONS ==>          ##  - Each cluster has a DATADIR (must have permissions 0700)
                       ##  - SHAREDIR, e.g. /usr/share/postgresql/9.3/

initdb [-D DATADIR]    ##Creates a cluster, i.e.:
                       ##  - populates or create DATADIR. Def: PGDATA
                       ##     - directory under which all data of the cluster are stored.
                       ##     - /usr/local/pgsql/data/ or /var/lib/pgsql/data/ are often used.
                       ##  - creates template0|1 and an empty database "postgres"
                       ##  - create cluster-specific system catalogs
                       ##  - initialize cluster-wide ENVVAR
                       ##User:
                       ##  - should have right to write in DATADIR
                       ##  - will use current OS_USER as the superuser ROLE (will create the ROLE in the cluster)
                       ##     - good idea to use a OS_USER which don't have any permissions on the filesystem appart from
                       ##       the directories used by the server. A OS_USER "postgres" is created by def. installation for
                       ##       this purpose.
                       ##  - can't be root
-U ROLE                ##Name of the superuser ROLE (by def. the current OS_USER).
                       ##Change the ROLE name but not the fact that OS_USER will be superuser, not the name of the default
                       ##database "postgres" (but could rename it)
-E STR                 ##See encoding in this doc.
--lc-collate|ctype|
messages|monetary|
numeric|time=LOCALE    ##See locales in this doc.
--locale=LOCALE        ##Same but for all
--auth-host|local=STR  ##
-A STR                 ##
-W
--pwfile=FILE          ##Default authentification, see pg_hba.conf
-X DIR                 ##Log DIR

LIBPQ VARIABLES ==>    ##Used in several places in this doc.
host[addr]             ##Def is localhost or PGHOST[ADDR]. Can be an absolute path to a Unix socket.
                       ##With addr:
                       ##  - specifies IP address
                       ##  - avoid host name lookup, faster.
                       ##  - Can't be used with Kerberos, GSSAPI, SSPI or verify-full SSL
port                   ##Def is PGPORT (5432)
dbname                 ##Def: OS_USER or PGDATABASE
user                   ##ROLE to use. Def: OS_USER or PGUSER
connect_timeout        ##In seconds. Def: 0 (unlim). Should not be <2. Def: PGCONNECT_TIMEOUT
client_encoding        ##Def: "auto" (defaults OS locales). Def: PGCLIENT_ENCODING
options                ##Set postgres flags at runtime, client-side. Def: PGOPTIONS
                       ##Can use -c ENVVAR=VAL to set ENVVAR.
[fallback_]            ##Name of the application (psql, pg_dump, etc.)
application_name       ##Used in logs (title of the client connection). Can be env. variable PGAPPNAME too.
                       ##If application_name or PGAPPNAME is blank, defaults to fallback_* (let users override it)
keepalives             ##1 to enable it (def). If connection seems lost, sends packet to check it actually is.
keepalives_            ##Send max *count packets, every *interval seconds. Start sending after non-activity from server for
[idle|iterval|count]   ##*idle packets.
                       ##Can also use ENVVAR tcp_keepalives_*
OTHERS ==>             ##See Authentication section
service                ##Additional parameters (def: PGSERVICE[FILE])

postgres [-D DATADIR]  ##Starts the server daemon for a specific cluster.
                       ##To start postgres on several clusters on same machine, use different ports.
                       ##Only one server can be launched for a given cluster. Each client session spans a new process.
                       ##Should be logged as the same OS_USER who used initdb. It should have permissions to access files that
                       ##will be used:
                       ##   - DIR of TABLESPACE
                       ##   - FILE referenced by copy from|to, load STR or create function ... as STR
                       ##DATADIR is PGDATA by def (unset by def). Can also use ENVVAR data_directory, and PGDATA or -D DATADIR
                       ##will point to the directory containing the postgresql.conf.
                       ##Print log messages on stderr. Should be launched in the background.
                       ##Can call version(), ENVVAR server_version[_num] and also a file DATADIR/PG_VERSION.
                       ##Commandes that started the server can be found under DATADIR/postmaster.opts
                       ##Connections are done:
                       ##  - using Unix sockets ("local"), on a socket at DIR/.s.PSQL.PORT (created at server start), where
                       ##    DIR is designated by ENVVAR unix_socket_directories (def: /tmp or /var/lib/postgresql/),
                       ##    comma-separated list by order of preference. Can be "" to disallow "local" connections.
                       ##    OS_USER must (to avoid spoofing) have:
                       ##      - permissions on DIR, preferably only him
                       ##      - socket itself will be own by OS_USER and OS_GROUP_USER (can be changed with ENVVAR
                       ##        unix_socket_group)
                       ##      - socket itself will have permission ENVVAR unix_socket_permissions (def: 0777). Could set to
                       ##        0770 or 0700 (only OS_GROUP_USER+OS_USER or only OS_USER can connect)
                       ##    Client must connect by using DIR as host
                       ##  - using TCP/IP ("host"), which uses socket designated by IP addresses specified in ENVVAR
                       ##    listen_addresses (comma-separated) with IPv4|6 addresses (def: "localhost"). Should be set at
                       ##    server start.
                       ##    "*" means all IPv4|6, "0.0.0.0" all IPv4, "::" all IPv6
                       ##If crash, sessions will restart automatically if ENVVAR restart_after_crash (def: on)
-p NUM                 ##Port number (def: 5432).
                       ##Can also use ENVVAR port at server start.
-k STR                 ##Sets ENVVAR unix_socket_directories.
-h STR                 ##Sets ENVVAR listen_addresses
-i                     ##Same as -h "*"
-N                     ##Sets ENVVAR max_connections (def: 100), which is the number of superuser + normal users max
                       ##connections. The last ENVVAR superuser_reserved_connections (def: 3) are kept for superuser only.
                       ##Look at work_mem to see estimate of memory used by this configuration.
                       ##Lower max_connections means more connections denied, higher means more chance to crash server.
                       ##Look at real life usage to set this setting.
                       ##Can also sets database-wise with connection limit (see create database)
--single               ##Single-user mode: start both the daemon and a superuser client session
                       ##Must be put before -D DATADIR
                       ##Quits with EOF (C-D). No readline.
                       ##By default use newlines instead of semicolons. Use -j to use EOF instead (then there will be only one
                       ##command).
-l                     ##Enable SSL connections

pg_isready             ##Return (exit code) according to server up or down status: 0 if OK, 1 if refusing connection, 2 if no
                       ##server response, 3 if could not send request.
-d -h -p -U -w|W       ##Connection options (see psql)
-t NUM                 ##Timeout (in sec, def 3, 0 to disable)
-q                     ##quiet

ENVVAR ==>             ##Environment variables. Can be specified with:
                       ##  - cluster-specific:
                       ##     - postgres -c VAR=VAL or postgres --VAR=VAL
                       ##     - editing DATADIR/postgresql.conf (ENVVAR config_file)
                       ##        - has lines VAR = VAL, and #comment
                       ##        - can have include[_if_exists] 'FILE' or include_dir 'DIR' (includes DIR/*.conf, by
                       ##        - alphabetical order)
                       ##        - can use pg_reload_conf() or send SIGHUP to server daemon postgres
                       ##        - can see load time with pg_conf_load_time()
                       ##     - PGOPTIONS
                       ##  - role-specific:
                       ##     - alter role ROLE ... set ENVVAR to VAL | from current
                       ##                           reset ENVVAR|all
                       ##  - database-specific:
                       ##     - alter role all in database DATABASE ... set ENVVAR from current
                       ##                                               reset ENVVAR|all
                       ##  - role-database-specific:
                       ##     - alter role ROLE in database DATABASE ... set ENVVAR to VAL | from current
                       ##                                                reset ENVVAR|all
                       ##  - session-specific:
                       ##     - set ...
                       ##     - setting PGOPTIONS with '-c VAR=VAL ...' before launching the client command
                       ##  - function-specific:
                       ##     - do create function ... set ENVVAR to VAL | from current:
                       ##        - from current: use current VAL as VAL
                       ##     - inside a function:
                       ##        - set local (if FUNC() used create function ... set ...)
                       ##  - transaction-specific:
                       ##     - set local ...
                       ##Some require superuser ROLE to write.
                       ##Some cannot be session-specific.
                       ##Are always STR. BOOL actually use 'on' and 'off'.
set [local] ENVAR      ##VAL can be default
to VAL                 ##For an ARR, write VAL...
                       ##Can also use set_config(ENVVAR_STR, VAL_STR, BOOL) (BOOL is is_local)
reset ENVVAR|all       ##Same as set ... to default
show ENVVAR|all        ##Print their values.
                       ##Can also use current_setting(ENVVAR_STR)
                       ##Can also use postgres ... -C ENVVAR, which is done against a running server
alter TYPE VAR ...
set ENVVAR
to VAL|from current    ##
alter TYPE VAR ...
reset ENVVAR|all       ##For database, function or role

discard plans|temp|all ##Remove session-specific information:
                       ##  - plans: cached explain plans
                       ##  - temp: TEMP
                       ##  - all: cached explain plans, TEMP, session-specific ENVVAR, PREP, CURSOR, unlisten *,
                       ##    pg_advisory_unlock_all(), put session_user and current_user to default

pg_postmaster_start_
time()                 ##
current_database()     ##Can also use information_schema.information_schema_catalog_name.catalog_name
current_query()        ##Current executing statement
inet_client|server_
addr|port()            ##

statement_timeout      ##ENVVAR (in ms) after which client requests fail (def: 0).

FILESYSTEM ACCESS ==>  ##
pg_ls_dir(STR)         ##Returns COL_STR. Must be relative path (.. not allowed)
pg_read[_binary]_file
(STR)                  ##Returns as STR|BYTEA
pg_stat_file(STR)      ##Returns as a TABLE with one row with COL size, access (atime), modification (mtime), change (ctime),
                       ##creation (creation time, only Windows) and isdir.
pg_file_write(STR,
STR2, BOOL)  adminpack###BOOL is append
pg_file_read(STR,STR2,
BOOL)                 ###BOOL is append
pg_file_rename(STR,
STR2)                 ###
pg_file_unlink(STR)   ###
pg_file_length(STR)   ###

DATADIR ==>            ##  - base/: databases main data.
                       ##    Querying database|tables oids:
                       ##      - utility oid2name:
                       ##         - without option: database oid -> name
                       ##         - -H -p -U -P: usual connection flags
                       ##         - -d DATABASE: show table oid -> name
                       ##           - -S: include system catalogs, views and TOAST
                       ##         - -f|o NUM: for tables with filenode|oid NUM
                       ##         - -t STR: same with name
                       ##         - -i: include SEQUENCE and INDEX
                       ##         - -s: info about TABLESPACE
                       ##      - pg_relation_filenode|filepath(OID|STR)
                       ##    6MB for an empty database. Each file is a table with optionally:
                       ##      - a FILE_vm: Visibility Map
                       ##      - a FILE_fsm: Free Space Map
                       ##    TOAST are special subfiles when a file is too big.
                       ##  - global/: cluster-wide data.
                       ##  - pg_xlog/: WAL
                       ##  - pg_log/: logs (see log_directory)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CLIENT             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


psql                   ##Starts interactive commands while connecting to a DATABASE
                       ##Read from standard input, so <<<"STR" or <FILE are possible.
                       ##Use readline so can use a .inputrc
                       ##The five first options are connection-related, and shared by other commands. When available, can
                       ##also always use a STR argument instead with either:
                       ##  - VAR=VAL... as space-separated libpq variables
                       ##  - postgres://[USER:PASSWORD@][HOST][:PORT]/[DATABASE][?VAR=VAL] (VAR are libpq variables)
-d DATABASE            ##Def: "postgres"
-h HOST                ##Def: PGHOST or (if absent) local host.
-p PORT                ##Def: PGPORT or (if absent) default port.
-U USER                ##Def: PGUSER or (if absent) current OS user.
-w|W                   ##Force to not use or use a password

-c STR
-f FILE                ##Redirect input from STR or FILE (non-interactive shell)
-o FILE                ##Redirect output to FILE
-L FILE                ##Prints queries to FILE (logging)

-X                     ##Do not read init files (files read at session start containing any command that PSQL understands)
                       ##Init file can be:
                       ##  - systemwide (/etc/postgresql-common/psqlrc)
                       ##  - user-specific (shell variable PSQLRC, by def HOME/.psqlrc)
                       ##Can append -NUM[.NUM2[.NUM3]] to target only specific PostgreSQL versions.
-n                     ##Don't use readline (useful when pasting)
-s                     ##Ask for confirmation before each commande
-1                     ##Wrap commands in a transaction block. Commands should not contain transaction blocks themselves.
                       ##Disable all EFUNC
-l                     ##Does \list then exits

\COMMANDE              ##psql can use special commandes. Finished by newline not semicolon.
                       ##Can use \n \t \r \000 \x00 in 'STR'. Can do shell substitution with `COMMAND`
                       ##Can appear anywhere a SQL command can.
                       ##Some commands use SREGEXP:
                       ##  - . only means SCHEMA separation (otherwise current schema).
                       ##    For all objects outside current SCHEMA, do *.*
                       ##  - $ not present
                       ##  - ? and * are globbing
                       ##  - case-insensitive unless ""

\q                     ##Exits
\h [COMMAND]           ##Help on SQL commands
\?                     ##Help on PSQL commands
\! [COMMAND]           ##Execute a shell command according to the shell variable SHELL
\timing [on|off]       ##Show time taken by commands

\pset VAR [VAL]        ##If VAL, sets printing options, if not either toggle or print current value.
                       ##VAR [VAL] can be:
format STR             ##How things are printed. Can be unaligned, aligned (def), wrapped, html, latex[-long table], troff-ms.
                       ##  - Wrapped: like aligned but wrap long lines according to columns INT (if 0, only on terminal
                       ##    output, and uses shell env variable COLUMNS or detected screen size)
                       ##  - unaligned: can control separators with field|recordsep STR or field|recordsep_zero (use \0)
x [on|off|auto]        ##Put in expanded mode (switch cols and rows, good for table with long width). If auto, do it only for
                       ##wide tables.
t                      ##No footer or headers
footer [on|off]        ##footer is command tag, etc.
title STR              ##Print STR in front of all tables
border INT             ##Column width
linestyle ascii|unicode##
null STR               ##What to print for null strings (def: "")
numericlocale on|off   ##Locale-specific NUM
pager on|off|always    ##Using env variable PAGER (def: less)
tableattr STR          ##In HTML output format, HTML attributes of <table>

END OF LINE ==>        ##The seven following commands should be put at the end of a command (instead of ;):
\w FILE| |COMMAND      ##Print current input to FILE or pipe to COMMAND (don't execute it)
\p                     ##Same as \w |cat
\g [FILE| |COMMAND]    ##Redirect ouput like \o, but for current input only.
\r                     ##Clears current input.
\e [FILE]              ##Edit current command (or if FILE, FILE) with editor (shell variable [PSQL_]EDITOR, i.e. vim), which
                       ##becomes the new command.
                       ##New command is only executed if terminated by ;
\ef [FUNC[(...)]]      ##Same but the command is a "create or replace function FUNC". If no FUNC, creates a empty definition.
\watch INT             ##Execute current input every INT seconds, until interrupted (or error).

\o [FILE| |COMMAND ]   ##Redirect stdout (not stderr) (def: to stdout)
\i[r] FILE             ##Execute command in FILE.
                       ##If r and non-interactive, relative path to script DIR, otherwise relative to PWD)
\copy ...              ##Like SQL copy but performed locally, not on the server.
                       ##Can use pstdin|out to avoid any stdin|stdout redirection (i.e. use terminal input|output)
\lo_export OID STR
\lo_import STR         ##Like lo_ex|import(...), but performed locally, not on the server.

\[q]echo [-n] VAL      ##Prints VAL (use q if \o has been used), without trailing newline if -n
                       ##Can appear anywhere in a SQL command.
\setenv VAR [VAL]      ##Sets shell environment variables

\d...[S][+] [SREGEXP]  ##Show info about VAR specified in ... (S for also system ones, + for more info) among:
                       ##  - nothing (all table-like), t (table), v (view), i (index), m (materialized view),
                       ##    s (sequence)
                       ##  - u (ROLE), dp (default user privilege), p (all table-like with privileges)
                       ##  - d (constraint, operator*, rule, trigger)
                       ##  - n (SCHEMA)
                       ##  - b (tablespace)
                       ##  - L (LANGUAGE), x (EXTENSION)
                       ##  - T (TYPE), D (domain), C (cast)
                       ##  - O (COLLATION), c (conversion)
                       ##  - E|et (ftable), es (foreign server), eu (user mapping), ew (foreign data wrapper)
                       ##  - f[n|a|w|t] (func, afunc, wfunc, tfunc), y (efunc), o (OPERATOR)
                       ##  - F (REGCONF), Fd (DICTIONARY), Fp (PARSER), Ft (TEMPLATE)
                       ##  - l (large objects)
                       ##  - rds: ENVVAR, ROLE-specific (SREGEXP) and optionally database-specific too (use a second SREGEXP)
                       ##  - \l[+] (not \d): DATABASE
\sf [FUNC[(...)]]      ##Prints definition of FUNC

\c [DATABASE [USER]
[HOST] [PORT]]         ##Connect to different database. Def is current
\conninfo              ##Print current connection info
\cd [DIR]              ##Change PWD (def: HOME)

\[un]set INTVAR [VAL]  ##Internal VAR. INTVAR is case-sensitive. Are not ENVVAR.
                       ##Can also use psql -v INTVAR[=[VAL]]
                       ##Can be created INTVAR. Substitution is done with:
                       ##  - :INTVAR: macro expansion of INTVAR
                       ##  - :'INTVAR': same but surround with '', unless already present (do with STR)
                       ##  - :"INTVAR": same with "" (do with VAR)
\gset [WORD]           ##Put at end of command (line \p, \r, etc.)
                       ##Command output is redirected to new INTVAR (one by column) called [WORD_]COL_VAR.
                       ##Columns must be named. Null give unset variables, failing commands don't change variables
\prompt [-f] [STR]
INTVAR                 ##Prompt for INTVAR, with message STR. Use terminal (no -f) or stdin/stdout (-f)

DBNAME                 ##
HOST                   ##
PORT                   ##
USER                   ##Connections info

ON_ERROR_STOP          ##When set, errors terminate script (non-interactive) or line (interactive)
                       ##ENVVAR exit_on_error is also available, where errors terminate whole session.
ON_ERROR_ROLLBACK      ##If on, errors in transactions are just ignored. If off (def), they abort the whole transaction.
on|interactive|off     ##Interactive means on for interactive and off for non-interactive.
IGNOREEOF INT          ##Number of EOF (C-D) to send to terminate a session (def: 1)

PROMPT1|2|3            ##Prompt. Literal STR with possible sequences escaped by %: M (full host), m (short host), > (port),
                       ##n (user), / (database), ~ (database, but ~ if default one), # (# if superuser, > otherwise),
                       ##R (= if normal, ! if disconnected), x (transaction block), NNN (octal), `command`, :VAR:,
                       ##[ and ] (ansi escaping sequences). Def is %/%#
                       ##PROMPT1 is normal, PROMPT2 when continuing on a new line, PROMPT3 when reading from stdin
COMP_KEYWORD_CASE
[preserve-]lower|upper ##Completion case. If preserve, tries to keep current word case.
QUIET                  ##Don't print welcome message
ECHO                   ##When set to '' (def), do nothing.
                       ##When 'queries', print all input to output.
                       ##When 'all', same but only for non-interactive input.
ECHO_HIDDEN            ##When set, prints commands behind \ commands

HISTFILE               ##Def: ~/.psql_history. Written at exit.
                       ##Can for example use user, database-specific, etc. hist files
                       ##Can also use shell variable PSQL_HISTORY
                       ##Can also use \s [FILE] (always relative to PWD) (def: print to stdout)
HISTSIZE               ##Number max of commands (def: 500)
HISTCONTROL
ignoredups|space|both  ##Like in Bash

AUTOCOMMIT on|off      ##By def (on), each individual command is wrapped in a single transaction. When off, a start
                       ##transaction is implicitly fired but needs to manually commit it
LASTOID                ##OID of last written object
FETCH_COUNT            ##Number of max rows in memory at once

PGADMIN3 ==>           ##GUI client. Can do almost anything that can be done with psql.
                       ##Installing/launching:
                       ##  - MaintenanceDB:
                       ##     - DATABASE where pgAdmin connects first (should use postgres)
                       ##     - should install adminpack extension
                       ##  - Should use the same OS_USER we would use for a normal psql session
                       ##Usage:
                       ##  - Is not refreshed automatically: needs to refresh it manually.
                       ##  - Some types of objects are hidden by default (e.g. AFUNC or TYPE): can change in options
                       ##  - Servers have names, and can be grouped.
                       ##Useful tools:
                       ##  - Easy access to objects, with properties and statistics, and conf files
                       ##  - Can open a psql session in a console
                       ##  - Edit/View data: simple spreadsheet.
                       ##    Read-only if no primary key.
                       ##    blank is null (needs to write '' for '')
                       ##  - Server status: current clients, locks, log (needs to edit path in options), transactions
                       ##Less useful tools:
                       ##  - Grant wizard (available when on a SCHEMA): generate SQL grant statements with GUI
                       ##  - Reports: generating HTML files for objects properties, statistics, dependencies.
                       ##  - Query tool: useless, use the console (unless needs a SQL debugger, which needs to be installed)

PGAGENT ==>            ##  - SQL "cron", inside pgadmin3
                       ##  - to install on a cluster:
                       ##     - connect to postgres
                       ##     - install plpgsql
                       ##     - as superuser ROLE, execute pgagent.sql (i.e. in pgadmin3 sharedir).
                       ##       It will create TABLE and FUNC used for storing/manipulating the jobs and schedules, in SCHEMA
                       ##       pagent
                       ##     - launch pgagent LIBPQ_STR
                       ##        - this daemon will look at those tables and determine if need to launch job
                       ##        - run as root
                       ##        - options:
                       ##           -s LOG_FILE
                       ##           -l1 (best verbosity)
                       ##           -t (poll time, def: 10 sec)
                       ##        - do no put password in LIBPQ_STR (use passfile instead)
                       ##        - automatically in background and detached from current terminal tab
                       ##        - should automatically launch it at startup
                       ##  - configure jobs using pgadmin, under "Jobs":
                       ##     - steps are:
                       ##         - SQL commands (will be executed as superuser ROLE)
                       ##         - or "batch" (shell commands, run as root, must start with #!/bin/bash)
                       ##     - schedules are when it is done
                       ##  - monitor with check_postgres

TEAMPOSTGRESQL ==>     ##Alternative to pgAdminIII. Less functions, but is a web interface instead of a dekstop app.

                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              IPC              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


notify WORD[, STR_LIT] ##Sends a message STR_LIT (def: "") on channel WORD. Also communicates the server PID.
                       ##Any client connected to the same server and listening to WORD will get a notification.
                       ##How this notification is handled depends on the client:
                       ##  - psql: print to stderr
                       ##If sends twice the same WORD + STR_LIT, might notify only once.
                       ##STR_LIT is max 8KB
                       ##pg_notify(STR, STR2) is also available: same but don't need to use constant STR_LIT.
[un]listen WORD        ##WORD can be * for unlisten
                       ##listening is session-wise: unlisten * is executed at end of each session
pg_listening_channels()##


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LOGGING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOGGING ==>            ##Controlled by several ENVVAR.
log_destination        ##Where to put log messages (comma-separated list):
                       ##  - stderr (def)
                       ##  - csvlog:
                       ##     - with logging_collector, will output as CSV files
                       ##     - goal is to import them in tables with copy from
                       ##  - syslog:
                       ##     - prefer using the logging_collector
                       ##     - Needs to put local0.* /var/log/postgresql/ in syslog conf file
                       ##     - Can use ENVVAR syslog_facility, syslog_ident and event_source
logging_collector      ##When on, use the logging facility:
                       ##  - redirect stderr to file in ENVVAR log_directory too (can be relative to DATADIR) (def: pg_log).
                       ##      - files are named according to ENVVAR log_filename, which can use %... (data escape)
                       ##        (def: "postgresql-%Y-%m-%d_%H%M%S.log") for file creation time
                       ##      - a new file is created every ENVVAR log_rotation_age (def: 1d)
                       ##        or every time the file is more than ENVVAR log_rotation_size (def: 10MB)
                       ##        or when pg_rotate_file() is called
                       ##    If csvlog, will be in CSV format and use name ENVVAR application_name.
                       ##  - Files have permission ENVVAR log_file_mode (def: 0600, only server owner can read/write)

log|client_min_messages##Between debug5-1,log,notice,warning,error,fatal,panic (def: notice for client, warning for log) for
                       ##either client messages or logging.
                       ##postgres -d NUM can set log_min_messages (from 0 to 5, def: 0), for DEBUG5-1
log_error_verbosity    ##Verbosity of messages: default, verbose (include SQLSTATE error code) or terse (no defail, hint,
                       ##query nor context)
log_min_error_statement##Same as log_min_message, but for logging the statements themselves (def: error).
log_statement          ##Which statements to log among none (def), all, ddl or mod (include ddl)
log_min_duration_      ##Logs time of statement execution, when it is higher than this limit (in ms, 0 to log all, -1 not to
statement              ##log it (def))
log_statement_stats    ##If on, server prints to stderr the statements executed and the time it took
log_line_prefix        ##What to put in beginning of each log line. Can include %-escapes:
                       ##  - a: application_name
                       ##  - u: user
                       ##  - d: database
                       ##  - r: host+port
                       ##  - h: host
                       ##  - p: PID
                       ##  - t|m: timestamp (to seconds|ms). Timezone is controlled by ENVVAR log_timezone (def: 'localtime')
                       ##  - s: process start time
                       ##  - c: session id (process start time + PID)
                       ##  - i: command
                       ##  - e: error code
                       ##  - l: log number, session-wise
                       ##  - x|v: [virtual] transaction ID
log_checkpoints        ##Logs checkpoints (def: off)
log_[dis]connections   ##Logs [dis]connections attempts (def: off)
log_lock_waits         ##Logs deadlocks (see deadlock_timeout)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MONITORING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


check_postgres         ##Performs several sanity monitoring tests, and outputs warnings.
-db -H -u -p           ##Connection options. Some actions requires several clusters/databases: then use STR...
--dbpass=STR...        ##Can also use STR... for any action to perform the check separately on several clusters/databases
                       ##(will return problem if any of them is wrong), or on master/slave
--output=STR           ##Output format (this doc only talks about nagios):
                       ##  - nagios (def):
                       ##     - compatible with NAGIOS (server network monitoring application)
                       ##     - one line with test name, then OK|WARNING|CRITICAL|UNKNOWN (and exit code 0 to 3 accordingly),
                       ##       followed by colon and description.
                       ##       UNKNOWN is when test cannot be performed, and WARNING|CRITICAL are set up according to
                       ##       -w|c VAL ("thresholds", depends on action). If warning = critical, turn off warnings.
                       ##     - can use option:
                       ##       --showperf=1|0: show performance at end of line (def: 1)
                       ##           --perflimit=NUM: limit --showperf to NUM items (def: 0, i.e. unlim)
                       ##           --showtime=1|0: show queries execution time (def: 1)
                       ##  - mrtg
                       ##     - compatible with MRTG (traffic load monitoring application)
                       ##     - four lines: NUM (usually main info), description (usually 0), blank and DATABASE
                       ##       (only when relevant)
                       ##     - can use option --mrtg=VAL to pass arguments to MRTG
                       ##     - usually don't issue warnings|critical
                       ##  - simple
                       ##     - like mrtg, but only first line (NUM)
                       ##     - can be followed by unit, e.g. --output=simple,MB
--action=STR...        ##Checks to perform, among:
                       ##Connections:
                       ##  - connection:
                       ##    Checks if server is up.
                       ##    CRITICAL + psql error description if no, OK + server version if yes.
                       ##  - backends:
                       ##    Checks if number of connections is more than threshold (NUM or % of max_connections) or if more
                       ##    than threshold connections are left (-NUM).
                       ##    Can only select --noidle connections.
                       ##    Look at --include below for how to specify per DATABASE.
                       ##  - pgbouncer_backends:
                       ##    Same but with pgBouncer (max_client_conn)
                       ##  - pgb_pool_maxwait|cl_active|waiting|sv_active|idle|used|tested|login:
                       ##    Checks if any pgBouncer pool has more than thresholds (see show pools in pgBouncer)
                       ##Space usage (look at --include below):
                       ##  - disk_space:
                       ##    Checks if any partition used by any data in the cluster (DATADIR, tablespaces, WAL dir, log dir)
                       ##    is using more than thresholds of memory (can use percentage, "MB", etc. units, and "and|or")
                       ##  - database|relation|index|table_size:
                       ##    Checks if any DATABASE|TABLE|INDEX is more than thresholds (can include "MB", etc.).
                       ##    Prints size in bytes (first line), and name (last line) of biggest one.
                       ##  - bloat:
                       ##    Checks if there are more than threshold (NUM (unit: 'KB', etc.) or % of TABLE size) dead rows
                       ##    in any TABLE|INDEX (only consider ones with > 10|15 pages)
                       ##  - wal_files:
                       ##    Checks if there are more than thresholds WAL files.
                       ##    Number of WAL files is usually comprised in a given range, unless there is a malfunction
                       ##    (long transaction, wrong archive_command, etc.), creating disk space usage risk.
                       ##Unusual state:
                       ##  - pgagent_jobs:
                       ##    Checks if all pgagent jobs since threshold (unit 's|m|h|d') have an exit code of 0
                       ##  - logfile:
                       ##    Checks if redirection to log file is happening correctly.
                       ##    Must provide log full path with --logfile=STR (can use "%Y|%m|%d|%H").
                       ##    Does not work if redirection to stderr without logging collector on.
                       ##  - commitratio:
                       ##    Checks if commit ratio (non-rollbacked transactions/transactions) is lower than thresholds.
                       ##  - disabled_triggers:
                       ##    Checks if number of disabled triggers is >= thresholds.
                       ##  - locks:
                       ##    Checks if number of locks held >= threshold.
                       ##  - txn_idle:
                       ##    Checks if there are more than thresholds idle current transactions (waiting for locks), and if
                       ##    any has lasted more than threshold (unit 's|m|h|d')
                       ##  - prepared_txns:
                       ##    Checks max. age of prepared transactions (not prepared statements).
                       ##Corruption:
                       ##  - sequence:
                       ##    Checks if sequence has been used more than threshold (%)
                       ##  - txn_wraparound:
                       ##    Checks if more than thresholds transactions have not been vacuumed, creating risk for
                       ##    xid wraparound. Wraparound happends every 2e9, so value should be e.g. 1.5e9
                       ##  - autovac_freeze:
                       ##    Checks if number of old transactions is more than threshold (%) of autovacuum_freeze_max_age
                       ##Performance (look at --include below):
                       ##  - query_runtime:
                       ##    Checks if queries specified by --queryname=STR (VIEW or FUNC) runs in more than time specified
                       ##  - txn|query_time:
                       ##    Same for running transactions|queries
                       ##  - hitratio:
                       ##    Checks if cache hit ratio is lower than thresholds.
                       ##  - last_[auto]analyze|vacuum:
                       ##    Checks if has been run (auto only checks autovacuum|analyze, other checks all) since threshold
                       ##    (in s|m|h|d, def: 1d for vacuum, 2d for analyze).
                       ##    Should exclude tables with no dead rows.
                       ##  - dbstats:
                       ##    For each DATABASE, print one line with backends (number of processes), commits|rollbacks
                       ##    (number since beginning), read|hit (number of blocks since beginning), ret|fetch|ins|upd|del
                       ##    (number of rows), dbname, idx..., seq...
                       ##    Cannot use alternate outputs.
                       ##Comparison:
                       ##  - same_schema:
                       ##    Compares two or more databases, schema-wise (not data-wise).
                       ##    If only one host: make a time-based comparaison: next time it will be executed, will compare
                       ##    with previous version.
                       ##    To do so, create a file at ./check_postgres.audit.port.PORT.db.DATABASE:
                       ##      - Use --replace to overwrite it.
                       ##      - can add .STR to the filename with suffix=STR
                       ##    Can exclude objects with:
                       ##      --filter=nouser|schema|table|view|index|sequence|constraint|trigger|perm|funcbody|
                       ##        function[=RGXSTR]
                       ##      --filter=noposition: don't compare columns positions
                       ##  - settings_checksum:
                       ##    Compares two settings (ENVVAR...) for a given user.
                       ##    First use -c 0 to get checksum, then do -w|c=CHECKSUM
                       ##  - pgbouncer_checksum:
                       ##    Same but with pgBouncer
                       ##  - timesync:
                       ##    Checks if local time diff >= threshold (in sec., should not be <5)
                       ##Standbies (can all test standby mode with --assume-standby|prod-mode):
                       ##  - hot_standby_delay:
                       ##    Checks if delay between current database (master) and slave >= threshold (number of WAL lines)
                       ##  - replicate_row:
                       ##    Checks that updates of a single row takes no more than threshold to replicate using replication.
                       ##    Should choose column to change (should pick one not likely to be changed by another process),
                       ##    with --repinfo=TABLE,PKEY,PKEY_VAL(to select row),COL_VAR,OLD_VAL,NEW_VAL
                       ##  - checkpoint:
                       ##    Checks if last checkpoint was run more than threshold ago (unit: 's|m|h|d').
                       ##    Meant to be run on a slave. Must supply --datadir DATADIR
                       ##Upgrades:
                       ##  - new_version_bc|cp|pg:
                       ##    Checks if new version of Bucardo|check_postgres|PostgreSQL is available.
                       ##    Only nagios. UNKNOWN if binary not here, CRITICAL is revision upgrade, WARNING is major upgrade.
                       ##  - version:
                       ##    Checks that server version is at least threshold
                       ##Custom:
                       ##  - custom_query:
                       ##    Checks a custom --query=STR, which returns a single column called "result", if any row value,
                       ##    depending on type of -w|c VAL:
                       ##      - NUM: >= NUM
                       ##      - NUM[KB,etc.]: >= NUM
                       ##      - STR's|m|h|d': older or same as STR
                       ##      - STR: same as STR
--in|exclude=STR...    ##Limit the objects checked:
                       ##  - DATABASE: for backends, database_size, locks, query_time, txn_idle, txn_time
                       ##  - TABLE|INDEX: for bloat, index|table|relation_size, last_[auto]vacuum|analyze
                       ##  - FILESYSTEM: disk_space
                       ##include alone means "include only", but not alone means "include also" (to reinstate objects that
                       ##have been excluded with --exclude).
                       ##STR:
                       ##  - ending with . matches a schema
                       ##  - starting with ~ is a REGEXP (otherwise full VAR name)
--in|excludeuser=STR...##Same for objects owned by ROLE_STR...
                       ##Works for relation|database_size, query|txn_time, last_[auto]vacuum|analyze.
-t NUM                 ##Timeout (in secs, def: 10) after which returns UNKNOWN status, per cluster.
-v ...                 ##Verbosity. Do several times to increase verbosity.
--debugoutput=LETTER...##Prints also the psql output for a (all), c (critical), w (warning), o (ok), u (unknown)
--PGBINDIR=STR         ##psql directory (see man page on precautions to use)

pgbadger FILE[...]     ##FILE... are the log files (stderr, csvlog (need Text::csv_xs module) or syslog format).
                       ##FILE can be - for stdin (not for csvlog).
                       ##Recognize compressed files from extensions .gz, .bz2 or .zip
                       ##Should put:
                       ##  - log_statement to 'none' (do not enable it)
                       ##  - log_min_duration_statement to 0
                       ##  - log_checkpoints|[dis]connections|lock_waits to 'on'
                       ##  - log_temp_files to 0
                       ##  - lc_messages to 'C'
                       ##If stderr:
                       ##  - log_line_prefix to '%t [%p]: [%l-1] user=%u,db=%d,host=%h,application=%a'
                       ##    Use pgbadger -p '%t ...' (same as above) -f stderr
                       ##Use latest release (3.3 is not)
                       ##Needs to put as much as possible in logs to get all graphs.
                       ##Can zoom it with shift button.
-f stderr|csvlog|syslog##Def: stderr
                       ##For syslog:
                       ##  -i STR: Program name used as ident for syslog
-o FILE                ##Output file and format (among .html, .txt and .tsung). Def is output.html
                       ##Can also use -x text|html|tsung. Tsung is <sessions> tag for XML config file with most usual session.
-q                     ##Quiet

-c HOST
-d DATABASE
-u USER
-N APPLICATION_NAME    ##Filter for only this parameter (can be used several times)
-U USER                ##Filter for excluding USER (can be used several times)
-b|e DATE              ##Start|end time to be processed.
-l FILE                ##Only use logs starting from this log file.

-a NUM                 ##Step (in min, def: 5) for the average number of query per second.
-s NUM                 ##Number of sample queries (def: 3)
-t NUM                 ##Number of top queries (def: 20)
--pie-limit NUM        ##Minimum percentage for pie chart slices

-S                     ##Only analyze select queries
--exclude-query STR    ##Exclude queries matching regexp STR
--exclude-file FILE    ##Same but regexps are in FILE
--include-...          ##Inverse: include only.
-T                     ##HTML <title> (def: "pgBadger")
-C                     ##Remove /*comment*/ from queries
--disable-error|hourly|
type|session|temporary|
connection|query|lock|
autovacuum|checkpoint  ##Remove a specific part in the report

-j|J NUM               ##Multiprocessing. Cannot be used with compressed files, csvlog or on Windows.
                       ##j is number of jobs/log file, J is number of log files in same time.

pgcluu_collectd DIR    ##GUI that gives info on resource and space usage (similar to pgbadger, but gives some different stats).
                       ##pgcluu_collectd is the daemon collecting stats, pgcluu the tool creating reports
                       ##Should be run as the OS_USER owning the cluster, on a DIR owned by this OS_USER.
                       ##Good idea is to put inside DATADIR, with same permissions as other folders.
                       ##psql, sar (from package sysstat) should be installed. Their path should be given with -P|s STR if not
                       ##in /usr/bin/
                       ##Can find a sar file and several CSV files in DIR/
-d -h -p -U            ##Connection options
-D                     ##Run as daemon. Can be killed with pgcluu_collectd -k
-i NUM                 ##Frequency in seconds (def: 60)
-f FILE                ##PID FILE (def: /tmp/pgcluu_collectd.pid)
--stat-type all        ##Includes also system catalogs stats.
-m STR                 ##Restrict data collection with a comma-separated list of metrics to perform (list can be found with
                       ##pgcluu_collectd --list-metric)
--pgbouncer-args=STR   ##If pgbouncer (connection pooling utility) is used, arguments to pass to it (e.g. connection options)

pgcluu DIR             ##Creates report. DIR is the pgcluu_collectd DIR
                       ##Should be run as same OS_USER as pgcluu_collectd
                       ##sadf (from package sysstat) should be installed. Its path should be given with -s STR if not
                       ##in /usr/bin/
                       ##Can zoom in graphs
-b|e DATETIME          ##Begin|end time when to report.
-d DATABASE            ##Filter for only DATABASE (can be used several times)
-T TABLE               ##Same for TABLE (don't seem to work)
-t                     ##Per table stats (don't seem to work)
-p DEVICE              ##Filter I/O info for only DEVICE (can be used several times)
-o DIR                 ##DIR to create the HTML files (def: $PWD)

pg_top [NUM]           ##Show info about running PostgreSQL clients and servers in realtime, tables|indexes read|write.
                       ##psql is shown as "postgresql" command.
                       ##Must be run as the OS_USER owning the server.
                       ##If NUM, only show NUM first processes.
                       ##Accepts the following keystrokes:
                       ##  C-L: refresh
                       ##  R|X: switch with table|index stats
                       ##    t: show cumulative, not instant stats
                       ##  i: toggle display of idle processes
                       ##  k: kill
                       ##  o: change sorting
                       ##  Q: show current query
                       ##  u: show only specific user
                       ##Also available for smartphones/tablets.
-I                     ##Do not display idle processes.
-o FIELD               ##Sorts according to FIELD
-z USER                ##Filter for only USER
-x [NUM]               ##Prints NUM first processes (def: "all"), then exits.
-c                     ##Show command name instead of full command line
-s NUM                 ##Delay in seconds (def: 5)
-r                     ##Connects to a remote database. Needs to use -h -p -U -W connection options.

DETAILED MONITORING ==>##Usually not needed, because there are higher-level monitoring tools:
                       ##   - ps auxww | grep ^postgres: see individual processes and description:
                       ##      - postgres master process
                       ##      - several master background processes: checkpoints, WAL, autovacuum, statistics collector
                       ##      - each client connection has one process with description showing CLIENT DATABASE HOST ACTIVITY
                       ##        (autoupdate can be turned on|off by ENVVAR update_process_title)
                       ##   - statistic collector:
                       ##      - daemon that fill in pg_stat* system views
                       ##      - used to collect statistics on activity
                       ##      - controlled by several ENVVAR BOOL:
                       ##         - track_activities (def: 'on') (COMMAND executed and time of execution)
                       ##            - track_activity_query_size (size of tracks in track_activities, def: 1024). Can only be
                       ##              set at server start.
                       ##         - track_counts (def: 'on') (general activity). Also allows explain.
                       ##         - track_io_timing (def: 'off') (I/O timing). Can be slow.
                       ##         - track_functions (def: 'none') (FUNC calls). Can also be 'pl' (PL/*) or 'all' (PL/*, SQL
                       ##           and C functions)
                       ##      - temp stats are stored in ENVVAR DATADIR/stats_temp_directory (def: 'pg_stat_tmp').
                       ##        Putting it in a RAM disk can improve performance.

pg_stat_activity       ##Server processes, with names, usernames, start|last time, addresses and command activity.
pg_stat_bgwriter       ##Background writer process's activity, e.g. for checkpoints.
pg_stat_replication    ##WAL sender processes.
pg_stat_database       ##DATABASE, with number of servers/clients, transactions, temp files, tuples manipulated
                       ##(fetch|select|insert|update|delete), blocks read|hits, time spent on I/O read|write, deadlocks.
pg_stat_database_
conflicts              ##DATABASE, with query cancelled due to recovery on standby servers.
pg_stat[io]_[xact_]    ##TABLE, with:
all|sys|user_tables    ##  - not io: number of sequential|indexed scans (and tuples they fetched), tuples manipulated
                       ##    (insert|update|delete), number of rows (live|dead), [auto]vacuum|analyze activity
                       ##  - io: disk read|hits for all, index-only, TOAST and TOAST index
                       ##Can be for only system catalogs (sys) or not (user).
                       ##If xact_, take the current transaction into account.
pg_stat[io]_           ##INDEX, with:
all|sys|user_indexes   ##  - not io: number of scan (with tuples fetched: bitmap + simple index scan, or simple index scan
                       ##    only)
                       ##  - io: index blocks read|hits (efficient if low read/hits %)
pg_statio_all_sequences ##SEQUENCE, with number of blocks read|hits
pg_stat_[xact_]user_   ##FUNC, with number of calls and total time (only FUNC, or FUNC called by it too).
functions              ##ENVVAR track_functions must be on.

pg_stat_statements     ##VIEW for all queries (query, time, number of rows, I/O).
                       ##Must put pg_stat_statements in ENVVAR shared_preload_libraries and use EXTENSION pg_stat_statements.
                       ##Can be reset with pg_stat_statements_reset().
                       ##Can use ENVVAR pg_stat_statementsmax (def: 1000)

OBJECT SIZES ==>       ##Can also look at pg_class.relpages (a page is 8KB)
pg_column_size(VAL)    ##
pg_database_size
(OID|STR)              ##
pg_tablespace_size
(OID|STR)
pg_indexes_size(OID|STR)
pg_relation_size       ##Size in bytes. Can be a TABLE, INDEX or TOAST.
(OID|STR[, STR2])      ##STR2 can be 'main' (def), 'vm' of 'fsm'
pg_total_relation_size ##Same but with INDEX included
(OID|STR)
pg_table_size(OID|STR) ##Same with INDEX excluded and only for TABLE
pg_size_pretty(UINT)   ##Convert a bytes size into human readable STR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         MAINTAINANCE          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


vacuum([full,][freeze,]##Options:
[verbose, ] [analyze]) ##  - analyze: do analyze ... too
[TABLE[( COL_VAR... )]]##  - full:
                       ##     - make database size shrink to its minimum (while non-full keep same size, but let rows space
                       ##       be used by further writes.
                       ##     - require exclusive lock
                       ##     - slower and should be avoided
                       ##  - verbose: print details
                       ##Goal:
                       ##  - With MVCC deleted rows are actually hidden so that other transactions can read them.
                       ##    Once they are not used anymore, those are "dead rows".
                       ##    Delete them, so optimize disk storage and thus speed (garbage-collection)
                       ##  - update visibility map: 1 and 0 map for rows that are not committed yet.
                       ##    Used to optimize index-only scans queries.
                       ##  - protect losing very old data due to xid (xmin or xmax) wraparound:
                       ##     - rows that will be wraparound but are still not committed can be "frozen" (frozenxid at the
                       ##       TABLE-level), so that their xid is skipped in the wraparound
                       ##        - vacuum freeze rows that are older than ENVVAR vacuum_freeze_max_age transactions or
                       ##          2e9 - ENVVAR vacuum_freeze_min_age (def: 5e7) transactions
                       ##            - if problems of wraparound, increase vacuum_freeze_max_age or decrease
                       ##              vacuum_freeze_min_age
                       ##        - force vacuuming freeze rows if "freeze" is used (can violate MVCC)
                       ##        - TABLE with no "dead rows" will not be checked for frozenxid removal unless it is older
                       ##          than ENVVAR vacuum_freeze_table_age (def: 1.5e8)
                       ##     - 2e9 xid, so need to vacuum once every 1e9 transactions
                       ##     - if there are wraparound risk, the server will not accept any new command
                       ##Def. TABLE is all TABLE that current ROLE has permissions. Def. COL_VAR is all COL_VAR.
                       ##Cannot be done inside a transaction.
                       ##Requires share update exclusive lock, and makes database slower during vacuuming.

AUTOVACUUM ==>         ##Daemon automatically doing vacuum [analyze] on individual TABLE... according to how many rows are
                       ##changed. Better than doing it manually.
                       ##Enabled by ENVVAR autovacuum (def: 'on')
                       ##  - do vacuum every time:
                       ##     - a vacuum freeze is needed
                       ##       (can set its own ENVVAR autovacuum_freeze_* to override vacuum_freeze_*)
                       ##     - or ENVVAR autovacuum_vacuum_threshold (def: 50) +
                       ##       (number of inserted|deleted|updated rows) * ENVVAR autovacuum_vacuum_scale_factor (def:0.2)
                       ##  - do vacuum analyze similary according to ENVVAR autovacuum_analyze_threshold (def: 50)
                       ##    and autovacuum_analyze_scale_factor (def: 0.1)
                       ##  - ENVVAR:
                       ##     - autovacuum_naptime INT: min. time (def: '1min') * number of databases before two launches
                       ##     - autovacuum_max_workers: numbers of DATABASE that can be vacuumed at same time (def: 3)
                       ##  - doesn't vacuum TEMP
                       ##Can set autovacuum_* at the TABLE-level with create table ... with ( autovacuum_* = VAL )

analyze [verbose]      ##Fill in pg_statistic.
[TABLE[( COL_VAR... )]]##Parameters are like for vacuum.
                       ##Requires only an access share lock.
                       ##FOREIGNTABLE are analyzed only when explicitely specified, and not always supported.

pg_statistic           ##Stats used by the planner to optimize queries (only indexed COL).
                       ##Not exact stats, because only a random sample of the rows is chosen for efficiency purpose.
                       ##Example of statistics: number of entries, distinct entries, histograms, size (number of disk blocks)
                       ##ENVVAR default_statistics_target (def: 100, from 0 to 10000) decides the sample size.
                       ##Can be set column-wise with alter table TABLE alter COL_VAR set statistics INT (-1 means default)
                       ##(same for FTABLE and materialized views).
                       ##  - starelid OID: of the TABLE. refers to pg_class.oid
                       ##  - staatnum UINT: numero de la COL. Refers to pg_attribute.attnum.
                       ##  - stainherit BOOL: all COL have false + (if inherited COL) a row with true, with inherited
                       ##    version of the TABLE
                       ##  - stanullfrac FLOAT: percentage of null values
                       ##  - stawidth INT: average size of null values
                       ##  - stadistinct FLOAT: number of repetitions: -NUM if repetitions (UNIQUE/TOTAL, closer to 0 if
                       ##    many repetitions), +NUM means no repetitions (UNIQUE), 0 means unknown
                       ##  - for NUM statistics:
                       ##     - stakindNUM INT: subtype of the statistic (code number)
                       ##     - staopNUM OID: FUNC used. Refers to pg_operator.oid
                       ##     - stanumbersNUM FLOAT_ARR: stats as FLOAT, or null if COL is not numerical
                       ##     - stavaluesNUM ARR: stats as the same type as the COL
pg_stats               ##User-friendly version og pg_statistic


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:      RECOVERY & BACKUPS       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DURABILITY ==>         ##PostgreSQL has durability: write operations will succeed even after a crash.
                       ##A crash will lead to:
                       ##  1) an inconsistent state if transaction was half written because of:
                       ##     - cache, which should be disabled:
                       ##        - ENVVAR fsync (def: 'on'): don't use cache, i.e. flush WAL records as they are written.
                       ##          Can be disabled at server startup with postgres -F
                       ##        - ENVVAR wal_sync_method tells which OS command to use to flush cache (when using fsync):
                       ##          open_datasync, fdatasync, fsync, fsync_writethrough or open_sync (def: fdatasync). Best one
                       ##          can be determined with:
                       ##             pg_test_fsync -f FILE (FILE must be on same filesystem than DATADIR)
                       ##        - HDD cache on Linux:
                       ##           - queried with: hdparm -I /dev/FILE | grep "Write cache" (* at beginning if cache enabled)
                       ##           - disabled with: hdparm -W 0 /dev/FILE
                       ##        - Filesystem caching through journaling: disable it with mount options (e.g. data=writeback
                       ##          on ext3).
                       ##     - partial writes, controlled by ENVVAR full_page_writes (def: 'on').
                       ##       Putting to 'off' can improve performance.
                       ##  2) lost transactions (but no inconsistency) because of:
                       ##     - no synchronous commit: doesn't wait for the WAL to be written to report success of operation.
                       ##       ENVVAR synchronous_commit can be activated (local|remote_write|on (def)) or not ('off')
                       ##     - WAL is written after ENVVAR commit_delay (def: 0ms) in hope several operations will happen in
                       ##       the delay, which will then use a single flush. Best value is half the time of a single
                       ##       8kB write, as reported by last line of pg_test_fsync
                       ##       Only happens when min. ENVVAR commit_siblings (def: 5) transactions are currently opened.
                       ##     - ENVVAR wal_writer_delay (INT in ms, def: 200) too high: delay between each WAL writes
                       ##1) is dangerous: database could not be restared without a restore.
                       ##2) gives similar performance gain without that problem.
                       ##Putting DATADIR in a RAM disk is hardcore non-durable, and limits space to RAM space, but highly
                       ##efficient.

BACKUP VS HIGH         ##Backup strategy (saving data) is different from, but should be combined with high availability
AVAILABILITY ==>       ##strategy (quick restore of system if a node falls down)

BACKUPS ==>            ##Either:
                       ##  - pg_dump: more stable from one PostgreSQL version to another, or from one architecture to another.
                       ##  - WAL archiving: provides continuous archiving and PITR
                       ##Best: do both too (do a pg_dumpall after each base backup).
                       ##Good idea to compress backups
                       ##Backups methods are all hot backups.

pg_dump [DATABASE]     ##Def DATABASE: PGDATABASE or OS_USER
                       ##Must be connect as superuser ROLE (for both backups and restores).
                       ##Unless "all", doesn't backup cluster-specific information. Remember then to restore them before
                       ##psql <FILE.
                       ##template1-specific information are backuped too: remember to restore from template0.
                       ##INTVAR ON_ERROR_STOP should be set.
                       ##Recommendations for restore:
                       ##  - psql --single can be used to make everything rollback if error
                       ##  - Make sure tablespace DIR are good
                       ##  - should analyze restored databases.
-F p|c|d|t             ##Format of the output:
                       ##  - p (def): sql command in text format
                       ##  - c: custom compressed format (must be restored with pg_restore)
                       ##  - d -f DIR: put in a directory DIR (must be restored with pg_restore) with one file by TABLE, and a
                       ##    table of content file
                       ##  - t -f DIR: same but use tar (not compressed). Limit of GB per table.
-a                     ##Don't save SCHEMAs (save only data)
-s                     ##Save only SCHEMAs (not data)
-n SREGEXP             ##Only VAR... in SCHEMA matching SREGEXP (see psql). Can be specified several times.
                       ##Caution: doesn't dump VAR of other SCHEMA2... that SCHEMA might depend on.
-N                     ##Inverse: SCHEMA not matching SREGEXP
-t SREGEXP             ##Same for TABLE matching SREGEXP. Incompatible with -n or -N
-T SREGEXP             ##Inverse: TABLE not matching SREGEXP.
--exclude-table-data=
SREGEXP                ##Same but only exclude TABLE data, not definition
-b                     ##Include large objects, which is the default unless -n, -s or -t is used
-o                     ##Includes OID
-O                     ##Don't save ROLE ownership (with -F p). Will be able to restore backups without being superuser, but
                       ##restorer will get ownership of all objects.
-x                     ##Don't save privileges (grant/revoke)
--no-tablespaces       ##Save everything in same, default tablespace.
--no-unlogged-table-data#Don't save content of unlogged TABLEs
--no-security-labels   ##Don't save seLINUX labels (when using it)
-c                     ##Put cleaning commands first (drop VAR before trying to create it)
-C                     ##Create the database in the beginning (otherwise need to create it).
--[column-]inserts     ##Use insert instead of copy (slower). With column, put COL_VAR names instead of using positions. Is
                       ##much slower.
--serializable-        ##Execute command in a serializable deferrable transaction (useful only when dumping to clone to
deferrable             ##another machine)
--disable-dollar-quoting#Use standard SQL quoting ' ' instead of $$ $$
--disable-triggers     ##Create commands (with -F p) which disable triggers on tables before dump is restored.
-E STR                 ##Encoding (def: PGCLIENTENCODING)
-j NUM                 ##Use several threads in same time (faster but uses more resources). Make sure ENVVAR max_connections
                       ##is high enough. Doesn't work if any exclusive lock is being requested meanwhile.
--lock-wait-timeout=NUM##Wait for NUM seconds when asking for locks (def: unlim)
-d -h -p -U -w|W       ##Connection options (see psql)
--role=ROLE            ##ROLE when getting the dump data

pg_dumpall             ##Same as pg_dump, but for the whole cluster (except template0)
                       ##Use same options as pg_dump, except ones that are irrelevant, and selection options (like -T).
                       ##All databases must already exist.
-g                     ##Only saves cluster-wide objects
-t                     ##Only saves tablespaces
-r                     ##Only saves ROLE

pg_restore [FILE]      ##Restore a backup produced by pg_dump -F (hors normal format, which should be restored with psql).
                       ##Def FILE is stdin. If no -d DATABASE is specified, print a text version of the restoration instead.
-a
-c
-C
-F c|d|t
-j NUM
-n SCHEMA
-O
-s
-S ROLE
-t SREGEXP
-x
--disable-triggers
--no-tablespaces
--no-security-labels
-d -h -p -U -w|W       ##Like pg_dump
-e                     ##Sets ENVVAR exit_on_error
-1                     ##Put in only one transaction
-L FILE                ##Restore only objects present in FILE (can be produced with pg_restore -l, then manipulated)

WAL ==>                ##Write-ahead logs. Logs that store every operation on the cluster before they are performed.
                       ##Goal:
                       ##  - when starting the server, if the last operations of the WAL have not been applied to the data
                       ##    (i.e. if the DATADIR data don't match the WAL), last operations are performed.
                       ##    Goal is to recover from crash.
                       ##  - can also be used for backups (see below)
                       ##Only WAL log are garanteed to be flushed (faster), not real operations, to ensure durability.
                       ##Structure:
                       ##  - Use 16MB segments. A log "line" is a "record".
                       ##    Every write on the cluster add a new record on the last segment.
                       ##    New segments are automatically added and rotated.
                       ##    Are in DATADIR/pg_xlog/ but could be moved to a faster storage using symlinks.
                       ##Checkpoints are when operations recorded by WAL are flushed to the disk (as opposed to flushing
                       ##the WAL itself, which is controlled by fsync, etc.):
                       ##  - last one is where to restart in crash recovery
                       ##  - are performed at min. time between ENVVAR checkpoint_segments (number of segments, def 3) and
                       ##    checkpoint_timeout (time between checkpoints, def '5min').
                       ##    Increasing it will improve performance but increase crash recovery time (values between 32 to 256
                       ##    are often used for checkpoint_segments, and checkpoint_timeout can be one day)
                       ##    If ENVVAR checkpoint_warning (def: '30s') is less than the time between checkpoints, but more
                       ##    than checkpoint_timeout, a warning will be issued to the server log.
                       ##  - can also issue SQL command checkpoint to do it
                       ##  - flushes performed by a checkpoints are spread to the next checkpoint. The spread is
                       ##    ENVVAR checkpoint_completion_target, i.e. percentage of size spread for the free time allowed
                       ##    between checkpoints (def: 0.5, best is 0.9). Can go up to 0.9 will improve performance, but
                       ##    increase recovery time. Can only be set at server start.
pg_xlogdump [FILE]     ##Show a WAL file in human readable format
                       ##When in DATADIR, can also use FILE FILE2 to go from FILE to FILE2
pg_resetxlog           ##To use when WAL is corrupted. Look at online doc
pg_controldata DATADIR ##Show debug info for WAL

WAL ARCHIVING /        ##  - goals:
ONLINE BACKUP ==>      ##     - "continuous archiving". Just need to archive new WAL segments.
                       ##     - point in time recovery (PITR): instead of single snapshots, can recover to specific time in
                       ##       past
                       ##  - enabled by ENVVAR wal_level to 'archive|hot_standby' (def: 'minimal') and archive_mode to 'on':
                       ##  - backing up WAL segments continuously, and DATADIR at regular times:
                       ##     - backup in different folders, let's call them DIR1 and DIR2
                       ##     - events since the last DATADIR since the crash are then restored thanks to the archived WAL
                       ##       segments
                       ##  - backup of WAL segments:
                       ##     - each time a new WAL segment is about to be erased (because of rotation), ENVVAR
                       ##       archive_command STR is fired to back it up:
                       ##        - can include %p for its path and %f for its filename, e.g.:
                       ##            '[ ! -f "DIR1/%f" ] && cp -a "%p" "DIR1/%f"'
                       ##        - should give exit code != 0 if error, so that it retries it
                       ##        - should not allow overwritting files
                       ##        - on Linux, use sh, not Bash
                       ##        - should be faster than the speed at which WAL segments appear
                       ##        - check permissions of server daemon to execute command
                       ##     - new segments are automatically made. But can be created manually by:
                       ##        - running pg_switch_xlog()
                       ##        - can be made every max. every ENVVAR archive_timeout (def: 0, in seconds). Should not be
                       ##          under 60s.
                       ##          Goal is for databases with low traffic: new segments are rarely created but still want to
                       ##          archive the little traffic.
                       ##     - archived WAL segments before the last "base backup" can be erased to save space, up until when
                       ##       we want to do a PITR
                       ##     - Can also use command pg_receivexlog -D DIR, which archive WAL segments to DIR, according to
                       ##       connection options (see psql) -d -h -p -U -w|W
                       ##  - backup of DATADIR ("base backups"):
                       ##     - manually, steps are:
                       ##        - connect to any DATABASE of the cluster and fire pg_start_backup(STR) as superuser.
                       ##          STR should be the number of this unique backup
                       ##           - creates a text file DATADIR/pg_xlog/FILE.*.backup, where FILE is the last WAL segment
                       ##             archived, with information used by the recovery process (e.g. last WAL segment of
                       ##             current DATADIR)
                       ##           - creates DATADIR/backup_label, which is a very similar file
                       ##        - backup DATADIR with any command (such as cp -a) to DIR2:
                       ##           - don't include postmaster.* nor pg_xlog/*
                       ##           - don't forget directories that might be elsewhere, e.g. tablespaces or directories using
                       ##             symlinks postgresql.conf, pg_hba.conf, pg_ident.conf could also be put somewhere else
                       ##             with ENVVAR config|hba|ident_file
                       ##           - copy might issue warnings because DATADIR files change on the fly (since cluster is
                       ##             running): it's fine
                       ##        - fire pg_stop_backup() as superuser.
                       ##           - removes backup_label file
                       ##        - utilities (not necessarily needed):
                       ##           - pg_is_in_backup(), pg_backup_start_time()
                       ##           - pg_start|stop_backup() returns the WAL segment as STR: to translate into filenames:
                       ##              - pg_xlogfile_name[_offset](STR)
                       ##              - pg_xlog_location_diff(STR, STR2)
                       ##     - pg_basebackup:
                       ##        - automate all this. Options are:
                       ##            -h -p -U -w      Connection options
                       ##            -D DIR           DIR to copy to. Can be - (stdout) for tar mode
                       ##            -F p|t           If p, do a simple copy. Files pointed by symlinks (such as tablespaces),
                       ##                             will be copied to the destination using the same absolute path
                       ##                             If t, will tar it under the filename base.tar (symlinks files are tar'd
                       ##                             too, under their abs. path)
                       ##                             Can also use -z to gzip it and -Z 1-9 for the compression level (def: 6)
                       ##            -R               Put a recovery.conf sample if the backup
                       ##            -X s             Includes first WAL segment in the backup (def: doesn't include any WAL
                       ##                             segment).
                       ##                             Will use two clients in max_wal_senders
                       ##            -l STR           Label used in backup_label (def: 'pg_basebackup base backup')
                       ##            -c fast|spread   Change the checkpoint_completion_target (def: spread)
                       ##            -P               Progress bar
                       ##        - use same privileges as streaming replication (max_wal_senders, replication privilege, etc.)
                       ##     - in all cases, need to be done regularly, e.g. with a cron script
                       ##        - more regular base backups require more storage, but make faster recoveries
                       ##  - recovery:
                       ##     - steps:
                       ##        - stop server
                       ##        - replace DATADIR by DIR2, but keeping the WAL segments:
                       ##           - move DATADIR/* to temporary DIR3 (including tablespaces, etc., see above)
                       ##           - copy DIR2/* to DATADIR (including tablespaces, etc., see above), with right ownership
                       ##             and permissions
                       ##           - replace DATADIR/pg_xlog/* by DIR3/pg_xlog/*, with right ownership|permissions
                       ##             (in case some WAL segments were not archived but still present in DATADIR)
                       ##        - copy archived WAL segments from DIR1 to DATADIR:
                       ##           - create DATADIR/recovery.conf (its presence instructs server start to be in recovery mode)
                       ##              - can copy template SHAREDIR/recovery.conf.sample
                       ##              - must set variables:
                       ##                 - restore_command STR: just like archive_command, but to copy the WAL segments from
                       ##                   DIR1 to DATADIR/pg_xlog/
                       ##                   Should overwrite existing ones.
                       ##                   Will emit warnings because try to copy files that might not exist.
                       ##                   Ex: 'cp -a "DIR1/%f" "%p"'
                       ##              - can recover to a specific time (PITR):
                       ##                 - by setting (in recovery.conf) any of:
                       ##                    - recovery_target_time TIMESTAMP
                       ##                    - recovery_target_xid STR: the transaction ID
                       ##                    - recovery_target_name STR: STR is a restore point, which must have been
                       ##                      previously created by pg_create_restore_point(STR)
                       ##                 - time must be after the creation time of DIR2/*
                       ##                 - recover just before|after according to variable (in recovery.conf)
                       ##                   recovery_target_inclusive (def: true, i.e. after)
                       ##                 - will stop (unless variable pause_at_recovery_target is set to false or if
                       ##                   hot_standby mode), so we can check if the state is fine. Can resume by firing
                       ##                   pg_xlog_replay_resume()
                       ##                 - must remove WAL segments that have been archived after that time, to restart
                       ##                   archiving them normally
                       ##           - start the server in single user mode
                       ##           - recovery will happen: when done, recovery.conf will be recovery.done
                       ##        - make sure everything is ok, then restart the server normally
                       ##     - timelines:
                       ##        - each time a recovery suceeds, it increments the first number of the WAL segment files, e.g
                       ##          00...00100..0034 to 00...00200..0034
                       ##        - the first number is the timeline ID. Goal it that following WAL archives doesn't overwrite
                       ##          previous WAL archives created between the recovery and the crash, in case we want to come
                       ##          back to that point.
                       ##        - by default, recover to the timeline that was used during the base backup, but can specify
                       ##          recovery_target_timeline STR with "latest" in recovery.conf, or with the specified
                       ##          timeline ID

HIGH AVAILABILITY ==>  ##Can use:
                       ##  - log shipping: master ships WAL to a DIR, then standby gets it from DIR
                       ##  - streaming replication: ships directly WAL from master to slave. Probably better.
                       ##     - async. (better performance) or sync. (better availability)
                       ##Any standby can also be a hot standby (makes more sense for a streaming replication one) to improve
                       ##load balancing (watch out precautions)

LOG SHIPPING /         ##  - Goal: not backup (but can be combined with backup) but to maintain a copy of the master server, so
WARM STANDBY ==>       ##    a switchover to the standby can happen quickly if there is a problem with the master
                       ##  - Idea: the standby machine keeps on reading the WAL archive (master must do WAL archiving) and
                       ##    applies them right away.
                       ##  - How:
                       ##     - start a cluster with a base backup, with a recovery.conf file in it.
                       ##       recovery.conf variable standby_mode should be on.
                       ##     - will continuously call recovery.conf variable restore_command (same format as archive_command)
                       ##       to copy WAL archive DIR1 to its own pg_xlog/, e.g. 'cp -a "DIR1/%f" "%p"'
                       ##        - will show error messages for next WAL segment, and .history file -> it's normal
                       ##     - Put recovery_target_timeline to "latest" (to stay sync. with the timeline chosen by the master)
                       ##     - If don't want to use DIR1 for backup purpose, clean every WAL archive that has been copied by
                       ##       setting variable archive_cleanup_command:
                       ##        - %r is the filename (not path) of the first WAL file to keep
                       ##        - pg_archivecleanup is a command line often used:
                       ##            - pg_archivecleanup "DIR" "%r"
                       ##            - flags are -d (verbose), -x STR (use it if WAL segments have this extension,
                       ##              e.g. -x .gz) and -n (dry-run)
                       ##     - Can stop standby mode and become a master:
                       ##        - by creating file specified by recovery.conf variable trigger_file, or firing
                       ##          pg_ctl promote.
                       ##           - change recovery.conf to recovery.done
                       ##        - never two masters at same time:
                       ##           - should turn off former master shortly before
                       ##           - before restarting, former master should become the new slave
                       ##        - good idea to prepare already the slave to become a master by setting up WAL archiving, etc.
                       ##        - recovery_end_command STR will be fired (%r is the same as archive_cleanup_command)
                       ##        - automatic failover is only possible using external packages.
                       ##  - Precautions:
                       ##     - DIR1 should not be on the master machine.
                       ##     - WAL segments are sent async (don't wait for shipping to execute), so there's a window for data
                       ##       loss, that can be reduce by lowering archive_timeout
                       ##     - standby and master should have similar config:
                       ##        - logically, e.g. symlinks (including table spaces)
                       ##        - software-wise
                       ##        - hardware wise. CPU architecture must be same.
                       ##     - switchover is manual: should have own mechanism to notify when the primary server is down, and
                       ##       to automatically failover

ASYNC. STREAMING       ##  - Goal: like log shipping, but smaller delay between master and slave state (still small one)
REPLICATION ==>        ##  - Idea: like log shipping, but doesn't use WAL archive DIR1 (nor restore_command,
                       ##    recovery_target_timeline, archive_cleanup_command), but directly get WAL from the server (over
                       ##    TCP connection).
                       ##  - How:
                       ##     - Set recovery.conf variable primary_conninfo (as "VAR=VAL ...", using libpq variables) for how
                       ##       to connect to the master.
                       ##     - Same as above for recovery_target_timeline
                       ##     - Must have privileges:
                       ##        - to connect to "replication" virtual DATABASE (in pg_hba.conf)
                       ##        - replication and login privileges (better to create a ROLE than to set up as superuser).
                       ##        - max number of connections is ENVVAR max_wal_senders (def: 0).
                       ##     - slave must keep up with the pace:
                       ##        - can increase ENVVAR wal_keep_segments on the master (number of segments that should be
                       ##          recycled but are kept, def: 0)
                       ##        - can use log shipping in parallel.
                       ##        - If fall behind, can redo a base backup.
                       ##        - Can tell by:
                       ##           - comparing pg_current_xlog_[insert_]location() on the master (current WAL),
                       ##             pg_last_xlog_receive|replay_location|timestamp() on the slave
                       ##           - use pg_stat_replication system view
                       ##        - Connection waits only for ENVVAR wal_receiver_timeout (def:60s) from slave to master, and
                       ##          wal_sender_timeout (def: 0, turned off) from master to slave.
                       ##     - Cascading replication:
                       ##        - Just use replication from downstream to upstream servers.
                       ##        - Goal: to reduce cost for master, but introduces more delay for other standbies.
                       ##        - sync. replication doesn't work for downstream servers.

SYNC. STREAMING        ##  - Goal: like async. streaming replication, but reduces data loss window to nothing (at expense of
REPLICATION ==>        ##    performance): every write transaction returns only after WAL is sent to standby ("2-safe
                       ##    replication").
                       ##  - How:
                       ##     - Master must set ENVVAR synchronous_standby_names with standbies:
                       ##        - comma-separated-list, only picks the first connected in the list
                       ##        - names must match application_name in primary_conninfo
                       ##           - can be * for any application_name
                       ##        - def is walreceiver
                       ##     - Actually waits according to ENVVAR synchronous_commit:
                       ##        - on (received and flushed to disk on slave), remote_write (only received) or local|off
                       ##          (nothing).
                       ##        - Makes it possible to set synchronous_commit specific values for databases, users or
                       ##          transactions, for different durability/performance tradeoff.
                       ##  - Precautions:
                       ##     - If last standby loses connection, will wait forever
                       ##        - if last standby needs to be down, must first put synchronous_commit to off in a
                       ##          pg_start|stop_backup() block

HOT STANDBY ==>        ##  - Goal: use a standby server (streaming replication or log shipping) for readonly queries (load
                       ##    balancing).
                       ##  - How:
                       ##     - Must set ENVVAR hot_standby to on on standby and ENVVAR wal_level to hot_standby for master
                       ##     - Must start with a new base backup (if switching from non hot standby to hot standby)
                       ##  - Precautions:
                       ##     - Watch out for the delay between master write and ability to read it in standby. If an arriving
                       ##       WAL archive is conflicting with a current query (e.g. if master dropped a table while standby
                       ##       is querying it), it will be wait ENVVAR max_standby_archive|streaming_delay (for streaming
                       ##       replication mode or not) (def: 30000 ms, -1 for unlim) then cancel
                       ##        - low value provokes more cancels, but standby and master are more in sync : good if goal is
                       ##          more High availability, bad if goal is more load balancing
                       ##        - could be set at approx max time of queries.
                       ##        - Can also increase ENVVAR vacuum_defer_cleanup_age if lot of vacuum-related conflicts.
                       ##          Cancels can be seen on system view pg_stat_database_conflicts.
                       ##     - Hot standby stops at startup when standby tries to catch up servers WAL segments. During that
                       ##       period, there might be seemingly weird behavior.
                       ##       pg_is_in_recovery() will return true.
                       ##     - those ENVVAR must be superior or equal on the standby than the master: max_connections,
                       ##       max_prepared_transactions, max_locks_per_transaction
                       ##     - advisory locks can't be shared between master and slave
                       ##     - isolation level serializable not available


repmgr                 ##Must have rsync, pg_ctl and pg_config in $PATH. Must be installed from source (see online doc).
                       ##Actions can be:
                       ##  - standby clone NODE_VAR: make it possible to put as standby (do a base backup).
                       ##  - master|standby register: put as master|standby (master should be done first)
                       ##  - standby promote|follow: in case of a failover, automatical new master to promoted, and followers
                       ##    will replicate from it. Automatical or manual???
-d -h -p -U            ##Connection options
-D DATADIR             ##Cluster to target
-f DIR                 ##repmgr.conf DIR (def: same as executable).
                       ##repmgr has three lines: cluster STR, node number INT, libpq_conninfo STR
--force                ##Do with standby clone when a master is up again after having being down, to get back the changes
                       ##since then from the new master.

repmgrd                ##Daemon doing automatic failover.
                       ##Needs to do all the standby register first.
-f DIR                 ##


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     CLUSTERING AND POOLING    :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


POSTGRES-XC ==>        ##Multi-master replication to provide write scalability:
                       ##  - 0.6*number of machines, so not good if 1 or 2 machines.
                       ##  - Doesn’t provide high availability, but can use standard Postgres features on each node (backups,
                       ##    streaming replication, hot standby).
                       ##Look online doc: http://static4.wikia.nocookie.net/__cb20130121060137/postgresxc/images/d/d6/
                       ##20120515_PGXC_Tutorial_global_1.pdf and official doc (similar to original Postgres doc).

pgbouncer PGBFILE      ##Does connection pooling: maintains a single connection to be reused for each DATABASE+USER pair.
                       ##Goal is to lower connection time.
                       ##Can be much faster when connection time is important (small sessions).
                       ##Disadvantages:
                       ##  - requires more fds, so might needs lower max_client_conn than PostgreSQL's server
                       ##  - hides original host+port information in logs (all traffic goes though pgbouncer)
                       ##  - cannot implement all authentication method
                       ##Act as layer of abstraction: connection to pgbouncer DATABASE can be redirected to DATABASE of
                       ##other names, or of different clusters/machines.
                       ##Should be run as same OS_USER as the server.
                       ##Should be installed:
                       ##  - on database server, if lot of web servers connect to it (because it is the center of all
                       ##    connections that should be pooled)
                       ##  - on web server, if connects to lot of database servers
                       ##Each pool (DATABASE+USER) has:
                       ##  - clients (to pgBouncer) and servers (connection of pgBouncer to PostgreSQL).
                       ##    1 client = 0|1 server: 0 server when client just connected (session pool_mode) or is not issuing
                       ##    a request (transaction pool_mode)
                       ##  - cl_active (from show pool, see below) is number of clients on a pool.
                       ##    If more than pool size, clients will be cl_waiting instead until cl_active is lower.
                       ##    Pool size is determined by:
                       ##      - [default_]pool_size: cluster-wide and database-specific pool size.
                       ##      - min_pool_size: at first client, opens at least NUM idle servers, to make it more responsive
                       ##        in the first requests.
                       ##      - reserver_pool_size: extra pool size used for clients waiting (cl_waiting) for more than
                       ##        reserve_pool_timeout
                       ##  - max_client_conn is max number of cl_active for all pools together.
                       ##    When reached, clients don't wait, they crash.
                       ##  - Optimize limits:
                       ##     - number of file descriptors used = 2 + 1 per client (max_client_conn) + 1 per server
                       ##       (pool_size * number of users * number of databases)
                       ##         - pool_size should be at max without creating more servers than PostgreSQL's max_connections
                       ##         - max_client_conn should be max number of servers + expected number of idle clients
                       ##         - total should not exceed max number of file descriptors
                       ##Pool mode:
                       ##  - when server is not used anymore, returns back to pool (sv_active -> sv_idle|used)
                       ##  - it is done according to pool_mode, either after each session (def), transaction or query (avoid).
                       ##    transaction doesn't support session states, i.e. [re]set ENVVAR, listen|notify, with hold CURSOR,
                       ##    PREP, load, user-defined volatile FUNC
                       ##    Use transaction if lot of idle times in sessions, or if long queries.
                       ##Look at check_postgres for monitoring.
-d                     ##Run in background.
                       ##Needs to give pidfile = FILE in [pgbouncer] in PGBFILE (FILE is created with the PID)
-R                     ##Online restart: closes current running pgbouncer and inherits its connections without interrupting
                       ##anything (current running pgbouncer will be closed). Useful to upgrade without interrupting anything.
-q                     ##Quiet mode
-v                     ##Verbosity (can do several times)

PGBFILE ==>            ##Usually called pgbouncer.ini
                       ##Has two parts, each started with [databases], then [pgbouncer] on a single line, and separated by
                       ##blank line.
                       ##Each part has VAR = VAL ... (STR don't have any quoting)

                       ##[databases]:
DATABASE               ##STR, libpq string, but with only:
                       ##  - dbname: def. is same as DATABASE
                       ##  - host: def. is using Unix socket.
                       ##  - port: def. 5432
                       ##  - user: def. is same user
                       ##  - password
                       ##When client asks PgBouncer to connect on DATABASE, will use STR to connect to server.
                       ##Can also specify:
                       ##  - pool_size: Per-database pool size
                       ##  - connect_query: query done at connection start.
                       ##    For connection end (not DATABASE-specific), use server_reset_query (def: "discard all")
                       ##Can use * DATABASE to mean "any other database"

                       ##[pgbouncer]:
listen_port            ##Proxy port (which client should connect to in order to reach server)
listen_addr            ##Same for proxy address. Can be *
unix_socket_dir|mode|  ##Like PostgreSQL ENVVAR unix_socket_directories|permissions|group
group                  ##Def. are /tmp, 0777 and ""
auth_type              ##Similar as in pg_hba.conf. Can be md5 (def), plain (like password in pg_hba.conf), trust or any.
                       ##any is like trust, except that users are not even remembered which means:
                       ##  - all DATABASE must specify user=VAL in their libpq STR
                       ##  - control with admin_users is not effective
auth_file              ##"USER" "PASSWORD" ...
                       ##Necessary (only USER with a line in it will be able to connect)
admin_users            ##USER... (pgBouncer USER) allowed to connect to pgBouncer and issue statements on it.
stats_users            ##Same but can only use show ENVVAR (except show fds)

logfile                ##Redirect stderr to FILE, without stopping stderr
log_[dis]connections   ##Logs them (def: 1)
log_pooler_errors      ##Logs errors sent to client (def: 1)
stats_period           ##Logs stats every NUM seconds (def: 60)
syslog[_ident|facility]##

pool_mode              ##See above (def: session)
                       ##If transaction, server_reset_query should be ""
max_client_conn        ##(def: 100)
default_pool_size      ##(def: 20)
min_pool_size          ##(def: 0)
reserve_pool_size|
timeout                ##(def: 0 and 5 seconds)

server_check_delay     ##After NUM seconds (def: 30), goes from sv_idle to sv_used, i.e. run sanity check query on server
                       ##connections when going from idle to active.
server_lifetime        ##Closes server connections opened for more than NUM seconds (def: 3600)
server_idle_timeout    ##Same but for idle server connections (def: 600)
server_connect_timeout ##Same but for connecting time (def: 15)
client_login_timeout   ##If client connects but does not login before NUM seconds (def: 60), drops it.
autodb_idle_timeout    ##Closes pools (using * in [databases]) that have been unused for more than NUM seconds (def: 3600)

server_login_retry     ##Waits NUM seconds (def: 15) after each failed authentification.
dns_max_ttl            ##DNS (host resolution) cache time in seconds (def: 15)
max_packet_size        ##Max packet size between PostgreSQL and pgBouncer, in bytes (def: 2GB)

server_round_robin     ##If 0 (def), reuse connections in LIFO manner. If 1, in a random manner (better if TCP round-robin
                       ##distributing load between servers)

pgbouncer DATABASE ==> ##Virtual DATABASE, where only show ENVVAR is allowed, with some commands:
reload                 ##Reload PGBFILE (can also use SIGHUP)
pause [DATABASE]       ##Safest way to stop pgBouncer: wait for clients to complete. Can also use SIGINT (CTRL-C)
shutdown               ##Like pause, but exit pgBouncer completely. Can also use SIGTERM
suspend                ##Drop clients, but flush buffers
kill DATABASE          ##Least saft way to stop DATABASE
resume [DATABASE]      ##Resume from pause|resume

                       ##Can also show the following ENVVAR:
lists                  ##Snapshot of all other info
databases              ##DATABASE: connection+pool_size
stats                  ##DATABASE:
servers                ##
clients                ##
pools                  ##
fds                    ##File descriptors
users                  ##All users in auth_file
config                 ##PGBFILE info
dns_hosts
dns_zones              ##Host resolution


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            TESTING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TSUNG ==>              ##See tsung in my doc (test load on server)

PGTAP UNITS ==>  pgtap###Needs to be installed.
                      ###Tested database must enable PL/PGSQL.
                      ###Idea is to create assertions (see below for list) in a separate test unit SQL file. Usually put in a
                      ###tests/ folder
                      ###Usually do test manipulation on the database between assertions, so in the end should rollback:
                      ###  - put in a transaction block that rollbacks.
                      ###  - put INTVAR ON_ERROR_ROLLBACK and ON_ERROR_STOP
                      ###For best formatting:
                      ###  - put INTVAR ECHO to nothing, QUIET to 1
                      ###  - \pset format unaligned, pager on, t true
                      ###  - if using psql, use -X to bypass init files
                      ###Each assertion produce a TAP format line (common text format for test results, that is runned and
                      ###parsed by "test harnesses")
select plan(NUM)      ###Starts a unit with exactly NUM assertions. If not sure use select * from no_plan() instead (avoid).
select * from finish()###Completes a test unit

PGTAP XUNIT TESTS ==> ###Same as normal pgTap units (same output and assertions) but:
                      ###  - use functions, not files. Functions just execute the assertions functions, and returns them as a
                      ###    STR_ARR:
                      ###      create or replace function SCHEMA.FUNC()
                      ###      returns setof text as
                      ###      $$begin
                      ###          return next ASSERTION_FUNC(ARGS)...
                      ###        end$$
                      ###      language pgsql
                      ###  - don't need anything to start|finish the unit but needs to run the unit with:
                      ###      select * from runtests( [STR][, STR2] )
                      ###    where STR is SCHEMA, and STR2 is a regular expression to match the FUNC to choose.
                      ###    Since STR and STR2 are both optional (def: 'public' and '^test') cast to name if using only STR
                      ###     - Use transaction on all tests as a whole, then on each individual test (including fixture
                      ###       functions)
                      ###  - can use fixture functions (same definition as above), special FUNC used at specific moments.
                      ###    Each set run in alphabetical order. FUNC name must start with the name of the phase:
                      ###     - startup, once before all tests
                      ###     - setup, once before each test
                      ###     - teardown|shutdown: same as setup|startup, but after tests
                      ###    Watch out to exclude the fixture functions in runtests(). Good practice is to use a SCHEMA, and
                      ###    '^test' for tests.

pg_prove [DIR|FILE...]###Test harness for pgTap (executes tests).
                      ###Like doing psql, but better output, and don't need to set \pset and INTVAR
                      ###(but should still put in a transaction block)
                      ###For DIR, recognize test files according to extension ".pg"
                      ###Same for xUnit-style tests, but:
                      ###  - don't use FILE... but -R, which fires runtests()
                      ###  - use -s SCHEMA and -x REGEXP to specify arguments to runtests(STR, STR2)
-d -U -h -p           ###Connection options
-P|S VAR=VAL          ###Does \[p]set VAR VAL
--ext STR             ###With DIR, use extension STR, not ".pg"
-r                    ###With DIR, recursive

--shuffle|reverse     ###Modify test execution order
--state STR,...       ###Which tests to run:
                      ###  - last: same as last time
                      ###  - all: in normal order
                      ###  - failed|passed: only ones that failed|passed
                      ###  - hot: most recent failure first
                      ###  - todo: only test with todos
                      ###  - slow|fast: in speed order
                      ###  - new|old: in mtime order
                      ###  - fresh: only the ones that have been modified
                      ###  - save: save state in a file ./.pg_prove (must be done first to be able to do --state next time)

-f                    ###Print failed tests
-q|Q                  ###Quiet, or even quieter
--verbose             ###Outputs full TAP format
--no-comments         ###Don't show diag() messages
--directives          ###Only show skip() messages and todo() tests
-D                    ###Dry-run
-t                    ###Show time of execution of each test

-j NUM                ###Number of jobs in parallel
-b FILE               ###PSQL location

PGTAP ASSERTIONS ==>  ###They are FUNC that all come with an optional (but recommended) last arg STR for error message
                      ###(def: '').
                      ###They print the result of the assertion as a STR

GENERAL ASSERTS ==>   ###
ok( BOOL )            ###Asserts that BOOL is true.
                      ###Prefer other function when possible, e.g. is(VAL, VAL2) over ok(VAL = VAL2), because more descriptive
                      ###output.
pass|fail()           ###Like ok( true|false ) (avoid them)
is[nt]( VAL, VAL2 )   ### is [not] distinct from
[i]matches(VAL,VAL2)  ### ~[*]
doesnt_[i]match(...)  ### !~[*]
[un][i]alike(VAL,VAL2)### [not] [a]like
cmp_ok(VAL,OP_STR,VL2)### VAL OP VAL2
isa_ok(VAL,STR)       ### pg_type(VAL) = STR

SQL QUERIES RESULTS==>###Asserts results of sql select ...:
SQL_STR               ##Means SQL statement STR (either as is, or name of a PREP (recommended)).
                      ###A PREP with arguments needs to be written as is, i.e. not 'PREP' but 'execute PREP(ARGS)'
throws_ok(SQL_STR     ###Asserts that SQL_STR throws an exception, with errcode STR2 and errmessage STR3 (each can be null
[, STR2 [, STR3]])    ###(def) for all errcode|errmessage)
lives_ok(SQL_STR)     ###Inverse of throws_ok(SQL_STR)
throws_[i]like|
matching(SQL_STR
[, STR2])             ###Same as throws_ok(SQL_STR, null, STR2), but STR2 needs to match with [i]like or ~[*], not = <>
performs_ok(SQL_STR,
INT)                  ###Asserts that SQL_STR performs in less than INT ms.
results_eq|ne         ###Asserts that both queries compare with = <>
(SQL_STR|CURSOR,      ###Is row-wise, so make sure they are ordered the same.
SQL_STR2|ARRAY|CURSR2)###CURSOR iterates over all rows (must be STR casted as refcursor)
                      ###ARRAY represents a single-column (values ... could also be used for several columns)
bag|set_eq|ne(SQL_STR,###Same but compares not row-wise, but the whole set of values together (so order doesn't matter).
SQL_STR2|ARRAY)       ###set removes duplicates, bag doesn't.
bag|set_has[nt]
( SQL_STR, SQL_STR2)  ###Same as bag|set_eq|ne, but only for subset, i.e. asserts that SQL_STR includes SQL_STR2
is[nt]_empty(SQL_STR) ###Asserts number of rows = <> 0
row_eq(SQL_STR, CTYPE)###Same as results_eq, but for a single row. CTYPE can't be a CTYPE_LIT (using row())

SCHEMA CONFORMANCE ==>###Asserts that current variables are exactly this.
schemas|tablespaces|
roles|languages|
casts_are(STR_ARR)    ###
tables|views|sequences
|functions|opclasses|
types|domains|enums|
operators_are         ###Can restrict to a SCHEMA, otherwise use search_path.
([SCHEMA_STR],STR_ARR)###Functions are only the name, without arguments.
columns|indexes|
triggers|rules_are(
[SCHEMA_STR],
TABLE_STR, STR_ARR)   ###

SCHEMA EXISTENCE ==>  ###Asserts that variable exist.
has[nt]_schema|role|
language(STR)         ###
has[nt]_table|view|
sequence|foreign_table
|type|composite|domain
|enum|opclass|relation
( [SCHEMA_STR, ]STR ) ###relation is table|view|sequence|ctype
has[nt]_index
( [SCHEMA_STR, ]
TABLE_STR, INDEX_STR,
[COL_STR[_ARR]] )     ###COL_STR[_ARR] not with hasnt.
has[nt]_trigger|rule
( [SCHEMA_STR, ]
TABLE_STR, STR )      ###
has[nt]_function
( [SCHEMA_STR, ]
FUNC_STR
[, ARGSTYPE_STR_ARR] )###
has[nt]_cast(TYPE_STR,
TYPE2_STR[,SCHEMA_STR]
[, FUNC_STR] )        ###
has_operator(TYPE_STR
[, SCHEMA_STR],OP_STR,
TYPE2_STR
[, RETURNTYPE_STR] )  ###
has_left|rightop
( [SCHEMA_STR],OP_STR,
TYPE_STR
[, RETURNTYPE_STR] )  ###
has[nt]_tablespace
(STR[, STR2])         ###Can use a STR2 as tablespace location (not with hasnt).

COL ATTRIBUTES ==>    ###
has[nt]_column(
[SCHEMA_STR,]TABLESTR,
COL_STR)              ###
col_not|is_null|
has[nt]_default|pk|fk|
unique|check(
[SCHEMA_STR,]TABLESTR,
COL_[ARR_]STR)        ###pk is primary key, fk foreign key constraint.
is_clustered|
index_is_unique|
primary( [SCHEMA_STR,]
[TABLE_STR,]INDEX_STR)###Asserts properties for an index COL
has[nt]_unique|check|
pk|fk( [SCHEMA_STR,]
TABL_STR)             ###TABLE has at least those constraints.
col_default_is(
[SCHEMA_STR,]TABLESTR,
COL_STR, VAL)         ###
fk_ok( [SCHEMA_STR, ]
TABLE_STR,
COL[_ARR]_STR,
TABLE2_STR,
COL2[_ARR]_STR )      ###Asserts that COL references COL2

TYPES ==>             ###
col_type_is(
[SCHEMA_STR,]TABLESTR,
COL_STR,[SCHEMA2_STR,]
TYPE_STR)             ###
index_is_type(
[SCHEMA_STR, ]
[TABLE_STR,]INDEX_STR,
TYPE_STR)             ###TYPE_STR is 'btree', 'hash', etc.
domain_type_is[nt](
[SCHEMA_STR, ]
DOMAIN_STR,
[,SCHEMA2_STR]TYP_STR)###TYPE_STR is 'btree', 'hash', etc.
enum_has_labels(
[SCHEMA_STR,]ENUM_STR,###
VAL_STR_ARR)

FUNCTIONS ==>         ###
can( [SCHEMA_STR, ]
FUNC_ARR_STR )        ###Same as has_function, but without ARGSTYPE_STR_ARR, and with FUNC_ARR
function_lang_is(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
LANGUAGE_STR )        ###
function_returns(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
TYPE_STR )            ###
volatility_is(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR],
STR )                 ###
function_is_definer|
strict|aggregate(
[SCHEMA_STR, ]FUNC_STR
[, ARGSTYPE_STR_ARR] )###
cast_context_is(
TYPE_STR,TYP2_STR,STR)###STR can be 'implicit', 'assignment', 'explicit'
trigger_is
( [SCHEMA_STR, ]
TABLE_STR, TFUNC_STR
[,SCHEMA2_STR]FUNCSTR)###Asserts that TFUNC executes FUNC
rule_is_instead(
[SCHEMA_STR, ]
TABLE_STR, RULE_STR)  ###
rule_is_on(
[SCHEMA_STR, ]
TABLE_STR, RULE_STR,
EVENT_STR )           ###

ROLES AND SECURITY ==>
db|schema|tablespace|
language_owner_is
(STR, ROLE_STR)       ###
table|view|sequence|
composite|
foreing_table|relation
|opclass|type_owner_is
([SCHEMA_STR, ]STR,
ROLE_STR)             ###
index_owner_is(
[SCHEMA_STR,]TABL_STR,
INDEX_STR, ROLE_STR)  ###
function_owner_is(
[SCHEMA_STR,]FUNC_STR,
ARGSTYPE_STR_ARR,
ROLE_STR)             ###
*_privs_are(...,      ###Same as *_owner_is(...), but asserts PRIVILEGE[_ARR] for ROLE. Differences:
PRIVILEGE_[ARR_]STR)  ###  - db -> database
                      ###  - no relation, view, composite, foreign_table, index, opclass, type
                      ###  - there is also:
                      ###     - column*( ..., TABLE_STR, COL_STR, ... ) and any_column*( ..., TABLE_STR )
                      ###     - fdw|server( FDW|FSERVER_STR, ... )

is[nt]_superuser(ROLE)###
is_member_of(ROLE_STR,
ROLE2[_ARR]_STR)      ###
language_is_trusted(
LANGUAGE_STR)

UTILITIES ==>         ###
diag( STR... )        ###Returns STR (separated by newline), in front of a #, to add comments to the output.
skip(STR[, INT])      ###Skip the next INT (def: 1) PGTAP functions, with explanation STR.
                      ###To put in a conditional branch (e.g. case when ...) when a test might provoke the whole unit test
                      ###to throw an exception (language or function not available).
collect_tap(          ###Do several PGTAP assertions functions at once.
ASSERT_FUNC(...)... ) ###Useful when can't be put several COMMAND; but only one, for example in a SQL case when
todo(STR[, INT])      ###Same, but instead of skipping, just declares that tests are expected to fail, because still on the
                      ###todo list.
todo_start|end(STR)   ###Do todo() for all tests between start and end.
in_todo()             ###Returns true if in a todo_start|end block.
os_name()             ###e.g. 'linux'

OWN ASSERTION_FUNC ==>###Just create a plpgsql function that returns text, with a last optional text argument, and which
                      ###returns ok() if test passes, or returns error message if not.

datafiller.py [FILE]   ##Script printing commands filling randomly some TABLE...
                       ##FILE (def: stdin) is a list of DDL commands creating the TABLE.
                       ##Hints on how to fill are provided with --comments:
                       ##  - syntax:
                       ##     -- df [MACRO]: VAR[=VAL]
                       ##       - with MACRO, can do elsewhere use=DIRECTIVE to repeat all the VAR[=VAL]...
                       ##          - some predefined MACRO:
                       ##             words: word=/etc/dictionaies-common/words
                       ##       - VAL can use '' for STR and TIMESTAMP
                       ##     - can also use -- df T=TABLE A=COL_VAR: ... to target a TABLE or COL_VAR on a separate line
                       ##       after it. TABLE cannot use skip=FLOAT.
                       ##     - to specify a VAR, I write $VAR, but it should be written VAR
                       ##  - supported VAR:
                       ##     - all TABLE (put comment on a line by itself)
                       ##        - size, offset, mangle, null, seed: see below
                       ##     - TABLE (put comment after the opening parenthesis of creation):
                       ##        - size=INT: number of tuples to fill.
                       ##          Can only be on TABLE, not COL (except for gen=serand)
                       ##        - mult=INT: multiply $size for this TABLE
                       ##          mult (def: 1) should be done on each TABLE (relative size with each other)
                       ##          size (def: 100) only once for all TABLE (to scale it)
                       ##        - skip=FLOAT: divide $size for this TABLE.
                       ##          As opposed to mult, actually produce the rows, but randomly don't output them
                       ##        - nogen, null=FLOAT: same as below
                       ##     - COL (put comment after it):
                       ##        - all:
                       ##           - type=TYPE: generate another TYPE, then casted to the actual type
                       ##           - nogen: no random data (use only default values)
                       ##           - null=FLOAT: percentage of nulls
                       ##           - seed=INT: set random seed (def: use OS (usually depends on current time))
                       ##        - BOOL:
                       ##           - rate=FLOAT: percentage of true (def: 0.5)
                       ##        - INT (integer, not int)|DATE|TIMESTAMP|INTERVAL|INET|CIDR|MAC:
                       ##           - gen=STR: distribution, among:
                       ##              - serial: counter, increments $step (def: 1, must not be divider of $size) from
                       ##                $shift (def:0), then modulo $size, then adds $offset (def: 1)
                       ##                If $mangle, choose random $shift and $step
                       ##              - uniform: uniform distribution, from $offset to $offset + $size - 1
                       ##              - serand: serial up to $size1, then uniform to $size2 - $size1 ($size1 and $size2
                       ##                are the COL-level, and TABLE-level $size)
                       ##              - for other distributions: just use type=float, then use float distributions
                       ##           - offset, shift, step, size, mangle: see above
                       ##        - FLOAT:
                       ##           - gen=WORD: distribution, among:
                       ##              - uniform, gauss|norm, log (lognormal), beta, gamma, weibull, vonmises: use $alpha and
                       ##                $beta
                       ##              - exp, pareto: use $alpha
                       ##           - alpha|beta: see above
                       ##        - STR, followed by:
                       ##           - nothing: prefix followed by repetition of number, separated by _
                       ##              - prefix=STR (def: COL_VAR)
                       ##              - length|lenvar=NUM: average length and diff from average of STR (def: 12 and 3)
                       ##           - chars=STR: choose random characters among a dictionary built with random words using
                       ##             characters in STR
                       ##              - cgen=MACRO: specifies INT parameters (to be used like use=) to specify how selection
                       ##                is done
                       ##              - length and lenvar: see above
                       ##           - text: can use INT parameters, choose from list of words
                       ##              - word=FILE|:STR,...: list of words, of 'size' words
                       ##              - length and lenvar: see above
                       ##           - word=FILE|:STR,...: same as above, but length to 1 and lenvar to 0. Can support unique.
                       ##        - DATE|TIMESTAMP|INTERVAL:
                       ##           - start|end=...
                       ##           - prec=NUM: in days for DATE, in seconds for TIMESTAMP
                       ##        - TIMESTAMP:
                       ##           - tz=STR
                       ##        - INTERVAL:
                       ##           - unit=s|m|h|d|mon|y (def: s)
                       ##        - BYTEA:
                       ##           - length and lenvar: see above
                       ##        - INET|CIDR:
                       ##           - network=STR
                       ##        - MAC
                       ##COL can't be unique for FLOAT, STR chars|text and BYTEA.
                       ##Uniqueness is tried 10 times (can be changed with datafiller.py --tries=NUM)
--[no-]filter          ##Output also FILE (commands creating the TABLE...), before commands filling the TABLE...
--drop                 ##Output also commands dropping the TABLE...
--truncate             ##Sale for truncating
--size|offset|seed INT
--null FLOAT
--mangle               ##Sets VAR
-T                     ##Put in a single transaction (normal isolation level)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            OTHERS             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


comment on ... is STR  ##Add a comment STR, that can be read with:
                       ##  - col_description(TABLEOID, NUM)
                       ##  - [sh]obj_description(OID, DATABASE): shobj is for cluster-wide objects
                       ##  - a psql command \d* under "description"
                       ##... is TYPE followed by the definition, e.g. table TABLE, trigger TFUNC on TABLE, column TABLE.COL
                       ##cast ( TYPE as TYPE2 ), aggregate AFUNC(...), etc.
                       ##TYPE can be any object that can be used with create TYPE ...
                       ##If STR is null, remove the comment.
                       ##To write must be owner or superuser

TO DO ==>              ##In online doc:
                       ##  - Clients all rely on a backend protocol.
                       ##  - libpq is a C library that implement it and often used by clients.
                       ##    Large objects are a specific type handled by libpq.
                       ##  - Embedded SQL is a preprocessor that allows to write SQL in C code (then processed to libpq)
                       ##  - C can be used to write functions: see online doc, including background worker processes, SPI,
                       ##    creating new types, writing a PL/* handler, writing a FDW


      
   PG  
      


VERSION ==>                   #3.6.2

PG                            #Installation requires libpq

new PG.Client( [STR|OBJ],     #Connect to the database specified by STR|OBJ, then fire FUNC.
FUNC( ERROR, CLIENT,          #FUNC2 must be fired when all operations are done to close connection.
FUNC2(ERROR)))                #STR is "[connectionname://][user[:password]@][host[:port]][/database]"
                              #(all defaults if no first arg) or a IPC socket folder path.
                              #Connectionname can be anything, it just differentiate sessions. OBJ has members :
                              #  - user (def: process.env.USER)
                              #  - database (def: process.env.USER)
                              #  - password (def: null)
                              #  - port (def: 5432)
                              #  - host (def: null): if not URL, use DIR/.s.PGSQL.PORT
                              #  - ssl (def: false)
                              #Defaults are in PG.defaults.VAR
                              #Other defaults:
                              #  - PG.defaults.parseInt8:
                              #     - PSQL bigint (such as result of count()) is too big for JavaScript INT.
                              #     - If false (def), bigint -> STR. If true, bigint -> INT
                              #Returns CLIENT, but should only be used for events. Use CLIENT in callback for connect|end()
CLIENT.connect
([FUNC(ERROR, CLIENT)])       #
CLIENT.end()                  #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            QUERIES            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CLIENT.query(STR[, VAL_ARR]   #Returns a QUERY from PostgreSQL command STR.
[,FUNC(ERROR, OBJ)])          #If FUNC, also execute it and event handlers of QUERY cannot be used (so QUERY is useless then).
                              #OBJ is same as in QUERY end event handler.
                              #If VAL_ARR, each "$1", "$2", etc. in STR will be replaced by those VAL, providing it does not
                              #point to a TABLE, a COL or a SCHEMA. It is slower but it prevents SQL injections (VAL_ARR are
                              #properly escaped instead of using risky STR concatenation).
CLIENT.query(OBJ[, FUNC])     #Same but OBJ can have members :
                              #  - text: same as STR
                              #  - values: same as VAL_ARR
                              #  - name STR3:
                              #     - make it PREP: but using PSQL Extended Protocol, so same effect (skip parsing phase when
                              #       calling came query with same name (will use same text|values)), but not actual PREP
                              #     - parsing is only done when values VAL_ARR is used, so only useful then

QUERY.on
("row", FUNC( OBJ ))          #Execute QUERY and fire event handler for each row OBJ: { VAR: VAL }...
QUERY.on("end", FUNC( OBJ ) ) #Execute QUERY and fire event handler for all rows. OBJ has members :
                              #  - command STR : SQL command
                              #  - rowCount UINT
                              #  - oid DOUBLE
                              #  - rows OBJ_ARR: { VAR: VAL }...
                              #  - fields OBJ_ARR:
                              #     - name STR
                              #     - format TYPE_STR
                              #     - tableID DOUBLE
                              #     - columnID DOUBLE
                              #     - dataTypeID DOUBLE
                              #     - dataTypeSize UINT
                              #     - dataTypeModifier UINT
QUERY.on("error",FUNC(ERROR)) #

CLIENT.query
(new PG-QUERY-STREAM(STR))    #Like CLIENT.query(STR) but returns as ISTREAM.
CLIENT.query
(new PG-CURSOR(STR)[,VAL_ARR])#Like CLIENT.query(STR[, VAL_ARR]) but returns a CURSOR (version 0.2.0).
CURSOR.read
(UINT, FUNC(ERROR, OBJ_ARR))  #OBJ_ARR is { VAR: VAL }... or [] if no more rows
CLIENT.copyFrom|To( STR )     #COPY...FROM|TO statement must use this instead of CLIENT.query().
                              #Returns a I|OSTREAM (must execute I|OSTREAM.end()).
                              #Can use stdin (not stdout) in STR if writing|reading from I|OSTREAM
CLIENT.pause|resumeDrain()    #Stops|resumes emission of drain events (useful when async operations need to complete first)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       OTHER OPERATIONS        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CLIENT.on("drain", FUNC())    #Fired each time all queries have been executed
CLIENT.on("error",FUNC(ERROR))#
CLIENT.on                     #Fired with listen/notify SQL statements
("notification", FUNC(OBJ))   #OBJ:
                              #  - name "notification"
                              #  - channel STR
                              #  - payload STR
                              #  - length NUM
                              #  - processId NUM
CLIENT.on("notice", FUNC(STR))#Fired with warning messages (otherwise printer in stdout)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            POOLING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


PG.pools.getOrCreate([OBJ])   #Returns POOL (from GENERIC-POOL) of CLIENT that has extra method:
                              #  - connect(FUNC(ERROR, CLIENT, FUNC2(ERROR2))): acquire a CLIENT and fires FUNC()
                              #Created with params (either OBJ or PG.defaults):
                              #  - name: OBJ stringified
                              #  - max <- poolSize
                              #  - idleTimeoutMillis <- poolIdleTimeout
                              #  - reapIntervalMillis <- reapIntervalMillis
                              #  - log <- poolLog
                              #Other OBJ passed to new PG.CLIENT(OBJ)
                              #If no POOL used, would use one new connection for each query.
PG.pools.all                  #POOL_OBJ

PG.connect(OBJ, FUNC)         #Like new PG.CLIENT(OBJ, FUNC).connect() but uses PG.pools.getOrCreate(OBJ)
PG.on
("error",FUNC(ERROR, CLIENT)) #
PG.end()                      #Close all CLIENT, even if currently querying.
