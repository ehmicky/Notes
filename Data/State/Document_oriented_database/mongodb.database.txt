
           
   MONGODB  
           


The following are docs of the updates from 3.0.2 to 3.4.9
Should wait for 3.6, then go through whole doc again for anything between 3.4.9 and 3.6, then actually add the new doc

$type:
https://docs.mongodb.com/manual/reference/operator/query/type/#op._S_type

find():
https://docs.mongodb.com/manual/reference/command/find/#dbcmd.find
https://docs.mongodb.com/manual/reference/command/getMore/#dbcmd.getMore

combined methods:
https://docs.mongodb.com/manual/reference/method/db.collection.findOneAndDelete/#db.collection.findOneAndDelete
https://docs.mongodb.com/manual/reference/method/db.collection.findOneAndReplace/#db.collection.findOneAndReplace
https://docs.mongodb.com/manual/reference/method/db.collection.findOneAndUpdate/#db.collection.findOneAndUpdate

one vs many:
https://docs.mongodb.com/manual/reference/method/db.collection.insertOne/#db.collection.insertOne
https://docs.mongodb.com/manual/reference/method/db.collection.insertMany/#db.collection.insertMany
https://docs.mongodb.com/manual/reference/method/db.collection.replaceOne/#db.collection.replaceOne
https://docs.mongodb.com/manual/reference/method/db.collection.updateOne/#db.collection.updateOne
https://docs.mongodb.com/manual/reference/method/db.collection.updateMany/#db.collection.updateMany
https://docs.mongodb.com/manual/reference/method/db.collection.deleteOne/#db.collection.deleteOne (not marked as new, but is
new)
https://docs.mongodb.com/manual/reference/method/db.collection.deleteMany/ (not marked as new, but is new)

NumberDecimal:
https://docs.mongodb.com/manual/reference/mongodb-extended-json/
https://docs.mongodb.com/manual/core/shell-types/
https://docs.mongodb.com/manual/tutorial/model-monetary-data/

cursors:
https://docs.mongodb.com/manual/reference/command/killCursors/#dbcmd.killCursors
https://docs.mongodb.com/manual/reference/command/cursorInfo/#dbcmd.cursorInfo

explain:
https://docs.mongodb.com/manual/reference/command/explain/#dbcmd.explain
https://docs.mongodb.com/manual/reference/method/db.collection.explain/#db.collection.explain

HTTP interface:
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/program/mongostat/
https://docs.mongodb.com/manual/reference/configuration-options/

Roles:
https://docs.mongodb.com/manual/core/security-built-in-roles/
https://docs.mongodb.com/manual/reference/built-in-roles/
https://docs.mongodb.com/manual/reference/program/mongodump/

Authentication:
https://docs.mongodb.com/manual/tutorial/enforce-keyfile-access-control-in-existing-replica-set-without-downtime/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/configuration-options/

system.profile:
https://docs.mongodb.com/manual/reference/database-profiler/

SSL:
https://docs.mongodb.com/manual/tutorial/configure-ssl/
https://docs.mongodb.com/manual/tutorial/configure-ssl-clients/
https://docs.mongodb.com/manual/tutorial/upgrade-cluster-to-ssl/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/program/mongo/
https://docs.mongodb.com/manual/reference/program/mongodump/
https://docs.mongodb.com/manual/reference/program/mongorestore/
https://docs.mongodb.com/manual/reference/program/mongoimport/
https://docs.mongodb.com/manual/reference/program/mongoexport/
https://docs.mongodb.com/manual/reference/program/mongostat/
https://docs.mongodb.com/manual/reference/program/mongotop/
https://docs.mongodb.com/manual/reference/program/mongofiles/
https://docs.mongodb.com/manual/reference/configuration-options/
https://docs.mongodb.com/manual/reference/parameters/

LDAP:
https://docs.mongodb.com/manual/core/authentication-mechanisms-enterprise/
https://docs.mongodb.com/manual/tutorial/kerberos-auth-activedirectory-authz/
https://docs.mongodb.com/manual/core/security-ldap/
https://docs.mongodb.com/manual/tutorial/configure-ldap-sasl-activedirectory/
https://docs.mongodb.com/manual/tutorial/configure-ldap-sasl-openldap/
https://docs.mongodb.com/manual/tutorial/authenticate-nativeldap-activedirectory/
https://docs.mongodb.com/manual/core/security-ldap-external/
https://docs.mongodb.com/manual/reference/program/mongoldap/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/configuration-options/

Audit log:
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/configuration-options/
https://docs.mongodb.com/manual/administration/monitoring/

Encryption:
https://docs.mongodb.com/manual/core/security-encryption-at-rest/
https://docs.mongodb.com/manual/tutorial/configure-encryption/
https://docs.mongodb.com/manual/tutorial/rotate-encryption-key/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/configuration-options/

Namespaces:
https://docs.mongodb.com/manual/reference/program/mongorestore/

Dry runs:
https://docs.mongodb.com/manual/reference/program/mongorestore/

Oplog:
https://docs.mongodb.com/manual/reference/program/mongooplog/
https://docs.mongodb.com/manual/reference/program/mongorestore/
https://docs.mongodb.com/manual/reference/glossary/
https://docs.mongodb.com/manual/administration/production-checklist-operations/

import/export:
https://docs.mongodb.com/manual/reference/program/mongoimport/

mongostat:
https://docs.mongodb.com/manual/reference/program/mongostat/

mongo files ID:
https://docs.mongodb.com/manual/reference/program/mongofiles/

mongoreplay:
https://docs.mongodb.com/manual/reference/program/mongoreplay/

Logging:
https://docs.mongodb.com/manual/reference/configuration-options/
https://docs.mongodb.com/manual/reference/parameters/
https://docs.mongodb.com/manual/reference/log-messages/

Network compression:
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/program/mongo/
https://docs.mongodb.com/manual/reference/configuration-options/

Dump compression:
https://docs.mongodb.com/manual/reference/program/mongorestore/

listDatabases:
https://docs.mongodb.com/manual/reference/command/listDatabases/#dbcmd.listDatabases

killOp:
https://docs.mongodb.com/manual/reference/privilege-actions/
https://docs.mongodb.com/manual/reference/command/killOp/#dbcmd.killOp
https://docs.mongodb.com/manual/reference/method/db.killOp/#db.killOp

compat:
https://docs.mongodb.com/manual/reference/command/setFeatureCompatibilityVersion/#dbcmd.setFeatureCompatibilityVersion

writeConcern/readConcern:
https://docs.mongodb.com/manual/reference/read-concern/
https://docs.mongodb.com/manual/reference/write-concern/
https://docs.mongodb.com/manual/core/read-isolation-consistency-recency/
https://docs.mongodb.com/manual/administration/production-notes/
https://docs.mongodb.com/manual/tutorial/perform-findAndModify-linearizable-reads/
https://docs.mongodb.com/manual/faq/concurrency/
https://docs.mongodb.com/manual/reference/command/findAndModify/#dbcmd.findAndModify
https://docs.mongodb.com/manual/reference/method/db.collection.count/#db.collection.count
https://docs.mongodb.com/manual/reference/method/db.collection.findAndModify/#db.collection.findAndModify
https://docs.mongodb.com/manual/reference/method/cursor.readConcern/#cursor.readConcern
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/configuration-options/
https://docs.mongodb.com/manual/reference/connection-string/

Read preference:
https://docs.mongodb.com/manual/core/read-preference/
https://docs.mongodb.com/manual/core/read-preference-mechanics/
https://docs.mongodb.com/manual/reference/connection-string/

journal:
https://docs.mongodb.com/manual/core/journaling/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/configuration-options/

capped collections:
https://docs.mongodb.com/manual/core/capped-collections/

indexes:
https://docs.mongodb.com/manual/indexes/
https://docs.mongodb.com/manual/core/index-partial/
https://docs.mongodb.com/manual/core/index-multikey/
https://docs.mongodb.com/manual/core/multikey-index-bounds/
https://docs.mongodb.com/manual/core/index-unique/
https://docs.mongodb.com/manual/core/index-case-insensitive/
https://docs.mongodb.com/manual/core/index-sparse/
https://docs.mongodb.com/manual/reference/command/create/#dbcmd.create
https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection
https://docs.mongodb.com/manual/reference/command/createIndexes/#dbcmd.createIndexes
https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/#db.collection.createIndex
https://docs.mongodb.com/manual/reference/method/cursor.returnKey/#cursor.returnKey

text search:
https://docs.mongodb.com/manual/tutorial/text-search-with-rlp/
https://docs.mongodb.com/manual/core/index-text/
https://docs.mongodb.com/manual/reference/text-search-languages/
https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/configuration-options/

geo search:
https://docs.mongodb.com/manual/core/2dsphere/
https://docs.mongodb.com/manual/reference/operator/aggregation/geoNear/#pipe._S_geoNear

storage engines:
https://docs.mongodb.com/manual/administration/production-notes/
https://docs.mongodb.com/manual/tutorial/backup-with-filesystem-snapshots/
https://docs.mongodb.com/manual/tutorial/backup-sharded-cluster-with-filesystem-snapshots/
https://docs.mongodb.com/manual/tutorial/backup-sharded-cluster-with-database-dumps/
https://docs.mongodb.com/manual/core/wiredtiger/
https://docs.mongodb.com/manual/tutorial/change-standalone-wiredtiger/
https://docs.mongodb.com/manual/tutorial/change-replica-set-wiredtiger/
https://docs.mongodb.com/manual/core/inmemory/
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/command/moveChunk/#dbcmd.moveChunk
https://docs.mongodb.com/manual/reference/command/create/#dbcmd.create
https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection
https://docs.mongodb.com/manual/reference/command/fsync/#dbcmd.fsync
https://docs.mongodb.com/manual/reference/method/db.fsyncLock/#db.fsyncLock
https://docs.mongodb.com/manual/reference/method/db.fsyncUnlock/#db.fsyncUnlock
https://docs.mongodb.com/manual/reference/command/buildInfo/#dbcmd.buildInfo
https://docs.mongodb.com/manual/reference/configuration-options/

JavaScript engines:
https://docs.mongodb.com/manual/reference/command/buildInfo/#dbcmd.buildInfo
https://docs.mongodb.com/manual/reference/program/mongo/
https://docs.mongodb.com/manual/reference/parameters/

views:
https://docs.mongodb.com/manual/core/views/
https://docs.mongodb.com/manual/reference/method/db.createView/#db.createView
https://docs.mongodb.com/manual/reference/command/create/#dbcmd.create
https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection
https://docs.mongodb.com/manual/reference/program/mongodump/

replication:
https://docs.mongodb.com/manual/replication/
https://docs.mongodb.com/manual/core/replica-set-sync/
https://docs.mongodb.com/manual/core/replica-set-elections/
https://docs.mongodb.com/manual/reference/replica-set-protocol-versions/
https://docs.mongodb.com/manual/core/master-slave/
https://docs.mongodb.com/manual/tutorial/adjust-replica-set-member-priority/
https://docs.mongodb.com/manual/reference/replica-configuration/
https://docs.mongodb.com/manual/reference/command/isMaster/#dbcmd.isMaster
https://docs.mongodb.com/manual/reference/command/replSetReconfig/#dbcmd.replSetReconfig
https://docs.mongodb.com/manual/reference/method/rs.reconfig/#rs.reconfig
https://docs.mongodb.com/manual/reference/command/replSetSyncFrom/#dbcmd.replSetSyncFrom
https://docs.mongodb.com/manual/reference/method/rs.syncFrom/#rs.syncFrom
https://docs.mongodb.com/manual/reference/command/replSetGetStatus/#dbcmd.replSetGetStatus
https://docs.mongodb.com/manual/reference/command/serverStatus/#dbcmd.serverStatus
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongodump/
https://docs.mongodb.com/manual/reference/program/mongoexport/
https://docs.mongodb.com/manual/reference/parameters/

sharding:
https://docs.mongodb.com/manual/administration/configuration/
https://docs.mongodb.com/manual/tutorial/upgrade-revision/
https://docs.mongodb.com/manual/tutorial/restore-sharded-cluster/
https://docs.mongodb.com/manual/sharding/
https://docs.mongodb.com/manual/core/sharded-cluster-config-servers/
https://docs.mongodb.com/manual/core/sharded-cluster-query-router/
https://docs.mongodb.com/manual/tutorial/create-chunks-in-sharded-cluster/
https://docs.mongodb.com/manual/core/sharding-balancer-administration/
https://docs.mongodb.com/manual/tutorial/manage-sharded-cluster-balancer/
https://docs.mongodb.com/manual/tutorial/replace-config-server/
https://docs.mongodb.com/manual/tutorial/upgrade-config-servers-to-replica-set/
https://docs.mongodb.com/manual/tutorial/upgrade-config-servers-to-replica-set-downtime/
https://docs.mongodb.com/manual/tutorial/migrate-sharded-cluster-to-new-hardware/
https://docs.mongodb.com/manual/tutorial/convert-replica-set-to-replicated-shard-cluster/
https://docs.mongodb.com/manual/tutorial/troubleshoot-sharded-clusters/
https://docs.mongodb.com/manual/reference/config-database/
https://docs.mongodb.com/manual/reference/command/addShardToZone/#dbcmd.addShardToZone
https://docs.mongodb.com/manual/reference/method/sh.addShardToZone/#sh.addShardToZone
https://docs.mongodb.com/manual/reference/command/removeShardFromZone/#dbcmd.removeShardFromZone
https://docs.mongodb.com/manual/reference/method/sh.removeShardFromZone/#sh.removeShardFromZone
https://docs.mongodb.com/manual/reference/method/sh.removeRangeFromZone/#sh.removeRangeFromZone
https://docs.mongodb.com/manual/reference/command/updateZoneKeyRange/#dbcmd.updateZoneKeyRange
https://docs.mongodb.com/manual/reference/method/sh.updateZoneKeyRange/#sh.updateZoneKeyRange
https://docs.mongodb.com/manual/reference/method/sh.addShardTag/#sh.addShardTag
https://docs.mongodb.com/manual/reference/method/sh.removeShardTag/#sh.removeShardTag
https://docs.mongodb.com/manual/reference/method/sh.addTagRange/#sh.addTagRange
https://docs.mongodb.com/manual/reference/method/sh.removeTagRange/#sh.removeTagRange
https://docs.mongodb.com/manual/reference/command/balancerStart/#dbcmd.balancerStart
https://docs.mongodb.com/manual/reference/method/sh.startBalancer/#sh.startBalancer
https://docs.mongodb.com/manual/reference/command/balancerStatus/#dbcmd.balancerStatus
https://docs.mongodb.com/manual/reference/command/balancerStop/#dbcmd.balancerStop
https://docs.mongodb.com/manual/reference/method/sh.stopBalancer/#sh.stopBalancer
https://docs.mongodb.com/manual/reference/command/shardCollection/#dbcmd.shardCollection
https://docs.mongodb.com/manual/reference/method/sh.shardCollection/#sh.shardCollection
https://docs.mongodb.com/manual/reference/command/currentOp/#dbcmd.currentOp
https://docs.mongodb.com/manual/reference/method/db.currentOp/#db.currentOp
https://docs.mongodb.com/manual/reference/method/sh.getBalancerLockDetails/#sh.getBalancerLockDetails
https://docs.mongodb.com/manual/reference/method/sh.getBalancerHost/#sh.getBalancerHost
https://docs.mongodb.com/manual/reference/method/sh.status/#sh.status
https://docs.mongodb.com/manual/reference/program/mongod/
https://docs.mongodb.com/manual/reference/program/mongos/
https://docs.mongodb.com/manual/reference/program/mongodump/
https://docs.mongodb.com/manual/reference/configuration-options/
https://docs.mongodb.com/manual/reference/parameters/
https://docs.mongodb.com/manual/reference/glossary/

bitwise operators:
https://docs.mongodb.com/manual/reference/operator/query/bitsAllClear/#op._S_bitsAllClear
https://docs.mongodb.com/manual/reference/operator/query/bitsAllSet/#op._S_bitsAllSet
https://docs.mongodb.com/manual/reference/operator/query/bitsAnyClear/#op._S_bitsAnyClear
https://docs.mongodb.com/manual/reference/operator/query/bitsAnySet/#op._S_bitsAnySet

aggregation:
https://docs.mongodb.com/manual/core/aggregation-pipeline/
https://docs.mongodb.com/manual/core/aggregation-pipeline-optimization/
https://docs.mongodb.com/manual/core/aggregation-pipeline-limits/
https://docs.mongodb.com/manual/core/aggregation-pipeline-sharded-collections/
https://docs.mongodb.com/manual/reference/operator/aggregation/collStats/#pipe._S_collStats
https://docs.mongodb.com/manual/reference/operator/aggregation/project/#pipe._S_project
https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/#pipe._S_unwind
https://docs.mongodb.com/manual/reference/operator/aggregation/group/#pipe._S_group
https://docs.mongodb.com/manual/reference/operator/aggregation/sample/#pipe._S_sample
https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup
https://docs.mongodb.com/manual/reference/operator/aggregation/indexStats/#pipe._S_indexStats
https://docs.mongodb.com/manual/reference/operator/aggregation/facet/#pipe._S_facet
https://docs.mongodb.com/manual/reference/operator/aggregation/bucket/#pipe._S_bucket
https://docs.mongodb.com/manual/reference/operator/aggregation/bucketAuto/#pipe._S_bucketAuto
https://docs.mongodb.com/manual/reference/operator/aggregation/sortByCount/#pipe._S_sortByCount
https://docs.mongodb.com/manual/reference/operator/aggregation/addFields/#pipe._S_addFields
https://docs.mongodb.com/manual/reference/operator/aggregation/replaceRoot/#pipe._S_replaceRoot
https://docs.mongodb.com/manual/reference/operator/aggregation/graphLookup/#pipe._S_graphLookup
https://docs.mongodb.com/manual/reference/operator/aggregation/abs/#exp._S_abs
https://docs.mongodb.com/manual/reference/operator/aggregation/ceil/#exp._S_ceil
https://docs.mongodb.com/manual/reference/operator/aggregation/exp/#exp._S_exp
https://docs.mongodb.com/manual/reference/operator/aggregation/floor/#exp._S_floor
https://docs.mongodb.com/manual/reference/operator/aggregation/ln/#exp._S_ln
https://docs.mongodb.com/manual/reference/operator/aggregation/log/#exp._S_log
https://docs.mongodb.com/manual/reference/operator/aggregation/log10/#exp._S_log10
https://docs.mongodb.com/manual/reference/operator/aggregation/pow/#exp._S_pow
https://docs.mongodb.com/manual/reference/operator/aggregation/sqrt/#exp._S_sqrt
https://docs.mongodb.com/manual/reference/operator/aggregation/trunc/#exp._S_trunc
https://docs.mongodb.com/manual/reference/operator/aggregation/indexOfBytes/#exp._S_indexOfBytes
https://docs.mongodb.com/manual/reference/operator/aggregation/indexOfCP/#exp._S_indexOfCP
https://docs.mongodb.com/manual/reference/operator/aggregation/split/#exp._S_split
https://docs.mongodb.com/manual/reference/operator/aggregation/strLenBytes/#exp._S_strLenBytes
https://docs.mongodb.com/manual/reference/operator/aggregation/strLenCP/#exp._S_strLenCP
https://docs.mongodb.com/manual/reference/operator/aggregation/substr/#exp._S_substr
https://docs.mongodb.com/manual/reference/operator/aggregation/substrBytes/#exp._S_substrBytes
https://docs.mongodb.com/manual/reference/operator/aggregation/arrayElemAt/#exp._S_arrayElemAt
https://docs.mongodb.com/manual/reference/operator/aggregation/concatArrays/#exp._S_concatArrays
https://docs.mongodb.com/manual/reference/operator/aggregation/filter/#exp._S_filter
https://docs.mongodb.com/manual/reference/operator/aggregation/in/#exp._S_in
https://docs.mongodb.com/manual/reference/operator/aggregation/indexOfArray/#exp._S_indexOfArray
https://docs.mongodb.com/manual/reference/operator/aggregation/isArray/#exp._S_isArray
https://docs.mongodb.com/manual/reference/operator/aggregation/reduce/#exp._S_reduce
https://docs.mongodb.com/manual/reference/operator/aggregation/reverseArray/#exp._S_reverseArray
https://docs.mongodb.com/manual/reference/operator/aggregation/slice/#exp._S_slice
https://docs.mongodb.com/manual/reference/operator/aggregation/zip/#exp._S_zip
https://docs.mongodb.com/manual/reference/operator/aggregation/dateToString/#exp._S_dateToString
https://docs.mongodb.com/manual/reference/operator/aggregation/isoDayOfWeek/#exp._S_isoDayOfWeek
https://docs.mongodb.com/manual/reference/operator/aggregation/isoWeek/#exp._S_isoWeek
https://docs.mongodb.com/manual/reference/operator/aggregation/isoWeekYear/#exp._S_isoWeekYear
https://docs.mongodb.com/manual/reference/operator/aggregation/switch/#exp._S_switch
https://docs.mongodb.com/manual/reference/operator/aggregation/type/#exp._S_type
https://docs.mongodb.com/manual/reference/operator/aggregation/stdDevPop/#grp._S_stdDevPop
https://docs.mongodb.com/manual/reference/operator/aggregation/stdDevSamp/#grp._S_stdDevSamp
https://docs.mongodb.com/manual/reference/command/aggregate/#dbcmd.aggregate
https://docs.mongodb.com/manual/reference/command/group/#dbcmd.group
https://docs.mongodb.com/manual/reference/method/db.collection.group/#db.collection.group
https://docs.mongodb.com/manual/reference/limits/
https://docs.mongodb.com/manual/tutorial/iterate-a-cursor/

meta operator / cursor methods:
https://docs.mongodb.com/manual/reference/operator/query-modifier/
https://docs.mongodb.com/manual/reference/operator/meta/comment/#metaOp._S_comment
https://docs.mongodb.com/manual/reference/operator/meta/explain/#metaOp._S_explain
https://docs.mongodb.com/manual/reference/operator/meta/hint/#metaOp._S_hint
https://docs.mongodb.com/manual/reference/operator/meta/maxScan/#metaOp._S_maxScan
https://docs.mongodb.com/manual/reference/operator/meta/max/#metaOp._S_max
https://docs.mongodb.com/manual/reference/operator/meta/maxTimeMS/#metaOp._S_maxTimeMS
https://docs.mongodb.com/manual/reference/operator/meta/min/#metaOp._S_min
https://docs.mongodb.com/manual/reference/operator/meta/orderby/#metaOp._S_orderby
https://docs.mongodb.com/manual/reference/operator/meta/query/#metaOp._S_query
https://docs.mongodb.com/manual/reference/operator/meta/returnKey/#metaOp._S_returnKey
https://docs.mongodb.com/manual/reference/operator/meta/showDiskLoc/#metaOp._S_showDiskLoc
https://docs.mongodb.com/manual/reference/operator/meta/snapshot/#metaOp._S_snapshot
https://docs.mongodb.com/manual/reference/operator/meta/natural/#metaOp._S_natural
https://docs.mongodb.com/manual/reference/command/aggregate/#dbcmd.aggregate
https://docs.mongodb.com/manual/reference/method/cursor.addOption/#cursor.addOption
https://docs.mongodb.com/manual/reference/method/cursor.comment/#cursor.comment
https://docs.mongodb.com/manual/reference/method/cursor.max/#cursor.max
https://docs.mongodb.com/manual/reference/method/cursor.maxScan/#cursor.maxScan
https://docs.mongodb.com/manual/reference/method/cursor.min/#cursor.min
https://docs.mongodb.com/manual/reference/method/cursor.showRecordId/#cursor.showRecordId
https://docs.mongodb.com/manual/reference/method/cursor.tailable/#cursor.tailable

document validation:
https://docs.mongodb.com/manual/core/document-validation/
https://docs.mongodb.com/manual/core/databases-and-collections/
https://docs.mongodb.com/manual/reference/privilege-actions/
https://docs.mongodb.com/manual/reference/opertaor/aggregation/out/#pipe._S_out
https://docs.mongodb.com/manual/reference/command/aggregate/#dbcmd.aggregate
https://docs.mongodb.com/manual/reference/command/mapReduce/#dbcmd.mapReduce
https://docs.mongodb.com/manual/reference/command/findAndModify/#dbcmd.findAndModify
https://docs.mongodb.com/manual/reference/command/insert/#dbcmd.insert
https://docs.mongodb.com/manual/reference/command/update/#dbcmd.update
https://docs.mongodb.com/manual/reference/command/collMod/#dbcmd.collMod
https://docs.mongodb.com/manual/reference/command/create/#dbcmd.create
https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/#db.collection.aggregate
https://docs.mongodb.com/manual/reference/method/db.collection.findAndModify/#db.collection.findAndModify
https://docs.mongodb.com/manual/reference/method/db.collection.mapReduce/#db.collection.mapReduce
https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection
https://docs.mongodb.com/manual/reference/method/db.getCollectionInfos/#db.getCollectionInfos
https://docs.mongodb.com/manual/reference/program/mongorestore/
https://docs.mongodb.com/manual/reference/program/mongoimport/

collation:
https://docs.mongodb.com/manual/reference/bson-type-comparison-order/
https://docs.mongodb.com/manual/reference/collation/
https://docs.mongodb.com/manual/reference/collation-locales-defaults/
https://docs.mongodb.com/manual/reference/method/cursor.collation/#cursor.collation
https://docs.mongodb.com/manual/reference/command/aggregate/#dbcmd.aggregate
https://docs.mongodb.com/manual/reference/command/distinct/#dbcmd.distinct
https://docs.mongodb.com/manual/reference/command/delete/#dbcmd.delete
https://docs.mongodb.com/manual/reference/command/update/#dbcmd.update
https://docs.mongodb.com/manual/reference/command/create/#dbcmd.create
https://docs.mongodb.com/manual/reference/command/createIndexes/#dbcmd.createIndexes
https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/#db.collection.aggregate
https://docs.mongodb.com/manual/reference/method/db.collection.createIndex/#db.collection.createIndex
https://docs.mongodb.com/manual/reference/method/db.collection.deleteOne/#db.collection.deleteOne
https://docs.mongodb.com/manual/reference/method/db.collection.deleteMany/#db.collection.deleteMany
https://docs.mongodb.com/manual/reference/method/db.collection.distinct/#db.collection.distinct
https://docs.mongodb.com/manual/reference/method/db.collection.findAndModify/#db.collection.findAndModify
https://docs.mongodb.com/manual/reference/method/db.collection.mapReduce/#db.collection.mapReduce
https://docs.mongodb.com/manual/reference/method/db.collection.remove/#db.collection.remove
https://docs.mongodb.com/manual/reference/method/db.collection.update/#db.collection.update
https://docs.mongodb.com/manual/reference/method/db.createCollection/#db.createCollection
https://docs.mongodb.com/manual/reference/method/Bulk.find.collation/#Bulk.find.collation

bulk write:
https://docs.mongodb.com/manual/reference/method/db.collection.bulkWrite/#db.collection.bulkWrite


VERSION ==>                       #3.0.2

GENERAL ==>                       #NoSQL database that uses JavaScript-like objects (BSON), based on V8 engine.

SHORT SUMMARY ==>                 #Goal:
                                  #  - document-oriented, close to JavaScript/JSON, schemaless
                                  #  - powerful queries
                                  #  - eleviates application logic with complex operations on search/update,
                                  #    include groupped (e.g. mapReduce), sorting, etc.
                                  #Other use cases:
                                  #  - FIFO
                                  #  - TTL
                                  #  - geolocation search
                                  #  - text search
                                  #Main properties:
                                  #  - optimized for speed
                                  #Other properties:
                                  #  - BASE (not ACID) with decent durability (WAL)
                                  #  - Scaling: replica sets (read-only), sharding (write)
                                  #  - High availability: replica sets
                                  #  - Monitoring: logging, stats, alerts, debugging
                                  #  - Security: fairly ok
                                  #  - Maintainance: backups, migrations

LONG SUMMARY ==>                  #Design:
                                  #  - document-oriented:
                                  #     - dynamic OBJ
                                  #     - BSON: like JavaScript but binary and extra types: function, double, int,
                                  #       longlong, oid, dbref, date, timestamp, binary
                                  #     - server > databases > collections > objects
                                  #  - normalized vs denormalized
                                  #  - unique index
                                  #  - capped COLL / tailable CUR (FIFO)
                                  #  - TTL
                                  #CRUD:
                                  #  - read:
                                  #     - &find():
                                  #        - declarative generic search
                                  #        - > >= <= < == != in nin && || ! typeof %
                                  #        - regex
                                  #        - ARR.contains(), ARR.length
                                  #     - Q$where(): imperative flexible search, using JavaScript FUNC
                                  #     - text search: tokenization, remove common words, simplist suffix stemming,
                                  #       case insensitive, "..." search, -WORD exclusion
                                  #     - geolocation search:
                                  #        - types: flat, on a sphere, haystack (points close to each other)
                                  #        - query: close to, within, intersects
                                  #     - return value manipulation:
                                  #        - projection: simple exclusion|inclusions
                                  #        - sorting
                                  #        - &aggregate():
                                  #           - operations:
                                  #              - > >= <= < == != && || ! + - * / %
                                  #              - STR.concat() STR.toLower|Upper() STR.slice() DATE.get*() DATE.format()
                                  #              - ARR.length ARR.some|any()
                                  #                Set_operations(unions,difference,intersection,equal,subset)
                                  #              - ARR -> several OBJ
                                  #              - several OBJs -> first|last|min|max|sum|avg|push|addToSet() (like "group by")
                                  #              - if|then|else
                                  #              - excluding sub-OBJs
                                  #           - operations output can be used for query, i.e. for complex search
                                  #             instead of manipulation
                                  #           - output: direct or to another collection
                                  #        - &group|count|distinct():
                                  #           - group by key
                                  #           - operations: ARR.reduce(), ARR.length, ARR.getUniqueValues()
                                  #        - &mapReduce():
                                  #           - map: group|unwind by arbitrary logic (JavaScript function)
                                  #           - reduce: like ARR.reduce()
                                  #  - create:
                                  #     - &insert()
                                  #  - create|update:
                                  #     - &update(), &save|findAndModify()
                                  #        - operations:
                                  #           - = += *= &= |= ^=
                                  #           - ARR.pop|shift() ARR.push|addToSet() ARR.removeIf()
                                  #           - new Date() min|max()
                                  #           - rename variable name
                                  #  - delete: &delete()
                                  #Limits:
                                  #  - max 1000 OBJ_ARR.length, unless using BULK writes
                                  #  - OBJ 16MB, unless using GridFS (files, non-atomic operations)
                                  #  - max. num connections
                                  #ACID:
                                  #  - atomicity: OBJ-level, never multi-OBJs
                                  #  - consistency/isolation:
                                  #     - write locks can yield in the middle, unless $isolated 1
                                  #     - cursors can have duplicated reads, unless $snapshot
                                  #  - durability:
                                  #     - always read uncommitted (can read a write that is received but not journaled)
                                  #     - WAL: writes are journaled, and can optionally only return success once journaled
                                  #High availability:
                                  #  - replica sets (automatic failover using async log shipping): arbiter, passive
                                  #Scalability:
                                  #  - replica sets (multiple read servers): hidden|read preferences (query routing)
                                  #  - sharding (multiple write servers): choose right shard key, per COLL
                                  #    (choose primary shard), restrictions
                                  #Performance:
                                  #  - cursors: pagination, skip|limit, maxScan|maxTimeMS
                                  #  - indexes: sorting, multikey (on ARR), compound vs several, hashed (HASH(VAL)),
                                  #    sparse (no undefined)
                                  #  - BULK writes
                                  #  - space allocation
                                  #  - compression
                                  #  - workingSet should fit in RAM
                                  #  - mongoperf
                                  #  - query plans/debugging
                                  #Security:
                                  #  - Users, roles, permissions on resources
                                  #  - Authentication: Kerberos, SSL, SASL, passwords
                                  #  - Audit log
                                  #Maintenance:
                                  #  - backups: manual vs snapshots (can use MMS) vs mongodump, delayed replica
                                  #  - migration: export|import as JSON or CSV
                                  #  - monitoring:
                                  #     - stat commands, mongotop|mongostat, MMS (graph|text)
                                  #     - alerts (MMS)
                                  #     - logging (can use MMS), profiling (can use MMS)
                                  #  - debugging:
                                  #     - bsondump
                                  #     - mongosniff
                                  #  - server management (MMS)
                                  #Clients:
                                  #  - Mongo shell (JavaScript)
                                  #  - Node.js driver, Mongoose
                                  #  - Robomongo, Genghisapp
                                  #  - MMS

UPGRADE ==>                       #Should upgrade in this order:
                                  #  - drivers
                                  #  - mongos: first disable balancer, then upgrade each mongos
                                  #  - config servers: starting with last one listed in configDB
                                  #  - mongod:
                                  #     - replica sets: starts with secondaries, then make failover to upgrade primary
                                  #How:
                                  #  - using OS mechanics (e.g. apt-get)
                                  #  - or using d,s--upgrade
                                  #  - or MMS
                                  #Can check upgrade minor number upgrades problems before starting with:
                                  #  - 2.4 to 2.6: DB.upgradeCheck[AllDBs]() and &&authSchemaUpgrade()
                                  #  - 2.6 to 3.0: DB.upgradeCheck[AllDBs]() and &&authSchemaUpgrade()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            DESIGN             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SCHEMA ==>                        #Schemaless, i.e.:
                                  #  - columns be added|removed runtime
                                  #  - stores OBJ.VAR names for each OBJ (not only OBJ.VAL), so should not be too long
                                  #  - columns can also mix types

NORMALIZATION ==>                 #Can choose to either include entities references as OID or to include them as sub-OBJ.
                                  #Is similar to normalization|denormalization tradeoffs.
                                  #Comparison:
                                  #  - performance:
                                  #     - OID -> lower network performance: needs several round-trip to the server
                                  #     - sub-OBJ -> (mmapv1 only) lower I/O performance (one|many-to-many relationship only):
                                  #        - embedded OBJ_ARR changes over time
                                  #        - but MongoDB is faster when documents don't change size
                                  #          (needs reallocate space otherwise, which is slow)
                                  #  - ACID:
                                  #     - OID -> lower atomicity: operations are atomic at the OBJ (and sub-OBJ) level only
                                  #     - sub-OBJ -> lower consistency (many-to-many relationship only):
                                  #        - denormalization problems, since updating one entity requires updating all
                                  #  - simplicity:
                                  #     - OID -> more complex, since application needs to rebuild the different pieces together
                                  #     - sub-OBJ -> harder to represent complex relationship/hierarchical patterns
                                  #  - scalability:
                                  #     - sub-OBJ -> can hit maximum OBJ_ARR size (one|many-to-many relationship only)
                                  #Conclusion:
                                  #  - for one-to-one relationship (except with complex relationship|hierarchical patterns),
                                  #    sub-OBJ is always better
                                  #  - for one-to-many, sub-OBJ is better as long as OBJ_ARR will not hit max OBJ_ARR size
                                  #  - for many-to-many, prefer references, unless performance is critical and application
                                  #    tries to guarentee consistency


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           NOTATION            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


NOTATION ==>                      #This doc uses the following notations.

VAR#TYPE,...                      #Means { VAR,... }
CMD()#VAR                         #CMD(...) first OBJ argument, key VAR
                                  #If no OBJ specified in definition, means OBJ as last argument.

VARR                              #Can be:
                                  #  - "VAR[.VAR2[...]]"
                                  #  - "ARR[.NUM][.VAR2[...]]": no NUM means any
VARR$                             #Like VARR but NUM can also be "$", meaning:
                                  #  - the first element matching Q_OBJ, and Q_OBJ must include ARR.
                                  #  - if Q_OBJ uses Q$ne(), Q$not(), Q$nor() or Q$nin(), must be wrapped in $elemMatch
VAR$                              #Same as VARR$ but NUM can *only* be "$"
VARRR                             #"VAR[.VAR2[...]]" or VAR: { VAR2: VAL }
AGG_EXPR                          #Can be:
                                  #  - any VAL.
                                  #    If STR starting with "$", must be written { "$literal": STR }
                                  #  - "$VARR"
                                  #    If VARR does not exist, no error, just undefined
                                  #  - "$$CURRENT": current OBJ
                                  #  - O$CMD(AGG_EXPR...)
                                  #  - O$literal(VAL): like VAL, but avoid syntax ambiguity to interpret it as an AGG_EXPR,
                                  #    on some edge cases
                                  #Used with &aggregate()

COMMANDS ==>                      #
COMMAND(...)                      #Run in mongo shell
&COMMAND([VAL[, VAR#TYPE...]])    #  - Mongo shell: DB.runCommand(OBJ)
                                  #  - Node.js: ~DB.command(OBJ[, OBJ2])
                                  #Where OBJ:
                                  #  - is { COMMAND: VAL, VAR... }
                                  #  - if no VAL, means any works (usually use 1)
&&COMMAND(...)                    #Same but require to be in admin DB.
                                  #Can also use:
                                  #  - Mongo shell: SH._adminCommand(OBJ[, BOOL]): like DB.runCommand(OBJ) but:
                                  #     - run in admin DB
                                  #     - if true, must connect to mongos
                                  #  - Node.js: ~DB.executeDbAdminCommand(OBJ[, OBJ2]) or ~ADMIN.command(OBJ[, OBJ2])
~COMMAND(...)                     #Node.js driver. When using a class, actually prepended by require("mongodb"), e.g.
                                  #new ~MongoClient is "new (require('mongodb')).MongoClient"
[c|p]^COMMAND(...)                #Mongoose driver. When using a class, actually prepended by require("mongoose")
                                  #  - c means: if not using final callback FUNC(ERROR,[A]CUR.toArray()), rreturns [A]CUR
                                  #  - p means: if not using final callback FUNC(ERROR, VAL), rreturns PROMISE, with methods:
                                  #     - fulfill(VAL), reject(ERROR), resolve(ERROR, VAL)
                                  #     - then(FUNC(VAL)[, FUNC(ERROR)])
                                  #     - end(): to use after then() if then() can throw error, so it propagates.

QQ$CMD(BOOL...)                   #{ $CMD: Q_OBJ_ARR }, where each Q_OBJ is transtyped to BOOL.
                                  #Used in queries.
Q$CMD(OBJ.VAR, VAL...)...         #{ VARR: { $CMD: VAL[_ARR], ... } }, used in queries.
P$CMD(OBJ.VAR, VAL...)...         #{ VAR$: { $CMD: VAL[_ARR], ... } }, used in queries.
C$CMD(VAL)                        #Can be:
                                  #  - { $query: Q_OBJ, $CMD: VAL } instead of Q_OBJ.
                                  #  - [A]CUR._addSpecial( CMD_STR, VAL )
                                  #  - ~CUR.addQueryModifier( "$CMD", VAL ): rreturns CUR
                                  #  - ~COLL.findOne()#CMD_VAR
                                  #Perform actions on returned cursors.
U$CMD(OBJ.VAR, VAL)...            #{ $CMD: { VARR: VAL, ... } }, used with write methods.
S$CMD(OBJ, VAL)                   #{ $CMD: VAL }, applied to current OBJ. Used with &aggregate()
O$CMD(VAL...)                     #{ $CMD: AGG_EXPR[_ARR] }, used with &aggregate()

COMMAND LINE ARGUMENTS ==>        #
ds--VAR                           #mongod|mongos --setParameter VAR=VAL
                                  #Can also runtime:
                                  #  - &setParameter(VAR#VAL)
                                  #  - &getParameter(VAR#1)
d--VAR                            #mongod
s--VAR                            #mongos
o--VAR                            #mongo
dmp--VAR                          #mongodump
rst--VAR                          #mongorestore
bsn--VAR                          #bsondump
opl--VAR                          #mongooplog
imp--VAR                          #mongoimport
exp--VAR                          #mongoexport
sta--VAR                          #mongostat
top--VAR                          #mongotop
fil--VAR                          #mongofiles
snf--VAR                          #mongosniff
prf--VAR                          #mongoperf

mnt--VAR                          #mongodump, mongorestore, mongooplog, mongoimport, mongoexport, mongofiles
cli--VAR                          #Same as mnt + mongostat, mongotop, mongod, mongos
inout--VAR                        #Same as mnt + mongostat, mongotop, mongo

OTHER ==>                         #
cf--VAR                           #Specified in YAML file specified with mongod|mongos --config FILE
cn--VAR                           #Specified as query variables to CONN_STR


RETURN VALUE ==>                  #  - "returns VAL": use final callback FUNC(ERROR, VAL)
                                  #    If returns nothing, callback FUNC(ERROR) can still be used.
                                  #  - "rreturns VAL": returns VAL
                                 ##Can also use MONGOOSE-Q() (version 0.0.13) instead of MONGOOSE so that if Func()
                                 ##"returns VAL", FuncQ() (e.g. MODEL.removeQ()) rreturns Q PROMISE instead


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            BASICS             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DB.runCommand({COMMAND:VAL}, ... )#Often a wrapper function VAL.COMMAND(...) exist.
                                  #Return an OBJ, with (except if if in Mongo shell):
                                  #  - ok#1
                                  #  - ok#0, errmsg#STR, code#NUM

OBJ                               #Uses BSON:
                                  #  - physically stored in a compact form, "binary"
                                  #     - Mongo shell can use Object.bsonsize(OBJ)
                                  #  - use extended JSON, i.e. adds the following types:
                                  #     - FUNC[_SCOPE]:
                                  #        - _SCOPE keeps the scope (e.g. &mapReduce())
                                  #        - With Node.js:
                                  #           - cannot &insert() FUNC, unless using either:
                                  #              - ~MONGO.connect()#db.serializeFunctions true
                                  #              - ~DB.collection|createCollection()#serializeFunctions true
                                  #              - ~COLL.insert*|bulkWrite()#serializeFunctions true
                                  #           - returned as { code FUNC_STR, scope OBJ }
                                  #     - DOUBLE: def for numbers
                                  #     - INT: 32 bits, constructed with NumberInt(INT)
                                  #     - LONGLONG: 64bits
                                  #        - constructed with:
                                  #           - Mongoshell: NumberLong(LONGLONG).
                                  #           - Node.js: ~new Long(NUM, NUM2): where each NUM is 32 bits of data.
                                  #             Has many methods for calculation and conversion (see online doc)
                                  #        - can use ~MONGO.connect()#db.promoteLongs true (def) to convert to DOUBLE
                                  #     - OID:
                                  #        - Object ID, 12 bytes with:
                                  #           - 1-4: Epoch time (seconds-precise)
                                  #           - 5-7: machine ID
                                  #           - 8-9: PID
                                  #           - 10-12: counter, with random initial value
                                  #        - As opposed to UUID:
                                  #           - less uniqueness but unique as long as always < 1.6 million records/secs,
                                  #             per mongod
                                  #           - provides info about origin and ctime
                                  #        - with methods:
                                  #           - constructors (def: random) rreturning OID:
                                  #              - ObjectId([HEXA_STR])
                                  #              - ~ObjectId.createFromHexString(HEXA_STR)
                                  #              - ~ObjectId.createFromTime(EPOCHNUM)
                                  #              - new ~ObjectId([OID])
                                  #              - new ^Types.ObjectId()
                                  #           - OID.valueOf() or ~OID.toHexString(): rreturns as HEXA_STR
                                  #           - OID.toString(): returns as ObjectId("HEXA_STR")
                                  #           - OID.getTimestamp() or ~OID.getTimestamp(): rreturns date as DATE
                                  #           - ~OID.generationTime: date as EPOCH_NUM
                                  #           - ~ObjectId.isValid(OID): rreturns BOOL
                                  #           - ~OID.equals(OID2): rreturns BOOL
                                  #     - DBREF:
                                  #        - not an actual type, but a simple OBJ convention, working with some drivers
                                  #        - prefer OID, because simpler. But DBREF adds COLL+DB info.
                                  #        - Is:
                                  #           - Mongo shell: OBJ with: $id OID, $ref COLL[, $db DB]
                                  #           - Node.js: ~new DBRef('COLL', OID[, 'DB'])
                                  #     - DATE: is different from JavaScript:
                                  #        - signed 64 bits INT (negative is before Epoch),
                                  #          millisecond-precise (range: 290 million years past|future)
                                  #        - methods: like JavaScript
                                  #     - TIMESTAMP:
                                  #        - 4 bytes of Epoch time, then 4 bytes of counter for a given second
                                  #        - constructor:
                                  #           - Mongo shell: new Timestamp([NUM, NUM2])
                                  #           - Node.js: ~new Timestamp(NUM, NUM2): for each 4 bytes
                                  #             Has many methods for calculation and conversion (see online doc)
                                  #        - prefer using DATE
                                  #     - BIN: binary data, with methods:
                                  #        - new BinData(TYPE_INT, BASE64_STR)
                                  #        - ~new Binary(BUFFER|BASE64_STR, Binary.SUBTYPE_*), where * can be:
                                  #           - DEFAULT (0)
                                  #           - FUNCTION (1): FUNC type
                                  #           - BYTE_ARRAY (2)
                                  #           - UUID (4)
                                  #           - MD5 (5)
                                  #           - USER_DEFINED (128)
                                  #        - ~BIN.buffer: rreturns as BUFFER
                                  #        - ~BIN.value(): rreturns as STR
                                  #        - ~BIN.length(): rreturns NUM
                                  #        - ~BIN.read([NUM[, NUM2]]): rreturns BUFFER. NUM is position. NUM2 length.
                                  #        - ~BIN.write(STR|BUFFER[ NUM])
                                  #        - ~BIN.put(STR): single char
                                  #  - those extra types can be represented as normal JSON:
                                  #     - use an OBJ instead, e.g. { "$oid": HEXA_STR }, etc.
                                  #     - understood in input by REST interface, mongoimport, --query, but not mongo
                                  #     - output by mongoexport and REST interface
                                  #  - OBJ comparison is always exact comparison, including order.
                                  #    Can use VARR sometimes to do partial comparison.
                                  #  - null and undefined:
                                  #     - read operations:
                                  #        - (explicit) VAR: undefined -> error
                                  #        - null -> null|undefined:
                                  #           - skips sparse index unless using [A]CUR.hint(), in which case -> null only
                                  #        - Q$exists() -> undefined
                                  #        - Q$type(VAR, 0) -> undefined
                                  #        - Q$type(VAR, 10) -> null
                                  #     - write operations:
                                  #        - (explicit) VAR: undefined -> null
                                  #        - null -> null
                                  #Limits:
                                  #  - OBJ max. 16MB (should use GridFS)
                                  #     - for INDEX: only 1MB
                                  #     - for shard key: only 512 bytes
                                  #     - when output can be written to a COLL (e.g. &aggregate() or &mapReduce()), can use
                                  #       incremental actions, i.e. querying only a specific range of input,
                                  #       then merge output COLLs
                                  #  - 100 sub-OBJs levels
                                  #  - OBJ_ARR: max length 1000 for write operations, unless using BULK

bsondump BSON_FILE                #Outputs a BSON file as JSON. Used usually to understand better mongoimport output.
                                  #Can use --pretty
                                  #Use bsn--type "debug" for verbose.

_id                               #Field automatically added by write methods if omitted (according to &create() autoIndexId),
                                  #with an index on it.
                                  #Drivers can create it to avoid server doing it:
                                  #  - ~MONGO.connect()#db.forceServerObjectId or ~COLL.insert*()#forceServerObjectId BOOL:
                                  #    if false (def), driver adds _id when missing
                                  #  - ^SCHEMA_OPT._id BOOL: same but must be true (def)
                                  #  - ~MONGO.connect()#db.pkFactory or ~DB.collection|createCollection()#pkFactory.createPk()
                                  #    ->VAL when forceServerObjectId false, how _id are generated (def: return new ObjectId())
                                  #By def. an OID, but can also choose:
                                  #  - UUID:
                                  #     - with helper UUID(STR), that returns it as a BIN (much smaller)
                                  #       Can change UUID() with cn--uuidRepresentation
                                  #       "standard|csharpLegacy|javaLegacy|pythonLegacy"
                                 ##     - can use Mongoose plugin MONGOOSE-UUID (version 0.0.2), which takes PL_OBJ.models
                                 ##       MODEL_STR_ARR (must use SCHEMA_OPT._id false). Use UUID v1 (time-based)
                                  #  - serial number:
                                 ##     - can use Mongoose plugin MONGOOSE-AUTO-INCREMENT (version 3.2.0), with PL_OBJ:
                                 ##        - model MODEL_STR
                                 ##        - field VAR_STR (def: "_id")
                                 ##        - startAt NUM (def: 0), incrementBy NUM (def: 1)
                                 ##       Also creates:
                                 ##        - MOBJ.nextCount(): returns NUM
                                 ##        - MOBJ.resetCount(): returns nextCount()

&listCommands()
  DB.listCommands()               #Returns all &CMD() as OBJ|STR_ARR
  help                            #Returns Mongo shell help
DB|RS|SH|PLANCACHE.help()         #Returns all DB|RS|SH|PLANCACHE.* as STR_ARR
DB.commandHelp(CMD_STR)           #Print help for &CMD()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           DATABASES           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DB                                #Sets of COLL
db                                #Current DB
use 'DB'                          #Same as db = DB.getSiblingDB('DB')

&&listDatabases()                 #Returns:
 ~ADMIN.listDatabases()           #  - databases OBJ_ARR
                                  #     - name 'DB'
                                  #     - sizeOnDisk UINT (in bytes)
                                  #     - empty BOOL
                                  #  - totalSize NUM
DB.getName()                      #
 ~DB.databaseName                 #
 ^DB.name                         #Rreturns 'DB'
DB.getSiblingDB('DB')
 ~DB.db('DB')
 ^DB.useDb('DB')                  #Rreturns DB
&&copydb(fromdb#STR, todb#STR
[, fromhost#HOST[:PORT]]
[, username#STR][, key#STR]
[, slaveOk#BOOL])
  DB.copyDatabase(STR, STR
  [, HOST[:PORT], STR, STR])      #Copy from another host or (def) same host
&clone("DB.HOST[:PORT]")
  DB.cloneDatabase("HOST[:PORT]") #Same with simpler syntax
&dropDatabase()
  DB.dropDatabase()
 ~DB.dropDatabase()               #Needs to also run &dropAllUsers|RolesFromDatabase()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          COLLECTIONS          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COLL                              #Are just OBJ_ARR, but similar to SQL tables.
                                  #'COLL' can contain "."
DB.COLL                           #Rreturns COLL. No need to escape "." in COLL
DB.getCollection('COLL')          #Don't use DB.COLL or ^DB.collections.COLL when COLL is a MongoDB reserved word
 ~DB.collection('COLL'[, OBJ])    #If OBJ.strict true:
 ^DB.collection('COLL'[, OBJ])    #  - returns COLL (instead of rreturning it)
 ^DB.collections.COLL             #  - non existing COLL produces ERROR. Otherwise, creates it.
 ^MODEL.collection                #^DB.collection() calls createCollection('COLL', OBJ) if not existing.

&create('COLL'[, capped#BOOL]     #Creates a COLL. Usually not needed since &insert() does it implicitely.
[, autoIndexId#BOOL][, size#NUM]  #  - capped BOOL (def: false): creates as capped COLL. See below
[, max#NUM][, flags#NUM]          #  - size|max: with capped COLL. See below
[, noPadding#BOOL])               #  - autoIndexId BOOL (def: true): auto creates _id
  DB.createCollection             #  - flags 0-3 (def: 1): (not with ~DB.createCollection())
  ('COLL'[, ...])                 #      - 1|3: same as powerOf2Sizes true (see below)
 ~DB.createCollection             #      - 2|3: same as noPadding true (see below)
  ('COLL'[, OBJ])                 #  - noPadding BOOL (def: false) (see below)
 ^SCHEMA_OPT.capped.size|max      #~DB.createCollection() can use OBJ.strict BOOL (like DB.collection())
                                  #Rreturns COLL
~COLL.options()                   #Returns null or { capped, autoIndexId, size, max, flags }

&cloneCollection('COLL',
from#HOST[:PORT][, query#Q_OBJ])
  DB.cloneCollection(HOST[:PORT],
  'COLL'[, Q_OBJ])                #
&&renameCollection
("DB.COLL", to#"DB[2].COLL[2]"
[, dropTarget#BOOL])
  COLL.renameCollection
  ("COLL2"[, BOOL])
 ~DB.renameCollection('COLL',
  'COLL2'[, dropTarget#BOOL])
 ~COLL.rename                     #Error if COLL2 exists unless BOOL true.
  ('COLL2'[, dropTarget#BOOL])    #Rreturns COLL
&listCollections([filter#Q_OBJ])  #Rreturns infos for each coll as OBJ2_ARR:
  DB.getCollectionInfos()         #  - name STR
  ~DB.listCollections()           #  - options:
                                  #     - capped BOOL, size NUM, max NUM
                                  #     - autoIndexId BOOL
                                  #     - flags NUM
                                  #     - storageEngine STR
                                  #&listCollections() rreturns RAW_CUR
  DB.getCollectionNames()         #Returns 'COLL'_ARR (including system.*)
  ~DB.collections()               #Returns COLL_ARR (no system.*)
&drop('COLL')
 ~DB.dropCollection('COLL')
  COLL.drop()
 ~COLL.drop()                     #Returns BOOL: false if problem, e.g. did not exist.

&collMod('COLL'[,noPadding#BOOL]  #Changes some COLL properties.
[, index#OBJ])                    #OBJ is { keyPattern#INDEX_STR, expireAfterSeconds#NUM }


CAPPED COLLECTIONS ==>            #COLL that works as a FIFO: preserve insertion order, but must sort according to
                                  #$natural: -1 in queries
                                  #Created with &create() capped true, then specifying:
                                  #  - size NUM (max size in bytes, min 4KB)
                                  #  - and (optional) max NUM (max number of OBJs)
                                  #Restrictions:
                                  #  - cannot remove OBJs
                                  #  - updates must not change OBJ size
                                  #Tailable cursor:
                                  #  - CUR with CUR_CONF tailable and CUR_CONF awaitData
                                  #    (or ^CUR.tailable(), ^CUR.setOptions()#tailable)
                                  #  - do not use index, and wait for incoming data on a capped COLL
                                  #  - in Mongo shell, if at the end and no more coming after 2 secs, stops reading
COLL.isCapped()
 ~COLL.isCapped()                 #Returns BOOL
&convertToCapped
('COLL', size#NUM)                #
&cloneCollectionAsCapped('COLL',
toCollection#'COLL2', size#NUM)   #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           CRUD READ           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COLL.find([Q_OBJ[, P_OBJ]])       #Rreturns CUR iterating over some OBJs of COLL:
 ~COLL.find([Q_OBJ])              #  - filter only the OBJs matching Q_OBJ (def: all):
 c^MODEL.find([Q_OBJ[,P_OBJ|STR]])#     - QQ$CMD(BOOL...)->BOOL2
 ^CUR.find(Q_OBJ)                 #     - Q$CMD(OBJ.VAR, VAL...)->BOOL2
                                  #        - if several Q$CMD() for same OBJ.VAR, results are and'd
                                  #           - i.e. OBJs matching all CMDs
                                  #        - if field is ARR, runs agains every element, and results are or'd
                                  #           - i.e. OBJ "matches" if at least one ARR element matches CMD
                                  #        - if several Q$CMD() for same OBJ.VAR are matched by at least one in ARR:
                                  #           - OBJ "matches" if each CMD is matches by at least one ARR element
                                  #           - unless using Q$elemMatch(CMD...) or ^CUR.elemMatch(Q_OBJ): OBJ "matches" if
                                  #             all CMD at once are matched by at least one ARR element
                                  #     - VARR: VAL: same as Q$eq(OBJ.VAR, VAL)
                                  #     - empty: means all
                                  #  - transform the OBJs fields according to P_OBJ:
                                  #     - VAR$: 1|0: filter OBJs fields. Must use only VAR$: 1 or only VAR$: 0
                                  #       (exception: can use _id: 0 with other VAR: 1).
                                  #     - P$CMD(OBJ.VAR, VAL...)->VAL2, return OBJ.VAR as VAL2 instead
                                  #c^MODEL.find() can use last argument OBJ same as ^CUR.setOptions()
COLL.findOne([Q_OBJ[, P_OBJ]])
 ~COLL.findOne([Q_OBJ[, OBJ]])
 c^MODEL.findOne
  ([Q_OBJ[, P_OBJ|STR]])
 ^CUR.findOne([Q_OBJ])            #Like COLL.find(...).limit(1), but returns OBJ not rreturns CUR
 c^MODEL.findById                 #OBJ has members limit NUM, sort S_OBJ, fields P_OBJ, skip NUM
 (OID[_STR][, P_OBJ|STR])         #c^MODEL.findOne|findId() can use last argument OBJ same as ^CUR.setOptions()
&parallelCollectionScan           #Like COLL.find|aggregate({}), but returns NUM CUR, with part of COLL assigned to each.
('COLL', numCursors#NUM)          #For Node.js, returned as CUR_ARR
 ~COLL.parallelCollectionScan     #For Mongo shell, returned as (not useful):
  ([S$CMD_ARR, ]numCursors#NUM)   #  - cursors OBJ_ARR:
                                  #     - cursor:
                                  #        - firstBatch OBJ_ARR
                                  #        - ns "DB.COLL"
                                  #        - id LONG_LONG
                                  #     - ok BOOL
                                  #Goal: increase throughput for big COLLs with concurrency.

Q$gt|lt[e]|eq|ne(VAL, VAL2)
 ^CUR.gt|lt[e]|equals|ne(VAL2)    #VAL >|>=|<|<=|==|!=
 ^CUR.where(VARR, VAL2)           #STR comparison is byte-wise.
Q$[n]in(VAL, ARR)                 #VAL ==|!= any(ARR)
 ^CUR.in(ARR)                     #ARR can be REGEXP_ARR to mean VAL matching any REGEXP pattern.
Q$exists(VAL, BOOL)
 ^CUR.exists(BOOL)                #VAL is [not] undefined (according to BOOL).

QQ$and(BOOL...)
 ^CUR.and(BOOL_ARR)               #Instead of $and: [ { ... }, { ...2 } ], using { ..., ...2 } is a shortcut in most cases.
QQ$or(BOOL...)
 ^CUR.or(BOOL_ARR)                #
QQ$nor(BOOL...)
 ^CUR.nor(BOOL_ARR)               #Like QQ$and(Q$not(...))
Q$not(BOOL)                       #

Q$type(VAL, NUM)                  #Checks typeof VAL, where NUM can be:
                                  #  - 0: undefined
                                  #  - 1: DOUBLE
                                  #  - 2: STR
                                  #  - 3: OBJ
                                  #  - 4: ARR
                                  #  - 5: BIN
                                  #  - 7: OID
                                  #  - 8: BOOL
                                  #  - 9: DATE
                                  #  - 10: null
                                  #  - 11: REGEXP
                                  #  - 13: FUNC
                                  #  - 14: SYM
                                  #  - 15: FUNC[_SCOPE]
                                  #  - 16: INT
                                  #  - 17: TIMESTAMP
                                  #  - 18: LONG_LONG

Q$mod(UINT, [ UINT2, UINT3 ])     #UINT % UINT2 == UINT3
 ^CUR.mod([ UINT2, UINT3 ])       #[ UINT2, UINT3 ] is an ARR.

Q$regex(STR, REGEXP_STR) [+       #Syntax 1) can use also Perl REGEXP ("PCRE")
  Q$options(STR, FLAGS_STR)]      #Both syntaxes can use those flags:
Q$regex(STR, REGEXP)              #  - "i": case insensitive
 ^CUR.regex(REGEXP)               #  - "m": makes ^ and $ take into account \n (match start|end of newlines, not of STR2)
                                  #Syntax 1)
                                  #  - can also use flags:
                                  #     - "s": make . match \n
                                  #     - "x": ignore whitespaces in REGEXP, except if escaped or in [].
                                  #       Also, can include comment with # (up to next newline in REGEXP)
                                  #  - cannot be used with QQ$not()
                                  #Index:
                                  #  - Must use /^/ at beginning
                                  #  - Remove /.*/ or /.*$/ from end for best performance

Q$all(ARR, ARR2)
 ^CUR.all(ARR2)                   #ARR2 is contained by ARR
Q$size(ARR, NUM)
 ^CUR.size(NUM)                   #ARR.length == NUM

P$elemMatch(ARR, Q_OBJ)           #Returns first ARR element matching Q_OBJ (undefined if none).
P$slice(ARR, [ NUM, NUM2 ])       #Returns ARR.slice(NUM, NUM2)
 ^CUR.slice([ NUM, NUM2 ])        #[ NUM, NUM2 ] is an ARR.
P$slice(ARR, NUM)
 ^CUR.slice(NUM)                  #Same as P$slice(ARR, [0, NUM])


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            CURSORS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CUR                               #Cursor to return OBJ by OBJ of an OBJ_ARR.
                                  #Streaming:
                                  #  - ~[A]CUR:
                                  #     - is an ISTREAM in Object mode (see ~CUR.stream() for not Object mode).
                                  #     - can use ~CUR.stream(transform#FUNC(OBJ)->VAL) to add a transform function:
                                  #        - e.g. to return JSON.stringify(OBJ) instead of normal object Mode
                                  #  - ^CUR.stream([transform#FUNC])->ISTREAM
                                  #Batches:
                                  #  - CUR iterations are cached to avoid each iteration to communicate with server
                                  #  - this is transparent to applications
                                  #  - can use CUR_CONF exhaust to make it faster when iterating over the whole cursor
                                  #    (usually done by drivers)
                                  #In Mongo shell, when returning a CUR without assigning to a variable:
                                  #  - assign to variable "it" (not real CUR, can just print it)
                                  #  - iterates 20 times and prints the results
                                  #    ("shell batches", different from normal batches).
                                  #    Can assign DBQuery.shellBatchSize = NUM in the shell to change from 20.
                                  #CUR_CONF must use:
                                  #  - [A]CUR.addOption(DBQuery.Option.VAR)
                                  #  - ~CUR.addCursorFlag(VAR_STR, BOOL): rreturns CUR
                                  #Cursor closes after 10mins of idleness unless CUR_CONF noCursorTimeout
^[A]CUR                           #Must finally:
                                  #  - call ^[A]CUR.exec(), returning VAL or rreturning PROMISE
                                  #  - or add final FUNC(ERROR,VAL) to ^CUR.count|distinct|find*|update|remove(), returning VAL
                                  #When using Q$CMD(OBJ.VAR, VAL) (not Q$CMD(VAL)), ^CUR must:
                                  #  - first initialize VARR with:
                                  #     - ^CUR.where(VARR_STR)
                                  #     - or ^MODEL.where(VARR_STR)->CUR, which will execute ^CUR.count|distinct|find*
                                  #       |update|remove() (def: find())
                                  #  - or add VARR_STR as first argument to ^CUR.elemMatch|regex|size|slice|all|exists|
                                  #    gt[e]|lt[e]|ne|[n]in|near|circle|polygon|maxDistance|mod()
                                  #All ^[A]CUR.FUNC() rreturns ^[A]CUR unless noted otherwise.
RAW_CUR                           #OBJ returned by some &CMD(), with OBJ.cursor:
                                  #  - id LONG
                                  #  - ns "DB.$cmd.CMD.COLL"
                                  #  - firstBatch OBJ_ARR
                                  #This cannot be manipulated directly through mongo, must use drivers.

[A]CUR.hasNext()                  #Returns true or NUM if more OBJ
[A]CUR.next()                     #
 ~[A|C]CUR.next()                 #Returns next OBJ or throw exception (Mongo shell) / returns null (Node.js) if none.
~[A|C]CUR.rewind()                #Put the CUR back to first element, and rreturns it
~[A|C]CUR.close()                 #
~[A|C]CUR.isClosed()              #Rreturns BOOL

~[A|C]CUR.clone()                 #Rreturns copy
 ^CUR.toConstructor()             #Returns a FUNC()->CUR2, where CUR2 is a clone of CUR
^CUR.merge(CUR2)                  #

[A]CUR.toArray()                  #
 ~[A|C]CUR.toArray()              #Returns as OBJ_ARR
[A]CUR.forEach(FUNC(OBJ))         #
 ~CUR.forEach(FUNC(OBJ))          #
[A]CUR.map(FUNC(OBJ)->VAL)        #

CUR.skip(NUM)
 ~[A]CUR.skip(NUM)
 ^CUR.skip(NUM)                   #Skips next NUM OBJs. O(n) complexity.
 ^CUR.setOptions()#skip           #Rreturns [A]CUR.
CUR.limit(INT)                    #Keeps only next INT OBJs.
 ~[A]CUR.limit(INT)               #If INT is 0, keep all.
 ^CUR.limit(INT)                  #If INT is negative, same as positive, except cuts the results so it fits in a single batch.
 ^CUR.setOptions()#limit          #Rreturns [A]CUR.

~CUR.filter(Q_OBJ)
 ^CUR.where(Q_OBJ)                #Adds Q_OBJ to query. Rreturns [A]CUR
~[A]CUR.project(P_OBJ)            #Adds P_OBJ to query. Rreturns [A]CUR.
 ^CUR.select(P_OBJ|P_STR)         #P_STR is "[-|+]VARR ..."

C$orderby(S_OBJ)                  #Sort OBJs according to S_OBJ being { VAR: 1|-1, ... }.
CUR.sort(S_OBJ)                   #ARR are sorted according to their min|max values.
 ~[A]CUR.sort(S_OBJ)              #Output must be <= 32MB (including further limit()), unless this is an indexed sort.
 ^CUR.sort(S_OBJ|S_STR)           #Sorting always applied before projection (P_OBJ).
 ^CUR.setOptions()#sort           #Can also use { $natural: 1|-1, ... } being an internal order usually representing the last
                                  #updated OBJs last (and otherwise the default order).
                                  #S_STR is "[-]VARR ..."
                                  #Rreturns CUR.
                                  #If mixed types, use sorting order:
                                  #  undefined < null < NUM < STR|SYM < OBJ < ARR < BIN < OID < BOOL < DATE < TIMESTAMP < REGXP
C$showDiskLoc()                   #Add OBJ.$diskLoc { file NUM, offset NUM2 } to the output, giving info about on-disk location
CUR.showDiskLoc()                 #of the underlying documents (same as $natural order)
                                  #Returns CUR.

CUR.batchSize(NUM)
 ~[A|C]CUR.batchSize(NUM)
 ^CUR.batchSize(NUM)
 ^CUR.setOptions()#batchSize
 ~COLL.parallelCollectionScan|    #Changes batches size. Def. is min(101 OBJs, 1MB) for first batch, 4MB for next ones.
  listIndexes()#batchSize         #Rreturns [A|C]CUR.

CUR.objsLeftInBatch()             #Returns number left before needing to communicate again with the server.

C$maxScan(UINT)                   #Stops scanning after INT OBJs scanned.
 ^CUR.maxScan(UINT)               #Scanning happens during find() on all COLL OBJs (as opposed to limit()), and only if the
 ^CUR.setOptions()#maxscan        #index was not enough for the query.
                                  #Returns CUR.
C$maxTimeMS(NUM)
CUR.maxTimeMS(NUM)
 ~[A|C]CUR.maxTimeMS(NUM)
 ~CUR.count()#maxTimeMS
 ~DB.[executeDbAdmin]command()
  #maxTimeMS                      #Keeps a cumulative time counter for each iteration/operation done on CUR, and kills the
 ~ADMIN.command()#maxTimeMS       #cursor when above NUMms
 ~COLL.findOne*()#maxTimeMS       #Rreturns CUR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          AGGREGATION          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


&aggregate('COLL',                #Similar to find() but performs transformations:
pipeline#S$CMD_ARR[,explain#BOOL] #  - each COLL's OBJ is transformed to OBJ2 using S$CMD(OBJ, VAL)->OBJ2.
[,allowDiskUse#BOOL][,cursor#OBJ])#    Can use the same CMD several times at different places
  COLL.aggregate                  #  - except for S$group(), each OBJs is transformed independently from other OBJs
  (S$CMD_ARR[, explain#BOOL]      #Also has:
  [,allowDiskUse#BL][,cursor#OBJ])#  - explain: if true, return info about aggregation processing through explain()
 ~COLL.aggregate(S$CMD_ARR[, ...])#  - allowDiskUse: if true, let write to dbPath + "/_tmp".
   ~ACUR.S$CMD(VAL)               #    Slower than using RAM, but allow exceeding 100MB/stage RAM limit.
 c^MODEL.aggregate(S$CMD[_ARR])   #  - cursor OBJ:
   ^ACUR.append(S$CMD[_ARR])      #     - batchSize NUM: change CUR batch size of only first batch.
   ^ACUR.S$CMD(VAL)               #       Using 0 is helpful to get server problems fast without further processing.
   ^ACUR.allowDiskUse(BOOL)       #Return value can be either:
   ^ACUR.cursor(OBJ)              #  - S$out(OBJ, 'COLL') (last S$CMD() only): creates/overwrites 'COLL'.
                                  #    'COLL' cannot be capped.
                                  #  - &aggregate()|~COLL.aggregate():
                                  #     - not using cursor OBJ (even empty {}): returns CUR.toArray()
                                  #     - using cursor OBJ:
                                  #        - Mongo shell: does not work
                                  #        - Node.js: rreturns ~ACUR
                                  #  - COLL.aggregate(): returns ACUR
                                  #  - ^MODEL.aggregate(): rreturns ACUR, returns OBJ_ARR
                                  #Note on ^ACUR.S$CMD(VAL):
                                  #  - ^ACUR.sort() and ^ACUR.select() can use S_STR and P_STR
                                  #  - geoNear() is near()

PERFORMANCE ==>                   #Can optimize by arranging S$CMD() order:
                                  #  - S$match(): If put first, can use indexes.
                                  #  - S$match|limit|skip(): should be early.
                                  #  - S$sort(): before S$project(), S$unwind() or S$group() to be able to use indexed sorts.
                                  #Some reordering is automatically done by optimizer.
                                  #On a sharded COLL, first S$CMD() after S$match|limit|skip() is run on several shards, but
                                  #later stages are performed on the primary shard.

S$match(OBJ, Q_OBJ)               #Like find() Q_OBJ.
                                  #Restrictions on Q_OBJ:
                                  #  - cannot use Q$where()
                                  #  - cannot use Q$text() unless first S$CMD()
S$limit(OBJ, NUM)                 #Like CUR.limit(NUM)
S$skip(OBJ, UINT)                 #Like CUR.skip(UINT)
S$sort(OBJ, S_OBJ)                #Like CUR.sort(S_OBJ)
S$project(OBJ, P_OBJ)             #Like find() P_OBJ, but:
                                  #  - can only use { VAR: 0 } with _id
                                  #  - cannot use P$CMD() nor C$CMD()
                                  #  - can use { VARRR: AGG_EXPR }, to assign AGG_EXPR to VARRR
                                  #    If AGG_EXPR is 1|0|BOOL|STR must be written { "$literal": 1|0|BOOL|STR }
S$group(OBJ, OBJ2)                #As opposed to other S$CMD(), operates on all OBJs at once:
                                  #  - grouped by OBJ2._id AGG_EXPR (can be null for only one group)
                                  #  - outputs OBJ2, { VAR: { CMD: AGG_EXPR } ... }, where OBJ2.VAR = CMD(AGG_EXPR...),
                                  #    AGG_EXPR... being AGG_EXPR for all OBJs at once in each group.
                                  #    OBJ2._id is included too.
                                  #    CMD() can be:
                                  #     - $first|last(VAL...)
                                  #     - $min|max(NUM...): null|undefined are ignored unless all VAL... are null|undefined
                                  #     - $sum|avg(NUM...): ignores non-NUM. $sum(1) can be used for counting.
                                  #     - $push(NUM...): returns as NUM_ARR
                                  #     - $addToSet(NUM...): same but removes duplicates
                                  #Can consider OBJ2._id similar to SQL "group by" and OBJ2 similar to SQL AFUNC()
S$unwind(OBJ, AGG_EXPR)           #AGG_EXPR must be:
                                  #  - a VARR_STR
                                  #  - pointing to an ARR or undefined (same as [])
                                  #It returns several OBJs, identical to OBJ but with each item of ARR.
                                  #It is somehow the opposite of S$group() + $push()
S$redact(OBJ, AGG_EXPR)           #Keeps/removes some sub-OBJs according to a condition.
                                  #Applied several times over each individual object inside OBJ, starting with top-level,
                                  #then going inside lower-level sub-OBJ[_ARR], one at a time.
                                  #Each S$redact() AGG_EXPR should resolve to either:
                                  #  - "$$PRUNE": remove this sub-OBJ recursively
                                  #  - "$$KEEP": keep this sub-OBJ recursively
                                  #  - "$$DESCEND": keep this sub-OBJ non-recursively
                                  #Usually AGG_EXPR uses O$cond().

O$let(vars#OBJ, in#AGG_EXPR)      #Same as AGG_EXPR, but can use "$$VAR" where VAR is defined in OBJ.VAR: AGG_EXPR2
                                  #Can also set "CURRENT" to AGG_EXPR_OBJ3, so that "$VAR" (in AGG_EXPR) means
                                  #"$OBJ3.VAR" instead.
                                  #Can still reference top-level object with "$$ROOT".
                                  #Overall goal is just a simplified syntax (avoid repetitions by using variables).
O$map(input#AGG_EXPR2, as#VAR_STR,#Same but:
in#AGG_EXPR)                      #  - different syntax: AGG_EXPR2 separate from VAR_STR instead of { VAR: AGG_EXPR2 }
                                  #  - AGG_EXPR2 must resolve to ARR. AGG_EXPR will be called with each element VAR_STR of ARR.

O$and|or(BOOL...)                 #True means: not 0|null|undefined (including OBJ, STR, etc.)
O$not(BOOL)                       #

O$eq|ne|gt[e]|lt[e](VAL, VAL2)    #STR comparison is byte-wise.
O$cmp(VAL, VAL2)                  #Returns -1, 0 or 1

O$add(NUM|DATE...)                #
O$subtract(NUM|DATE, NUM|DATE2)   #
O$multiply(NUM...)                #
O$divide(NUM, NUM2)               #
O$mod(NUM, NUM2)                  #NUM % NUM2

O$concat(STR...)                  #Same as STR.concat(STR...)
O$toLower|Upper(STR)              #Not Unicode-friendly.
O$substr(STR, UINT, INT)          #If:
                                  #  - INT > 0, returns STR from index UINT, length INT
                                  #  - INT < 0, returns STR from index INT (from end), until end
                                  #Byte-wise, not char-wise.
O$strcasecmp(STR, STR2)           #Like STR.localCompare(STR2), without locale handling.

O$year|month|dayOfYear|dayOfMonth|
dayOfWeek|week|hour|minute|second|
millisecond(DATE)                 #Returns NUM
O$dateToString                    #Formats DATE to a STR2, according to STR, which can contain date format:
(format#STR, date#DATE_AGG_EXPR)  #  - %Y, %m, %d, %H, %M, %S
                                  #  - %L: milliseconds
                                  #  - %j: day of year, %w: day of week, %U: week of year
                                  #  - %%: literal %

O$size(ARR)                       #Returns ARR.length
O$setUnion(ARR...)                #Returns ARR3 containing members in at least one ARR, removing duplicates.
O$setIntersection(ARR...)         #Same for members in all ARR
O$setDifference(ARR, ARR2)        #Same for members in ARR but not ARR2
O$setEquals(ARR, ARR2)            #Returns true if ARR members == ARR2 members (after removing duplicates)
O$setIsSubset(ARR, ARR2)          #Same for ARR members are contained in ARR2 members
O$any|allElementsTrue(ARR)        #Returns true if any|all ARR members are true.
                                  #True means: not 0|null|undefined (including OBJ, STR, etc.)

O$cond(if#BOOL,then#VAL,else#VAL2)#
O$cond(BOOL, VAL, VAL2)           #
O$ifNull(VAL, VAL2)               #If VAL is null|undefined, returns VAL2, otherwise VAL.


&group(ns#'COLL'[, cond#Q_OBJ],   #Like &aggregate() but simpler and less flexible and slower.
key#OBJ|$keyf#FUNC(OBJ)->OBJ,     #COLL.group() uses keyf and reduce, not $keyf and $reduce
initial#OBJ1,                     #Steps:
$reduce#FUNC2(OBJ0, OBJ1)         #  - filter COLL OBJ0s according to Q_OBJ
[, finalize#FUNC3(OBJ1)])         #  - group them according to:
  COLL.group(...)                 #     - key { VAR: 1, ... }, meaning { VAR: OBJ0.VAR, ... }
 ~COLL.group(keys#OBJ|FUNC,       #     - $keyf(OBJ0)-> { ... }
  condition, initial, reduce      #  - for each group:
  [, finalize])                   #     - fill in an initial object by merging OBJ1 with object used for grouping
                                  #     - calls FUNC2(OBJ0, OBJ1) for each COLL OBJ0 of the group, which should modify OBJ1
                                  #     - finally calls FUNC3(OBJ1) which should modify OBJ1
                                  #     - initial, FUNC2() and FUNC3() can all be empty to only do grouping
                                  #&group() returns:
                                  #  - retval OBJ1_ARR
                                  #  - count NUM: number of OBJ0s after filtering
                                  #  - keys NUM: number of OBJ1s
                                  #COLL.group() only returns OBJ1_ARR
                                  #FUNC[2|3]() should be pure functions.
                                  #Max 20000 distinct groups.
&count('COLL'[, query#Q_OBJ]
[, limit#NUM][, skip#NUM]
[, hint#INDEX_STR|OBJ])
  COLL.[it]count([Q_OBJ])
 ~COLL.count                      #Returns length of COLL.find(Q_OBJ).limit(NUM).skip(NUM).hint(INDEX_STR|OBJ) output.
  ([Q_OBJ[, limit|skip|hint#VAL]])#On a sharded COLL, prefer using &aggregate() with S$match(Q_OBJ) and
  [A]CUR.size()                   #S$group({ _id: null, count: { $sum: 1 } })
 c^MODEL|^CUR.count([Q_OBJ])      #itcount() actually performs the query, exhausting the cursor.
  [A]CUR.count([BOOL])
 ~CUR.count(BOOL[, READPREF
  [, limit|skip|hint#VAL]])       #Same but any previous CUR.limit|skip() is only taken into account if BOOL true.
&distinct('COLL', key#VARR_STR
[, query#Q_OBJ])
  COLL.distinct(VARR_STR[, Q_OBJ])
 ~COLL.distinct(VARR_STR[, Q_OBJ])
 ^CUR.distinct([Q_OBJ, ]VARR_STR) #Returns COLL.find( Q_OBJ ).toArray(), but with only VARR_STR members as an ARR, and without
 c^MODL.distinct(VARR_STR[,Q_OBJ])#duplicates.


&mapReduce('COLL', map#FUNC(),    #Like &aggregate() but slower and more flexible (can use any JavaScript function)
reduce#FUNC2(KEY, VALL_ARR)       #Overview:
, out#'COLL2'|OBJ                 #  - Q_OBJ: initial filter
[, finalize#FUNC3()][,query#Q_OBJ]#  - FUNC(): like SQL "group by", but can transform values as well
[, sort#S_OBJ][, limit#NUM]       #  - FUNC2(): like SQL AFUNC(), which should only reduce not transform
[, scope#OBJ]                     #  - FUNC3(): optional final transformation
[, verbose#BOOL][, jsMode#BOOL])  #FUNC():
COLL.mapReduce(FUNC, FUNC2, ...)  #  - called for each COLL's OBJ, assigned to this
 ~COLL.mapReduce                  #  - should call emit(KEY, VALL) 0 or several times
  (FUNC[_STR], FUNC2[_STR], ...)  #FUNC2(KEY, VALL_ARR)->VALL2:
 p^MODEL.mapReduce(...)           #  - called with all VALL emit()'d with KEY
                                  #  - not called if VALL_ARR.length === 1
                                  #  - might split up VALL_ARR in several chunks and call FUNC2() several times,
                                  #    with previous return values being input to FUNC2(), i.e.:
                                  #      - VALL2 must be same type as VALL
                                  #      - associative: FUNC2(KEY, [ A, VALL2(B,C) ]) === FUNC2(KEY, [ A, B, C ] )
                                  #      - idempotent: FUNC2(KEY, [ VALL2 ] ) === VALL2
                                  #      - commutative: FUNC2(KEY, [ A, B ]) === FUNC2(KEY, [ B, A ])
                                  #FUNC3(KEY, VALL2)->VALL2
                                  #FUNC(), FUNC2() and FUNC3:
                                  #  - should be pure: no side effects, read-only and not access database outside "this"
                                  #  - can access variables defined in scope OBJ
                                  #  - must not input (FUNC2 args) nor output (emit()) VALL > 8MB
                                  #Final output is:
                                  #  - results OBJ_ARR:
                                  #     - _id: KEY
                                  #     - value: VALL2
                                  #  - timeMillis NUM: execution time (only if verbose true (def))
                                  #  - counts:
                                  #     - input: number of FUNC() invocations
                                  #     - emit: number of emit() invocations
                                  #     - reduce: number of FUNC2() invocations
                                  #     - output: results OBJ_ARR length
                                  #Additional arguments:
                                  #  - out: how the final output is done, among:
                                  #     - { inline: 1 }: normal return
                                  #     - OBJ:
                                  #        - ACTION: "COLL", where ACTION can be:
                                  #           - replace: overwrites if same collection
                                  #           - merge: overwrites if same collection and same key
                                  #           - reduce: apply FUNC2() if same collection and same key
                                  #        - db 'DB' (def: current)
                                  #        - sharded BOOL (def: false)
                                  #        - nonAtomic BOOL (def: false): if true and ACTION != "replace",
                                  #          can yield write lock while write to COLL
                                  #       Final output will write results OBJ_ARR to 'COLL' and add result 'COLL' or
                                  #       { db: 'DB', collection: 'COLL' }
                                  #     - "[DB.]COLL": same as { replace: "COLL", db: "DB" }
                                  #  - query Q_OBJ, limit#NUM, sort S_OBJ: applied before FUNC()
                                  #  - jsMode BOOL: if true, faster but can only use 500000 distinct KEY for a single call.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            INDEXES            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SORTING ==>                       #Indexes can be used to sort faster since they are already sorted, providing they are sorted
                                  #in the same direction, with exception of the last field which can have any direction.

MULTIKEY INDEXES ==>              #Index on ARR or ARR.VAR[...]. Stores one index entry for each ARR element.
                                  #Cannot have compound with several multikey indexes.
                                  #Cannot use with "hashed" indexes

COMPOUND INDEXES ==>              #Can add indexes to several fields with either:
                                  #  - compound indexes: one index with several fields (max 31 fields)
                                  #  - index intersection: several indexes combined.
                                  #Compound indexes have, compared to index intersection:
                                  #  - disadvantages:
                                  #     - for sorting, position and (except last field) ascending|descending direction matters
                                  #     - for querying, position matters (must use only first, or first two together, etc.)
                                  #  - advantages: S_OBJ can use a field not used in Q_OBJ
                                  #Cannot mix in a query indexes of different types, e.g. geospatial with text

PERFORMANCE ==>                   #  - indexes takes at least 8KB
                                  #  - should be selective:
                                  #     - find OBJs quickly -> use many fields (compound/intersection)
                                  #       and fields with many different values
                                  #     - prefer == over < > over !=
                                  #  - in compound indexes, most selective first

SPECIAL INDEXES ==>               #By specifying STR in &createIndexes():
                                  #  - "hashed": stores hash. Good for == and != on big STR values. Do not use with DOUBLE.
                                  #  - "geolocation": querying a point proximity or inclusion in an area. See below.
                                  #  - "text": querying text with tokenization, common words removal, stemming and weights.
                                  #    See below.

&createIndexes                    #Creates indexes on COLL, i.e. COLL2 that real-time copies few fields of COLL, and is
(COLL, indexes#OBJ2_ARR)          #searched first before COLL to determine the OBJs matching Q_OBJ.
  COLL.createIndex(OBJ[, OBJ2])   #Is faster for read, but slower writes (require updating indexes).
 ~DB.createIndex                  #_id always have a unique index.
  ('COLL', OBJ[, OBJ2])           #Max 64 INDEX per COLL.
 ~COLL.createIndex(OBJ[, OBJ2])   #For other than &createIndexes():
                                  #  - OBJ is like &createIndexes key, and OBJ2 other members of OBJ_ARR
                                  #Does nothing if index already exist
                                  #OBJ2_ARR (each OBJ2 is an index):
                                  #  - key { VARR: VAL }:
                                  #     - populates index with OBJ.VAR field
                                  #     - VAL can be sorting order 1|-1, or "2d[sphere]|geoHaystack|text" for special indexes
                                  #  - background BOOL: if false (def), gets write locks while building the index, but is
                                  #    faster. If several OBJ2_ARR, must all have same background BOOL.
                                  #  - unique BOOL: if true (def: false), creates unique constraints on indexed fields.
                                  #    Cannot be done with hashed indexes.
                                  #  - name INDEX_STR (def: VAR_VAL): DB.COLL.INDEX_STR name cannot be > 128 chars
                                  #  - sparse BOOL:
                                  #     - def: false, but always true for "2d[sphere]", "geoHaystack" and "text" indexes
                                  #     - if true, COLL2 skips OBJs that have all indexed fields as undefined (not null).
                                  #     - If there is "2d[sphere]", "geoHaystack" and "text" fields, only consider those
                                  #       fields.
                                  #     - index will be skipped when used with some commands that usually would check null
                                  #       values too, e.g. sort()
                                  #        - to keep using sparse index (in order to return only non-undefined OBJs), use
                                  #          [A]CUR.hint():
                                  #           - e.g. comparison with null skip index. Using [A]CUR.hint() allow comparing with
                                  #             null only (not undefined)
                                  #     - usage:
                                  #        - on a unique field allowing undefined: without sparse, creates problem
                                  #           - undefined are converted to null in index and second null creates duplicate
                                  #             error
                                  #        - more space efficient if lot of undefined OBJs
                                  #  - expireAfterSeconds UINT:
                                  #     - mark the inserted OBJs as "to be removed" after UINT seconds after DATE.
                                  #     - must be simple index on DATE[_ARR] and not on _id nor capped COLL.
                                  #     - if DATE_ARR, use min DATE.
                                  #     - a background process that runs every minute remove OBJs, so there is delay.
                                  #     - if never used, can disable it with ds--ttlMonitorEnabled BOOL
                                  #     - cannot use usePowerOf2Sizes false.
                                  #To modify index:
                                  #  - first drop it
                                  #  - to change expireAfterSeconds, use &collMod() instead.
&listIndexes(COLL)                #Returns COLL indexes as OBJ_ARR:
  COLL.getIndexes()               #  - key OBJ
  ~COLL.indexes()                 #  - ns "DB.COLL" and name INDEX_STR
  ~COLL.listIndexes()             #  - &createIndexes() OBJ2 members
  ~DB.indexInformation            #&listIndexes() rreturns RAW_CUR
   ('COLL'[, full#BOOL])          #~COLL.listIndexes() rreturns CUR
  ~COLL.indexInformation          #If  BOOL false (def: true), returns a smaller OBJ_ARR instead:
   ([full#BOOL])                  #  - INDEX_STR: [ KEY_OBJ_AS_ARR ]
~COLL.indexExists(INDEX_STR|OBJ)  #Returns BOOL
&dropIndexes(COLL, index#STR)
  COLL.dropIndex(STR|OBJ)
 ~COLL.dropIndex(STR)
  COLL.dropIndexes()
 ~COLL.dropAllIndexes()           #
&reIndex()                        #Indexes are compacted routinely. However if big change, might take too much space, so this
  COLL.reIndex()                  #command drops and rebuild the index.
 ~COLL.reIndex()                  #Can acquire long-running write lock

C$returnKey(BOOL)                 #If true, does not return anything if the index was not enough to complete the query.
                                  #Can also set ds--notablescan 0|1 (def: 0) to 1 so any query without using an index fails.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          QUERY PLAN           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


&explain(CMD_STR[, STR])          #Show query plan used.
COLL.explain(STR).CMD()           #STR is verbosity among "queryPlanner|executionStats|allPlansExecution" (last is def)
[A]CUR.explain(STR)               #Query plan is:
 ~[A]CUR.explain()                #  - chosen among several possible
                                  #  - cached after the first time calculated, but recalculated from time to time
                                  #    i.e. when 1000 write operations, &reIndex(), adding|removing index,
                                  #    mongod restart or using explain()
                                  #Returns EXPL_OBJ (see online doc)

C$hint(INDEX_STR|OBJ)
[A]CUR.hint(INDEX_STR|OBJ)
 ^CUR.hint(INDEX_STR|OBJ)
 ^CUR.setOptions()#hint           #Forces query plan to choose an INDEX. Usually not needed. Returns CUR.
C$min|max(VAR#NUM)                #Forces query plan to choose index on VAR, and indexBounds according to NUM.
[A]CUR.min|max(VAR#NUM)           #Usually not needed. Returns CUR.

DB.collection.getPlanCache()      #Returns PLANCACHE
&planCacheListQueryShapes(COLL)   #Returns query shapes as OBJ_ARR:
  PLANCACHE.listQueryShapes()     #   - query Q_OBJ
                                  #   - projection P_OBJ
                                  #   - sort OBJ
                                  #Query shapes allow regrouping query plans.
&planCacheListPlans(COLL
[, query#Q_OBJ]
[, projection#P_OBJ][, sort#OBJ])
  PLANCACHE.getPlansByQuery
  ([Q_OBJ][, P_OBJ][, OBJ])       #Returns EXPL_OBJ_ARR
&planCacheClear(COLL,
[, query#Q_OBJ]
[, projection#P_OBJ][, sort#OBJ])
  PLANCACHE.clearPlansByQuery
  ([Q_OBJ][, P_OBJ][, OBJ])
  PLANCACHE.clear()               #

&planCacheListFilters(COLL)       #Returns index filters, forced query plans on an index.
                                  #Cannot even be overriden with hint().
                                  #Returns filters OBJ_ARR:
                                  #  - query Q_OBJ
                                  #  - projection P_OBJ
                                  #  - sort OBJ
                                  #  - indexes INDEX_STR|OBJ_ARR
&planCacheSetFilter(COLL,
query#Q_OBJ[, projection#P_OBJ]
[, sort#OBJ]
indexes#INDEX_STR|OBJ_AR)         #
&planCacheClearFilters(COLL
[, query#Q_OBJ]
[, projection#P_OBJ][, sort#OBJ]) #


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          TEXT SEARCH          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


TEXT INDEX ==>                    #For queries STR using ==, with extra functions:
                                  #  - tokenizes according to any punctuation or whitespace
                                  #  - removes common words
                                  #  - simplist suffix stemming
                                  #  - case insensitive
                                  #According to language:
                                  #  - depends on &createIndexes() OBJ_ARR:
                                  #     - default_language "en|fr|sv|de|it|es|..." (def: "en"),
                                  #       used unless specified by Q$text()
                                  #     - language_override VAR_STR (def: "language"): a field OBJ.VAR to override language
                                  #       on an OBJ-basis
                                  #  - "none": only tokenizes
                                  #Restrictions:
                                  #  - at most one per COLL
                                  #  - in compound index: not with geospatial or multikey, and if not first field,
                                  #    first field must use == or !=
                                  #  - cannot use usePowerOf2Sizes false.

Q$text(STR, $search#STR2          #Counts the number of occurences.
[, $language#STR3])               #Q_OBJ must be as { $text: OBJ } not { VAR: { $text: OBJ } }
                                  #STR can:
                                  #   - contain \"...\" meaning it is a phrase (no tokenization).
                                  #     If mixed with non-phrases, only consider strings at least matching the phrases.
                                  #   - -STR, meaning it excludes OBJs. Must be mixed with normal STR.
                                  #Words match occurences are multiplied according to &createIndexes() weights { VAR: NUM }
                                  #(def: 1, from 1 to 99999). Adds 10% if exact match.
                                  #Which gives a total score.
                                  #Restrictions:
                                  #  - does not work with QQ$nor()
                                  #  - with QQ$or(), all must be Q$text()
P$meta(OBJ.VAR, "textScore")      #Assigns Q$text() score to OBJ.VAR, e.g. to use it for sorting.
O$meta("textScore")               #Returns Q$text() score.

^MODEL.textSearch(STR2)          ##From Mongoose plugin MONGOOSE-TEXT-SEARCH (version 0.0.2)
                                 ##Do ^MODEL.find() with Q$text()
                                 ##Returns OBJ:
                                 ##  - results OBJ_ARR:
                                 ##     - score NUM
                                 ##     - obj OBJ
                                 ##  - language STR
                                 ##  - stats: from CUR.explain(): nscanned[Objects], n, nfound, timeMicros
                                 ##  - ok 1|0
                                 ##PL_OBJ:
                                 ##  - project P_OBJ|P_STR
                                 ##  - filter Q_OBJ
                                 ##  - limit NUM
                                 ##  - language STR3
                                 ##  - lean BOOL


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          GEOLOCATION          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


GEOLOCATION INDEXES ==>           #Required to use any related commands/operators.

2DSPHERE INDEXES ==>              #Calculate distance on a spherical surface. For Geolocalisation on Earth/sphere.
                                  #  - index on GeoJSON Point GEO_OBJ:
                                  #     - type STR
                                  #     - coordinates ARR, where COOR is [ LONG_NUM, LAT_NUM ] depends on type STR:
                                  #        - "[Multi]Point": COOR[_ARR]
                                  #        - "[Multi]LineString": [ COOR, COOR2 ][_ARR], noted COOR_LINE below
                                  #        - "[Multi]Polygon": COOR_LINE_ARR[_ARR]:
                                  #           - COOR_LINE_ARR: at least length 4, and first must == last
                                  #           - if several COOR_LINE_ARR, must contain each other, the first being the outer,
                                  #             with no intersection
                                  #        - "GeometryCollection": GEO_OBJ_ARR.
                                  #          Use geometries GEO_OBJ_ARR instead of coordinates ARR
                                  #     - crs OBJ, used to define a custom coordinate system:
                                  #        - type "name"
                                  #        - properties: name: "urn:..."
                                  #  - restrictions:
                                  #     - cannot be used in compound multikey indexes
                                  #     - cannot use &geoNear() if compound index

2D INDEXES ==>                    #For normal 2d plane.
                                  #  - use 2D_ARR, i.e. [X_NUM, Y_NUM]
                                  #  - &createIndexes() has additional OBJ_ARR:
                                  #     - bits NUM: resolution of the grid, from 1 to 32 (def: 26).
                                  #       Increasing by 2 multiply the grid by 4
                                  #     - min|max NUM: for longitude|latitide (def: -180|180)
                                  #  - restrictions:
                                  #     - can only be used in compound index with two fields, as the first one, with the
                                  #       second being not multikey
                                  #     - cannot use QQ$or()

GEOHAYSTACK INDEXES ==>           #Like "2d" but optimized for queries over small areas (keeps closed locations together into
                                  #buckets).
                                  #  - &createIndexes must use additional OBJ_ARR: bucketSize NUM: number of units to consider
                                  #    closed locations a "group"
                                  #  - Must use &geoSearch().

&geoNear('COLL',
near#GEO_OBJ|2D_ARR
[, min|maxDistance#NUM][...])
  Q$near[Sphere]
  (GEO_OBJ, $geometry#GEO_OBJ,
  #$min|maxDistance#NUM)          #Filter according to min|maxDistance and sort from closest to furthest.
  Q$near[Sphere](2D_ARR) +        #  - min|maxDistance: in meters for GEO_OBJ, in radians for 2D_ARR.
  Q$min|maxDistance(NUM)          #    minDistance not available with 2D_ARR
 ^CUR.near(center#2D_ARR          #  - limit NUM (def: 100, 50 for &geoSearch()): does limit(NUM)
  [, min|maxDistance#NUM]         #  - query Q_OBJ: optional additional filter
  [, spherical#BOOL])             #  - spherical true: required if GEO_OBJ
 ^CUR.maxDistance(NUM)            #  - distanceMultiplier NUM: multiply distances.
 ~COLL.geoNear(X_NUM, Y_NUM[,...])#    Multiplying by radius of Earth can convert radians to kilometers.
 p^MODEL.geoNear                  #  - includeLocs BOOL: if true, output location in case an OBJ has several
  (GEO_OBJ|2D_ARR[, ...])         #If no use of sorting, faster to use Q$geoWithin() with $center[Sphere]
S$geoNear(OBJ, ...)               #Same members as &geoNear() except:
                                  #  - no minDistance
                                  #  - includeLocs is VARR_STR, to specify the output variable name
                                  #  - distanceField VARR_STR (required): same as includeLocs but for the distance
                                  #Also:
                                  #  - must be first S$CMD()
&geoSearch('COLL',
near#GEO_OBJ|2D_ARR[, limit#NUM]
[,maxDistance#NUM][,search#Q_OBJ])
 ~COLL.geoHaystackSearch
  (X_NUM, Y_NUM[, ...])
 p^MODEL.geoSearch(GEO_OBJ|2D_ARR
  [, ...])                        #Like &geoNear() but for "geoHaystack" index.
Q$geoWithin|Intersects
(GEO_OBJ, GEO_OBJ2)
 ^CUR.within|intersects().geometry
  (GEO_OBJ)
 ^CUR.within|intersects().box
  (2D_ARR, 2D_ARR2)
 ^CUR.within|intersects().polygon
  (2D_ARR...)
 ^CUR.within|intersects().circle  #Filter GEO_OBJ completely|partially contained in GEO_OBJ2.
  (center#2D_ARR, radius#NUM,     #GEO_OBJ2 can also be OBJ:
  spherical#BOOL, unique#BOOL)    #  - $box: [ BOTTOM_LEFT_2D_ARR, TOP_RIGHT_2D_ARR ]
 ^CUR.within|intersects           #  - $polygon: 2D_ARR_ARR
  (GEO_OBJ|box#OBJ|circle_OBJ|    #  - $center[Sphere]: [ 2D_ARR, NUM ] for a circle.
  polygon#2D_ARR_ARR)             #    NUM is in radians for $centerSphere, meters for $center


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          CRUD WRITE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


&insert|update|delete|save()      #&CMD():
 RETURN VALUE ==>                 #  - n NUM
                                  #  - nModified NUM (&update())
                                  #  - upserted OBJ_ARR { index NUM, _id VAL } (&update())
                                  #Mongo shell:
                                  #  - nInserted NUM: number of OBJs inserted, with &insert()
                                  #  - nUpserted NUM: number of OBJs updated with &update() and upsert
                                  #  - nModified NUM: number of OBJs updated with &update() and no upsert
                                  #  - nRemoved NUM: number of OBJs removed
                                  #  - nMatched NUM: number of OBJs selected by Q_OBJ, with &update() and &remove()
                                  #    If &update() does not change value (e.g. assign same value), can be > other n*ed
                                  #  - _id OID: if &update() + upsert
                                  #  - write[Concern]Error:
                                  #     - writeConcernError only handles writeConcern problems, writeError does not
                                  #     - can also use return value OBJ.hasWrite[Concern]Error()->BOOL
                                  #     - Node.js uses ERROR with class ~Write[Concern]Error instead.
                                  #       WRITEERROR.index can be used to see which OBJ fired the problem.
                                  #     - is OBJ:
                                  #        - code UINT
                                  #        - errmsg STR
                                  #        - errInfo OBJ
                                  #Mongo shell with insert() with OBJ_ARR also has:
                                  #  - upserted OBJ_ARR (instead of _id):
                                  #    - _id OID
                                  #    - index UINT: order added to BULK
                                  #  - writeErrors OBJ_ARR (instead of writeError), with extra:
                                  #    - index UINT
                                  #    - op OBJ: arguments to command
                                  #Node.js:
                                  #  - result OBJ: &insert() return value
                                  #  - ops OBJ_ARR: arguments (not with save() without _id)
                                  #  - inserted|upserted|modified|removed|matchedCount NUM (not with save())
                                  #  - insertedId[s] OID[_ARR] or upsertedId OID (not with save())
                                  #Mongoose:
                                  #  - update|remove(): like &CMD() except:
                                  #     - updatedExisting instead of nModified NUM
                                  #  - save(), MOBJ.remove(), MODEL.find[And]*(): return MOBJ

&insert(COL_STR, documents#OBJARR)#Adds new OBJ in COLL.
  COLL.insert(OBJ[_ARR])          #Creates COLL if it does not exist.
 ~COLL.insertOne|Many(OBJ[_ARR])  #Adds _id (as ObjectId()) if not specified in OBJ_ARR (according to &create() autoIndexId)
                                  #If using OBJ_ARR, actually does a Bulk writes, with a different return value then.

&update('COLL', updates#OBJ_ARR)  #updates#OBJ: q Q_OBJ, u U_OBJ[, upsert BOOL][, multi BOOL]
  COLL.update(Q_OBJ, U_OBJ        #Perform actions specified by U_OBJ on COLL OBJ filtered by Q_OBJ.
  [, upsert#BOOL][, multi#BOOL])  #If multi is false (def), only does it on first OBJ matching Q_OBJ.
 ~COLL.updateOne|Many|replaceOne  #If upsert true (def: false):
  (Q_OBJ, U_OBJ[, upsert#BOOL])   #  - create empty OBJ first before updating if no OBJ matches Q_OBJ.
 ^CUR.update(Q_OBJ, U_OBJ         #  - in order to avoid duplicates (two concurrent upserts at same time),
  [, upsert#BOOL][, multi#BOOL]   #should only be used if unique index
  [, safe#BOOL][, overwrite#BOOL])#  - VARR$ is VARR
 c^MODEL.update(Q_OBJ, U_OBJ      #Mongoose only:
  [, upsert#BOOL][, multi#BOOL]   #  - safe BOOL: if false, do not use SCHEMA default values
  [, safe#BOOL][, overwrite#BOOL])#  - overwrite BOOL: if false (def), if U_OBJ is normal OBJ -> { $set OBJ }, meaning it
 ^MOBJ.update(U_OBJ[, ...])       #    only overwrites specific fields not full object. If true, cannot use normal OBJ.
                                  #  - U_OBJ cannot include _id
                                  #U_OBJ can be (only first for ~COLL.replaceOne(), second for ~COLL.updateOne|Many()):
                                  #  - { VAR: VAL, ... }: performs OBJ = { VAR: VAL, ... } (erase all except _id).
                                  #    Cannot use multi true.
                                  #  - { CMD: { VARR$: VAL, ... } }: performs U$CMD(OBJ.VAR, VAL), for each OBJ.VAR
                                  #    If this sets OBJ.VAR and it does not exist, creates it first
                                  #    (as default value, e.g. 0 for NUM or [] for ARR).
                                  #If using OBJ_ARR, actually does a Bulk writes, with a different return value then.

&delete('COLL', deletes#OBJ_ARR)
  COLL.remove(Q_OBJ[,justOne#BOL])
 ~COLL.deleteOne|Many(Q_OBJ)
 ^CUR.remove([Q_OBJ])             #OBJ is { q Q_OBJ, limit 0|1 }
 ^MOBJ.remove()                   #Erases OBJ in COLL, matching Q_OBJ, either all (limit 0) or just one (limit 1).
 c^MODEL.remove([Q_OBJ])          #If using OBJ_ARR, actually does a Bulk writes, with a different return value then.

COLL.save(OBJ)
 ~COLL.save(OBJ)                  #Calls either COLL.insert() or COLL.update() (with upsert: true) depending
 ^MOBJ.save()                     #on whether OBJ has _id.
p^MODEL.create(OBJ[_ARR])         #Same as (new ^MODEL(OBJ)).save()

&findAndModify('COLL'
[, query#Q_OBJ][, fields#P_OBJ]
[, sort#S_OBJ][, update#U_OBJ]
[, remove#BOOL][, new#BOOL]
[, upsert#BOOL])
  COLL.findAndModify
  (query#Q_OBJ, ...)
 ~COLL.findAndModify
  ([Q_OBJ[,S_ARR[,U_OBJ[, ...]]]])
 ~COLL.findAndRemove
  ([Q_OBJ[, S_ARR]])
 ~COLL.findOneAndDelete
  ([Q_OBJ[, projection#P_OBJ]
  [, sort#S_OBJ]])
 c^MODEL.findOneAndRemove
  (Q_OBJ[, sort#S_OBJ|STR]
  [, select#P_OBJ|STR])
 ^CUR.findOneAndRemove
  ([Q_OBJ[, sort#S_OBJ|STR]])
 c^MODEL.findByIdAndRemove
  (OID[, sort#S_OBJ|STR]
  [, select#P_OBJ|STR])
 ~COLL.findOneAndReplace|Update
  (Q_OBJ, U_OBJ[,projection#P_OBJ]
  [,sort#S_OBJ][, upsert#BOOL]    #Does any of:
  [, returnOriginal#BOOL])        #  - &update(): with U_OBJ, with optional upsert true
 c^MODEL.findOneAndUpdate         #  - &remove(): with remove true
  ([Q_OBJ[, U_OBJ[,sort#S_OBJ|STR]#Both use Q_OBJ and S_OBJ first. Only update|remove one document.
  [, upsert#BOOL][, new#BOOL]     #P_OBJ is only used for the return value.
  [, select#P_OBJ|STR]]])         #Returns:
 ^CUR.findOneAndUpdate            #  - value OBJ: new or old value, depending on new|returnOriginal#BOOL (def: false).
  ([Q_OBJ[, U_OBJ[,sort#S_OBJ|STR]#null if none
  [, upsert#BOOL][, new#BOOL]]])  #Comparison with &update() and &remove():
 c^MODEL.findByIdAndUpdate        #  - cannot modify several OBJs at once
  (OID[, U_OBJ[,sort#S_OBJ|STR]   #  - but:
  [, upsert#BOOL][, new#BOOL]     #     - can use sort order before modification
  [, select#P_OBJ|STR])           #     - returns new or old value
                                  #S_ARR is like S_OBJ but as [ [ KEY_STR, VAL ]... ]

U$currentDate(OBJ.VAR, true)      #OBJ.VAR = new Date()
U$inc(OBJ.VAR, NUM2)              #OBJ.VAR += NUM2
U$mul(OBJ.VAR, NUM2)              #OBJ.VAR *= NUM2
U$bit(OBJ.VAR, and|or|xor#INT)    #OBJ.VAR &= |= ^= INT (not NUM)
U$min|max(OBJ.VAR, VAL2)          #OBJ.VAR = min|max(OBJ.VAR, VAL)
U$rename(OBJ.VAR, VAR2_STR)       #Renames OBJ.VAR to OBJ.VAR2
U$set[OnInsert](OBJ.VAR, VAL)     #OBJ.VAR = VAL. If VAL is OBJ, merges it.
                                  #If OnInsert, does nothing if did not upsert (upsert: true + Q_OBJ do not match any)
U$unset(OBJ.VAR, "")              #delete OBJ.VAR

U$pop(OBJ.ARR, 1|-1)
 ^MOBJ.ARR.$shift|pop()           #OBJ.ARR.shift|pop()
U$pull(OBJ.ARR, Q_OBJ|VAL)        #Removes any element from OBJ.ARR matching Q_OBJ.
 ^MOBJ.ARR.pull|remove            #VAL is like { $eq: VAL }.
  (Q_OBJ|VAL...)                  #Q_OBJ is not possible when OBJ.ARR contains objects.
                                  #^pull() uses U$set() instead.
U$pullAll(OBJ.ARR, ARR2)          #Removes any element from OBJ.ARR matching any element from ARR2.
U$push(OBJ.ARR, OBJ2|VAL)         #If VAL, OBJ.ARR.push(VAL)
                                  #OBJ2:
                                  #  - $each VAL_ARR (required but can be []): same but can add several VAL
                                  #  - $position UINT: insert at index UINT instead of pushing at end
                                  #  - $slice INT: does ARR.slice(INT) afterwards
                                  #  - $sort 1|-1 (ARR_VAL) or { VAR: 1|-1, ... } (ARR_OBJ): sort ARR afterwards
U$addToSet(OBJ.ARR, OBJ2|VAL)     #Adds VAL to OBJ.ARR, if it is not === any element of OBJ.ARR
 ^MOBJ.ARR.addToSet(VAL)          #Can use { $each VAL_ARR } also (see U$push())


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          BULK WRITE           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COLL.initialize[Un]OrderedBulkOp()#Rreturns BULK, which registers operations to perform all at the same time:
 ~COLL.                           #  - from ACID point of view though, this behaves as if each command was send individually
  initialize[Un]OrderedBulkOp()   #    at the same time
                                  #  - but this sends all commands at once over the network which can be more performant
                                  #If "Ordered", will stop (but no rollback) if one command fails.
                                  #If several &insert|remove|update() in a row, group them together,
                                  #in up to 1000 commands groups.
BULK.insert(OBJ)
 ~BULK.insert(OBJ)                #Adds &insert() to BULK, and rreturns BULK
BULK.find([Q_OBJ])[.upsert()]     #Adds &update() to BULK, using upsert: true if upsert() and multi: true if not "One".
.update[One]|replaceOne(U_OBJ)    #U_OBJ uses U$CMD() if update[One](), { VAR: VAL } if replaceOne()
 ~BULK.find([Q_OBJ])              #Rreturns BULK
BULK.find([Q_OBJ]).remove[One]()  #Adds &remove() to BULK. Rreturns BULK
BULK.execute()                    #Executes operations added to BULK.
 ~BULK.execute()                  #Returns:
                                  #  - Mongo Shell: same OBJ as COLL.insert() with OBJ_ARR
                                  #  - Node.js: BULKWRITERESULT with:
                                  #     - properties:
                                  #        - ok FUNC()->BOOL
                                  #        - nInserted|nUpdated|nUpserted|nModified|nMatched FUNC()->NUM
                                  #     - methods (rreturns):
                                  #        - getInserted|UpsertedIds()->OID_ARR, getUpsertedIdAt(UINT)->OID
                                  #        - getLastOp()->OBJ
                                  #        - toString|toJSON()
BULK.toString|tojson()            #Returns BULK to execute, before execute() as OBJ|STR:
                                  #  - nInsert|Update|RemoveOps NUM
                                  #  - nBatches NUM
BULK.getOperations()              #Returns BULK executed, after execute(), as OBJ_ARR:
                                  #  - originalZeroIndex UINT: order added to BULK
                                  #  - batchType NUM: 1 for &insert(), 2 for &update(), 3 for &remove()
                                  #  - operations OBJ_ARR: arguments to command

~COLL.bulkWrite(OBJ_ARR)          #Simpler way, with OBJ_ARR being any of:
                                  #  - insertOne: document OBJ
                                  #  - updateOne|Many: filter Q_OBJ, update U_OBJ, upsert BOOL
                                  #  - replaceOne: filter Q_OBJ, replacement OBJ, upsert BOOL
                                  #  - deleteOne|Many: filter Q_OBJ
                                  #Returns:
                                  #  - inserted|upserted|matched|modified|deletedCount NUM
                                  #  - inserted|upsertedIds OBJ
                                  #  - same members as BULKWRITERESULT


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             FILES             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


GRIDFS ==>                        #Goal:
                                  #  - store OBJ >16MB (files),
                                  #  - random access without loading full value in memory
                                  #How:
                                  #  - uses COLL fs.files and fs.chunks
                                  #  - divide files into chunks of 255kB, ~new GridStore()#chunk_size or
                                  #    GRIDSTORE.chunk_size INT
                                  #Restrictions:
                                  #  - atomicity is at the chunk-level, not file-level

fs.files                          #COLL with OBJ:
                                  #  - _id FILE_OID
                                  #  - length INT: in bytes
                                  #  - chunkSize INT
                                  #  - uploadDate DATE
                                  #  - md5 STR: can be computed with:
                                  #     - &filemd5(OID, root#STR), where OID is _id, and root "fs"
                                  #     - md5sumFile(STR), where STR is filename
                                  #     - GRIDSTORE.md5
                                  #  - filename STR
                                  #  - contentType MIME_STR: must use mongofiles put --type MIME_STR or
                                  #    ~new GridStore()#content_type MIME_STR
                                  #  - aliases STR_ARR
                                  #  - metadata OBJ: must use ~new GridStore()#metadata OBJ

fs.chunks                         #COLL with OBJ (index on files_id + n):
                                  #  - _id CHUNK_OID
                                  #  - files_id FILE_OID
                                  #  - n INT: chunk sequence number, starting from 0
                                  #  - data BIN

mongofiles                        #Command line to manipulate GridFS files.
                                  #Cannot access replica slaves.
list [PREFIX]                     #
search STR                        #Same but can search anywhere not only the PREFIX
get|put FILE [--local FILE2]      #GridFS FILE -> local filesystem FILE2 (def: FILE) or inverse
                                  #If no --replace, put will not overwrite but create another file with same name
delete FILE                       #


~GridStore.list(DB[, FS_STR])     #Returns FILE_STR_ARR
                                  #FS_STR: using different COLL than "fs"
~GridStore.exist(DB[, FILE_STR
 [, FS_STR]])                     #Returns BOOL

~new GridStore(DB[, FILE_OID],    #Rreturns GRIDSTORE, i.e. a file in GridFS
 FILE_STR, "r|w|w+"[, OBJ])       #"w" mode truncates.
                                  #OBJ: root FS_STR
~GRIDSTORE.collection()           #Rreturns fs.files as ~COLL
~GRIDSTORE.chunkCollection()      #Rreturns fs.chunks as ~COLL
~GRIDSTORE.stream([BOOL])         #Rreturns as IOSTREAM.
                                  #If true, ~GRIDSTORE.close() will be called on EOF or close event.

~GRIDSTORE.open()                 #Needs to be done before any read|write
~GRIDSTORE.close()                #Close and saves file
~GRIDSTORE.destroy()              #Close file
~GRIDSTORE.unlink()
~GridStore.unlink
 ('DB', FILE_STR_ARR)             #Delete file

~GRIDSTORE.read([NUM[, NUM2]])
~GridStore.read('DB', FILE_STR
 [, NUM[, NUM2]])                 #Returns STR, read from offset NUM2 with length NUM
~GRIDSTORE.readlines([STR])
~GridStore.readlines('DB',
 FILE_STR[, STR2])                #Returns entire file as STR_ARR, split according to STR2 (def: newline)
~GRIDSTORE.getc()                 #Returns single char as STR
~GRIDSTORE.puts(STR)              #Writes STR + newline
~GRIDSTORE.write(STR|BUFFER[,BOL])#Writes STR|BUFFER. If true, calls ~GRIDSTORE.close()
~GRIDSTORE.writeFile(PATH_STR)    #Writes file content
~GRIDSTORE.tell()                 #Returns cursor position NUM
~GRIDSTORE.seek(NUM, NUM2)        #Moves cursor to position NUM, based on NUM2 as ~GridStore.IO_SEEK_SET|CUR|END
~GRIDSTORE.rewind()               #Put cursor back to original position. If "w|w+", truncates file.
~GRIDSTORE.eof()                  #True if at EOF


GRIDFS-STREAM(~DB, MONGODB)      ##Rreturns GFS
                                 ##Version 0.5.3
GFS.createWrite|ReadStream(OBJ)  ##Returns a GridFS file as OSTREAM|ISTREAM. OBJ:
                                 ##  - filename STR or _id FILE_OID
                                 ##  - mode STR, chunkSize NUM, content_type STR, root STR, metadata OBJ: like ~GridStore
                                 ##  - range: startPos|endPos NUM: to get a partial range (ISTREAM only)
                                 ##The OSTREAM close event has OBJ2 as argument, where OBJ2 is the one from fs.files.
GFS.remove(OBJ)                  ##OBJ: filename STR or _id FILE_OID, root STR. Returns nothing.
GFS.exist(OBJ)                   ##OBJ: filename STR or _id FILE_OID, root STR. Returns BOOL.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        JAVASCRIPT EVAL        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


JAVASCRIPT ==>                    #Can use:
                                  #  - DB.system.js: similar to .bashrc for MongoDB JavaScript func declarations
                                  #  - mongo FILE or mongo --eval JAVASCRIPT_STR
                                  #  - Q$where(): for queries
                                  #As opposed to mongo shell:
                                  #  - cannot use interactive features like: show *, use DB, etc.
                                  #  - For pretty prints, use printjson(STR)
                                  #Can be disabled for security reason with cf--security.javascriptEnabled false or
                                  #d,s--noscripting.
                                  #Beware of injections.

DB.system.js                      #OBJ:
                                  #  - _id FUNC_NAME_STR
                                  #  - value FUNC
                                  #Available to Q$where(), etc.
                                  #Use DB.loadServerScripts() to make them available to the Mongo shell too
                                  #(for current client connection).

o--eval JAVASCRIPT_STR            #Can use o--shell to still open Mongo shell.
                                  #Can also use:
                                  #  - mongo FILE directly.
                                  #  - load(FILE) in Mongo shell (keep local variables in same environment)

Q$where(FUNC[_STR])               #Filters according to FUNC|JAVASCRIPT_STR, where this|obj is OBJ.VAR.
 ^CUR.$where(FUNC[_STR])          #Restrictions:
                                  #  - Cannot use indexes
                                  #  - Much slower than others Q$CMD(). Good idea to add others members in Q_OBJ in order to
                                  #    restrict number of OBJs where Q$where() apply.
                                  #  - Can only use pure JavaScript, no DB, COLL, &COMMAND, etc., except type-related
                                  #    (e.g. ObjectId()) or utils (e.g. print())
                                  #  - Must be read-only
                                  #  - Must not be nested inside Q_OBJ (must be top-level)
^MODEL.$where(...)                #Same as ^MODEL.find().$where(...)

new Mongo(["HOST[:PORT]"])        #Creates and returns a MONGO, i.e. client connection, from a "JavaScript eval" function.
DB.getMongo()                     #Returns MONGO
MONGO.getDB('DB')                 #Returns DB
connect("HOST[:PORT]/DB")         #Same as (new Mongo("HOST[:PORT]")).getDB("DB")


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          PERFORMANCE          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


STORAGE ENGINE ==>                #How database is layed out physically. Has great impact on performance.
                                  #Specify with cf--storage.engine STR or ds,d--storageEngine STR:
                                  #  - "mmapv1" (def)
                                  #  - "wiredTiger": faster
                                  #To change storage engine, must mongodump, change then mongorestore.

STORAGE_OBJ ==>                   #All the cf--storage.ENGINE.* are STORAGE_OBJ, which can also be specified with:
                                  #  - &createIndexes() OBJ2_ARR.storageEngine STORAGE_OBJ
                                  #  - &create() OBJ.storageEngine STORAGE_OBJ
                                  #  - ds--wiredTigerEngineRuntimeConfigSetting or d--wiredTigerEngineConfig STORAGE_OBJ

COMPRESSION ==>                   #The following applies only to WiredTiger.
                                  #Compression reduces space size, but takes more CPU:
                                  #  - indexes using prefix compression if
                                  #    cf--storage.wiredTiger.indexConfig.prefixCompression or
                                  #    d--wiredTigerIndexPrefixCompressor true (def)
                                  #  - the rest (other collections, including WAL) using
                                  #    cf--storage.wiredTiger.collectionConfig.blockCompressor or
                                  #    d--wiredTigerCollectionBlockCompressor "none|zlib|snappy"
                                  #    Def: "snappy", using less CPU but more space than zlib.

SPACE ALLOCATION ==>              #The following applies only to mmapv1.
                                  #Allocating new space is slow:
                                  #  - prefer updates that do not change size to insert or updates that change size
                                  #  - by def. adds padding, i.e. space + padding takes always power of 2 bytes,
                                  #    with min. 32 bytes.
                                  #  - padding takes extra space:
                                  #     - if document size does not change, can disable with:
                                  #        - &create() flags 0 or noPadding true
                                  #        - &collMod() noPadding true: for a single COLL
                                  #     - can erase current padding but let new insertions add padding with:
                                  #        - &compact('COLL'[, force#BOOL][, paddingFactor|Bytes#NUM]):
                                  #           - can add initial padding as a multiple of current size (paddingFactor, 1 to 4)
                                  #             or addition (paddingBytes)
                                  #           - gets a write lock, so must use force true on primary server
                                  #           - can be long running
                                  #           - free space for MongoDB, but not for an OS perspective
                                  #             (as opposed to &repairDatabase())
                                  #           - does not replicate (must be done on each replica set members)
                                  #           - on secondaries, put in RECOVERING state
                                  #           - requires at least 2GB of RAM
                                  #        - &repairDatabase(), DB.repairDatabase() or d,dmp--repair: does &compact() on all
                                  #          COLLs, and free OS space too

PERFORMANCE TIPS ==>              #Logic:
                                  #  - working set size (can be seen with serverStatus()):
                                  #     - i.e. size of data being used, should fit in RAM
                                  #     - if not, use sharding
                                  #     (wiredTiger only)
                                  #     - the space available to put working set is
                                  #       cf--storage.wiredTiger.engineConfig.cacheSizeGB NUM or d--wiredTigerCacheSizeGB
                                  #       (def: 1/2 RAM, min. 1GB)
                                  #  - improve concurrency with not too much locks
                                  #  - use indexes
                                  #  - cf--net.wireObjectCheck false (def: true) or d--[no]objcheck: no BSON check,
                                  #    so slightly faster (but not recommended)
                                  #OS:
                                  #  - do not use 32 bits OS (would limit RAM to 2GB and disables journalling by def.)
                                  #  - time should be sync. accross hosts in a sharded cluster
                                  #  - more important to have fast dbPath than log than journal:
                                  #     - can use different volumes with faster volumes (e.g. EC2 PIOPS) for the first ones.
                                  #     - however, this prevents using OS snapshots
                                  #     - can also use different volumes for different DB inside same cluster if
                                  #       cf--storage.directoryPerDB true (def: false) or d--directoryperdb
                                  #  - use ext4 or xfs
                                  #  - turn off atime for the partition holding dbPath (noatime,nodiratime in /etc/fstab)
                                  #  - disable NUMA (if enabled)
                                  #  - disable transparent hugepages, i.e. do hugeadm --thp-never (package hugepages)
                                  #  - use low readahead:
                                  #     - get: sudo blockdev --report
                                  #     - set: sudo blockdev --setra 32 PARTITION
                                  #Hardware:
                                  #  - use SSD
                                  #  - RAM > IO > CPU Hz > Num. CPU cores
                                  #  - use Swap partition
                                  #  - use RAID-10, avoid RAID-0|5|6


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             ACID              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DURABILITY ==>                    #Always read uncommitted: can read when write command received, before acknowledged,
                                  #so can read:
                                  #  - before write returns (should not be a problem)
                                  #  - if crash happens before journalled (for replica sets, only when majority
                                  #    has journalled), data that will never be written
                                  #Write:
                                  #  - according to any of:
                                  #     - &*Role|User*()#writeConcern, ~DB|ADMIN.addUser|removeUser()#w|wtimeout|j,
                                  #       DB.*Role|User*()#writeConcern
                                  #     - &insert|update|delete()#writeConcern, COLL.insert|update|remove|save()#writeConcern,
                                  #       ~COLL.findAnd*|insert*|update*|replace*|delete*|save()#w|wtimeout|j,
                                  #       ~COLL.initialize[Un]OrderedBulkOp|bulkWrite()#w|wtimeout|j
                                  #     - BULK.execute() OBJ, ~BULK.execute()#w|wtimeout|j
                                  #     - rst--w, cn--w|wtimeoutMS|journal, ~MONGO.connect()#db.w|wtimeout|j, ~DB.writeConcern
                                  #     - system.replset.settings.getLastErrorDefaults (only replica sets, does not
                                  #       have priority over others)
                                  #     - ~DB.createIndex()#w|wtimeout|j, ~COLL.createIndex|dropIndex()#w|wtimeout|j
                                  #     - ~DB.collection|createCollection()#w|wtimeout|j
                                  #     - ~new GridStore()#w|wtimeout|j
                                  #     - ^SCHEMA_OPT.safe OBJ or ^CUR.setOptions()#safe OBJ
                                  #  - OBJ is:
                                  #     - w:
                                  #        - 0: after network transmission ok
                                  #        - 1 (def): after received by server and no constraint (e.g. unique field) problem
                                  #        - NUM|"majority": after received by NUM|majority of the voting members
                                  #        - Replica Set TAG_SET_STR, after using system.replset.settings.getLastErrorModes
                                  #          { TAG_SET: { TAG_VAR: 0|1|NUM|"majority" } }
                                  #     - j BOOL (def: false): if true, also after primary committed to journal
                                  #       (only primary even if writeConcern.w NUM|"majority")
                                  #     - wtimeout NUM (in ms): if not received then, returns error
                                  #       (even if operation eventually succeeds)

ATOMICITY/CONSISTENCY/ISOLATION   #At OBJ-level, operations are atomic.
  ==>                             #For multi-OBJs writes:
                                  #  - never rollbacks if error in middle (i.e. not atomic, no transactions)
                                  #  - &insert|update|delete()#ordered or COLL.insert()#ordered: if true (def),
                                  #    when insert fails, do not process further OBJs
                                  #  - can yield write lock between two OBJs:
                                  #     - more efficient but can give inconsistent view of a COLL in the middle of an
                                  #       update/insert/delete
                                  #     - can be prevented with Q_OBJ.$isolated 1

DUPLICATE READ ==>                #CUR can read twice same OBJ if there are intervening write operations.
                                  #To avoid this, use C$snapshot(), [^]CUR.snapshot() or ^CUR.setOptions()#snapshot:
                                  #  - before the first iteration
                                  #  - cannot be used with CUR.sort() or [A]CUR.hint()
                                  #  - implicitely done on results <= 1MB

CONCURRENCY ==>                   #WiredTiger:
                                  #  - one read lock per OBJ
                                  #  - max per cluster: d--wiredTigerConcurrentRead|WriteTransactions NUM
                                  #mmapv1:
                                  #  - each COLL has a single writer / multiple readers lock per mongod, with preference
                                  #    given to single writer.
                                  #  - is per mongod, not per cluster, so sharding improves concurrency.
                                  #  - there is also a global lock (accross DB), used for few operations.
                                  #  - locks can be yielded when long operation, i.e.:
                                  #     - between two OBJs in a multi-OBJs operation
                                  #     - when operation needs to load data in memory first


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              WAL              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


cf--storage.journal.enabled       #Use Write-ahead log (WAL) for write operations (def: true) in order to cache
d--journal                        #writes (performance) without losing consistency:
                                  #  - write in bulk to journal ("commit")
                                  #     (mmapv1 only)
                                  #     - cf--storage.mmapv1.journal.commitIntervalMs / ds,d--journalCommitInterval INT
                                  #       (in ms, 2 to 300, def: 100 usually, 30 if journal on different filesystem)
                                  #       trades performance for durability
                                  #       If writeConcern.j true reduced to one third.
                                  #  - then in bulk to disk ("sync"):
                                  #     (mmapv1 only)
                                  #     - cf--storage.syncPeriodSecs / ds,d--syncdelay INT (in sec, def: 60)
                                  #       trades performance for recovery time
                                  #     (WiredTiger only)
                                  #     - d--wiredTigerCheckpointDelaySecs NUM (def: 60)
                                  #     (all)
                                  #     - can force it with &fsync([async#BOOL][, lock#BOOL]) or DB.fsync[Un]lock():
                                  #         - can run sync. (def) or async.
                                  #         - if lock true, also blocks further writes and reads until called again with
                                  #           lock false (not with WiredTiger)
                                  #WAL:
                                  #  - located at dbPath/journal/
                                  #  - contains:
                                  #     - redo files j._*:
                                  #        - removed when all WAL has been applied to physical data, so usually only two
                                  #          or three files
                                  #        (mmapv1 only)
                                  #        - rotated at 1GB or if cf--storage.mmapv1.smallFiles or d--smallfiles true, 128MB
                                  #          (good for small databases)
                                  #     - last-sequence-number file
                                  #  - cleaned on normal shutdown, dirty leaves it


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            BACKUP             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


BACKUPS ==>                       #Options:
                                  #  - copy files manually:
                                  #     - must stop writes. Can use &fsync() for this purpose.
                                  #  - partition snapshots:
                                  #     - must enable journalling
                                  #  - mongodump/mongorestore (command line):
                                  #     - extra features:
                                  #        - PITR with dmp--oplog + rst--oplogReplay + rst--oplogLimit TIMESTAMP
                                  #        - filter what's backed up with dmp--query or
                                  #          dmp--excludeCollection[WithPrefix] STR_ARR
                                  #     - performance:
                                  #        - does not store indexes, so smaller backups but needs time to restore them
                                  #          (unless using rst--noIndexRestore)
                                  #        - same things for &collMod() options, unless using rst--noOptionsRestore
                                  #        - can impact performance while backing up, especially if data size > available RAM
                                  #        - use like MONGO.setReadPref() "secondaryPreferred"
                                  #        - rst--numInsertionWorkers NUM (def: 1) and rst--numParallelCollections NUM (def: 1)
                                  #     - output (mongogodump):
                                  #        - uses dmp--out DIR (def: "./dump/"). Can be "-" for stdout (unless for compressing)
                                  #        - does not overwrite OBJs with same _id, so consider:
                                  #           - running against an empty DB
                                  #           - using rst--drop, which drop all COLLs first
                                  #     - input (mongorestore): a file or stdin
                                  #     - also:
                                  #        - should use dmp--forceTableScan when _id is custom and with no index
                                  #          (can create problems)
                                  #        - with dmp--db, can use dmp--dumpDbUsersAndRoles and rst--restoreDbUsersAndRoles
                                  #          (otherwise def.)
                                 ##     - can use script backup.sh from MONGODB-S3-BACKUP (no version, last mod. 2 years ago)
                                 ##       to send as tar.gz archive to S3, named as backup-YYYY-mm-DD-HHMMSS. Use options:
                                 ##        - -u MONGO_USER, -p MONGO_PWD
                                 ##        - -k AWS_ACCESS_KEY, -s AWS_SECRET_KEY, -r AWS_REGION, -b BUCKET
                                 ##        - does a fsync[Un]lock()
                                  #If sharded:
                                  #  - disable balancer
                                  #  - &fsync() each shard (preferably a secondary)
                                  #  - backup one config server
                                  #  - backup each shard member
                                  #  - to restore:
                                  #     - shutdown everything
                                  #     - stop (in this order): mongos, mongod, config server
                                  #     - copy data files
                                  #     - restart (in this order): config server, mongod, mongos

RESTORE A REPLICA SET ==>         #  - start a mongod without replica set (with backed up dbPath)
                                  #  - restart it with replica set (being the only member of it)
                                  #  - fire RS.initiate()
                                  #  - add secondaries (ensuring their dbPath is empty)

&repairDatabase                   #Checks and repairs for inconsistency. To do after an abnormal shutdown with no journaling,
([backupOriginalFiles#BOOL,]      #before anything else, unless a replica has taken over.
[preserveClonedFilesOnFailure     #Uses a temp DIR at cf--storage.repairPath or d--repairpath DIR
#BOOL])                           #(def: storage.dbPath + "/_tmp"), which is erased always unless backupOriginalFiles true,
  DB.repairDatabase()             #and on error unless preserveClonedFilesOnFailure true (both available only with mmapv1)
  d,dmp--repair                   #Requires at least 2GB of RAM.
                                  #On startup, partial indexes are rebuild unless cf--storage.indexBuildRetry false or
                                  #d--noIndexBuildRetry


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            EXPORTS            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


mongoexport                       #Export COLLs:
                                  #  - to --out FILE
                                  #  - as --type STR:
                                  #     - "json": OBJ_ARR (def)
                                  #        - if --jsonArray: no newline but enclosed by [ ... ]
                                  #        - can --pretty
                                  #     - "csv": must use --fields[File]
                                  #  - can limit OBJs with --query Q_OBJ, --skip INT, --limit INT, --sort S_OBJ
                                  #  - can limits fields with --fields VARRR_STR... or --fieldsFile FILE (one VARRR per line)
                                  #    Implicitely incude _id if JSON output
                                  #  - can use --forceTableScan (like mongodump)
                                  #Do not use for backups (does not store type info) but for transfer between different systems
                                  #(e.g. SQL <-> MongoDB)
mongoimport                       #Import COLLs:
                                  #  - from --file FILE
                                  #  - from --type "json|csv|tsv":
                                  #     - JSON (def): can use --jsonArray (like mongoexport)
                                  #     - CSV|TSV:
                                  #        - can limit fields with --fields[File] (like mongoexport) or --headerline
                                  #          (use first row)
                                  #        - ignore empty fields if --ignoreBlanks
                                  #        - CSV format uses "" to escape " (not \)
                                  #  - will insert unless using --upsert based on _id or using --upsertFields VARRR_STR...
                                  #    Can also use --drop (like mongorestore)
                                  #  - can use:
                                  #     - --stopOnError
                                  #     - --numInsertionWorkers NUM (def: 1)


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         REPLICATION           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


REPLICATION ==>                   #One read/write master, several read-only secondaries, with possibility of chain.
                                  #Use async. log shipping ("oplog")
                                  #Can use sync. replication, called "master/slave replication", but is deprecated legacy way.
                                  #Automatic failover:
                                  #  - replica set members use pings to each other to check states every 2 secs
                                  #     - failover after a timeout (def: 10 secs)
                                  #     - must be able to communicate with each other
                                  #  - elections use priority:
                                  #     - can use one arbiter for election purpose only (votes but no replication):
                                  #       will vote if elections is 50/50
                                  #        - prefer odd members number from the original configuration
                                  #        - can disallow journalling since do not hold data
                                  #     - passive (priority 0): cannot become primary
                                  #        - if cold standby (used only for backup, e.g. for mongodump), can consider building
                                  #          less indexes with cf--replication.secondaryIndexPrefetch or
                                  #          ds,d--replIndexPrefetch "none|_id|all" (def: "all").
                                  #          If not "all", must be hidden too.
                                  #        - frozen: like passive but temporary
                                  #     - hidden: replicates but cannot be targeted by queries
                                  #        - maintainance: similar to hidden but temporary
                                  #  - when former primary reconnects, if substantial part it previously received is not
                                  #    in current state, it must rollback:
                                  #     - writes data in dbPath/rollback/DB.COLL.TIMESTAMP.bson. Can apply it manually
                                  #     - max 300MB
                                  #Also:
                                  #  - delayed replication: to keep historical replication (e.g. one day ago)
                                  #     - must be hidden and passive.
                                  #     - delay must be higher than the oplog size
                                  #  - read preference: how to redirect read queries to primary/secondaries
                                  #How:
                                  #  - RS.conf(), that can be changed with sets of methods (see below)
                                  #  - maintainance procedure:
                                  #     - restart temporary as standalone while doing maintainance
                                  #     - if primary, first step down
                                  #  - if lagged behind, must resync i.e. restart with empty dbPath or copy from another member
                                  #Limits: 50 members, and 7 voting members.

rs                                #Returns RS in Mongo shell

[local.]system.replset            #OBJ:
                                  #  - _id REPL_SET_STR: set by cf--replication.replSetName, d--replSet or cn--replicaSet
                                  #    REPL_SET_STR
                                  #  - version INT: configuration version number
                                  #  - members OBJ_ARR:
                                  #     - _id STR
                                  #     - host HOST[:PORT]
                                  #     - arbiterOnly BOOL: if true, arbiter
                                  #     - hidden BOOL: if true, hidden member
                                  #     - slaveDelay INT (in sec): delayed members delay
                                  #     - buildIndexes BOOL: if false, like --replIndexPrefetch "_id"
                                  #     - priority INT (0 to 1000, def: 1):
                                  #        - higher means more eligible to become primary during failover election
                                  #        - changing priority triggers failover election
                                  #     - votes 0|1: in a failover election
                                  #     - tags { TAG_VAR: TAG_VAL_STR, ... }
                                  #  - settings:
                                  #     - getLastErrorModes OBJ
                                  #     - chainingAllowed BOOL: if true (def):
                                  #        - secondary can replicate from other secondaries.
                                  #        - automatically done based on other secondaries ping times and replication states.
                                  #           - but can manually change targets with
                                  #             &replSetSyncFrom|RS.syncFrom("HOST[:PORT]"), but this will be temporary only.
                                  #        - reduce load on primary, but increase replication lag
                                  #     - heartbeatTimeoutSecs UINT (in sec, def: 10)
&&replSetGetConfig()
  RS.conf()                       #Gets system.replset
&&replSetReconfig|Initiate        #Sets system.replset (should get it with RS.conf() first)
(OBJ[,force#BOL])                 #Must be done on the primary member unless using force true.
  RS.reconfig|initiate(OBJ)       #Majority of members must be available. Will incur downtime of 10-60sec.
                                  #Difference reconfig|initiate():
                                  #  - do initiate first time, reconfig() after
                                  #  - no force option
RS.add("HOST[:PORT]"|OBJ[, BOOL]) #Like RS.reconfig() but only for adding an element to members OBJ_ARR
                                  #If true, sets arbiterOnly true.
                                  #New member must have empty dbPath (or copied from another member)
RS.addArb(...)                    #Same as RS.add(..., true)
RS.remove("HOST[:PORT]")          #Like RS.reconfig() but removing an element from members OBJ_ARR
                                  #Should shutdown it first.
&&replSetStepDown(NUM2[,force#BOL]#Make primary step down as secondary and ask for election.
[,secondaryCatchupPeriosSecs#NUM])#Cancelled if no secondary is elected within NUM (def: 10) seconds unless force true.
  RS.stepDown(NUM)                #Also calls &&replSetFreeze(NUM) (def: 60)
                                  #Closes all current connections.
&&replSetFreeze(NUM)
  RS.freeze(NUM)                  #Make it not seek election for NUM seconds.

local.*                           #Database storing replication-related info for the specific mongod (not replicated).
                                  #As opposed to system.replset which is replicated among mongods.
local.startup_log                 #Capped COLL filled upon startime:
                                  #  - _id STR: includes hostname + startTime
                                  #  - hostname STR
                                  #  - startTime[Local] DATE
                                  #  - cmdLine:
                                  #     - dbpath STR
                                  #     - VAR: VAL
                                  #  - pid UINT
                                  #  - buildInfo: OBJ returned by &hostInfo()
local.oplog.rs                    #Capped COLL containing the oplog:
                                  #  - history of write operations performed, used to replicate on other members
                                  #    (similar to PostgreSQL WAL async logging)
                                  #     - secondaries can replicate between each other if one is ahead
                                  #  - can change max size with cf--replication.oplogSizeMB or d--oplogSize
                                  #    (def: max. of 1GB or 5% of available memory, but no more than 50GB).
                                  #      - Can only be done the first time the oplog is created, otherwise more complicated:
                                  #         - restart members as standalone
                                  #         - save local.oplog.rs last OBJ, recreate local.oplog.rs as capped COLL with new
                                  #           size and saved last OBJ
                                  #         - restart members as not-standalone
                                  #      - Size should be enough for replicas to have access to all missing info even if
                                  #        lag behind
                                  #      - Could size down|up according to workflow (def. is quite high)
local.me                          #OBJ: { host: STR }

&isMaster()                       #Returns:
  DB.isMaster()                   #  - ismaster|secondary|arbiterOnly|passive|hidden BOOL
                                  #  - msg "isdbgrid": when is mongos
                                  #  - me "[HOST]:[PORT]"
                                  #  - primary "[HOST]:[PORT]"
                                  #  - hosts STR_ARR, as "[HOST]:[PORT]", excluding arbiter, hidden or passive
                                  #  - passives|arbiters STR_ARR, as "[HOST]:[PORT]"
                                  #  - electionId STR
                                  #  - tags OBJ
                                  #  - localTime DATE
                                  #  - min|maxWireVersion STR
                                  #  - setName REPL_SET_STR
                                  #  - maxBsonObjectSize UINT
&replSetGetStatus()               #Returns OBJ:
  RS.status()                     #  - set REPL_SET_STR
 ~ADMIN.replSetGetStatus()        #  - date DATE
                                  #  - myState NUM, among:
                                  #     - 0 ("STARTUP"): startup, before syncing
                                  #     - 5 ("STARTUP2"): syncing
                                  #     - 3 ("RECOVERING"): recovering
                                  #     - 9 ("ROLLBACK"): rollbacking.
                                  #     - 1 ("PRIMARY"): is up and is primary
                                  #     - 2 ("SECONDARY"): is up and is secondary
                                  #     - 7 ("ARBITER"): is up and is arbiter
                                  #     - 8 ("DOWN"): is down
                                  #     - 10 ("REMOVED"): removed from replica set
                                  #     - 6 ("UNKNOWN"): no info yet for this members (no pings)
                                  #  - members OBJ_ARR:
                                  #     - _id STR
                                  #     - name HOST[:PORT]
                                  #     - self BOOL: true if current member
                                  #     - health 1|0 (only if self false)
                                  #     - state[Str] NUM|STR: like myState (see above)
                                  #     - uptime NUM (in sec)
                                  #     - optime[Date] TIMESTAMP|DATE: last oplog entry time
                                  #     - electionTime|Date TIMESTAMP|DATE: for the primary, election time
                                  #     - lastHeartbeat[Recv] DATE: last sent|received heartbeat
                                  #     - lastHeartbeatMessage STR
                                  #     - pingMs NUM (in ms): ping time
                                  #     - syncingTo HOST[:PORT]: for secondaries
DB.getReplicationInfo()           #Returns OBJ:
                                  #  - logSizeMB NUM: max oplog size
                                  #  - usedMB NUM: used oplog size
                                  #  - errmsg STR: if no entries in oplog
                                  #  - oplogMainRowCount NUM
                                  #  - now DATE
                                  #  - tFirst|Last DATE: first|last entries in oplog
                                  #  - timeDiff[Hours] NUM: diff between first and last entries in oplog in sec|hours
DB|RS.printReplicationInfo()      #On:
                                  #  - a primary: pretty print DB.getReplicationInfo()
                                  #  - a secondary: pretty print RS.status() (only syncingTo and lags behind primary)
DB|RS.printSlaveReplicationInfo() #Show lag and last sync time for each slave.

MONGO.setReadPref                 #Sets how read queries are routed accross the replica set. STR can be:
(STR[, TAG_OBJ_ARR])              #  - "primary" (def): only on primary.
                                  #  - "secondary": only on secondaries. More performant but replication lag can
                                  #    introduce inconsistent view of data ("eventual consistency")
                                  #  - "primary|secondaryPreferred": like "primary|secondary" except if becomes unavailable,
                                  #    uses secondaries|primary instead.
                                  #  - "nearest": any of primary|secondaries with lowest latency
                                  #~*()#*readPreference uses READPREF, i.e.:
                                  #  - "NEAREST|..."
                                  #  - new ReadPreference(ReadPreference.NEAREST|..., TAG_OBJ_ARR) (for tags)
                                  #Can target specific secondaries with TAG_OBJ_ARR { TAG_VAR: TAG_VAL_STR, ... } or
                                  #cn--readPreferenceTags "VAR:VAL,...". Order indicates preference.
                                  #If several possible:
                                  #  - INT0 = lowest latency among those possible
                                  #  - random pick among any that have a latency <= INT0 +
                                  #    cf--replication.localPingThresholdMs, s--localThreshold or
                                  #    ~MONGO.connect()#replSet.secondaryAcceptableLatencyMS INT (in ms, def: 15)
                                  #Can also use:
                                  #  - cn--readPreference STR
                                  #  - [A]CUR.readPref(STR[, TAG_OBJ_ARR])
                                  #  - ~CUR.setReadPreference(READPREF)
                                  #  - ^[A]CUR.read(READPREF_STR[, TAG_OBJ])
                                  #  - ~MNGO.connect()#db.readPreference
                                  #  - ~DB.collection|createCollection|[executeDbAdmin]command|indexInformation()
                                  #    #readPreference
                                  #  - ~ADMIN.command()#readPreference
                                  #  - ~COLL.parallelCollectionScan|mapReduce|aggregate|count|distinct|findOne|geo*()
                                  #    #readPreference
                                  #  - ~CUR.count() READPREF
                                  #  - ~new GridStore()#readPreference
                                  #  - ~GridStore.exist|list|read*()#readPreference
                                  #  - ^SCHEMA_OPT.read STR or [ STR, TAG_OBJ ]
MONGO.setSlaveOk()
RS.slaveOk()
exp--slaveOk
~DB.slaveOk
CUR_CONF slaveOk
^CUR.setOptions()#slaveOk         #Same as MONGO.setReadPref("nearest").
MONGO.getReadPrefMode()           #Returns STR
MONGO.getReadPrefTagSet()         #Returns OBJ

&&replSetMaintenance(BOOL)        #Enable/disable maitainance mode:
                                  #  - only on secondaries
                                  #  - puts in RECOVERING: not accessible for read operations but continue replicating

mongooplog                        #Tool to apply oplog read --from "HOST[:PORT]" to another mongod.
                                  #The length of the oplog is --seconds INT (in sec, def: 1 day).
                                  #Copy from --oplogns STR (def: "local.oplog.ns", which should always be good)
                                  #Used to artificially recreate replication.


MASTER/SLAVE REPLICATION ==>      #Older version of replication.
                                  #Few cases when it might be better:
                                  #  - no limit on number of slaves
                                  #  - can be per-database
                                  #Uses different local.*:
                                  #  - local.oplog.$main: Master oplog (capped COLL)
                                  #  - local.slaves|sources:
                                  #     - on the master|slaves, info about the slaves|master
                                  #     - write to it to change config runtime
                                  #Must start mongod with either:
                                  #  --master
                                  #  --slave --source HOST[:PORT]
                                  #    --only DB: for single DB replication
                                  #    --slavedelay NUM (in sec.)
                                  #    ds--replApplyBatchSize NUM (d--1024, def: 1): how many entries to apply at once
                                  #Failover:
                                  #  - no automatic
                                  #  - must stop slave, removes data files for DB "local", then restart it as master
                                  #Slaves can &resync() if they are outdated:
                                  #  - done if >10 secs lags, but only once every 10 minutes unless --autoresync
                                  #  - if dbPath has been copied from master, can use --fastsync to make start faster
                                  #Can also use:
                                  #  - DB.serverStatus()
                                  #  - DB|RS.print[Slave]ReplicationInfo()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           SHARDING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


SHARDING ==>                      #Distribute COLLs across several mongod for easy horizontal scaling.
                                  #Use only if large amount of data, otherwise add complexity for nothing.
                                  #Architecture:
                                  #  - mongos are load balancers that clients access
                                  #     - must be start with command line mongos
                                  #     - do not hold any state/data
                                  #     - all sharding commands (unless stated otherwise) must be done on mongos
                                  #  - config servers maintain the system sharding conf to mongos:
                                  #     - each config servers is a copy of the others:
                                  #        - however they technically do not and cannot be an actual replica set
                                  #        - only need to backup one
                                  #     - there can be 1-3 servers, but there should be 3.
                                  #     - mongod started with cf--sharding.clusterRole "configsvr" or d--configsvr
                                  #       this also changes default port to 27019 and dbPath to "/data/configdb/"
                                  #     - mongos must target config servers with cf--sharding.configDB "HOST[:PORT],..." or
                                  #       s--configdb "HOST[:PORT],..."
                                  #     - when changed, either:
                                  #        - mongos must be restarted
                                  #        - &flushRouterConfig() must be called (called automatically after a chunk migration)
                                  #     - when HOST[:PORT] change or all config servers restarted, must restart all
                                  #       mongos + mongod
                                  #     - if:
                                  #        - 1/2 config servers is unavailable, becomes read-only,
                                  #          i.e. no more chunk migration nor splitting
                                  #        - all are unavailable, mongos will work unless they are restarted
                                  #  - mongod have the data split in chunks between them:
                                  #     - must be started with cf--sharding.clusterRole "shardsvr" or d--shardsvr
                                  #       This also changes default port to 27018
                                  #     - each mongod is a shard, the set is a cluster
                                  #     - for each sharded COLL, according to its shard key (def: _id):
                                  #        - cannot be multikey or special index
                                  #        - cannot change shard key, nor update it in any OBJ once inserted
                                  #        - "query isolation": should use shard key (starting with first ones for compound
                                  #          indexes) to only target few shards
                                  #           - required for single OBJ write operations
                                  #        - "read|write scalability": should have lot of different values
                                  #           - to distribute OBJs evenly accross shards
                                  #           - and to avoid unsplittable chunk because cannot split if whole shard has only
                                  #             one shard key value
                                  #              - marked as "jumbo" in SH.status(), i.e. will not try to split anymore
                                  #              - must manually &split()
                                  #           - can use compound index or hashed to do so
                                  #        - can use shard key hashing (i.e. hashed index):
                                  #           - advantage (read|write scalability):
                                  #              - distribute evenly queries accross shards.
                                  #              - good when would otherwise always target same shard
                                  #                 - e.g. inserts on incremental field (like _id or TIMESTAMP) or read
                                  #                   same ranges of incremental field
                                  #           - disadvantages (query isolation):
                                  #              - multiply multi-OBJs read|write accross shards even for operations that
                                  #                would select only one, and would select it quite evenly, e.g. read random
                                  #                ranges of incremental field
                                  #     - can mix sharded COLL with some non-sharded COLLs specific to a shard
                                  #       (the "primary shard")
                                  #         - primary shard also perform second stage of sort operations
                                  #     - chunks:
                                  #        - a shard can have several chunks
                                  #        - has cf--sharding.chunkSize INT (in MB, 1 to 1024, def: 64). Def. should be good.
                                  #          Smaller make distribution more even but require migrations more often.
                                  #          Also cannot move a chunk with more than 250000 docs, or 1.3 times chunkSize.
                                  #          Prefer larger.
                                  #        - operations:
                                  #           - chunk migration:
                                  #              - move from one cluster to another when adding/removing shards, or when data
                                  #                distribution becomes uneven after some time
                                  #              - while migrating keep temp copy if cf--sharding.archiveMovedChunks true (def)
                                  #              - done by mongos "balancer":
                                  #                 - Enabled by default
                                  #                 - Started on a random mongos from time to time
                                  #                 - Should be stopped during maintainance and backups.
                                  #           - split:
                                  #              - to create new chunks when data increases.
                                  #              - Can disable with cf--sharding.autoSplit false or s--noAutoSplit
                                  #        - when aborts, create orphans.
                                  #          Can &cleanupOrphaned("DB.COLL"[, secondaryThrottle#BOOL]), to run on mongod,
                                  #          not mongos.
                                  #All members must be able to connect to any other member.
                                  #To "unshard" a cluster, must create new servers, then copy sharded ones data
                                  #using mongodump/mongorestore.

RESTRICTIONS ==>                  #Not supported:
                                  #  - unique index:
                                  #     - except for non-hashed shard key using &shardCollection() unique true
                                  #  - index only queries without the shard key
                                  #  - geohaystack index, &geoNear|geoSearch()
                                  #  - &group()
                                  #  - S$out()
                                  #  - &&renameCollection()
                                  #  - capped COLL
                                  #  - Q_OBJ.$isolated 1, CUR|C$snapshot()
                                  #  - being specified in Q$where()
                                  #  - PITR
                                  #  - delayed replication
                                  #Only on single mongod (so must be done on each):
                                  #  - &compact()

sh                                #Returns SH in Mongo shell.

config.*                          #DB present on config servers. Should not be modified unless necessary because internal.
                                  #COLL are:
                                  #  - mongos:
                                  #     - ping DATE
                                  #     - up INT
                                  #     - waiting BOOL
                                  #     - mongoVersion STR
                                  #  - shards:
                                  #    - _id SHARD_STR
                                  #    - host "HOST[:PORT]"
                                  #  - chunks: list of chunks and their associated shards:
                                  #    - shard SHARD_STR
                                  #    - ns "DB.COLL"
                                  #    - min|max OBJ
                                  #    - lastmod[Epoch] TIMESTAMP|INT
                                  #  - databases: [non-]sharded DB:
                                  #     - _id 'DB'
                                  #     - partitioned BOOL: sharded or not
                                  #     - primary SHARD_STR
                                  #  - collections: sharded COLL (including previous ones):
                                  #     - _id "DB.COLL"
                                  #     - lastmod[Epoch] DATE|INT
                                  #     - dropped BOOL
                                  #     - key OBJ
                                  #     - unique BOOL
                                  #  - tags
                                  #  - settings: chunk size, stopped status (can use activeWindow: start|stop DATE for
                                  #    scheduled maintainance), etc.:
                                  #     - _id STR, e.g. "chunksize" or "balancer"
                                  #     - VAL, depends on _id. For "chunksize", value NUM. For "balancer", stopped BOOL
                                  #  - version: metadata version number:
                                  #     - clusterId OID
                                  #     - version NUM
                                  #     - minCompatibleVersion NUM
                                  #     - currentVersion NUM
                                  #  - changelog: previous dropCollection|Database, moveChunks or split events:
                                  #     - server "HOST"
                                  #     - clientAddr STR
                                  #     - time DATE
                                  #     - what STR
                                  #     - ns "DB.COLL"
                                  #     - details OBJ
                                  #  - lockpings: pings to processes
                                  #     - ping DATE
                                  #  - locks: locks so only one mongos does admin tasks at once:
                                  #     - state 0|1
                                  #     - who STR
                                  #     - ts OID
                                  #     - process STR
                                  #     - when DATE
                                  #     - why STR
&isdbgrid()                       #Returns:
                                  #  - ok 1|0: if mongos or mongod
                                  #  - hostname STR: if mongos
&&shardingState()                 #Returns:
                                  #  - if not primary in replica set or not sharded: ok 0
                                  #  - if mongos: error
                                  #  - if config server:
                                  #     - enabled false
                                  #  - otherwise:
                                  #     - enabled true
                                  #     - configServer STR
                                  #     - shardName STR
                                  #     - shardHost "HOST[:PORT]"
                                  #     - versions: DB.COLL: TIMESTAMP
SH.status([BOOL])                 #Returns:
  DB.printShardingStatus([BOOL])  #  - sharding-version:
                                  #     - clusterId CLUSTER_STR
                                  #     - version STR: config server version
                                  #     - currentVersion STR
                                  #     - minCompatibleVersion STR: config server min. compatible version
                                  #  - shards OBJ_ARR:
                                  #     - _id SHARD_STR
                                  #     - host "HOST[:PORT]"
                                  #     - tags OBJ
                                  #  - databases OBJ_ARR:
                                  #     - _id 'DB'
                                  #     - partitioned BOOL: where sharding is enabled
                                  #     - primary SHARD_STR
                                  #     - COLL:
                                  #        - shard-key INDEX_OBJ
                                  #        - chunks OBJ_ARR (limited to 20 items unless BOOL true):
                                  #           - SHARD_STR: NUM (of chunks)
                                  #        - chunk-details OBJ_ARR: shard key range for each SHARD, and mtime
                                  #        - tag OBJ
                                  #  - balancer:
                                  #     - currently-enabled|running BOOL
                                  #     - collections-with-active-migrations 'COLL'_ARR|undefined
                                  #     - failed-balancer-rounds-in-last-5-attempts NUM
                                  #     - last-reported-error STR
                                  #     - time-of-reported-error DATE
                                  #     - migration-results-for-the-last-24-hours OBJ_ARR
COLL.getShardDistribution()       #Prints COLL repartition in each shard: data size / number of docs [per chunk] and number
                                  #of chunks.

&enableSharding('DB')
SH.enableSharding('DB')           #Make it possible to use &shardCollection() on DB
&shardCollection("DB.COLL"        #By def., COLL are not sharded. Make COLL sharded. Cannot be undone (including INDEX choice).
[, key#INDEX_OBJ][, unique#BOOL]  #The INDEX must exist, unless COLL is empty, in which case it is created.
[, numInitialChunks#INT])         #unique BOOL (def: false): for index.
SH.shardCollection("DB.COLL",     #numInitialChunks NUM (<8192, def: 1): only if COLL empty.
key#INDEX_OBJ[, unique#BOOL])     #COLL must be <256GB while performing this (but can grows more afterwards).

SH.start|stopBalancer(NUM, NUM2)  #Timeout after NUM ms, and check if command completed every NUM2 ms.
SH.setBalancerState(BOOL)         #Enable/disable balancer
SH.enable|disableBalancing
("DB.COLL")                       #Enable/disable balancer for a single COLL
SH.getBalancerState()             #Gets if balancer is disabled or not started.
SH.isBalancerRunning()            #Gets if balancer is currently moving chunks.
SH.getBalancerHost()              #Gets "HOST[:PORT]" of the mongos currently running the balancer.

&&addShard                        #Adds a shard to the cluster.
("[REPL_SET_STR/]HOST[:PORT]"     #maxSize is in MB (def: 0, i.e. unlim), useful when machines have different capacities.
[, maxSize#NUM][, name#SHARD_STR])#Def. SHARD_STR is automatically generated.
  SH.addShard("[RPL/]HOST[:PORT]")#
&&removeShard(SHARD_STR)          #Removes a non-primary shard from the cluster. Balancer must be on.
                                  #Return status (can be fire again to see progress):
                                  #  - shard SHARD_STR
                                  #  - state "started|ongoing|completed"
                                  #  - msg STR: verbose version of state
&&listShards()                    #Returns SHARD_STR_ARR

&&movePrimary                     #Modifies which one SHARD is the primary.
(SHARD_STR, to#SHARD2_STR)        #Takes some time. While doing it, should not access non-sharded COLLs.
                                  #After doing it, must either restart all mongos or &flushRouterConfig() before writing to
                                  #any COLL.

SH._lastMigration("DB.COLL")      #Returns last chunk move:
                                  #  - _id STR
                                  #  - server HOST
                                  #  - clientAddr IP[:PORT]: of the server
                                  #  - time DATE
                                  #  - what STR: type
                                  #  - ns "DB.COLL"
                                  #  - details OBJ

MANUAL MOVES ==>                  #Manually do what balancer does automatically. Only for special cases.
&&moveChunk("DB.COLL",            #Manually move chunks to another shard.
find#OBJ|bounds|OBJ_ARR,          #OBJ specifies according to shard key:
to#SHARD_STR                      #  - find: single document
[, _secondaryThrottle#BOOL])      #  - bounds: several documents. To use if hashed shard key.
  SH.moveChunk("DB.COLL",         #Indexes must already exist on SHARD_STR
  query#OBJ,destination#SHARD_STR)#If BOOL true (def), waits for replication while performing the migration.
&&mergeChunks("DB.COLL",          #Merge two chunks into one, one chunk being empty, and both being on same shard.
bounds#OBJ_ARR)                   #Only for testing (balancer does it automatically).
&&split("DB.COLL",                #Divides a chunk into two:
find|middle#OBJ|bounds#OBJ_ARR)   #  - find: select a chunk then divides into two egal sized chunk
  SH.splitAt|splitFind            #  - bounds: to use if hashed shard key
  ("DB.COLL", OBJ)                #  - middle: divide in a specific place. To use if empty COLL.

SH.add|removeTagRange             #Direct objects between OBJ (included) and OBJ2 (excluded) (same OBJ as &&split()) to a
("DB.COLL", OBJ, OBJ2, STR)       #specific shard, specified with SH.add|removeShardTag(SHARD_STR2, STR).
                                  #If hashed shard key, must target whole COLL.
                                  #Good for example to direct data close to data center geographically close to where it will
                                  #be used.

partial                           #CUR_CONF that let queries return partial results if some shards are down.

[A]CUR.explain() EXTRA OUTPUT ==> #  - shards: SHARD: EXPL_OBJ
                                  #  - clusteredType "ParallelSort|SerialServer"
                                  #  - millisShardTotal|Avg NUM
                                  #  - numQueries NUM
                                  #  - numShards NUM: number of shards queried


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MONITORING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


&dbStats([scale#NUM])             #Size/storage stats about a DB.
  DB.stats([NUM])                 #NUM allow multiplying output bytes values.
 ~DB.stats([scale#NUM])           #Return:
                                  #  - db 'DB'
                                  #  - collections NUM: number of COLLs
                                  #  - objects NUM: number of OBJs
                                  #  - avgObjSize NUM
                                  #  - dataSize|storageSize NUM: dataSize also takes into account padding
                                  #  - indexes NUM: number of indexes
                                  #  - indexSize NUM
                                  #  (only with mmapv1)
                                  #  - fileSize NUM: like dataSize but includes OS preallocation
                                  #  - nsSizeMB NUM: size to register COLLs names
                                  #  - dataFileVersion: major|minor STR
                                  #  - numExtents NUM: number of extents (sub-division of an OBJ)
                                  #  - extentFreeList: num NUM, size NUM
&collStats('COLL'[, scale#NUM]    #Size/storage stats about a COLL
[, verbose#BOOL])                 #Return:
  COLL.stats([scale#NUM]          #  - ns "[DB.]COLL"
  [, indexDetails#BOOL]           #  - count NUM: number of OBJs
  [, indexDetailsName|Field#STR]) #  - [storage]size NUM: like dataSize|storageSize above.
 ~COLL.stats([scale#NUM])         #    Can also use COLL.data|storageSize()
                                  #  - avgObjSize NUM
                                  #  - capped BOOL
                                  #  - max NUM: for capped COLL
                                  #  (if indexDetails true (def: false). Can filter with indexDetailsName|Field)
                                  #  - nindexes NUM
                                  #  - totalIndexSize NUM
                                  #    Can also use COLL.totalIndexSize()
                                  #  - indexSizes: VAR: NUM
                                  #  (mmapv1 only, with verbose true)
                                  #  - numExtents NUM
                                  #  - lastExtentSize NUM
                                  #  - [system|user]Flags 0-3: if using padding or not
                                  #  (wiredTiger only)
                                  #  - wiredTiger OBJ
  COLL.totalSize()                #Like COLL.totalIndexSize() + COLL.storageSize()
  DB.printCollectionStats()       #Do COLL.stats() on each COLL
&validate('COLL'[, full#BOOL])    #Similar to &collStats() but provide more info about low-level (extents and deleted records),
  COLL.validate([BOOL])           #and possible file-level errors.
 ~ADMIN.validateCollection        #Is resource intensive, and can get long-running write lock.
  ('COLL'[, full#BOOL])           #Returns:
                                  #  - ns 'COLL'
                                  #  - datasize|bytesWith[out]Headers NUM: second also include records headers.
                                  #    datasize should == bytesWithoutHeaders
                                  #  - nrecords|objectsFound NUM: should be same
                                  #  - nIndexes NUM
                                  #  - keysPerIndex: VAR: NUM
                                  #  - deletedCount NUM
                                  #  - deletedSize NUM
                                  #  - padding NUM: 1 to 2
                                  #  - first|lastExtent OBJ: location on disk
                                  #  - extentCount NUM
                                  #  - firstExtentDetails|extents OBJ_ARR
                                  #     - first|lastRecord OBJ
                                  #     - loc OBJ
                                  #     - xnext|prev OBJ
                                  #     - nsdiag 'COLL': should be same as ns
                                  #     - size NUM
                                  #  - lastExtentSize NUM
                                  #  - invalidObjects NUM: objects not BSON
                                  #  - valid BOOL
                                  #  - errors OBJ_ARR: empty if valid true

&hostInfo()                       #Returns OBJ:
  DB.hostInfo()                   #  - system:
                                  #     - currentTime DATE_STR
                                  #     - hostname STR: can also use hostname|getHostName()
                                  #     - cpuAddrSize 32|64
                                  #     - memSizeMB UINT
                                  #     - numCores UINT
                                  #     - cpuArch x86[_64]
                                  #     - numaEnabled BOOL
                                  #  - os:
                                  #     - type STR, e.g. "Linux". Can also use _isWindows()->BOOL
                                  #     - name STR
                                  #     - version STR
                                  #  - extra:
                                  #     - versionString STR: long OS name
                                  #     - libcVersion STR
                                  #     - kernelVersion STR
                                  #     - cpuFrequencyMHz STR
                                  #     - cpuFeatures STR: from /proc/cpuinfo
                                  #     - pageSize UINT: in bytes
                                  #     - numPages UINT
                                  #     - maxOpenFiles UINT: from ulimit
d--sysinfo                        #Show pages info then exits.
getMemInfo()                      #Returns OBJ about OS memory with two members resident OBJ and virtual OBJ
&getCmdLineOpts()                 #mongod invocation. Returns OBJ:
  DB.serverCmdLineOpts()          #  - argv STR_ARR
                                  #  - parsed OBJ: same as OBJ
DB.version()                      #Returns MongoDB version
&buildInfo()                      #Returns OBJ about mongoDB:
  DB.serverBuildInfo()            #  - version STR: mongoDB version. Can also use version()
 ~ADMIN.buildInfo|serverInfo()    #  - versionArray UINT_ARR: mongoDB version
                                  #  - gitVersion STR: mongoDB version
                                  #  - sysInfo STR: OS, hostname, date, Boost version
                                  #  - loaderFlags|compilerFlags STR: while building mongoDB
                                  #  - allocator "tcmalloc|system"
                                  #  - javascriptEngine "SpiderMonkey|V8"
                                  #  - bits 32|64
                                  #  - debug BOOL
                                  #  - maxBsonObjectSize UINT
~ADMIN.ping()                     #Returns { ok: 1 }

&&serverStatus([OBJ])             #All fields are returned by def., except workingSet. Can use OBJ.workingSet: 1 to add it.
  DB.serverStatus([OBJ])          #Can use OBJ.VAR: 0 to suppress any returned value, e.g. slow ones like recordStats.
 ~ADMIN.serverStatus()            #Returns:
                                  #  - host STR
                                  #  - version STR
                                  #  - process "mongos|mongod"
                                  #  - uptime[Estimate] NUM
                                  #  - localTime STR
                                  #  - locks: DB ("." for global): OBJ (like DB.currentOp() lockStats)
                                  #  - globalLock:
                                  #     - totalTime NUM: similar to uptime
                                  #     - lockTime NUM: how long lock has been held
                                  #     - currentQueue|activeClients: total|readers|writers NUM
                                  #  - mem:
                                  #     - bits 32|64
                                  #     - supported BOOL: whether supports extended mem info
                                  #     - resident NUM (in MB): RAM used
                                  #     - virtual NUM (in MB): memory used (including RAM)
                                  #     - mapped NUM (in MB): mapped memory used (usually database size)
                                  #     - mappedWithJournal NUM (in MB): if journalling, mapped NUM * 2
                                  #  - connections:
                                  #     - current NUM
                                  #     - available NUM
                                  #     - totalCreated NUM: includes previously closed
                                  #  - extra_info:
                                  #     - heap_usage_bytes NUM
                                  #     - page_faults NUM
                                  #  - indexCounters:
                                  #     - hits|misses NUM: whether index was in RAM
                                  #     - accesses NUM: hits + misses
                                  #     - missRatio DOUBLE: hits/misses
                                  #     - resets NUM: number of times indexCounters stats have been reset since last server
                                  #       start
                                  #  - backgroundFlushing:
                                  #     - flushes NUM
                                  #     - total_ms|average_ms|last_ms NUM: time spent flushing
                                  #     - last_finished TIMESTAMP
                                  #  - network:
                                  #     - bytesIn|Out NUM
                                  #     - numRequests NUM
                                  #  - repl:
                                  #     - setName STR
                                  #     - ismaster|secondary BOOL
                                  #     - hosts STR_ARR, primary STR, me STR
                                  #     - slaves OBJ_ARR:
                                  #        - host STR
                                  #        - optime STR
                                  #        - memberID
                                  #  - opcounters[Repl]: insert|query|update|delete|getmore|command NUM
                                  #  - asserts: errors flagged by MongoDB:
                                  #     - regular|warning|msg|user NUM: number of errors by type
                                  #     - rollovers NUM: number of times asserts stats have been reset since last server
                                  #  - writeBacksQueued BOOL: true if some operations are queued for retrying
                                  #  - dur: (journalling)
                                  #     - commits NUM: for last commit
                                  #     - commitsInWriteLock NUM: commits while a write lock held. Indicates heavy write load.
                                  #     - earlyCommits NUM: commits happening before normal time. Indicates too low commit
                                  #       interval.
                                  #     - journaledMB NUM: last time to journal
                                  #     - writeToDataFilesMB NUM: last time from journal to hard drive
                                  #     - compression DOUBLE: journal size / uncompressed size
                                  #     - timeMs (for last commit):
                                  #        - dt NUM (in ms): how long timeMs collected info
                                  #        - prepLogBuffer NUM (in ms): how long preparing to write to journal
                                  #        - writeToJournal NUM (in ms): how long writing to journal
                                  #        - writeToDataFiles NUM (in ms): how long writing from journal to data
                                  #        - remapPrivateView NUM (in ms): how long cleaning up
                                  #  - recordStats: [DB:]
                                  #     - accessesNotInMemory NUM: number of times data was not in RAM
                                  #     - pageFaultExceptionsThrown NUM: number of times data launched a page fault
                                  #  - workingSet:
                                  #     - pagesInMemory NUM: number of pages used. Multiply by 4KB to get working set size.
                                  #       If equals RAM size and overSeconds is small, means data is too big to fit.
                                  #     - overSeconds NUM (in ms): how long pages have been tracked for pagesInMemory
                                  #     - computationTimeMicros NUM (in ms): how long calculating workingSet took
                                  #  - rangeDeleter OBJ: info on &cleanupOrphaned() and cleanup by &moveChunk()
                                  #     - lastDeleteStats: last cleanup operations OBJ_ARR:
                                  #        - deletedDocs NUM
                                  #        - queue|delete|waitForReplStart|End TIMESTAMP
                                  #  - security:
                                  #     - SSLServerSubjectName STR
                                  #     - SSLServerHasCertificateAuthority BOOL
                                  #     - SSLServerCertificateExpirationDate DATE
                                  #  - storageEngine: name STR
                                  #  - wiredTiger:
                                  #     - concurrentTransactions.read|write: out NUM, available NUM, totalTickets NUM
                                  #     - LSM OBJ
                                  #     - block-manager OBJ
                                  #     - cache OBJ
                                  #     - connection OBJ
                                  #     - cursor OBJ
                                  #     - data-handle OBJ
                                  #     - log OBJ
                                  #     - reconciliation OBJ
                                  #     - session OBJ
                                  #     - thread-yield OBJ
                                  #     - transactions OBJ
                                  #  - metrics:
                                  #     - document: deleted|inserted|returned|updated NUM: number of OBJs
                                  #     - record: moves NUM: number of records moves due to &update() changing OBJs size
                                  #     - commands: CMD: failed|total NUM
                                  #     - operation:
                                  #        - fastmod NUM: number of &update() that did not change OBJ size
                                  #        - idhack NUM: queries using _id
                                  #        - scanAndOrder NUM: queries sorting without being able to use index for it
                                  #     - queryExecutor: scanned: same as explain() nscanned
                                  #     - repl:
                                  #        - apply:
                                  #           - ops NUM: number of replication operations applied
                                  #           - batches:
                                  #              - num NUM: number of replication batches applied
                                  #              - totalMillis NUM (in ms): how long spent applying replication batches
                                  #        - buffer:
                                  #           - count NUM: number of replication operations buffered
                                  #           - [max]sizeBytes NUM: current|max buffer size
                                  #        - network:
                                  #           - bytes NUM: read during replication
                                  #           - getmores:
                                  #              - ops NUM: number of operations read from replication
                                  #              - num NUM: number of getmores on the replication CUR
                                  #              - totalMillis NUM: time spent on getting new replication data
                                  #              - readersCreated NUM: number of CUR created
                                  #        - oplog:
                                  #           - insert:
                                  #              - num NUM: number of insertions operations in the oplog
                                  #              - totalMillis NUM (in ms): how long inserting in the oplog
                                  #              - insertBytes NUM
                                  #        - preload:
                                  #           - docs|indexes OBJ: like insert above, but for prefetched OBJs or indexes
                                  #    - storage: freelist: search:
                                  #       - requests NUM: number of times looked for memory records
                                  #       - bucketExhausted NUM: same but did not find any
                                  #          - scanned NUM: number of records scanned
                                  #    - ttl:
                                  #       - deletedDocuments NUM
                                  #       - passes NUM: number of times background process looked for docs to remove
                                  #    - cursor:
                                  #       - timedOut NUM
                                  #       - open:
                                  #          - total NUM
                                  #          - noTimeout NUM: number with this flag on
                                  #          - pinned NUM
                                  #          - singleTarget|multiTarget NUM: targets one or several shards

&connPoolStats()                  #Returns info about open connections:
                                  #  - totalAvailable|Created NUM
                                  #  - numDBClientConnection NUM: normal connections
                                  #  - numAScopedConnection NUM: connections that close only on socket exceptions
                                  #  - hosts: between cluster components
                                  #     - HOST: available|created NUM
                                  #  - replicaSets: between replica sets components
                                  #     - SHARD:
                                  #        - host OBJ_ARR:
                                  #           - addr "[HOST]:[PORT]"
                                  #           - ok BOOL: false if connection problem
                                  #           - ismaster|secondary|hidden BOOL
                                  #           - tags OBJ
                                  #           - pingTimeMillis NUM (in ms)
                                  #        - master NUM: index in host OBJ_ARR of the primary
                                  #  - createdByType:
                                  #     - master NUM: between clusters
                                  #     - set NUM: to replica sets components
                                  #     - sync NUM: to config databases
&shardConnPoolStats()             #Similar but for connections between members of a cluster.
                                  #Returns same OBJ as &connPoolStats() except:
                                  #  - no num*Connection
                                  #  - threads:
                                  #     - hosts:
                                  #        - host STR
                                  #        - created|avail NUM
                                  #     - seenNS STR

DB.currentOp([Q_OBJ|BOOL])        #Can query by Q_OBJ. BOOL true means including all OBJs.
                                  #Returns pending operations as OBJ_ARR:
                                  #  - opid NUM
                                  #  - threadId STR
                                  #  - active BOOL: false if queued/waiting for lock
                                  #  - waitingForLock BOOL
                                  #  - [micro]secs_running LONGLONG|NUM
                                  #  - op "query|insert|update|remove|getmore|command|none|killcursors"
                                  #     - "getmore" is when requesting CUR batches
                                  #  - ns "DB.COLL"
                                  #  - insert OBJ: if op "insert"
                                  #  - query OBJ: if op not "insert"
                                  #  - planSummary STR
                                  #  - client IP_STR: "0.0.0.0" for some operations
                                  #  - connectionId NUM
                                  #  - desc STR: of the client, including connectionId
                                  #  - locks: "DB": OBJ:
                                  #      - type "Global|MMAPV1Journal|Database|Collection|Metadata|oplog"
                                  #      - mode "r|w|R|W": see below
                                  #  - progress: done|total NUM (for &mapReduce() or indexes)
                                  #  - msg STR: progress as STR
                                  #  - killPending BOOL: true if DB.killOp() used
                                  #  (mmapv1 only)
                                  #  - numYield INT: number of locks yielded
                                  #  - lockStats: timeAcquiring|LockedMicros:
                                  #     - r|w LONGLONG (database lock waited|held in microseconds)
                                  #     - R|W LONGLONG (global lock)
DB.killOp(NUM)                    #Terminates an operation (NUM is DB.currentOp() opid)
                                  #Do not use on system operations.
&&top()                           #Returns current activities per COLL:
                                  #  - totals:
                                  #     - COLL:
                                  #        - readLock|writeLock:
                                  #           - time NUM (in ms)
                                  #           - count NUM
                                  #        - total|queries|insert|update|remove|commands|getmore:
                                  #           - time NUM (in ms)
                                  #           - count NUM

mongostat NUM                     #Realtime stats at DB level.
                                  #Show stats updated every NUM seconds, for --rowcount NUM seconds (def: 0)
                                  #Also:
                                  #  --noheaders: do not show first row
                                  #  --discover: show info about other members of the same replica set or shards
                                  #  --http: use HTTP instead of MongoDB protocol (if firewall restrictions)
                                  #  sta,top--json
                                  #Show following fields (use --all to show all of them):
                                  #  - inserts NUM: number of OBJs inserted per sec
                                  #  - query|update|delete|getmore|command|flushes NUM: number of such operations per sec
                                  #  - [non-]mapped NUM (in MB): virtual memory
                                  #  - size NUM: mapped + non-mapped
                                  #  - res NUM: resident memory
                                  #  (wiredTiger only)
                                  #  - dirty|used NUM: percentage of cache dirty|used
                                  #  (mmapv1 only)
                                  #  - mapped NUM: data mapped in MB (i.e. total data size)
                                  #  - faults NUM: page faults per sec
                                  #  - idx miss NUM: % of time loading an INDEX required a page fault
                                  #  (all)
                                  #  - conn NUM: open connections
                                  #  - ar|aw NUM: number of clients reading|writing
                                  #  - qr|qw NUM: number of clients waiting for read|write
                                  #  - netIn|Out NUM: traffic in bytes
                                  #  - set STR: replica set
                                  #  - repl: "M" (master), "SEC" (secondary), "REC" (recovery), "UNK" (unknown),
                                  #    "SLV" (slave), "RTR" (mongos)
                                  #  - flushes NUM: number of journal sync
mongotop [NUM]                    #Real time stats about current activity, at COLL level, active COLLs only.
                                  #Show stats updated every NUM second (def: 1)
                                  #Outputs one row per COLL:
                                  #  - ns "DB.COLL"
                                  #  - db 'DB' ("." for global) (only if --locks)
                                  #  - total|read|write: amount of time spent reading|writing (in ms)
                                  #  - TIMESTAMP
d--cpu                            #Show CPU info every 4 secs

cf--net.http.RESTInterfaceEnabled #If true (def: false):
d--rest                           #  - gives REST interface, with only HOST/DB/COLL/ and query variables limit, skip,
                                  #    filter_VAR (Q_OBJ.VAR)
                                  #     - use DB "$cmd" for $CMD()
                                  #  - if also cf--net.http.enabled true (def: false) or d,s--httpinterface,
                                  #    or cf--net.http.JSONPEnabled true (def: false) or d,s--jsonp,
                                  #    gives HTTP/JSONP console at HOST with PORT + 1000:
                                  #     - can run various monitoring commands, and show current operations
                                  #  - does not support "SCRAM-SHA-1" authentication mode
                                  #  - can allow security problems if not managed

mongosniff [PORT...]              #Similar to Wireshark but for MongoDB protocol. Requires libpcap.
                                  #PORT is 27017 by def.
                                  #Must specify:
                                  #  - source:
                                  #     --source NET INTERFACE: e.g. eth0
                                  #     --source FILE FILE: a PCAP file
                                  #   - destination: --forward HOST[:PORT] or (replica set) STR/HOST[:PORT],...
                                  #For driver development. Use --objcheck to only show invalid BSON objects.

mongoperf                         #Show I/O performance (independently from MongoDB).
                                  #Can be used together with iostat -xtm 1 (%util show I/O percentage use)
                                  #Gets arguments from stdin as JSON object:
                                  #  - nThreads NUM (def: 1, 16 is good idea)
                                  #  - fileSizeMB NUM (def: 1): test file size
                                  #  - sleepMicros NUM (def: 0): pause for NUM/nThreads secs between operations
                                  #  - mmf BOOL: if:
                                  #     - false (def): test direct/physical I/O, without caching. Use big fileSizeMB to
                                  #       simulate lot of traffic.
                                  #     - true: test mmap, i.e. caching system
                                  #  - r|w BOOL: must use either one as true to specify either read or write operations
                                  #  - recSizeKB NUM (def: 4): size of write operations
                                  #  - syncDelay NUM (def: 0, i.e. do not change): changes cf--storage.syncPeriodSecs

A FILE IN DBPATH ==>              #Holds stats if cf--storage.wiredTiger.engineConfig.statisticsLogDelaySecs NUM or
                                  #d--wiredTigerStatisticsLogDelaySecs (def: 0)

SNMP ==>                          #MongoDB Enterprise only.
                                  #Can use cf--snmp.subagent|master or d--snmp-subagent|master BOOL
                                  #See online doc.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            LOGGING            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOGGING ==>                       #Logs are "TIMESTAMP SEVERITY COMPONENT [CONTEXT] MSG":
                                  #  - TIMESTAMP: cf--systemLog.timeStampFormat or d,s--timeStampFormat STR, among:
                                  #     - "ctime", e.g. "Wed Dec 31 18:17:54.811"
                                  #     - "iso860d--utc", e.g. "1970-01-01T00:00:00.000Z"
                                  #     - "iso860d--local", e.g. "1969-1s--31T19:00:00.000+0500" (def)
                                  #  - SEVERITY: F (fatal), E (error), W (warning), I (info), D (debug).
                                  #  - COMPONENT can be:
                                  #     - QUERY: read commands and query planner
                                  #     - WRITE: write commands
                                  #     - GEO: geo* commands
                                  #     - COMMAND: other commands
                                  #     - INDEX
                                  #     - ACCESS: access control / authentication
                                  #     - REPL
                                  #     - SHARDING
                                  #     - NETWORK
                                  #     - CONTROL: initialization, etc.
                                  #     - STORAGE (includes JOURNAL)
                                  #     - JOURNAL
                                  #     - -: others
                                  #Can modify:
                                  #  - verbosity:
                                  #     - cf--systemLog.verbosity, ds--logLevel INT (def: 1, 0 to 5) or cli,o,bsn--verbose
                                  #       (once increase logLevel by 1). 0 does not include D (debug), 1-5 include it.
                                  #     - for specific COMPONENT verbosity, use:
                                  #        - DB.setLogLevel(INT[, COMPONENT_STR]) (where COMPONENT is camelCase)
                                  #        - cf--systemLog.component.verbosity INT (where component is lowercase)
                                  #        - ds--logComponentVerbosity[.component].verbosity (where component is camelCase)
                                  #     - DB.getLogComponents()->OBJ[.component].verbosity INT (-1 if not set, component is
                                  #       camelCase)
                                  #     - cf--systemLog.quiet BOOL or ds--quiet 0|1: erase logs of some things like connection
                                  #       or replication events
                                  #     - ds--logUserIds 0|1 (def: 0)
                                  #  - destination:
                                  #     - can be:
                                  #        - stdout (def)
                                  #        - file: cf--systemLog.destination "file" + cf--systemLog.path or d,s--logpath FILE
                                  #           - at restart, only append if cf--systemLog.logAppend true or d,s--logappend
                                  #           - can use &&logRotate() or send SIGUSR1 signal to, according to
                                  #             d,s--logrotate STR or --cf--systemLog.logRotate STR:
                                  #              - "rename" (def): add timestamp to filename, saves it then create new
                                  #                log file without timestamp in filename.
                                  #              - "reopen": close then reopen same file. To use when using Unix
                                  #                command logrotate
                                  #        - syslog: cf--systemLog.destination "syslog" or d,s--syslog, and
                                  #          cf--systemLog.syslogFacility or d,s--syslogFacility STR (def: "user")
&&getLog(STR)                     #Returns recent logs for STR:
                                  #  - "global": general logs
                                  #  - "rs": replica-set-related logs
                                  #  - "startupWarnings": startup problems
                                  #  - "*": all


~new Logger(STR                   #Rreturns LOGGER, doing Node.js client logging (not server logging).
 [,logger#FUNC(STR2[, OBJ])->STR2]#Used by Node.js driver but can use it too.
 [, loggerLevel#STR])             #STR is prepended to log messages, and is the category. Node.js uses this as
                                  #"Db|Server|ReplSet|Mongos|Cursor|Pool|Connection|Ping" by thoses classes.
                                  #loggerLevel is "error" by def. FUNC is console.log by def
~Logger.currentLogger()           #Rreturns FUNC(STR2[, OBJ])->STR2
~Logger.setCurrentLogger
 (FUNC(STR2[, OBJ])->STR2)        #
~LOGGER.isDebug|Info|Error()      #Check logLevel, rreturns BOOL
~LOGGER.debug|info|error
 (STR[, OBJ])                     #OBJ are metadata
~Logger.setLevel(STR)             #
~Logger.filter(STR, STR2_ARR)     #Only log specific objects named in STR2_ARR.
                                  #STR is the first STR of Logger constructor.
~Logger.reset()                   #Reset parameters


edda FILE                         #Produces picture of the servers, using a log FILE.
                                  #Connects to a mongod|mongos using --port and --host. Can specify --db and --collection
                                  #Install with pip


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           PROFILING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


DB.system.profile                 #Capped COLL registering received commands.
                                  #OBJ:
                                  #  - ts TIMESTAMP
                                  #  - op, ns, query, client, numYield, lockStats: see DB.currentOp()
                                  #  - updateobj U_OBJ
                                  #  - command STR (if "command")
                                  #  - cursorid OID (if "getmore")
                                  #  - ntoreturn INT: number OBJ asked to return (if "command|getmore|query")
                                  #  - nreturned INT: number OBJ actually returned
                                  #  - nskip INT: number OBJ skip()'d
                                  #  - responseLength INT: in bytes
                                  #  - nscanned INT: number OBJ scanned in the index
                                  #  - scanAndOrder BOOL: if false, could use the index for sorting
                                  #  (mmapv1 only)
                                  #  - moved BOOL: whether the update physically moved OBJ on the disk, which happens when
                                  #    COLL size grows
                                  #  - nmoved INT: number of OBJ physically moved
                                  #  (all)
                                  #  - nupdated INT: number of OBJ updated
                                  #  - keyUpdates INT: number of OBJ updated that updated an index key as well (slower than
                                  #    other fields)
                                  #  - millis INT: operation time from mongod perspective
                                  #  - user STR
show profile
~ADMIN.profilingInfo()            #Show DB.system.profile (only important entries)
&profile(NUM, slowms#INT)         #Sets how DB.system.profile is filled:
                                  #  - -1: no change
                                  #  - 0 (def): not
                                  #  - 1: only slow operations according to
                                  #       cf--operationProfiling.slowOpThresholdMs, slowms or d--slowms INT (def: 100)
                                  #  - 2: all operations
                                  #Can also use cf--operationProfiling.mode "off|slowOp|all" or d--profile NUM.
                                  #Affects performance.
DB.getProfilingStatus()           #Wrapper returning { profile NUM, slowOpThresholdMs NUM2 }
 ~ADMIN.profilingLevel()          #
DB.setProfilingLevel(NUM, NUM2)   #
 ~ADMIN.setProfilingLevel(STR)    #Wrapper.
 ^DB.setProfiling(NUM, NUM2)      #STR is "off|slow_only|all"

C$comment(STR)
[A]CUR.comment(STR)
 ~CUR.comment(STR)
 ^CUR.comment(STR)
 ^CUR.setOptions()#comment        #Add arbitrary comment, so it appears in DB.system.profile.query. Rreturns CUR.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           SECURITY            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


OS USER ==>                       #Should create a "mongodb" OS_USER (like postgres for PostgreSQL) with no permissions.
                                  #Unless ds--enableLocalhostAuthBypass 0|1, localhost can create first user in admin database,
                                  #providing it does not already exist.

USERS AND ROLES ==>               #A user must authenticate.
                                  #Each user has roles, which defines possible actions on defined resources.
                                  #All this is database-wise.
                                  #Must be enabled first (def: off) with cf--security.authorization "enabled|disabled" or
                                  #d--[no]auth
                                  #Sharding:
                                  #  - mongos cache information, renewing it every ds--userCacheInvalidationIntervalSecs INT
                                  #    (def: 30) or when &invalidateUserCache()
                                  #  - can create users/roles that only appear while connected directly to mongod, not through
                                  #    mongos
                                  #Sharding and replica sets:
                                  #  - members authenticate with each other using identical file-password at
                                  #    cf--security.keyFile FILE or d,s--keyFile FILE

KERBEROS ==>                      #Use $external DB

SASL ==>                          #MongoDB Enterprise only (LADP/Kerberos)
                                  #Use:
                                  #  - inout--authenticationMechanism "GSSAPI"
                                  #  - cf--security.sasl.saslauthdSocketPath FILE
                                  #  - cf--security.sasl.hostName STR, ds--saslHostName STR or inout--gssapiHostName STR
                                  #  - cf--security.sasl.serviceName STR, ds--saslServiceName STR, cn--gssapiServiceName STR or
                                  #    inout--gssapiServiceName STR (def: "mongodb")

SSL ==>                           #Must:
                                  #  - use cf--net.ssl.mode STR or ds,d,s--sslMode STR:
                                  #     - "disabled": no SSL
                                  #     - "allowSSL": no SSL between servers, optional normal clients
                                  #     - "preferSSL": SSL between servers, optional normal clints
                                  #     - "requireSSL", inout--ssl, cn--ssl true or ~MONGO.connect()#mongos|replSet|server.ssl
                                  #       true: SSL
                                  #  - use inout--authenticationMechanism "MONGODB-X509"
                                  #Credentials:
                                  #  - ~MONGO.connect()#mongos|replSet|server.sslCert BUFFER|STR
                                  #  - cf--net.ssl.PEMKeyFile STR, inout,d,s--sslPEMKeyFile STR or
                                  #    ~MONGO.connect()#mongos|replSet|server.sslKey BUFFER|STR
                                  #  - cf--net.ssl.PEMKeyPassword STR, inout,d,s--sslPEMKeyPassword STR or
                                  #    ~MONGO.connect()#mongos|replSet|server.sslPass BUFFER|STR
                                  #  - cf--net.ssl.CAFile FILE, inout,d,s--sslCAFile FILE or
                                  #    ~MONGO.connect()#mongos|replSet|server.sslCA BUFFER_ARR|STR_ARR
                                  #  - cf--net.ssl.CRLFile FILE or inout,d,s--sslCRLFile FILE
                                  #Also:
                                  #  - cf--net.ssl.allowConnectionsWithoutCertificates true,
                                  #    d,s--sslAllowConnectionsWithoutCertificates or
                                  #    ~MONGO.connect()#mongos|replSet|server.sslValidate true (def: false):
                                  #    allow self-signed certificates
                                  #  - cf--net.ssl.allowInvalidCertificates|Hostnames BOOL,
                                  #    inout,d,s--sslAllowInvalidCertificates|Hostnames
                                  #  - cf--net.ssl.FIPSMode BOOL or inout,d,s--sslFIPSMode: use FIPS 140-2 version of OpenSSL
                                  #    (MongoDB Enterprise only)
                                  #For authentication between cluster members:
                                  #  - cf--security.clusterAuthMode "x509" or ds,d,s--clusterAuthMode "x509"
                                  #    (instead of "keyFile")
                                  #  - cf--net.ssl.clusterFile STR or d,s--sslClusterFile STR
                                  #  - cf--net.ssl.clusterPassword STR or d,s--sslClusterPassword STR

ACTIONS ==>                       #All available can be found in builtin roles. Notation (for the related resource):
                                  #  - ACTION#: cluster
                                  #  - ACTION##: collection or database
                                  #  - ACTION###: database

BUILTIN ROLES ==>                 #Roles in every database:
read                              #Can read the DB data
                                  #On all COLL except system.*, but system.indexes|js|namespaces:
                                  #  - collStats##, dbHash##, dbStats###, find##, killCursors##
readWrite                         #Can read|write the DB data
                                  #On all COLL except system.*, but system.js:
                                  #  - like read
                                  #  - emptycapped##, insert##, remove##, update##
                                  #  - createCollection##, dropCollection##, renameCollectionSameDB##, convertToCapped##
                                  #  - createIndex##, dropIndex##
dbAdmin                           #Can change DB schema and gets more info
                                  #On all COLL except system.*:
                                  #  - createCollection##, dropCollection##, renameCollectionSameDB##, convertToCapped##
                                  #  - createIndex##, dropIndex##
                                  #  - dropDatabase###
                                  #  - collMod##, reIndex##
                                  #  - compact##, repairDatabase###
                                  #  - collStats##, dbStats###, enableProfiler###, indexStats##, storageDetails##, validate#
                                  #On system.indexes|profile|namespaces: like read
                                  #On system.profile:
                                  #  - dropCollection##, createCollection##
dbOwner                           #readWrite + dbAdmin + userAdmin
userAdmin                         #Can change users/roles
                                  #On the database:
                                  #  - changeCustomData|Password###, create|view|dropRole|User###, grant|revokeRole###

                                  #Roles in admin DB:
read[Write]|dbAdminAnyDatabase    #Like read|readWrite|dbAdmin on all DB, plus listDatabases#
userAdminAnyDatabase              #Like userAdmin on all DB, plus the following.
                                  #On the cluster:
                                  #  - authSchemaUpgrade#, invalidateUserCache#, listDatabases#
                                  #On admin.system.users|roles:
                                  #  - like read + planCacheRead|Write##, createIndex##, dropIndex##
root                              #read[Write]|dbAdmin|userAdminAnyDatabase combined.

                                  #Roles in admin DB:
clusterMonitor                    #Can get sharding config
                                  #On the cluster:
                                  #  - connPoolStats#, cursorInfo#, getCmdLineOpts#, getLog#, getParameter#, getShardMap#,
                                  #    hostInfo#, inprog#, listDatabases#, listShards#, netstat#, replSetGetStatus#,
                                  #    serverStatus#, shardingState#, top#
                                  #On every database:
                                  #  - collStats##, dbStats###, getShardVersion###
                                  #On every DB.system.profile:
                                  #  - find##
                                  #On config.* except config.system.*, but config.system.indexes|js|namespaces: like read
clusterManager                    #Can change sharding config
                                  #On the cluster:
                                  #  - addShard#, removeShard#, listShards#,
                                  #  - flushRouterConfig#, replSetConfigure#, replSetGetStatus#, replSetStateChange, resync#,
                                  #    replSetHeartbeat#
                                  #  - applicationMessage#, appendOplogNote#
                                  #  - cleanupOrphaned#,
                                  #On every database:
                                  #  - enableSharding#, moveChunk#, splitChunk#, splitVector#
                                  #On config.settings:
                                  #  - insert##, remove##, update##
                                  #On config.system.indexes|js|namespaces and local.replset: like read
hostManager                       #Can manipulate server process
                                  #On the cluster:
                                  #  - applicationMessage#, closeAllDatabases#, connPoolSync#, cpuProfiler#,
                                  #    flushRouterConfig#, fsync#, invalidateUserCache#, killop#, logRotate#, resync#,
                                  #    setParameter#, shutdown#, unlock#
                                  #On every database:
                                  #  - killCursors##, repairDatabase###
clusterAdmin                      #clusterManager + clusterMonitor + hostManager, with also dropDatabase###

                                  #Roles in admin DB:
backup                            #Allow using mongodump.
                                  #On admin.mms.backup:
                                  #  - insert##, update##
                                  #On the cluster:
                                  #  - listDatabases#
                                  #On any DB.COLL except DB.system.*, but DB.system.indexes|js|namespaces,
                                  #admin.system.users|roles:
                                  #  - find##
restore                           #Allow using mongorestore.
                                  #On any DB.COLL except DB.system.*, but DB.system.js, admin.system.users|roles: like read
                                  #On admin.system.users:
                                  #  - find##, remove##, update##
                                  #On system.namespaces:
                                  #  - find##


&authenticate(STR, STR2[, STR3])  #User authentication, where:
  DB.auth(...)                    #  - STR is username. Can also use inout--username
 ~DB|ADMIN.authenticate(STR, STR2 #  - STR2 is password. Can also use inout--password
 [, authMechanism#STR3])          #  - STR3 is authentication mechanism.
                                  #    Can also use ds--authenticationMechanisms STR, inout--authenticationMechanism STR or
                                  #    cn--authMechanism STR
                                  #    Can be:
                                  #     - "SCRAM-SHA1" (def)
                                  #     - "MONGODB-CR" (former def)
                                  #     - "PLAIN": SASL
                                  #     - "MONGODB-X509": not with ds--*
                                  #     - "GSSAPI": not with ds--*
                                  #Authentication is for inout--authenticationDatabase 'DB', cn--authSource 'DB',
                                  #~MONGO.connect()#db.authSource or ~DB.logout()#dbName (def: current one)
                                  #Fires authenticated(OBJ) event in Node.js
&logout()
  DB.logout()                     #
 ~DB.logout()                     #
 ~ADMIN.logout()                  #
&connectionStatus()               #Returns, for current client connection:
                                  #  - authInfo:
                                  #     - authenticatedUsers OBJ_ARR:
                                  #        - user STR
                                  #        - db STR
                                  #     - authenticatedUserRoles OBJ_ARR:
                                  #        - role STR
                                  #        - db STR

admin.system.roles                #Roles are database-specific.
                                  #User-defined ones only.
                                  #OBJ:
                                  #  - role ROLE_STR
                                  #  - db 'DB'
                                  #  - privileges OBJ_ARR: are or'd
                                  #     - resource:
                                  #        - db 'DB2':
                                  #           - if 'DB' <> "admin", 'DB2' must === 'DB'
                                  #           - can be "" for all DB (only if DB === "admin")
                                  #        - collection 'COLL':
                                  #           - must be "" if action targets DB2 not COLL
                                  #           - can be "" for all COLL except system.*
                                  #        - cluster BOOL:
                                  #           - if true, no db nor collection
                                  #           - for actions that don't target any collection nor databases, but the whole
                                  #             server
                                  #     - actions ACTION_STR_ARR
                                  #  - roles OBJ_ARR: to inherit from other roles
                                  #     - role STR
                                  #     - db DB3_STR
&create|updateRole(ROLE_STR,
privileges#OBJ_ARR, roles#OBJ_ARR)
  DB.create|updateRole(role#STR,
  privileges#OBJ_ARR,
  roles#OBJ_ARR, OBJ)             #
&grant|revokePrivilegesTo|FromRole
(ROLE_STR, privileges#OBJ_ARR)
  DB.grant|revokePrivilegesTo|
  FromRole(ROLE_STR,OBJ_ARR)      #
&grant|revokeRolesTo|FromRole
(ROLE_STR, roles#STR_ARR|OBJ_ARR)
  DB.grant|revokeRolesTo|FromRole
  (ROLE_STR,STR_ARR|OBJ_ARR)      #
&dropRole(ROLE_STR)
  DB.dropRole(STR)                #
&dropAllRolesFromDatabase()
  DB.dropAllRoles()               #
&rolesInfo                        #ROLE_OBJ is role#ROLE_STR, db#'DB'. 1 means all roles.
(ROLE_STR[_ARR]|OBJ[_ARR]|1       #showBuiltinRoles is false by def.
[, showPrivileges#BOOL]           #Returns OBJ_ARR:
[, showBuiltinRoles#BOOL])        #  - role ROLE_STR
  DB.getRoles([BOOL[, BOOL]])     #  - db 'DB'
  DB.getRole(ROLE_STR[, BOOL])    #  - isBuiltin BOOL
                                  #  - roles|inheritedRoles ROLE_STR_ARR: only direct|all inherited roles
                                  #  - [inherited]privileges OBJ_ARR

admin.system.users                #Applies role according to authentication
                                  #OBJ:
                                  #  - user USER_STR
                                  #  - db 'DB'
                                  #  - credentials OBJ: depends on authentication system
                                  #  - roles OBJ_ARR:
                                  #     - role ROLE_STR
                                  #     - db: 'DB'
                                  #  - customData OBJ
&create|updateUser(USER_STR,
  pwd#STR, roles#OBJ_ARR
  [, customData#OBJ]
  [, digestPassword#BOOL])
  DB.create|updateUser
  (user#USER_STR, ...[, OBJ])
 ~DB|ADMIN.addUser(USER_STR,
  PWD_STR[, customData#OBJ]
  [,roles#OBJ_ARR])               #If digestPassword true (def), server creates password hash. If false, client must do it.
  DB.changeUserPassword
  (USER_STR, STR[, STR2])         #STR2 is --authenticationMechanism
&grant|revokeRolesTo|FromUser
(USER_STR,
roles#ROLE_STR_ARR|OBJ_ARR)
  DB.grant|revokeRolesTo|FromUser
  (USER_STR, ROLE_STR_ARR|OBJ_ARR)#
&dropUser(USER_STR)
  DB.dropUser(STR)                #
 ~DB|ADMIN.removeUser(STR)        #
&dropAllUsersFromDatabase()
  DB.dropAllUsers()               #
&usersInfo(USER_STR[_ARR]|OBJ[_AR]
[, showCredentials#BOOL]
[, showPrivileges#BOOL])
  DB.getUser(USER_STR)
  DB.getUsers()                   #


admin.system.version              #Of the roles and users


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          AUDIT LOG            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


AUDIT LOG ==>                     #Authorization-wise logs. MongoDB Enterprise only.
                                  #Format is OBJ:
                                  #  - atype STR: name of the action (see online doc)
                                  #  - param OBJ: parameters to the action (see online doc)
                                  #  - result UINT: result of the action (see online doc)
                                  #  - ts:
                                  #     - $date: DATE
                                  #  - local|remote OBJ: for the server|client
                                  #     - ip STR
                                  #     - port UINT
                                  #  - users OBJ_ARR:
                                  #     - user USER_STR
                                  #     - db 'DB'
                                  #  - roles OBJ_ARR:
                                  #     - role ROLE_STR
                                  #     - db 'DB'
                                  #Can add custom messages with &logApplicationMessage(STR)
                                  #Authorization failures always logged, but successes also if ds--auditAuthorizationSuccess
                                  #Log:
                                  #  - stored as cf--auditLog.destination or d,s--auditDestination "syslog|console|file"
                                  #     - if file, must specify cf--auditLog.format or d,s--auditFormat "JSON|BSON" and
                                  #       cf--auditLog.path or d,s--auditPath FILE
                                  #  - can only include some using cf--auditLog.filter Q_OBJ or d,s--auditFilter Q_OBJ


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:            SERVER             :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


mongod                            #Starts server.
                                  #Background job if cf--processManagement.fork true or d,s--fork
                                  #Connect to server by specifying:
                                  #  - inout--host HOST[:PORT] (def: localhost:27017)
                                  #  - inout--port INT (def: 27017)
                                  #  - mnt--db STR
                                  #  - mnt--collection STR
&&shutdown([timeoutSecs#NUM]      #Clean shutdown. Can also use CTRL-C (if not in background) or SIGKILL.
[, force#BOOL])                   #In a replica set, does not shutdown if there is not one secondary with <10 secs lags:
  DB.shutdownServer(...)          #  - let them catchup for NUM secs
  d--shutdown                     #  - can force if BOOL true


STORAGE LOCATION ==>              #Locations:
                                  #  - Main data: cf--storage.dbPath or d--dbpath DIR (def: "/data/db")
                                  #     - names:
                                  #        - "DB": <= 64 bytes
                                  #        - "DB.COLL": <= 123 bytes.
                                  #        - "DB.COLL.$INDEX": <= 128 bytes.
                                  #        - files DB.ns
                                  #        (WiredTiger only)
                                  #        - if cf--storage.wiredTiger.engineConfig.directoryForIndexes BOOL or
                                  #          d--wiredTigerDirectoryForIndexes (def: false) true,
                                  #          physically stores indexes in separate dirs
                                  #        (mmapv1 only)
                                  #        - max number depends on file max size, i.e. cf--storage.mmapv1.nsSize INT
                                  #          or d--nssize INT (in MB, def: 16, i.e. approx 24000 names.
                                  #          Max 2047, i.e. approx 3M names)
                                  #     (mmapv1 only)
                                  #     - data files:
                                  #        - files DB.NUM
                                  #        - if cf--storage.mmapv1.quota.enforced true or d--quota, creates limit of
                                  #          cf--storage.mmapv1.quota.maxFilesPerDB INT or d--quotaFiles INT (def: 8) data
                                  #          files per DB
                                  #     - mongod.lock
                                  #     - _tmp/
                                  #  - cf--processManagement.pidFilePath or d,s--pidfilepath FILE (def: none)
                                  #  - Logs: see above
                                  #  - WAL: see above

CONN_STR                          #Means "mongodb://[USER:PASSWORD@]HOST[:PORT],...[/DB[?VAR=VAL&...]]"
                                  #If connects to replica set or mongos, must specify at least two HOST[:PORT],... and
                                  #cn--replicaSet STR
                                  #HOST is FILE for Unix socket (including first /)

NETWORK ==>                       #Uses:
                                  #  - cf--net.bindIp STR or d,s--bind_ip STR (def: all), as IP_STR,...
                                  #    Can also use cf--net.ipv6 BOOL (def: false) or cli--ipv6
                                  #  - cf--net.port INT or d,s--port INT (def: 27017)
                                  #  - or if cf--net.unixDomainSocket.enabled true (def: false) and no d,s--nounixsocket,
                                  #    Unix socket in cf--net.unixDomainSocket.pathPrefix DIR or
                                  #    d,s--unixSocketPrefix DIR (def: "/tmp")
                                  #  - when using HOST:PORT in commands/configuration, must enclose HOST in [] if IPv6
                                  #Max number of connections:
                                  #  - from clients:
                                  #     - cf--net.maxIncomingConnections or d,s--maxConns INT (def: 1M)
                                  #     - ulimit:
                                  #        - should be unlim for -f (file size), -t (cpu), -m (mem) and -v (virtual mem)
                                  #        - can be 64000 for -n (open files): approx. connections * shards + number of
                                  #          data files
                                  #        - can be 64000 for -u (processes): approx. connections
                                  #        - can persist with /etc/security/limits.conf and limits.d/*
                                  #  - from others:
                                  #     - ds--connPoolMaxConnectionsPerHost INT (def: 200):
                                  #        - mongod connection pool to other mongod for authentication
                                  #        - to adjust only if driver does not provide pooling
                                  #     - ds--connPoolMaxShardedConnectionsPerHost INT (def: 200): mongos connection to
                                  #       other mongos.
                                  #  - client pooling:
                                  #     - cn--min|maxPoolSize NUM (def: 0 and 100) or
                                  #       ~MONGO.connect()#mongos|replSet|server.poolSize NUM (def: 5)
                                  #     - cn--maxIdleTimeMS NUM (def: none)
                                  #     - cn--waitQueueTimeoutMS NUM (def: none)
                                  #     - cn--waitQueueMultiple NUM: multiplied to maxPoolSize to know how many can wait
                                  #Client network problems:
                                  #  - cn--connect|socketTimeoutMS NUM (def: none) or
                                  #    ~MONGO.connect()#mongos|replSet|server#socketOptions.connect|socketTimeoutMS NUM (def:0)
                                  #  - ~MONGO.connect()#mongos|replSet|server#socketOptions.autoReconnect BOOL (def: trye)
                                  #  - ~MONGO.connect()#db.bufferMaxEntries, ~DB.bufferMaxEntries NUM (def: -1, i.e. unlim) or
                                  #     ^SCHEMA_OPT.bufferCommands BOOL (def: true): buffer for operations pending to be sent
                                  #     to the server while reconnecting
                                  #  - ~MONGO.connect()#db.reconnectTries NUM (def: 30)
                                  #  - ~MONGO.connect()#db.reconnectInterval NUM (def: 1000)
                                  #Client TCP options:
                                  #  - ~MONGO.connect()#mongos|replSet|server#socketOptions.noDelay BOOL (def:true)
                                  #  - ~MONGO.connect()#mongos|replSet|server#socketOptions.keepAlive NUM (def:true)

WINDOWS SERVICE ==>               #To launch at startup daemon on Windows:
                                  #  - cf--processManagement.windowsService.serviceName|displayName STR (def: "MongoDB")
                                  #  - cf--processManagement.windowsService.description STR (def: "MongoDB Server")
                                  #  - cf--processManagement.windowsService.serviceUser|Password STR


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          MONGO SHELL          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


mongo                             #Shell client.
                                  #Is also JavaScript client.
                                  #Provides following helpers in the shell only:
                                  #  - use 'DB': like db = DB.getSiblingDB('DB')
                                  #  - quit() or CTRL-C
                                  #  - print[json](VAR): like JavaScript console.log()
                                  #  - [A]CUR.pretty(): pretty print
                                  #  - edit VAR: use ENVVAR EDITOR to edit a variable (including a FUNC)
                                  #  - show dbs: like &listDatabases()
                                  #  - show collections: like DB.getCollectionNames()
                                  #  - show users|roles: like DB.getUsers|Roles()
                                  #  - show log STR: like &&getLog(STR)
                                  #And the following filesystem commands:
                                  #  - cd(DIR)
                                  #  - pwd(): returns DIR
                                  #  - ls(): returns DIR_ARR
                                  #  - listFiles(): returns OBJ_ARR: name STR, isDirectory BOOL, size NUM
                                  #  - cat(FILE): returns STR
                                  #  - mkdir(DIR)
                                  #  - removeFile(FILE)
                                  #Can change prompt by assigning prompt = STR|FUNC()->STR
                                  #History stored in ~/.dbshell
                                  #Read conf files (unless o--norc): ~/.mongorc.js, /etc/mongorc.js
                                  #If o--nodb, need to connect manually using new Mongo()


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:        NODE.JS DRIVER         :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


VERSION ==>                       #2.0.13

new ~MongoClient()                #Returns MONGO
~MONGO.connect(CONN_STR[, OBJ])   #Returns DB.
                                  #OBJ:
                                  #  - uri_decode_auth BOOL: if false (def), does not parse USER:PASSWORD in CONN_STR
                                  #  - db:
                                  #     - native_parser BOOL: if false (def: true), choose JavaScript parser instead of C++ one
                                  #       Can also access with ~DB.native_parser
                                  #     - raw BOOL: if true (def: false), make commands return raw BSON as BUFFER
                                  #       Can also use ~DB.collection|createCollection()#raw, ~DB.indexInformation()#full,
                                  #       ~COLL.findOne()#raw
                                  #  - mongos|replSet|server OBJ:
                                  #     - must choose only one: mongos if cluster, replSet if non-cluster but replica set,
                                  #       server if non-cluster and non-replica set
                                  #     - members:
                                  #        - ha BOOL (def: true) and haInterval NUM (def: 5000): checks replica sets are
                                  #          working correctly every NUM seconds (mongos|replSet only)
                                  #        - replicaSet REPL_SET_STR (replSet only): can also use cn--replicaSet
                                  #        - connectWithNoPrimary BOOL (def: false) (replSet only)
                                  #     - Can access with ~DB.serverConfig, that have events:
                                  #        - connect(OBJ)
                                  #        - reconnect(OBJ) (server only). Also on DB.
                                  #        - fullsetup(OBJ): when all servers are connected. Also on DB.
                                  #        - open(OBJ): replica set started (replSet|mongos only)
                                  #        - ha(OBJ): replica set events (replSet|mongos only), where OBJ:
                                  #           - type "start|end"
                                  #           - data:
                                  #              - norepeat BOOL: repeating event
                                  #              - id NUM
                                  #              - state STR
                                  #        - joined|left(STR, OBJ): change in replica sets members (replSet|mongos only).
                                  #          STR is "primary|secondary|arbiter"
                                  #        - close(OBJ)
                                  #        - timeout(OBJ). Also on DB.
                                  #        - error|parseError(OBJ). Also on DB.
~DB.close([BOOL])                 #Close the connection. If true, forces it and emits no events.
~DB.options                       #
~DB.admin()                       #Rreturns ADMIN


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:           MONGOOSE            :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CONCEPT ==>                       #Like Node.js driver, but enforces a schema client-side.
                                  #Can also add getters/setters to models.
                                  #Version 3.8.23
                                  #Underlying Node.js driver is former version (1.4) so careful.

^mongo                            #Returns ~MONGO
^DB.db                            #Returns ~DB
^COLL                             #Is ~COLL

^createConnection([CONNSTR[,OBJ]])#Rreturns ^DB
                                  #If CONN_STR, calls ^DB.open[Set](OBJ)
^connection                       #Default ^DB (set up by ^connect())
^connections                      #Rreturns ^DB_ARR
^MODEL.db                         #Rreturns ^DB
^DB                               #Has events:
                                  #  - [dis]connecting|[dis]connected|reconnected(): can also access ^DB.readyState 0|1|2|3
                                  #    for disconnected|connected|connecting|disconnecting
                                  #  - open|close()
                                  #  - fullsetup(): like Node.js driver
                                  #  - error(ERROR)
^DB.open[Set](CONN_STR[, 'DB2']   #Open connection. Returns nothing. Must be done before using.
[, PORT][, OBJ])                  #OBJ: same as ~MONGO.connect()
                                  #Buffers commands while opening is on process, so can just start commands as if connected.
^connect(CONN_STR[, OBJ])         #Same as ^connection = ^createConnection(CONN_STR); ^connection.open[Set](OBJ)
                                  #Returns nothing.
^DB.close()                       #Returns nothing. Rreturns DB.
^disconnect()                     #Calls all ^DB.close()
                                  #Returns nothing.

new ^Schema(OBJ)                  #Returns a ^SCHEMA, i.e. a COLL type.
                                  #OBJ is a document of this collection, except:
                                  #  - values are:
                                  #     - OBJ[_ARR]:
                                  #        - type: String|Number|Date|Buffer|Boolean, {} (or) ^Schema.Types.Mixed,
                                  #          ^Schema.Types.ObjectId, ^SCHEMA or a Schema() OBJ[_ARR]
                                  #           - {} means any type
                                  #             After changes on ^MOBJ.VAR, must call ^MOBJ.[OBJ.]markModified(VAR_STR)
                                  #           - ^MOBJ.VAR DATE, when modified with DATE.FUNC(...) must call
                                  #             ^MOBJ.[OBJ.]markModified(VAR_STR)
                                  #        - ref ^MODEL2_STR:
                                  #           - makes it possible to use:
                                  #              - ^CUR.populate("VARR ..."[, P_STR|OBJ][, MODEL_STR][, Q_OBJ][, OBJ])
                                  #              - ^MOBJ.populate("VARR ...") or MOBJ.populate(path#VARR_STR[,select#P_STR|OBJ]
                                  #                [, model#MODEL_STR][, match#REGEXP][, options#OBJ]), returning MOBJ
                                  #              - p^MODEL.populate(MOBJ[_ARR][, ...])->MOBJ[_ARR]
                                  #           - replace value by another ^MODEL2 by looking up its _id === ^MOBJ.VARR.
                                  #           - can further filter/manipulate with P_STR|OBJ, Q_OBJ, OBJ.sort|limit|skip|...
                                  #           - The populated ^MODEL can be written onto.
                                  #           - Works on ARR too
                                  #           - can use MOBJ.populated(VARR_STR) to rreturn _id[s] used during population
                                  #        - default VAL|OBJ|FUNC()->VAL: if using OBJ, it is shared among instances
                                  #        - auto BOOL (def: true): on OID, adds default new ObjectId()
                                  #        - index BOOL: can rreturn with ^SCHEMA.indexes().
                                  #          For multikey indexes, use SCHEMA.index(OBJ, OBJ2)
                                  #        - unique|sparse BOOL, expires NUM: sets index.
                                  #        - required BOOL: not ""|null|undefined
                                  #          Can get list with SCHEMA.requiredPaths()->VARR_STR_ARR
                                  #        - select BOOL: allow to exclude|include by def. in P_OBJ
                                  #        - min|max NUM
                                  #        - enum STR_ARR
                                  #        - match REGEXP: ""|null|undefined always work
                                  #        - lowercase|uppercase BOOL
                                  #        - trim BOOL
                                  #        - get|set(VAL, SCHEMA_TYPE)->VAL: fired when reading|writing to the object
                                  #          As opposed to virtual getters/setters, the member persists in Mongo.
                                  #        - validate OBJ_ARR|REGEXP|FUNC(VAL[, FUNC2(BOOL)])->BOOL:
                                  #            - called with validate(). Async with FUNC2. OBJ_ARR is { validate ... }
                                 ##            - can use MONGOOSE-VALIDATOR(OBJ) as FUNC (version 1.0.3)
                                 ##               - where OBJ is:
                                 ##                  - validator FUNC2_STR [and arguments VAL2[_ARR]]: to call
                                 ##                    VALIDATOR.FUNC2(VAL[, VAL2...])->BOOL (see Validator.js)
                                 ##                  - message STR: error message
                                 ##                  - passIfEmpty BOOL: if false (def), return false if null|undefined
                                 ##               - must use MONGOOSE-VALIDATOR.extend(...) instead of VALIDATOR.extend(...):
                                 ##                 same but does not convert input to STR
                                  #     - all above can also call SCHEMA_TYPE.FUNC(VAL), e.g. SCHEMA_TYPE.required(BOOL)
                                  #     - or String|...: like { type: String|... }
                                  #  - cannot use VAR names from ^Schema.reserved: on, emit, get, set, init, isNew, errors,
                                  #    schema, options, modelName, collection, toObject
                                  #The only method that use Mongoose addons (get/setters, virtual methods, validate(),
                                  #pre|post(), etc.) is save() and Mongoose-only methods (e.g. MOBJ.ARR, etc.)
                                  #Others (e.g. find|update|remove|mapReduce()) do not.
SCHMA.virtual[path](VARR_STR).    #Make ^MOBJ.VAR fire FUNC(), with ^MOBJ as this. ^MOBJ.VAR will not be saved on MongoDB.
get(FUNC())                       #"path" is when virtual FUNC() already exist.
^SCHEMA.virtual[path](VARR_STR).  #Make ^MOBJ.VAR = VAL fire FUNC(VAL) instead and return its return value, with ^MOBJ as this.
set(FUNC(VAL))                    #MOBJ.VAR must have a virtual getter assigned first.
SCHEMA.method|static
(FUNC_VAR, FUNC(...))
SCHEMA.method|static              #Make it possible to use as ^MOBJ|MODEL.FUNC(...).
(FUNC_VAR#FUNC(...))              #With "methods", this is ^MOBJ.
^SCHEMA.pre(STR[, true],          #Fires FUNC() before ^MOBJ.STR(), with STR "save|remove|init|validate"
FUNC(FUNC2([ERROR])               #"init" is called when retrieved a MOBJ, but not when creating it.
[, FUNC3([ERROR])]))              #FUNC2() gives hand to next ^SCHEMA.pre(). done function is FUNC2() or (if true) FUNC3()
^SCHEMA.post(STR, FUNC(MOBJ))     #Fires FUNC() after ^MOBJ.STR()
^[SCHEMA.]plugin(FUNC[, PL_OBJ])  #Fires FUNC(SCHEMA[, PL_OBJ]). Usually FUNC(...) is a module.
                                  #If no SCHEMA, will fire for any future SCHEMA declaration (not already declared ones).
SCHEMA.add(OBJ)                   #Merges it
SCHEMA.path(VARR_STR)             #Rreturns as SCHEMA_TYPE
SCHEMA.path(VARR_STR, SCHEMA_TYPE)#Sets it
SCHEMA.pathType(VARR_STR)         #Rreturns "real" (normal), "virtual", "nested" (sub-OBJ) or "adhocOrUndefined"
SCHEMA.eachPath(FUNC(...))        #Does SCHEMA_TYPEs.forEach(...)
^MODEL|MOBJ.schema                #SCHEMA

^SCHEMA_OPT                       #Can be set with:
                                  #  - ^SCHEMA.set(VAR_STR, VAL).
                                  #  - new ^MODEL() OBJ2
                                  #Is:
                                  #  - autoIndex BOOL: if true (def), calls &createIndex() and emits index(EROR) event on MODEL
                                  #    Good for dev, but should be false during production.
                                  #    Can also manually p^MODEL.createIndexes()
                                  #  - id BOOL: if true (def), adds virtual getter MOBJ.id = _id.toString()
                                  #  - shardKey OBJ

^[DB.]model(MODEL_STR, SCHEMA     #Defines and rreturns ^MODEL, i.e. a wrapper around a COLL.
[, 'COLL'])                       #If not specifying ^DB, use ^connection
                                  #Can be done only one for a given MODEL_STR.
                                  #'COLL' is the one used by MongoDB, by def MODEL_STR + "s".
                                  #Can also use SCHEMA_OPT.collection
                                  #Events:
                                  #  - error(): instead of checking ERROR in callbacks
^[DB|MOBJ.]model(MODEL_STR)       #Rreturns ^MODEL
^[DB.]modelNames()                #Rreturns ^MODEL_STR_ARR
^MODEL.modelName                  #Rreturns ^MODEL_STR
^MODEL.base                       #Rreturns MONGOOSE
^CUR.cast(MODEL)                  #Force casting CUR to MODEL
^MODEL.discriminator              #Be SCHEMA the schema of MODEL, rreturns a MODEL2 using same COLL as MODEL, but based on
(MODEL2_STR, SCHEMA2)             #SCHEMA2, which must an expanded (child) version of SCHEMA.
                                  #Both SCHEMA and SCHEMA2 must call new ^Schema2 (not ^Schema), where ^Schema2 is inheriting
                                  #from ^Schema (normal JavaScript inheritance) but with constructor using this.add(...).
                                  #Allow using a base model for a COLL, with different sub-models possible as objects.
                                  #Will add member MOBJ.__t MODEL2_STR.
                                  #MODEL2.discriminators is set to children MODEL_ARR

new ^MODEL(OBJ[, OBJ2])           #Returns ^MOBJ, i.e. a container of an OBJ:
                                  #  - can be used as plain OBJ directly but also has extra methods, including
                                  #    ^MOBJ.get(VARR) and ^MOBJ.set(VARR, VAL)
                                  #  - is printed as plain OBJ on console
                                  #  - returned by MODEL.find[And]*(), save() and MOBJ.remove()
                                  #Members with some types are different types that must be used with their methods (otherwise,
                                  #changed locally but not on the server)
                                  #  - MOBJ.ARR:
                                  #     - the following ARR methods are wrapped with U$set(): pop|shift()|push|unshift(VAL...)|
                                  #       splice(), sort() (sets it) and MOBJ.ARR.set(NUM, VAL) for ARR[NUM] = VAL
                                  #     - can also use indexOf()
                                  #     - MOBJ.__v (or SCHEMA_OPT.versionKey VAR_STR, false to disable): member UINT
                                  #       modified by MOBJ.increment() from 0.
                                  #       MOBJ.increment() is called when a MOBJ.ARR makes a modification that can change
                                  #       array order: push|pull[All]|addToSet|pop().
                                  #       Goal is to be used by Q_OBJ when U_OBJ references MOBJ.ARR.NUM in a concurrent
                                  #       environment, in case another client changed array order.
                                  #  - MOBJ.OBJ_ARR: (does not seem to work)
                                  #     - id(OID)->OBJ|null
                                  #     - create(OBJ): inserts an OBJ
                                  #  - MOBJ.OBJ: (does not seem to work)
                                  #     - ownerDocument|parent(): rreturns top-level|direct MOBJ
                                  #     - parentArray(): rreturns parent chain as MOBJ_ARR
                                  #     - remove(): returns null
                                  #  - MOBJ.BUFFER:
                                  #     - the following BUFFER methods are supported: write(), copy()
                                  #     - equals(BUFFER2)->BOOL
                                  #Each can use toObject(...) to return as native type.
^MOBJ.toObject([OBJ2])            #Rreturns as plain OBJ. Called by other methods when converting MOBJ to OBJ.
                                  #Does:
                                  #  - remove keys not present in ^SCHEMA (does not invalidate) if SCHEMA_OPT.strict true (def)
                                  #    or MODEL.update()#strict true
                                  #    Only works on write, not read.
                                  #  - BUFFER -> BIN
                                  #  - OBJ2.getters BOOL (def: false): applies virtual getters
                                  #  - OBJ2.virtuals BOOL (def: false): applies virtual getters/setters
                                  #  - OBJ2.minimize BOOL: remove empty OBJ
                                  #  - OBJ2.transform FUNC(MOBJ,OBJ[, OBJ2])->OBJ. Applied to sub-OBJ recursively.
                                  #  - OBJ2.depopulate BOOL: calls MOBJ.populated()
                                  #Can set default values for OBJ2 with SCHEMA_OPT toObject|toJSON OBJ2
                                  #Can be done automatically with:
                                  #  - ^CUR.lean(true)
                                  #  - ^CUR.setOptions()#lean
                                  #  - ^MODEL.geoNear|Search()#lean BOOL
^MOBJ.toJSON([OBJ2])              #Same as JSON.stringify(^MOBJ.toObject([OBJ2]))
^MOBJ.equals(MOBJ2)               #Rreturns BOOL.
                                  #  - If MOBJ or MOBJ2 has _id, compare _id
                                  #  - Otherwise, does deepEqual()
^MOBJ.validate()                  #Returns ERROR if a MOBJ.VARR:
                                  #  - does not conform to SCHEMA.
                                  #  - MOBJ.[OBJ.]invalidate(VARR, ERROR_STR, ERROR) has been called previously
                                  #Also fills MOBJ.errors.VARR
                                  #Called before MOBJ.save()
^MOBJ.isNew                       #BOOL if _id is not present in COLL
^MOBJ.is[Direct]Modified          #Rreturns BOOL if modified. "Direct" excludes when it was not modified only because some
 ([VARR_STR])                     #children were modified.
^MOBJ.isSelected(VARR_STR)        #Rreturns BOOL if was part of P_OBJ (implicitely or not)
^MOBJ.modifiedPaths()             #Rreturns VARR_STR_ARR that have been modified


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          VERSIONING           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


MOBJ.createdAt|updatedAt         ##DATE automatically assigned by Mongoose plugin MONGOOSE-TIMESTAMP (version 0.4.0),
                                 ##using pre-"save" hook.
                                 ##Can change field names with PL_OBJ.created_at|updated_at VAR_STR
                                 ##A virtual getter returns created_at when no field exist yet, using _id.getTimestamp()

COLL_history                     ##COLL keeping past modifications on COLL, with Mongoose plugin MONGOOSE-HISTORY
                                 ##(version 0.3.3). Use pre-"save|remove" hooks. Has objects:
                                 ##  - t DATE
                                 ##  - o "i" (insert), "u" (update), "r" (remove)
                                 ##  - d OBJ: modified OBJ before the change
                                 ##PL_OBJ:
                                 ##  - customCollectionName 'COLL'
                                 ##  - indexes INDEX_OBJ_ARR
                                 ##  - historyConnection CONN_STR: to send to another database
                                 ##MODEL now have methods:
                                 ##  - MODEL.historyModel(): rreturns COLL_history MODEL2
                                 ##  - MODEL.clearHistory()
MODEL.VersionedModel             ##MODEL2 created by Mongoose plugin MONGOOSE-VERSION (version 0.2.4), stored at collection
                                 ##PL_OBJ.collection (def: "versions")
                                 ##  - has fields:
                                 ##     - refId OID, reflecting MODEL _id
                                 ##     - created|modified DATE: timestamps updated by pre-"save" hook
                                 ##     - versions OBJ_ARR:
                                 ##        - versioned copies (without _id), with refVersion NUM (not sure it works)
                                 ##  - can query with MODEL2.latest(NUM), returning NUM latest OBJ_ARR
                                 ##PL_OBJ:
                                 ##  - maxVersions NUM
                                 ##  - ignorePaths VAR_STR_ARR: don't version when modifying only those fields
                                 ##  - removeVersion BOOL: if true (def), remove versioned copied when original is removed
                                 ##  - mongoose (def: MONGOOSE)
                                 ##  - strategy "collection": use several flat OBJs instead of versions OBJ_ARR.
                                 ##    Less features: no created|modified, and other things.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:             REST              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


EXPRESS-RESTIFY-MONGOOSE.         #Creates default Express APP routes that maps MODEL to create a REST API
serve(ROUTER, ^MODEL[, OBJ])      #Version 0.7.2
                                  #ROUTER is EXPRESS.Router([OBJ2]). Express APP must then APP.use(ROUTER)
                                  #Must include BODY-PARSER for JSON and URL encoded (extended).
                                  #Creates:
                                  #  - GET HOSTT/MODEL: ^MODEL.find()
                                  #  - GET HOSTT/MODEL/count: ^MODEL.count(): returns { count: NUM }
                                  #  - PUT|POST HOSTT/MODEL: ^MODEL.create()
                                  #     - 201 on success
                                  #     - returns the created OBJ3[_ARR]
                                  #  - DELETE HOSTT/MODEL: ^MODEL.remove()
                                  #     - 204 on success (even if nothing matched)
                                  #  - GET HOSTT/MODEL/ID[/shallow]: ^MODEL.findOne(). Shallow replaces OBJs by true
                                  #  - PUT|POST HOSTT/MODEL/ID: findOneAndUpdate()
                                  #     - change fields, not necessary whole object
                                  #     - returns the object
                                  #  - DELETE HOSTT/MODEL/ID: findOneAndRemove()
                                  #     - returns nothing
                                  #Returns 404 if not found
                                  #Validation:
                                  #  - returns 400 if validation failed.
                                  #  - Mongoose features (validation, pre|post hooks, etc.) do not trigger on PUT|POST|DELETE
                                  #    HOSTT/MODEL/ID unless OBJ.findOneAndUpdate|Remove false (slower), which uses findOne()
                                  #    + save|remove() instead
                                  #GET|DELETE HOST/MODELs[/count] can use URL variables:
                                  #  - VARR=|!=|>|>=|<|<=VAL
                                  #  - VARR=[VAL,...]: in
                                  #  - VARR=~REGEXP_STR: adds /i flag. REGEXP_STR must be encodeURIComponent()'d'
                                  #  - select=VARR,...
                                  #  - projection=P_OBJ
                                  #  - query=Q_OBJ: must be JSON.stringify()'d, then encodeURIComponent()'d
                                  #  - sort=S_STR|S_OBJ
                                  #  - skip|limit=NUM
                                  #  - populate=VARR,...
                                  #  - distinct=VARR
                                  #HOSTT is HOST OBJ.prefix OBJ.version (def: HOST/api/v1)
                                  #MODEL:
                                  #  - is MODEL_STR or OBJ.name STR
                                  #  - smart pluralization if OBJ.plural true (def)
                                  #  - is made OBJ.lowercase (def)
                                  #OBJ:
                                  #  - idProperty VARR_STR: make methods by ID query on VARR_STR instead of on _id
                                  #  - lean BOOL: if false (def: true), returns MOBJ, not OBJ (bad for performance)
                                  #  - fullErrors BOOL: if true (def: false), returns Mongoose errors in HTTP (for testing)
                                  #  - strict BOOL: if true (def: false), read-only methods + update (PUT|POST without id) only
                                  #    so no delete nor creation
                                  #  - middleware MIDWR[_ARR]
                                  #  - postProcess MIDWR: same but only called on success
                                  #  - outputFn(RES, OBJ, NUM): how to send result. Def is RES.status(NUM).json(OBJ)
                                  #  - postCreate|Delete(RES, OBJ, FUNC(ERROR)): post document creation|deletion
                                  #  - prereq(REQ)->BOOL: middleware that makes it return 403 if false and not GET
                                  #  - access(REQ)->"public|private|protected": determines request access level, which will
                                  #    filter out fields according to OBJ.protected|private "VARR,..." (across all MODELs)
                                  #  - contextFilter(^MODEL, REQ, FUNC(^MODEL)): called to filter ^MODEL according to REQ
                                  #Cannot query on virtual fields.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              GUI              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


ROBOMONGO ==>                     #Explore data in table/JSON/tree view (full or according to queries).
                                  #Can execute queries/scripts, with autocomplete.

GENGHISAPP ==>                    #Explore data in JSON view. Can be run as a server.


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:              MMS              :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


FEATURES ==>                      #Deployment:
                                  #  - add|remove, including replica set and sharding
                                  #  - version manager (upgrades)
                                  #  - edit conf files, including agents ones
                                  #Monitoring:
                                  #  - alerts: lot of possible events. Send by email, SMS, PagerDuty or HipChat.
                                  #  - stats graph:
                                  #    - 1min. granularity up to 48 hours ahead, 1 hour granularity up to 3 months ahead
                                  #    - all stats available with MongoDB monitoring commands
                                  #    - can add CPU, I/O with munin-node (cpu, iostat[_ios]):
                                  #       - install munin-node
                                  #       - port 4949 must be open
                                  #  - info (last and daily):
                                  #    - all mongod conf vars
                                  #    - general stats: &serverStatus(), &connPoolStats()
                                  #    - replication: RS.status(), local DB, local.oplog.rs.stats()
                                  #    - sharding: &isMaster()
                                  #    - host: &hostInfo(), &buildInfo(), &getCmdLineOpts()
                                  #  - profiling data
                                  #Logs:
                                  #  - view raw mongod|mongos logs
                                  #  - located in each dbPath, rotated files (can change conf according to time as well)
                                  #  - view main last activities
                                  #  - view agent logs
                                  #Backups:
                                  #  - snapshots:
                                  #     - saved every NUM hours, kept for NUM2 days (regular ones), and one daily for NUM3
                                  #       days, one weekly for NUM4 weeks and one monthly for NUM5 months
                                  #     - can exclude some COLLs
                                  #  - cluster checkpoint:
                                  #     - use snapshots + oplog PITR, to restore before|after the snapshot
                                  #     - stops balancer
                                  #     - done every NUM minutes
                                  #  - restore: through dbPath archive download or through scp
                                  #  - only on replica sets
                                  #Security:
                                  #  - users/roles
                                  #  - comes with default users on admin DB (for each agent) for admin DB:
                                  #    - mms-automation, with roles clusterAdmin, readWrite|userAdmin|dbAdminAnyDatabase
                                  #    - mms-monitoring-agent, with roles clusterMonitor
                                  #    - mms-backup-agent, with roles clusterAdmin, read|useAdminAnyDatabase, readWrite and
                                  #      (for local DB) readWrite
                                  #REST API (do same as dashboard)
                                  #Account:
                                  #  - two-factor auth
                                  #  - user management
                                  #  - billing history

INFRASTRUCTURE ==>                #How:
                                  #  - package to install
                                  #  - start mongodb-mms-automation-agent
                                  #  - use port 27000 to 27020
                                  #  - directories:
                                  #     - OS_USER running daemon must have permissions on them
                                  #     - must be owned by mongod:mongod
                                  #  - only run on single server in a replica set or sharded cluster (can move it using the
                                  #    dashboard)
                                  #Deploy on:
                                  #  - local computer (testing purpose)
                                  #  - normal server
                                  #  - EC2:
                                  #     - automatically create EC2 resources
                                  #     - must be minimum:
                                  #        - m3.medium instances
                                  #        - need to use ECB if m3.*
                                  #        - 25GB root volume
                                  #        - Amazon Linux or Ubuntu
                                  #     - must:
                                  #        - enter AWS credentials
                                  #        - create user with permissions EC2 full access
                                  #        - public SSH key
                                  #        - security group allowing (inbound):
                                  #           - TCP traffic on MongoDB ports between all mongod|mongos
                                  #           - SSH rule allowing 4.71.186.128/25 and 4.35.16.128/25
                                  #        - must auto-assign public IP and public DNS
                                  #     - SSH username and domain: can see on dashboard (servers > connect)
                                  #What:
                                  #  - automation agent: creation of servers, and launches two other agents
                                  #  - backup agent
                                  #  - monitoring agent
                                  #Where:
                                  #  - /var/lib/mongodb-mms-automation/:
                                  #     - backup|monitoring agents
                                  #     - MongoDB binaries (server + client)
                                  #  - dbPath/CLUSTER_SHARD_SHARDNO_REPL-NO: actual dbPath
                                  #Conf files
                                  #  - /var/lib/mongodb-mms-automation/: for backup|monitoring agents
                                  #  - /etc/mongodb-mms/automation-agent.config: automation agent
                                  #     - mmsGroupId, mmsApiKey: credentials
                                  #     - automation agent logging
                                  #  - /var/lib/mongodb-mms-automation/workspace/mongos-CLUSTER_mongos_10.conf: mongos

PRICING ==>                       #  - 2.5$/GB/month of snapshots (first GB / replica set free)
                                  #  - 50$/month per mongod (excluding arbiter or config server) (first 8 free)

ALTERNATIVES ==>                  #  - objectrocket: very expensive
                                  #  - compose.io: very expensive
                                  #  - mlab: cheaper for small, but then very expensive
                                  #Besides MMS is done by MongoDB, and better quality
