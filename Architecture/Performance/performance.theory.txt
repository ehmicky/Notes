
                                  ┏━━━━━━━━━━━━━━━━━┓
                                  ┃   PERFORMANCE   ┃
                                  ┗━━━━━━━━━━━━━━━━━┛

                                  ┌────────────────┐
                                  │   COMPLEXITY   │
                                  └────────────────┘

COMPLEXITY ==>                    #Function expressing amount of resource taken by another function.
                                  #Can be:
                                  #  - time complexity: amount of time
                                  #  - space complexity: amount of memory

ORDER ==>                         #Function order, i.e. asymptotic complexity (see math analysis doc)
                                  #Either upper|lower bound, or both
                                  #Using asymptotic notation like O(...) θ(...) Ω(...) (see math analysis doc)
                                  #Usually try to use bound as tight as possible
                                  #  - e.g. O(n**2) => O(n**4)
                                  #In real-life, non-asymptotic complexity matters too
                                  #  - e.g. with n = 5 and k = 30, O(1) is slower than O(n) 2n

CASES ==>                         #Function but with only specific input:
                                  #  - best case:
                                  #     - input with lowest function order
                                  #  - average case:
                                  #     - any input
                                  #  - worst case
                                  #     - input with highest function order
                                  #     - can be used by attacker
                                  #Orthogonal from function order:
                                  #  - function order is about asymptotic output's upper|lower bound
                                  #  - best|average|worst case is about specific input impacting that function order
                                  #Asymptotic notation applies to function order, not to best|average|case, but:
                                  #  - average case O(...) <=> worst case O(...)
                                  #  - average case Ω(...) <=> best case Ω(...)

COMPLEXITY TRADEOFF ==>           #Decreasing time complexity usually decreases maintainability
                                  #Ex: loop unrolling

P VERSUS NP PROBLEM ==>           #Unproven assumption that for some algorithms:
                                  #  - solution can be computed from input in O(poly(n)) ("P")
                                  #  - but not: computed solution can be verified in O(poly(n)) ("NP")

                                  ┌──────────────────────────┐
                                  │   AMORTIZED COMPLEXITY   │
                                  └──────────────────────────┘

AMORTIZED COMPLEXITY ==>          #Also called "aggregate analysis"
                                  #Total complexity of n operations in a row
                                  #  - as opposed to complexity of 1 single operation ("actual complexity")
                                  #  - i.e. only useful when complexity varies per operation
                                  #Orthogonal from concepts of order and of best|average|worst case.

AGGREGATE METHOD ==>              #Compute amortized complexity f(x) by:
                                  #  - summing the actual complexity of each operation
                                  #  - dividing by count of operations
                                  #  - deducing f(x) based on it
                                  #Example:
                                  #  - array geometric expansion with:
                                  #     - actual complexity 1 for each operation
                                  #     - actual complexiy n if resizing (doubling size) needed
                                  #  - then amortized complexity:
                                  #     - each operation: (1 1 1 1 1 1 1 1  ...)
                                  #       resizing:       (1 2 0 4 0 0 0 8  ...)
                                  #     -       actual complexity / count
                                  #     - each operation: (1 2 3 4 5 6 7 8  ...) / (1 2 3 4 5 6 7 8 ...) -> n
                                  #       resizing:       (1 3 3 7 7 7 7 15 ...) / (1 2 3 4 5 6 7 8 ...) -> 2n
                                  #     -> f(x) = 3n

ACCOUNTING METHOD ==>             #Compute amortized complexity f(x) by:
                                  #  - guessing f(x)
                                  #  - proving that:
                                  #     - for any n
                                  #     - Σ n first operations' actual complexity <= Σ f(x)
                                  #  - done by:
                                  #     - computing difference of each operation's actual complexity and f(x), one by one
                                  #     - re-using any excess as "budget" for next operations
                                  #     - ensuring this budget always >= 0
                                  #Using same example:
                                  #  - f(x) = 3n
                                  #  - then:
                                  #     - amortized complexity: 3 3 3  3 3 3 3  3 ...
                                  #     - actual complexity:    2 3 1  5 1 1 1  9 ...
                                  #     - difference:           1 0 2 -2 2 2 2 -6 ...
                                  #     - budget:               1 1 3  1 3 5 7  1 ...
                                  #  -> f(x) always >= 0

POTENTIAL METHOD ==>              #Compute amortized complexity f(x) by:
                                  #  - guessing f(x), but with value 0 representing recurring|normal state
                                  #  - proving that f(x) always >= 0
                                  #Using same example:
                                  #  - f(x) = 2n - N, with N being next size that needs resize
                                  #  - then:
                                  #     - 2n:     2 4 6 8 10 12 14 16
                                  #     - N:      2 4 4 8  8  8  8 16
                                  #     - 3n - N: 0 0 2 0  2  4  6  0
                                  #  -> f(x) always >= 0

                                  ┌──────────────────────┐
                                  │   SPACE COMPLEXITY   │
                                  └──────────────────────┘

IN-PLACE ALGORITHM ==>            #Algorithm with space complexity O(log n) or less.
                                  #I.e. either need:
                                  #  - no additional memory, i.e. modifies data directly
                                  #  - constant|logarithmic auxiliary data
                                  #Opposite: "out-of-place" or "not-in-place"

SPACE-TIME TRADEOFF ==>           #Also called "time-memory tradeoff":
                                  #  - time complexity can often be reduced by increasing space complexity, e.g. precalculation
                                  #  - space complexity can often be reduced by increasing time complexity, e.g. compression

                                  ┌───────────────────────────┐
                                  │   LOCALITY OF REFERENCE   │
                                  └───────────────────────────┘

LOCALITY OF REFERENCE ==>         #Information close to each other.
                                  #Also called "principle of locality"
                                  #Can be:
                                  #  - "temporal": over time
                                  #  - "spatial": over space
                                  #     - "memory": over memory
                                  #     - "branch": over branches, i.e. source code
                                  #     - "equidistant": when spatial locality of reference leads to time locality of
                                  #       reference, since information close to each other is more likely to be used at once
                                  #High locality of reference leads to predicting information use pattern for:
                                  #  - information reuse:
                                  #     - caching (see its doc): temporal
                                  #     - compression (see its doc): spatial
                                  #  - idleness reduction:
                                  #     - parallelism (see its doc): increasing current work
                                  #     - throttling: decreasing current work
                                  #     - precalculation (see its doc): move work to past
                                  #     - lazy calculation: move work to future

DATA-ORIENTED DESIGN ==>          #Organizing code to optimize locality of reference and optimize CPU cache re-use.
                                  #Often opposed to OOP which often leads to poor locality of reference because subclassing
                                  #inherits logic and data that is not always used.
                                  #Often uses records of AoS (see aggregate theory)

MEMORY ACCESS PATTERN ==>         #How information is accessed in time.
                                  #The locality of reference depends on the memory access pattern.
                                  #It also affects how parallelism should be performed.
                                  #Can be:
                                  #  - "sequential": increment of 1
                                  #  - "strided": increment of x
                                  #  - "random"
                                  #  - "scatter": sequential reads, random writes
                                  #  - "gather": random reads, sequential writes
                                  #  - "nearest neighbor": nearest nodes from a graph

FLYWEIGHT ==>                     #Also called "hash consing" or (for strings) "string interning"
                                  #Replacing several copies of same data to references ("flyweights") to a single copy of it.
                                  #Can also work with equivalent data, i.e. after normalizing it.
                                  #Reference can be done at:
                                  #  - language-level
                                  #     - e.g. VAR& in C++
                                  #  - application-level
                                  #     - e.g. hash table
                                  #Pros:
                                  #  - more memory efficient
                                  #  - faster comparison
                                  #Cons:
                                  #  - writes propagates to each reference
                                  #     - less granular
                                  #     - less concurrent-friendly
                                  #  - slower read|write due to indirection
                                  #  - less memory efficient for data that does not have several copies
