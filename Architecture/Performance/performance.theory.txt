
               
   PERFORMANCE  
               



GOAL ==>                          #Improving UX, i.e. only perceived performance matters

HOW ==>                           #Can reduce idleness, or amount of work

IDLENESS ==>                      #Can use:
                                  #  - parallelism (concurring|async, array programming)
                                  #  - precalculation, lazy calculation

AMOUNT OF WORK ==>                #Reducing means improving information reuse
                                  #Should optimize where there is high "locality of reference":
                                  #  - i.e. information reuse close to each other
                                  #  - can be:
                                  #     - "temporal": over time
                                  #     - "spatial": over space
                                  #        - "memory": over memory
                                  #        - "branch": over branches, i.e. source code
                                  #Can be:
                                  #  - temporal: caching
                                  #  - spatial: compression|minification

SPACE-TIME TRADEOFF ==>           #Also called "time-memory tradeoff":
                                  #  - performance can often be reduced by using memory, e.g. precalculation
                                  #  - memory can often be reduced by decreasing performance, e.g. compression

COMPLEXITY TRADEOFF ==>           #Optimization usually decreases maintainability
                                  #Ex: loop unrolling


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          PARALLELISM          :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CONCURRING PROGRAMMING ==>        #Running several pieces of code at once
                                  #Good for performance but creates synchronization problems
                                  #Usually implies async
                                  #See concurrency doc

ARRAY PROGRAMMING ==>             #See parallelism doc


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:         PRECALCULATION        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


OBJECT POOL ==>                   #Efficient initialization (when it is costly):
                                  #  - pool creates objects in advance
                                  #  - new objects:
                                  #     - are taken from the pool
                                  #     - if not created yet, creates it
                                  #        - there can be a pool size limit ("high water mark")
                                  #  - destroyed objects are returned to the pool for reuse
                                  #     - pool must reset object
                                  #     - if destroyed objects are not returned, this will create starvation

MEMORY POOL ==>                   #Like object pool but for memory allocation

PRECALCULATION ==>                #Precalculating computation-intense, e.g. graphics (alpha, shadows)

PREFETCHING ==>                   #Precalculating IO/network request

LOOKUP TABLE ==>                  #Precalculating a search algorithm, so it is constant time


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       TEMPORAL LOCALITY       :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


CACHING ==>                       #Refactoring information used several times to a single time
                                  #Usually conceptually a hash table with function input as key
                                  #Cache hit|miss:
                                  #  - whether a specific function invocation can use cache
                                  #  - hit ratio: percentage of cache hits
                                  #  - "trashing":
                                  #     - very low hit ratio, i.e. time spent updating cache > time saved by caching
                                  #     - is when locality of reference (how much data is reused) < cache max size
                                  #If cache has max size, it requires a replacement policy ("cache algorithm")
                                  #  - tradeoff between hit ratio (fewer cache misses) and latency (how long to process cache hits)
                                  #  - can be:
                                  #     - LRU:
                                  #        - remove Least Recently Used item
                                  #        - i.e. queue
                                  #     - PLRU (Pseudo-LRU)
                                  #        - remove not recently used item (not necessarily least)
                                  #     - MRU:
                                  #        - remove Most Recently Used item
                                  #        - i.e. stack
                                  #        - useful when items are rarely used twice in a row
                                  #     - LFU: remove Least Frequently Used item
                                  #     - Bélády:
                                  #        - remove item that will be used in the furthest future
                                  #        - usually unknown, i.e. can only be approximated
                                  #     - RR (Random Replacement): remove random item
                                  #"Write-back": cache also acting as a buffer on write requests

MEMOIZATION ==>                   #Caching function calls:
                                  #  - by storing a hash table inside that function with input as key, return value as value
                                  #  - return value must be predictable, i.e. must be pure function


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:       SPATIAL LOCALITY        :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


FLYWEIGHT ==>                     #Also called "hash consing" or (for string constants|symbols) "string interning"
                                  #Refactoring information used in several places (i.e. duplicated, denormalized) to a single place (i.e. normalized):
                                  #  - i.e. using reference to single place ("flyweight"), instead of several copies
                                  #     - "reference" can be language-level (e.g. VAR& in C++) or application-level (e.g. hash table)
                                  #  - flyweight can be shared:
                                  #     - inside each object (i.e. as member)
                                  #     - outside each object (e.g. as function arguments)
                                  #Pros:
                                  #  - more memory efficient
                                  #  - fast to compare equal value, i.e. can compare reference instead of value
                                  #Cons:
                                  #  - less granularity

IMMUTABILITY ==>                  #Read-only instance
                                  #Pros:
                                  #  - easier to maintain, e.g. easier to be thread-safe
                                  #Cons:
                                  #  - less flexible
                                  #Can:
                                  #  - change by making copy
                                  #     - faster to test if "has changed" with immutability + copy, than with mutability
