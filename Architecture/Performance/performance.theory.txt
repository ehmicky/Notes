
               
   PERFORMANCE  
               



                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:          COMPLEXITY           :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


COMPLEXITY ==>                    #Function expressing amount of resource taken by another function.
                                  #Can be:
                                  #  - time complexity: amount of time
                                  #  - space complexity: amount of memory

UPPER BOUND ==>                   #Also called "majorant"
                                  #Upper bound of a set: any value > max set value.
                                  #That value is said to "majorize"/"bound from above" the set.

LEAST UPPER BOUND ==>             #Min upper bound

LOWER BOUND ==>                   #Also called "minorant"
                                  #Inverse of upper bound

CLASSES ==>                       #Noted O(...)
                                  #  - n is input size
                                  #     - poly(n): any possible polynome with n as sole variable
                                  #     - other variables can be used, e.g. if there are several inputs
                                  #  - k is constant
                                  #Describe asymptotic complexity:
                                  #  - i.e. when n is scaled up
                                  #  - constants (except as exponents) are not kept
                                  #  - however in real-life, those constants matter
                                  #     - e.g. with n = 5 and k = 30, O(1) is slower than O(n) 2n
                                  #
                                  #  +--------------------+-----------------+---------------+
                                  #  | NAME               | O(...) NOTATION | EXAMPLE       |
                                  #  +--------------------+-----------------+---------------+
                                  #  | constant           | 1               | ARR[NUM]      |
                                  #  | log-logarithmic    | log(log(n))     |               |
                                  #  | logarithmic        | log(n)          | binary search |
                                  #  | polylogarithmic    | (log(n))^k      |               |
                                  #  | fractional power   | n^k (0<k<1)     |               |
                                  #  | square root        | n^0.5           |               |
                                  #  | linear             | n               | max(ARR)      |
                                  #  | linearithmic /     |                 |               |
                                  #  | loglinear          | n.log(n)        | heap sort     |
                                  #  | quadratric         | n²              | bubble sort   |
                                  #  | cubic              | n³              |               |
                                  #  | polynomial         | poly(n)         |               |
                                  #  | exponential        | 2^poly(n)       |               |
                                  #  | double-exponential | 2^(2^(poly(n))  |               |
                                  #  | factorial          | n!              |               |
                                  #  +--------------------+-----------------+---------------+

CASES ==>                         #Can be:
                                  #  - best case:
                                  #     - notation: Ω(...) (big Omega)
                                  #  - average case:
                                  #     - notation: θ(...) (big Theta)
                                  #  - worst case
                                  #     - notation: O(...) (big O)
                                  #     - can be used by attacker

AMORTIZED COMPLEXITY ==>          #Also called "aggregate analysis"
                                  #Total complexity of n operations in a row
                                  #  - as opposed to complexity of any single operation, which is assumed otherwise
                                  #Orthogonal from concepts of best|average|worst case
                                  #Operations can be stateful.
                                  #Variants:
                                  #  - "aggregate method"
                                  #  - "accounting method":
                                  #     - each operation gets a constant amount of credit
                                  #     - each operations uses that credit, depending on its complexity
                                  #     - must ensure current credit always above 0
                                  #  - "potential method":
                                  #     - each operation adds cost related to how much disorder it introduces
                                  #     - e.g. for array geometric expansions, getting closer to upper limit introduces disorder

IN-PLACE ALGORITHM ==>            #Algorithm with space complexity O(log n) or less.
                                  #I.e. either need:
                                  #  - no additional memory, i.e. modifies data directly
                                  #  - constant|logarithmic auxiliary data
                                  #Opposite: "out-of-place" or "not-in-place"

SPACE-TIME TRADEOFF ==>           #Also called "time-memory tradeoff":
                                  #  - time complexity can often be reduced by increasing space complexity, e.g. precalculation
                                  #  - space complexity can often be reduced by increasing time complexity, e.g. compression

COMPLEXITY TRADEOFF ==>           #Decreasing time complexity usually decreases maintainability
                                  #Ex: loop unrolling


                                             /=+===============================+=\
                                            /  :                               :  \
                                            )==:     LOCALITY OF REFERENCE     :==(
                                            \  :_______________________________:  /
                                             \=+===============================+=/


LOCALITY OF REFERENCE ==>         #Information close to each other.
                                  #Also called "principle of locality"
                                  #Can be:
                                  #  - "temporal": over time
                                  #  - "spatial": over space
                                  #     - "memory": over memory
                                  #     - "branch": over branches, i.e. source code
                                  #     - "equidistant": when spatial locality of reference leads to time locality of
                                  #       reference, since information close to each other is more likely to be used at once
                                  #High locality of reference leads to predicting information use pattern for:
                                  #  - information reuse:
                                  #     - caching (see its doc): temporal
                                  #     - compression (see its doc): spatial
                                  #  - idleness reduction:
                                  #     - parallelism (see its doc): increasing current work
                                  #     - throttling: decreasing current work
                                  #     - precalculation (see its doc): move work to past
                                  #     - lazy calculation: move work to future

DATA-ORIENTED DESIGN ==>          #Organizing code to optimize locality of reference and optimize CPU cache re-use.
                                  #Often opposed to OOP which often leads to poor locality of reference because subclassing
                                  #inherits logic and data that is not always used.
                                  #Often uses records of AoS (see aggregate theory)

MEMORY ACCESS PATTERN ==>         #How information is accessed in time.
                                  #The locality of reference depends on the memory access pattern.
                                  #It also affects how parallelism should be performed.
                                  #Can be:
                                  #  - "sequential": increment of 1
                                  #  - "strided": increment of x
                                  #  - "random"
                                  #  - "scatter": sequential reads, random writes
                                  #  - "gather": random reads, sequential writes
                                  #  - "nearest neighbor": nearest nodes from a graph

FLYWEIGHT ==>                     #Also called "hash consing" or (for strings) "string interning"
                                  #Replacing several copies of same data to references ("flyweights") to a single copy of it.
                                  #Can also work with equivalent data, i.e. after normalizing it.
                                  #Reference can be done at:
                                  #  - language-level
                                  #     - e.g. VAR& in C++
                                  #  - application-level
                                  #     - e.g. hash table
                                  #Pros:
                                  #  - more memory efficient
                                  #  - faster comparison
                                  #Cons:
                                  #  - writes propagates to each reference
                                  #     - less granular
                                  #     - less concurrent-friendly
                                  #  - slower read|write due to indirection
                                  #  - less memory efficient for data that does not have several copies
