
                                  ┏━━━━━━━━━━━━━━━━━━━━━━━━━┓
                                  ┃   GENERAL_PROGRAMMING   ┃
                                  ┗━━━━━━━━━━━━━━━━━━━━━━━━━┛

TODO ==>                          #  - agile software development, part "Agile methods" and "Agile practices", also
                                  #    "list of software dvt philosophies", and all categories in software development process
                                  #  - look at pointnorth.io

                                  ┌───────────────┐
                                  │   PARADIGMS   │
                                  └───────────────┘

Waterfall model:
  - traditional model where each phase follows another.
  - Going to next phase requires reviews to make sure it's OK ("gates").
  - Should not go back: criticized for lack of flexibility, since clients don't know what they want before getting first
    versions, change their mind.
  - Importance of early phases: BDUF (Big Design Up Front). BDUF is criticized for analysis paralysis.

Incremental:
  - split project into several subprojects, that each apply a Waterfall model. After each step, there is a validation phase.

Iterative:
  - evolutionary prototyping
  - short loops
  - validation phase, which if not accepted, creates a new loop.
  - several phases/subprojects (interative and incremental) are going on at the same time thanks to feedback got by short loops

Agile development:
  - Principles:
     - Focus on client:
        - deliver value. Non valuable yet things (e.g. that could deliver value later) should not be implemented until needed.
        - continuous client involvement. There should be a designated customer representative, "Product Owner", available to
          answer questions.
     - Flexibility to change:
        - Code refactoring is frequent and to be expected
        - Architecture/design is done in a way to be modified later
     - Iterative and incremental:
        - Fast implementations:
           - weeks-long loops
           - All phases going on at the same time
           - Continuous integration:
              - each time a unit is finished (unit test passed), commit it to trunk, and perform testing, documentation
                generation, build (automatic building), etc. in the same time.
              - Continuous delivery: when performing testing (unit and e2e) + build, so just need end-user acceptance to
                release
              - Goal is to avoid "integration hell" (big conflicts of commit of long-held local copies), and do verification
                in the same time as coding.
              - Usually done on a dedicated build server, close to end-user hardware.
              - Avoiding branching, prefer using the trunk, if possible without using feature toggles (integrating directly).
           - TDD (test-driven development): write system tests before implementing, with goal of passing the test.
             Good to try the test before to make sure it fails. One test is passed, do code refactoring, then run test again.
           - BDD (behavior-driven development): like TDD, but:
              - tests cases should be related to acceptance tests
              - Reporting and function names should be self-descriptive: test suite name, test case name and assertion names.
                Ex: describe("A test suite"), it(" should do sth"), expec(val).toBe(val2).
              - Domain Driven Design (DDD): tests written in ubiquituous language, ideally with a layer between test cases and
                requirements (like Cucumber)
        - Documentation less important, but is done continuously, and at later stage of each unit
        - Current progress is displayed physically as a backlog
     - Social aspects:
        - pair-programming
        - face-to-face communication
        - Daily stand-up meetings
        - Planning pocker: decide user story score (estimated time) with each one playing one card (usually use an
          exponential from 0 to 100 with fewer large numbers because harder to estimate), and then people discuss to come to
          a consensus number. Card is put face down to avoid biases. Can use some apps.
  - Might have problems if: large team, geographically disparate, risk-critical software.
  - types:
     - extreme programming (XP): extremely small steps. testing (unit tests are goals to achieve), then coding, then only
       design/architecture (comes from refactoring, done by same people who code), then showing to client
     - scrum
     - Sprint: 2-7 days long meeting of usually open source developers to same physical place (usually next to a conference),
       coding together with pair programming. Focus on socialization of the project.

cowboy coding:
  - when coding with only one developer, and lack of structure (usually used derogatory).
    Pressure on time results in "code and fix": needs code quickly, but then find bugs and needs to fix them.

                                  ┌────────────┐
                                  │   PHASES   │
                                  └────────────┘

  - Inception: requirements and specification
  - Elaboration: architecture and design
  - Construction:
     - coding
     - verification
     - debugging
     - validation from client
  - Transition: deployment and maintenance

                                  ┌──────────────────┐
                                  │   REQUIREMENTS   │
                                  └──────────────────┘

See requirements doc

                                  ┌──────────────────┐
                                  │   ARCHITECTURE   │
                                  └──────────────────┘

Architecture:
  - Choice of the high-level software components: platforms, applications, programming languages, libraries/frameworks,
    coding paradigm, network type, relation between subcomponents (modules, files, machines, etc.)
  - Steps:
     - Analysis: of the requirements
     - Synthesis: creation of the architecture
     - Evaluation: validating that the design meet the requirements
     - Evolution: changing the architecture as requirements change
  - Can be formalized using a specific software (ADL, Architecture Description Language)
  - Architecture erosion:
     - gap between architecture and actual implementation
     - Can try to prevent/minimize, discover or repair
  - Reverse engineering (or "architecture recovery"):
     - Getting architecture from the implementation
  - Often used architecture are "patterns", e.g. Data warehouse, ETL, event-driven, async. messaging, plugin, client-server,
    shared-nothing, peer-to-peer, etc.
     - Reference architectures are barebones APIs/frameworks that aim at providing a specific architecture pattern
     - Different from design patterns

Design:
  - Can either mean architecture + lower-level (application-specific things/functional requirements), or only the later
  - Design patterns: factory, builder, etc.

                                  ┌────────────┐
                                  │   CODING   │
                                  └────────────┘

  - Coding convention: file organization, white spaces, comments, declaration, statements, naming,
    programming practices/principles.
  - Standard formats, programming languages, APIs, etc.
  - Common design (files organisation, classes, etc.), possibly with UML charts, etc.
  - Building: compilation, packaging, documentation creation, deployment.
    Should be as automated as possible, and can be triggered by demand, schedule time or event (such as commit).
  - Documentation
  - Obfuscation
  - Tuning, optimization
  - Refactoring: for non-functional requirements:
     - Abstraction: create an encapsulating interface, using polymorphism instead of usual types
     - Breaking classes/methods into separate pieces
     - Moving code: change to another class, to a subclass/superclass, or rename
  - Version control:
     - Neutral build is last version of code build on an end-user machine.
       Nightly build is an automatic neutral build (usually at night).
     - Can be:
        - client-server: only one repository
        - distributed: no repository has the "right" version

                                  ┌──────────────────┐
                                  │   VERIFICATION   │
                                  └──────────────────┘

Types of verification:
  - Dynamic (testing):
     - When program is running
     - Goal: finding requirement gaps
     - Types:
        - Automated:
           - Test suites: functional requirements
           - Non-functional test tools
        - Manual: testing as a end-user. Functional and non-functional.
  - Static (analysis):
     - Goal: improve code correctness. Can also check non-functional requirements.
     - Use code reviews (called also walkthrough, inspection, etc. with slight nuances), with goals:
        - Finding injection vulnerabilities, race conditions, memory leaks, buffer overflow, etc.
        - Finding anti-patterns
     - Manual/Automatic:
        - Automated: linting
        - Manual:
           - Formal: use a specific method, e.g. Fagan inspection (group review, line by line)
           - Informal/lightweight:
              - over-the-shoulder: one person looks another programming
              - pair programming: switch observer and coder ("driver") place
              - email pass-around: notification of source code changes are being checked

Test suites:
  - Structure:
     - One file = one topic:
        - one/several test suites
     - One test suite (e.g. "describe()")
        - several test cases.
        - Should be limited to scope (unit, integration or system).
        - Can be nested, e.g. the different behavior expectations (test cases) of the members (subtopic) of a class (topic).
     - One test case (e.g. "it()"):
        - should be self-sufficient.
        - Should be as high-level as possible by covering a stable higher-level API (don't spend too much time writing tests,
          and writing too often because too close to implementation details) but still covers enough to ensure quality
        - Sometimes called "specification", which can also have wider meaning of how to test and what to test documentation.
        - There should be a positive and a negative test case when possible.
        - Test fixture: preconditions, usually performed through a setup|beforeEach/teardown|afterEach() function.
          Often several unit tests in same test suite share some, so good to separate them for a specific test suite.
     - Test suites are batch-tested using a test harness/runner
     - Kept in a separate file from runtime code.
     - Data-driven testing:
        - when separating data from logic in test cases. Input is in separate files, e.g. CSV files.
        - Can test with more input and more flexible than hardcoded one
        - Guidelines: input of one test case can change, but expectation and logic should be same.
          Number of args should be same too.
     - Fakes/stubs/spies:
        - Test stub: test case that cannot be tested yet because not implemented, too slow or could have wrong state now
          (e.g. database with not the right data for testing).
        - Fake/mock: substitute an object/function during the time of the test case/suite (for same reason as test stub).
        - Spy: wrapper around an object/function that implements extra logging/testing function,
          e.g. checking if function is called.
     - Traceability matrix: table with the requirements as rows, and the test cases as columns (one test case might fill
       several requirements, and one requirement might need several test cases), to check the progress of the project
  - End to end (e2e) testing:
     - System test using scenarios based on acceptance tests.
     - From an end-user perspective, so will often use GUI simulation testing frameworks (or API).
     - Like unit testing, must test a higher-level stable API to avoid rewriting tests too often, i.e. an abstraction of
       the UI:
        - The abstraction should not include any assertion/tests, only abstract the common operations (get/set)
        - Can either be Page Objects (abstract UI elements in an OO approach, with e.g. LoginPage.enterPassword|getUsername())
          or Bot Actions (abstract UI actions in an imperative programming approach, with e.g. type(), etc.)
     - Keyword-driven testing: create different functions ("keywords") for actions that can be performed by end-user,
       optionally with arguments (UI state, differences in the action) ("keyword library").
       Use these functions as building blocks in test cases, which can be done using a GUI which will be understood by
       end-user. Keyword libraries can be provided in CSV.
  - Scope and goals:
     - Unit test: single module (e.g. class). Should not use anything beyond that module.
       Goal is to test bugs, not user stories. Should be requirements-oriented, not implementation details-based.
     - Integration testing: same as unit test, but with several modules.
       Can be bottom-up (from modules to system), top-down (inverse) or big bang (all at once).
     - System test: whole program. Test user stories / requirement gaps not bugs.
     - Acceptance test: whole program, but by end-user

Testing as end-user:
  - Knowledge types:
     - White-box
     - Black-box (end-user)
     - Grey-box: behaves like a end-user, but know the inside (developer knowledge) so can make interesting end-user actions
  - Test audience:
     - alpha testing: developers test
     - beta testing: end-user test (limited audience (closed) or not (open))
     - Release candidate (RC): all end-user, and there should be no more known bugs, but could still fix them if some are
       discovered

Non-functional automated testing:
  - performance:
     - load testing.
        - Normal one. Usually record typical sessions, then replays it.
        - Stress testing: goal is more reliability. Load is not normal behavior, but above what's expected until a breakpoint
          is reached and/or do weird/breaking behavior.
        - Soak testing: do for a long time, to check if performance decreases over time, and if there is memory leak.
        - Spike testing: when giving a sudden spike load.
     - volume testing: with high files or database sizes.
     - real-time testing: check that timing requirements are met
  - security: test confidentiality, integrity, authentication/authorization, non-repudiation and availability.
  - compatibility: e.g. using several OS, browsers, etc.
  - scalability
  - localization:
     - Pseudo-localization: giving random UTF8 characters, with varying length, direction, etc.
     - Actual translation check, and also all internationalization like currency format, etc.
  - accessibility
  - usability:
     - Checking UI issues with:
        - Test users (usability inspection)
        - Real end users (usability testing)
     - Can test:
        - Single part (cognitive)
        - Whole software (holistic)
     - Can use:
        - Several people at once (pluralistic)
        - Or not
  - User experience
  - robustness: Fault injection/"destructive testing":
     - When injecting exceptions/errors manually for testing purpose
     - Techniques:
        - Mutation testing: changing code

Corruption of memory space
Network level fault injection, e.g. modifying packets
Syscall: throwing exception manually

Fuzz testing: giving bad and random input
  - Equivalence partitioning: instead of uniform random data, divides each input into useful partitions where random are taken
    (e.g. if input should be from 0 to 10, into "<0", "0-10" and ">10")
  - Boundary-value analysis is case when it's based on min/max values
  - All-pair testing: test all combinations
  - State-transition table: test all states

Testing general ideas:
  - Goal:
     - Construction: set as a goal to consider a user story completed
     - Regression testing: find bugs on parts that worked before, but that don't work anymore because of new features.
       Good idea to create unit tests after correcting a bug to make sure it doesn't appear anymore.
     - "sanity/smoke test": fast test to to check there is so serious bug
  - Types of bug:
     - Bohrbug: normal bug
     - Heisenbug: disappears when studied, e.g. debugging introduce change which make the bug not appear: different compiler
       optimization, timing, etc.
     - Mandelbug: so complex that appears non-deterministic
  - Beyond runtime, can also test deployment and maintenance.
  - Code coverage: how many percents of the code are tested. Can be according to number of lines, of functions, of possible
    argument values, etc.

                                  ┌───────────────┐
                                  │   DEBUGGING   │
                                  └───────────────┘

  - Print debugging: using console.log(), print(), etc.
  - Remote debugging: using a different machine
  - Post-mortem debugging: studying a core dump
  - Anti-debugging: to avoid reverse engineering, checks existence of debugge, signal handlers, process/threads manipulation,
    code manipulation, hardware breakpoints, timing of execution

                                  ┌────────────────┐
                                  │   DEPLOYMENT   │
                                  └────────────────┘

  - software release life cycle:
     - pre-alpha (coding), alpha, beta, release candidate (see above), release to manufacturing (RTM),
       general availability (GA).
     - Rolling release: no different stages, just incremental releases.
       Might have some snapshots (releases aimed only at fixing bugs, not implementing features)
  - Install/uninstall, activate/deactivate
  - Adapt: modifying installed software because of change of environment. Manual or automatic.
  - Update. Manual or automatic.

                                  ┌─────────────────┐
                                  │   MAINTENANCE   │
                                  └─────────────────┘

Types:
  - corrective: fixing bug/crash, including recovery
  - adaptive: modification because of new environment
  - perfective: improving feature
  - preventive: prevent bug
